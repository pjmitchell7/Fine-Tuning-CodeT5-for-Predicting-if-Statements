Input function with masked if condition,Expected if condition,tokens_in_method,Predicted if condition,Whether the prediction is correct,BLEU-4 prediction score (0-100),CodeBLEU prediction score (0-100)
"def build_standard_field(self, field_name, model_field):
    field_class, field_kwargs = super().build_standard_field(field_name, model_field)
<mask>:
        field_class = EnumSerializerField
        field_kwargs['enum'] = model_field.enum
        field_kwargs.update(self.enumfield_options)
    return (field_class, field_kwargs)","isinstance(model_field, EnumFieldMixin) and field_class in self.enumfield_classes_to_replace",20,field_name == 'enum',False,1.127952363426135,N/A
"@pytest.mark.parametrize('int_names', (False, True))
def test_serialize(int_names):
    inst = MyModel(color=Color.BLUE, color_not_editable=Color.BLUE, taste=Taste.UMAMI, taste_not_editable=Taste.UMAMI, int_enum=IntegerEnum.B, int_enum_not_editable=IntegerEnum.B)
    data = (LenientIntNameSerializer if int_names else MySerializer)(inst).data
    assert data['color'] == data['color_not_editable'] == Color.BLUE.value
    assert Color.BLUE.value
<mask>:
        assert data['taste'] == data['taste_not_editable'] == 'umami'
        assert data['int_enum'] == data['int_enum_not_editable'] == 'b'
    else:
        assert data['taste'] == data['taste_not_editable'] == Taste.UMAMI.value
        assert data['int_enum'] == data['int_enum_not_editable'] == IntegerEnum.B.value",int_names,54,int_names,True,100.00000000000004,N/A
"@pytest.mark.django_db
@pytest.mark.parametrize('lenient_serializer', (False, True))
@pytest.mark.parametrize('lenient_data', (False, True))
def test_deserialize(lenient_data, lenient_serializer):
    secret_uuid = str(uuid.uuid4())
    data = {'color': Color.BLUE, 'taste': Taste.UMAMI.value, 'int_enum': IntegerEnum.B.value, 'random_code': secret_uuid}
<mask>:
        data.update({'color': 'b', 'taste': 'Umami', 'int_enum': 'B'})
    serializer_cls = LenientIntNameSerializer if lenient_serializer else MySerializer
    serializer = serializer_cls(data=data)
    if lenient_data and (not lenient_serializer):
        assert not serializer.is_valid()
        return
    assert serializer.is_valid(), serializer.errors
    validated_data = serializer.validated_data
    assert validated_data['color'] == Color.BLUE
    assert validated_data['taste'] == Taste.UMAMI
    assert validated_data['int_enum'] == IntegerEnum.B
    inst = serializer.save()
    assert inst.color == Color.BLUE
    assert inst.taste == Taste.UMAMI
    assert inst.int_enum == IntegerEnum.B
    inst = MyModel.objects.get(random_code=secret_uuid)
    assert inst.color == Color.BLUE
    assert inst.taste == Taste.UMAMI
    assert inst.int_enum == IntegerEnum.B",lenient_data,97,lenient_data,True,100.00000000000004,N/A
"@pytest.mark.django_db
def test_enum_int_field_validators():
<mask>:
        return pytest.skip('Needs connection.ops.integer_field_range')
    orig_method = connection.ops.integer_field_range
    connection.ops.integer_field_range = lambda *args: (-100, 100)
    m = MyModel(color=Color.RED)
    for f in m._meta.fields:
        if f.name == 'taste_int':
            if 'validators' in f.__dict__:
                del f.__dict__['validators']
    m.full_clean()
    connection.ops.integer_field_range = orig_method","not hasattr(connection.ops, 'integer_field_range')",37,connection.ops.integer_field_range is None,False,19.561803044642364,N/A
"def test_formfield_labels():
    form_field = EnumField(Color).formfield()
    expectations = {val.value: str(val) for val in Color}
    for value, text in form_field.choices:
<mask>:
            assert text == expectations[value]",value,23,value in expectations,False,27.516060407455225,N/A
"def __init__(self, hosts, connection_class=AIOHttpConnection, connection_pool_class=AIOHttpConnectionPool, host_info_callback=get_host_info, serializer=JSONSerializer(), serializers=None, sniff_on_start=False, sniffer_timeout=None, sniff_timeout=0.1, sniff_on_connection_fail=False, default_mimetype='application/json', max_retries=3, retry_on_status=(502, 503, 504), retry_on_timeout=False, send_get_body_as='GET', *, loop, **kwargs):
    self.loop = loop
    self._closed = False
    _serializers = DEFAULT_SERIALIZERS.copy()
    _serializers[serializer.mimetype] = serializer
<mask>:
        _serializers.update(serializers)
    self.deserializer = Deserializer(_serializers, default_mimetype)
    self.max_retries = max_retries
    self.retry_on_timeout = retry_on_timeout
    self.retry_on_status = retry_on_status
    self.send_get_body_as = send_get_body_as
    self.serializer = serializer
    self.sniffer_timeout = sniffer_timeout
    self.sniff_on_connection_fail = sniff_on_connection_fail
    self.last_sniff = self.loop.time()
    self.sniff_timeout = sniff_timeout
    self.host_info_callback = host_info_callback
    self.connection_pool_class = connection_pool_class
    self.connection_class = connection_class
    self._connection_pool_lock = asyncio.Lock(loop=self.loop)
    self.kwargs = kwargs
    self.hosts = hosts
    self.set_connections(hosts)
    self.seed_connections = set(self.connection_pool.connections)
    self.seed_connection_opts = self.connection_pool.connection_opts
    self.initial_sniff_task = None
    if sniff_on_start:

        def _initial_sniff_reset(fut):
            self.initial_sniff_task = None
        task = self.sniff_hosts(initial=True)
        self.initial_sniff_task = asyncio.ensure_future(task, loop=self.loop)
        self.initial_sniff_task.add_done_callback(_initial_sniff_reset)",serializers,110,serializers,True,100.00000000000004,N/A
"def set_connections(self, hosts):
<mask>:
        raise RuntimeError('Transport is closed')

    def _create_connection(host):
        if hasattr(self, 'connection_pool'):
            existing_connections = self.connection_pool.connection_opts + self.seed_connection_opts
            for connection, old_host in existing_connections:
                if old_host == host:
                    return connection
        kwargs = self.kwargs.copy()
        kwargs.update(host)
        kwargs['loop'] = self.loop
        return self.connection_class(**kwargs)
    connections = map(_create_connection, hosts)
    connections = list(zip(connections, hosts))
    if len(connections) == 1:
        self.connection_pool = DummyConnectionPool(connections, loop=self.loop, **self.kwargs)
    else:
        self.connection_pool = self.connection_pool_class(connections, loop=self.loop, **self.kwargs)",self._closed,61,self.closed,False,45.13864405503391,N/A
"def __init__(self, hosts=None, transport_class=AIOHttpTransport, *, loop=None, **kwargs):
<mask>:
        loop = asyncio.get_event_loop()
    self.loop = loop
    kwargs['loop'] = self.loop
    super().__init__(hosts, transport_class=transport_class, **kwargs)",loop is None,20,loop is None,True,100.00000000000004,N/A
"def __init__(self, host='localhost', port=9200, http_auth=None, use_ssl=False, ssl_context=None, verify_certs=False, maxsize=10, headers=None, *, loop, **kwargs):
    assert not (kwargs.get('session') and kwargs.get('session_factory')), 'Provide `session` or `session_factory`, not both.'
    super().__init__(host=host, port=port, use_ssl=use_ssl, **kwargs)
<mask>:
        headers = {}
    self.headers = headers
    self.headers.setdefault('Content-Type', 'application/json')
    self.loop = loop
    if http_auth is not None:
        if isinstance(http_auth, aiohttp.BasicAuth):
            pass
        elif isinstance(http_auth, str):
            http_auth = aiohttp.BasicAuth(*http_auth.split(':', 1))
        elif isinstance(http_auth, (tuple, list)):
            http_auth = aiohttp.BasicAuth(*http_auth)
        else:
            raise TypeError('Expected str, list, tuple or aiohttp.BasicAuth as http_auth parameter,got {!r}'.format(http_auth))
    self.http_auth = http_auth
    self.verify_certs = verify_certs
    self.base_url = URL.build(scheme='https' if self.use_ssl else 'http', host=host, port=port, path=self.url_prefix)
    self.session = kwargs.get('session')
    self.close_session = False
    if self.session is None:
        self._session_factory = kwargs.get('session_factory', session_factory)
        self.session = self._session_factory(auth=self.http_auth, loop=self.loop, ssl=ssl_context if self.verify_certs else False, limit=maxsize, use_dns_cache=kwargs.get('use_dns_cache', False))
        self.close_session = True",headers is None,120,headers is None,True,100.00000000000004,N/A
"def _build_headers(self, headers):
<mask>:
        final_headers = self.headers.copy()
        final_headers.update(headers)
    else:
        final_headers = self.headers
    return final_headers",headers,14,headers,True,100.00000000000004,N/A
"def __init__(self, connections, dead_timeout=60, timeout_cutoff=5, selector_class=RoundRobinSelector, randomize_hosts=True, *, loop, **kwargs):
    self._dead_timeout = dead_timeout
    self.timeout_cutoff = timeout_cutoff
    self.connection_opts = connections
    self.connections = [c for c, _ in connections]
    self.orig_connections = set(self.connections)
    self.dead = asyncio.PriorityQueue(len(self.connections), loop=loop)
    self.dead_count = collections.Counter()
    self.loop = loop
<mask>:
        random.shuffle(self.connections)
    self.selector = selector_class(dict(connections))",randomize_hosts,45,randomize_hosts,True,100.00000000000004,N/A
"def resurrect(self, force=False):
<mask>:
        if force:
            return random.choice(list(self.orig_connections))
        return
    timestamp, connection = self.dead.get_nowait()
    if not force and timestamp > self.loop.time():
        self.dead.put_nowait((timestamp, connection))
        return
    self.connections.append(connection)
    logger.info('Resurrecting connection %r (force=%s).', connection, force)
    return connection",self.dead.empty(),32,not self.dead,False,28.087083270446133,N/A
"def get_connection(self):
    self.resurrect()
<mask>:
        conn = self.resurrect(force=True)
        assert conn is not None
        return conn
    if len(self.connections) > 1:
        return self.selector.select(self.connections)
    return self.connections[0]",not self.connections,22,self.connections is None,False,39.76353643835252,N/A
"def __init__(self, connections, *, loop, **kwargs):
<mask>:
        raise ImproperlyConfigured('DummyConnectionPool needs exactly one connection defined.')
    self.loop = loop
    self.connection_opts = connections
    self.connection = connections[0][0]
    self.connections = [self.connection]
    self.orig_connections = set(self.connections)",len(connections) != 1,29,len(connections) != 1,True,100.00000000000004,N/A
"def __init__(self, es, query=None, scroll='5m', raise_on_error=True, preserve_order=False, size=1000, clear_scroll=True, scroll_kwargs=None, **kwargs):
    self._es = es
<mask>:
        query = query.copy() if query else {}
        query['sort'] = '_doc'
    self._query = query
    self._scroll = scroll
    self._raise_on_error = raise_on_error
    self._size = size
    self._clear_scroll = clear_scroll
    self._kwargs = kwargs
    self._scroll_kwargs = scroll_kwargs or {}
    self._scroll_id = None
    self._total = 0
    self._initial = True
    self._done = False
    self._hits = []
    self._hits_idx = 0
    self._successful_shards = 0
    self._total_shards = 0",not preserve_order,72,preserve_order,False,71.65313105737896,N/A
"def __aiter__(self):
<mask>:
        raise RuntimeError('Scan operations should be done inside async context manager')
    return self",self._initial,15,self.async_scan,False,23.643540225079384,N/A
"@property
def scroll_id(self):
<mask>:
        raise RuntimeError('Scan operations should be done inside async context manager')
    return self._scroll_id",self._initial,16,self._scroll_id is None,False,20.556680845025987,N/A
"@property
def total(self):
<mask>:
        raise RuntimeError('Scan operations should be done inside async context manager')
    return self._total",self._initial,16,self._total is None,False,30.213753973567677,N/A
"@pytest.fixture
def loop(request):
    asyncio.set_event_loop(None)
    loop = asyncio.new_event_loop()
    yield loop
<mask>:
        loop.call_soon(loop.stop)
        loop.run_forever()
        loop.close()
    gc.collect()
    asyncio.set_event_loop(None)",not loop._closed,15,request.method == 'post_run',False,6.567274736060395,N/A
"def pytest_generate_tests(metafunc):
<mask>:
        tags = set(metafunc.config.option.es_tag)
        if not tags:
            tags = ['6.0.0']
        else:
            tags = list(tags)
        metafunc.parametrize('es_tag', tags, scope='session')",'es_tag' in metafunc.fixturenames,19,'es_tag' in metafunc.config.option,False,58.73949094699213,N/A
"@pytest.fixture(scope='session')
def es_container(docker, session_id, es_tag, request):
    image = 'docker.elastic.co/elasticsearch/elasticsearch:{}'.format(es_tag)
<mask>:
        docker.images.pull(image)
    es_auth = ('elastic', 'changeme')
    if request.config.option.local_docker:
        es_port_9200 = es_access_port = unused_port()
        es_port_9300 = unused_port()
    else:
        es_port_9200 = es_port_9300 = None
        es_access_port = 9200
    container = docker.containers.run(image=image, detach=True, name='aioelasticsearch-' + session_id, ports={'9200/tcp': es_port_9200, '9300/tcp': es_port_9300}, environment={'http.host': '0.0.0.0', 'transport.host': '127.0.0.1'})
    if request.config.option.local_docker:
        docker_host = '0.0.0.0'
    else:
        inspection = docker.api.inspect_container(container.id)
        docker_host = inspection['NetworkSettings']['IPAddress']
    delay = 0.1
    for i in range(20):
        es = elasticsearch.Elasticsearch([{'host': docker_host, 'port': es_access_port}], http_auth=es_auth)
        try:
            es.transport.perform_request('GET', '/_nodes/_all/http')
        except elasticsearch.TransportError:
            time.sleep(delay)
            delay *= 2
        else:
            break
        finally:
            es.transport.close()
    else:
        pytest.fail('Cannot start elastic server')
    yield {'host': docker_host, 'port': es_access_port, 'auth': es_auth}
    container.kill(signal=9)
    container.remove(force=True)",not request.config.option.no_pull,102,image.endswith('.jpg'),False,5.11459870708889,N/A
"@pytest.mark.tryfirst
def pytest_pycollect_makeitem(collector, name, obj):
<mask>:
        item = pytest.Function(name, parent=collector)
        if 'run_loop' in item.keywords:
            return list(collector._genfunctions(name, obj))",collector.funcnamefilter(name),17,collector.funcnamefilter(name),True,100.00000000000004,N/A
"@pytest.mark.tryfirst
def pytest_pyfunc_call(pyfuncitem):
<mask>:
        funcargs = pyfuncitem.funcargs
        loop = funcargs['loop']
        testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}
        assert asyncio.iscoroutinefunction(pyfuncitem.obj)
        loop.run_until_complete(pyfuncitem.obj(**testargs))
        return True",'run_loop' in pyfuncitem.keywords,23,"isinstance(pyfuncitem, Call)",False,6.870636427700047,N/A
"def system_certs_file():
<mask>:
        return os.path.join(txclib.utils.get_base_dir(), 'cacert.pem')
    else:
        POSSIBLE_CA_BUNDLE_PATHS = ['/etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem', '/etc/ssl/certs/ca-bundle.crt', '/etc/pki/tls/certs/ca-bundle.crt', '/etc/ssl/certs/ca-certificates.crt', '/usr/local/share/certs/ca-root-nss.crt', '/etc/ssl/ca-bundle.pem']
        for path in POSSIBLE_CA_BUNDLE_PATHS:
            if os.path.exists(path):
                return path
        return resource_filename(__name__, 'cacert.pem')",platform.system() == 'Windows',26,txclib.utils.is_installed(),False,10.552670315936318,N/A
"def write(self, fp):
    """"""Write an .ini-format representation of the configuration state.""""""
    section_prefix = ''
<mask>:
        fp.write('[%s]\n' % DEFAULTSECT)
        for key in sorted(self._defaults):
            fp.write('%s = %s\n' % (key, str(self._defaults[key]).replace('\n', '\n\t')))
        section_prefix = '\n'
    for section in self._sections:
        fp.write('%s[%s]\n' % (section_prefix, section))
        for key in sorted(self._sections[section]):
            if key != '__name__':
                fp.write('%s = %s\n' % (key, str(self._sections[section][key]).replace('\n', '\n\t')))
        section_prefix = '\n'",self._defaults,58,self._defaults,True,100.00000000000004,N/A
"def __setitem__(self, key, val):
    k = self._flip.get(val, _NOTFOUND)
<mask>:
        raise MalformedConfigFile(""Your lang map configuration is not correct. Duplicate entry detected with value '%s'. Keys and values should be unique."" % val)
    v = self.get(key, _NOTFOUND)
    if v is not _NOTFOUND:
        dict.__delitem__(self._flip, v)
    dict.__setitem__(self, key, val)
    dict.__setitem__(self._flip, val, key)",not (k is _NOTFOUND or k == key),48,k is not _NOTFOUND,False,7.878071481927813,N/A
"def update(self, other=None, **kwargs):
<mask>:
        pass
    elif hasattr(other, 'items'):
        for k, v in six.iteritems(other):
            self[k] = v
    elif hasattr(other, 'keys'):
        for k in list(other.keys()):
            self[k] = other[k]
    else:
        for k, v in other:
            self[k] = v
    if kwargs:
        self.update(kwargs)",other is None,39,other is None,True,100.00000000000004,N/A
"def main(argv=None):
    """"""
    Here we parse the flags (short, long) and we instantiate the classes.
    """"""
    parser = tx_main_parser()
    options, rest = parser.parse_known_args()
<mask>:
        parser.print_help()
        sys.exit(1)
    utils.DISABLE_COLORS = options.color_disable
    if options.quiet:
        set_log_level('WARNING')
    elif options.debug:
        set_log_level('DEBUG')
    web.cacerts_file = options.cacert
    path_to_tx = options.root_dir or utils.find_dot_tx()
    cmd = options.command
    try:
        utils.exec_command(cmd, rest, path_to_tx)
    except SSLError as e:
        logger.error('SSL error %s' % e)
    except utils.UnknownCommandError:
        logger.error('Command %s not found' % cmd)
    except AuthenticationError:
        authentication_failed_message = '\nError: Authentication failed. Please make sure your credentials are valid.\nFor more information, visit:\nhttps://docs.transifex.com/client/client-configuration#-transifexrc.\n'
        logger.error(authentication_failed_message)
    except Exception as e:
        import traceback
        if options.trace:
            traceback.print_exc()
        else:
            msg = 'Unknown error' if not str(e) else str(e)
            logger.error(msg)
    else:
        return
    sys.exit(1)",not options.command,108,not options.command,True,100.00000000000004,N/A
"def check_file_exists(file=None):
<mask>:
        raise argparse.ArgumentTypeError('certificate file %s not found' % file)
    return file",file and (not os.path.isfile(file)),13,file is None,False,0.9816077559181531,N/A
"def parse_csv_option(option):
    """"""Return a list out of the comma-separated option or an empty list.""""""
<mask>:
        return option.split(',')
    else:
        return []",option,20,option,True,100.00000000000004,N/A
"def __init__(self, token=None, username=None, password=None, path_to_tx=None, host=None):
    self.hostnames = self.map_paths_to_hostnames(path_to_tx, host)
<mask>:
        self.token = token
        self.username = self.USERNAME
    elif username and password:
        self.token = password
        self.username = username
    else:
        logger.error('Authorization credentials are missing. Make sure that you have run `tx init` to setup your credentials.')",token,45,token and username,False,27.516060407455225,N/A
"def get(self, api_call, *args, **kwargs):
    """"""
        Performs the GET API call specified by api_call and
        parses the response
        """"""
<mask>:
        raise Exception('Tried to perform unsupported API call {}'.format(api_call))
    hostname = self.hostnames[api_call]
    url = API_URLS[api_call] % kwargs
    url = '{}{}'.format(hostname, url)
    try:
        response = requests.get(url, auth=HTTPBasicAuth(self.username, self.token))
        response.raise_for_status()
        all_data = response.json()
    except Exception as e:
        logger.debug(six.u(str(e)))
        raise
    next_page = response.links.get('next')
    while next_page:
        try:
            response = requests.get(next_page['url'], auth=HTTPBasicAuth(self.USERNAME, self.token))
            response.raise_for_status()
            all_data.extend(response.json())
            next_page = response.links.get('next')
        except Exception as e:
            logger.debug(six.u(str(e)))
            raise
    return all_data",api_call not in self.VALID_CALLS,80,api_call not in self.hostnames,False,65.48907866815301,N/A
"def choice_prompt(l, key):
    """"""
    l: A list of tuples (key, display_value) with the valid choices
    key: one of 'formats', 'organizations', 'projects'
    returns the key of the selected choice
    """"""
    a = '\n'.join(['  {}. {}'.format(i + 1, f[1]) for i, f in enumerate(l)])
    a = a + '\n'
    print(a)
    choice = ''
    first_time = True
    r = '1' if len(l) == 1 else '1-{}'.format(len(l))
    while not validate_int(choice, len(l)):
<mask>:
            print(messages.TEXTS[key]['error'])
        choice = input(utils.color_text(messages.TEXTS[key]['message'].format(r=r), COLOR))
        first_time = False
    return l[int(choice) - 1][0]",not first_time,80,first_time,False,71.65313105737896,N/A
"def input_prompt(key, validation_method):
    user_input = ''
    first_time = True
    while not validation_method(user_input):
<mask>:
            print(messages.TEXTS[key]['error'])
        user_input = input(utils.color_text(messages.TEXTS[key]['message'], COLOR))
        first_time = False
    return user_input",not first_time,23,first_time,False,71.65313105737896,N/A
"def get_formats(self, filename):
    _, extension = os.path.splitext(filename)
    try:
        formats = self.api.get('formats')
    except Exception as e:
        logger.error(e)
        raise

    def display_format(v):
        return '{} - {}'.format(v['description'], v['file-extensions'])
    formats = [(k, display_format(v)) for k, v in formats.items() if extension in v['file-extensions']]
<mask>:
        raise Exception(messages.TEXTS['formats']['empty'])
    return sorted(formats, key=lambda x: x[0])",not formats,45,not formats,True,100.00000000000004,N/A
"def run(self):
    """"""
        Runs the interactive wizard for `tx set` command and populates the
        parser's options with the user input. Options `local` and `execute`
        are by default True when interactive wizard is run.

        Returns: the options dictionary.
        """"""
    TEXTS = messages.TEXTS
    print(TEXTS['source_file']['description'])
    source_file = input_prompt('source_file', validate_source_file)
    print(TEXTS['expression']['description'].format(source_file=source_file))
    expression = input_prompt('expression', validate_expression)
    formats = self.get_formats(os.path.basename(source_file))
    print(TEXTS['formats']['description'])
    i18n_type = choice_prompt(formats, 'formats')
    organizations = self.get_organizations()
    print(TEXTS['organization']['description'])
    org_slug = choice_prompt(organizations, 'organization')
    projects = []
    first_time = True
    create_project = ('tx:new_project', 'Create new project (show instructions)...')
    project = None
    while not project:
<mask>:
            retry_message = 'Hit Enter to try selecting a project again: '
            input(utils.color_text(retry_message, COLOR))
        projects = self.get_projects_for_org(org_slug)
        p_choices = [(p['slug'], p['name']) for p in projects]
        p_choices.append(create_project)
        if projects:
            print(TEXTS['projects']['description'])
        else:
            print('We found no projects in this organization!')
        first_time = False
        project_slug = choice_prompt(p_choices, 'projects')
        if project_slug == 'tx:new_project':
            print(messages.create_project_instructions.format(host=self.host, org=org_slug))
        else:
            project = [p for p in projects if p['slug'] == project_slug][0]
            source_language = project['source_language']['code']
    resource_slug = slugify(os.path.basename(source_file))
    resource = '{}.{}'.format(project_slug, resource_slug)
    options = {'source_file': source_file, 'expression': expression, 'i18n_type': i18n_type, 'source_language': source_language, 'resource': resource}
    return options",not first_time,173,first_time,False,71.65313105737896,N/A
"def get_base_dir():
    """"""PyInstaller Run-time Operation.

    http://pythonhosted.org/PyInstaller/#run-time-operation
    """"""
<mask>:
        basedir = os.path.join(sys._MEIPASS, 'txclib')
    else:
        basedir = os.path.dirname(os.path.abspath(__file__))
    return basedir","getattr(sys, 'frozen', False)",18,sys._MEIPASS is not None,False,5.693025330278465,N/A
"def find_dot_tx(path=os.path.curdir, previous=None):
    """"""Return the path where .tx folder is found.

    The 'path' should be a DIRECTORY.
    This process is functioning recursively from the current directory to each
    one of the ancestors dirs.
    """"""
    path = os.path.abspath(path)
<mask>:
        return None
    joined = os.path.join(path, '.tx')
    if os.path.isdir(joined):
        return path
    else:
        return find_dot_tx(os.path.dirname(path), path)",path == previous,52,not os.path.isdir(path),False,4.767707020457095,N/A
"def parse_tx_url(url):
    """"""
    Try to match given url to any of the valid url patterns specified in
    TX_URLS. If not match is found, we raise exception
    """"""
    for type_ in list(TX_URLS.keys()):
        pattern = TX_URLS[type_]
        m = re.match(pattern, url)
<mask>:
            return (type_, m.groupdict())
    raise Exception('tx: Malformed URL given. Please refer to our docs: http://bit.ly/txcconfig')",m,53,m,True,100.00000000000004,N/A
"def determine_charset(response):
    content_type = response.headers.get('content-type', None)
<mask>:
        message = Parser().parsestr('Content-type: %s' % content_type)
        for charset in message.get_charsets():
            if charset:
                return charset
    return 'utf-8'",content_type,23,content_type,True,100.00000000000004,N/A
"def _prepare_url_request(host, username, password):
    """"""
    Return a ProxyManager object (as defined in urllib3 [1]) that can be used
    to perform authorized requests to a specific host.

    Authorization header is constructed and set using ""username"" and ""password""
    parameters. Also set the common HTTP headers that we want to be sent with
    each request.

    [1]: http://urllib3.readthedocs.io/en/latest/reference/#urllib3.poolmanager.ProxyManager  # noqa
    """"""

    def get_proxy_url(env_proxy, scheme):
        parsed_proxy = urllib3.util.url.parse_url(env_proxy)
        return urllib3.util.url.Url(scheme=scheme, auth=parsed_proxy.auth, host=parsed_proxy.host, port=parsed_proxy.port, path=parsed_proxy.path, query=parsed_proxy.query, fragment=parsed_proxy.fragment)
    num_pools = 1
    managers = {}
<mask>:
        scheme = 'http'
        if os.environ.get('http_proxy'):
            proxy_url = get_proxy_url(os.environ['http_proxy'], scheme)
            managers['http'] = urllib3.ProxyManager(proxy_url=proxy_url.url, proxy_headers=urllib3.util.make_headers(user_agent=user_agent_identifier(), proxy_basic_auth=proxy_url.auth), num_pools=num_pools)
        else:
            managers['http'] = urllib3.PoolManager(num_pools=num_pools)
    elif host.lower().startswith('https://'):
        scheme = 'https'
        if os.environ.get('https_proxy'):
            proxy_url = get_proxy_url(os.environ['https_proxy'], scheme)
            managers['https'] = urllib3.ProxyManager(proxy_url=proxy_url.url, proxy_headers=urllib3.util.make_headers(user_agent=user_agent_identifier(), proxy_basic_auth=proxy_url.auth), num_pools=num_pools, cert_reqs=CERT_REQUIRED, ca_certs=certs_file())
        else:
            managers['https'] = urllib3.PoolManager(num_pools=num_pools, cert_reqs=CERT_REQUIRED, ca_certs=certs_file())
    else:
        raise Exception('Unknown scheme')
    headers = urllib3.util.make_headers(basic_auth='{0}:{1}'.format(username, password), accept_encoding=True, user_agent=user_agent_identifier(), keep_alive=True)
    manager = managers[scheme]
    return (headers, manager)",host.lower().startswith('http://'),139,host.lower().startswith('http://'),True,100.00000000000004,N/A
"def hostname_tld_migration(hostname):
    """"""
    Migrate transifex.net to transifex.com.

    :param hostname: The hostname to migrate (if needed).
    :returns: A hostname with the transifex.com domain (if needed).
    """"""
    parts = urlparse.urlparse(hostname)
<mask>:
        hostname = hostname.replace('transifex.net', 'transifex.com', 1)
    return hostname",parts.hostname.endswith('transifex.net'),36,parts.scheme == 'http' and parts.scheme == 'https',False,6.837203339116283,N/A
"def hostname_ssl_migration(hostname):
    """"""
    Migrate Transifex hostnames to use HTTPS.

    :param hostname: The hostname to migrate (if needed).
    :returns: A https hostname (if needed).
    """"""
    parts = urlparse.urlparse(hostname)
    is_transifex = parts.hostname[-14:-3] == '.transifex.' or parts.hostname == 'transifex.net' or parts.hostname == 'transifex.com'
    is_https = parts.scheme == 'https'
<mask>:
        if not parts.scheme:
            hostname = 'https:' + hostname
        else:
            hostname = hostname.replace(parts.scheme, 'https', 1)
    return hostname",is_transifex and (not is_https),62,is_transifex and is_https,False,38.734798110329905,N/A
"def cmd_init(argv, path_to_tx):
    """"""Initialize a new Transifex project.""""""
    parser = init_parser()
    options = parser.parse_args(argv)
    path_to_tx = options.path_to_tx or os.getcwd()
    print(messages.init_intro)
    save = options.save
    config_file = os.path.join(path_to_tx, '.tx', 'config')
<mask>:
        if not save:
            if options.no_interactive:
                parser.error('Project already initialized.')
            logger.info(messages.init_initialized)
            if not utils.confirm(messages.init_reinit):
                return
        os.remove(config_file)
    if not os.path.isdir(os.path.join(path_to_tx, '.tx')):
        logger.info('Creating .tx folder...')
        os.mkdir(os.path.join(path_to_tx, '.tx'))
    default_transifex = 'https://www.transifex.com'
    transifex_host = options.host or default_transifex
    if not transifex_host.startswith(('http://', 'https://')):
        transifex_host = 'https://' + transifex_host
    if not os.path.exists(config_file):
        config = OrderedRawConfigParser()
        config.add_section('main')
        config.set('main', 'host', transifex_host)
        logger.info('Creating config file...')
        fh = open(config_file, 'w')
        config.write(fh)
        fh.close()
    if not options.skipsetup and (not options.no_interactive):
        logger.info(messages.running_tx_set)
        cmd_config([], path_to_tx)
    else:
        prj = project.Project(path_to_tx)
        prj.getset_host_credentials(transifex_host, username=options.user, password=options.password, token=options.token, force=options.save, no_interactive=options.no_interactive)
        prj.save()
        logger.info('Done.')",os.path.isfile(config_file),109,os.path.exists(config_file),False,65.80370064762461,N/A
"def cmd_config(argv, path_to_tx, is_legacy=False):
    """"""Add local or remote files under Transifex""""""
    from_wizard = False
<mask>:
        default_options = {'execute': True, 'subcommand': MAPPING, 'minimum_perc': 0, 'mode': None, 'expression_legacy': None}
        try:
            wizard_options = Wizard(path_to_tx).run()
            wizard_options.update(default_options)
            options = Namespace(**wizard_options)
            parser = set_parser(is_legacy=is_legacy)
            from_wizard = True
        except SystemExit:
            print('\n')
            sys.exit(1)
    else:
        if '--auto-local' in argv:
            argv.pop(argv.index('--auto-local'))
            argv.insert(0, MAPPING)
        if '--auto-remote' in argv:
            argv.pop(argv.index('--auto-remote'))
            argv.insert(0, MAPPINGREMOTE)
        is_subcommand = argv[0] in CONFIG_SUBCOMMANDS.keys()
        parser = set_parser(subparser=is_subcommand, is_legacy=is_legacy)
        options = parser.parse_args(argv)
    _validate_set_arguments(parser, options)
    if not hasattr(options, 'subcommand'):
        bare_set(path_to_tx, options)
    else:
        try:
            CONFIG_SUBCOMMANDS[options.subcommand](path_to_tx, options, from_wizard=from_wizard)
        except KeyError:
            parser.print_help()
            sys.exit(2)",len(argv) == 0,89,argv[0] in CONFIG_SUBCOMMANDS.keys(),False,4.789232204309912,N/A
"def _validate_set_arguments(parser, options):
    """"""
    Do any extra required validation for set command arguments
    """"""
<mask>:
        if options.is_source and (not options.language):
            parser.error('Please specify a source language.')
    if hasattr(options, 'expression'):
        if options.expression and '<lang>' not in options.expression:
            parser.error('The expression you have provided is not valid.')
    if hasattr(options, 'resource') and (not utils.valid_resource_slug(options.resource)):
        parser.error('Invalid resource slug. The format is <project_slug>.<resource_slug> and the valid characters include [_-\\w].')","hasattr(options, 'is_source') and hasattr(options, 'language')",62,"hasattr(options, 'is_source')",False,41.686201967850856,N/A
"def bare_set(path_to_tx, options):
    filename = options.filename
    path_to_file = os.path.relpath(filename, path_to_tx)
<mask>:
        resource = options.resource
        _set_source_file(path_to_tx, resource, options.language, path_to_file)
    elif options.resource or options.language:
        resource = options.resource
        lang = options.language
        _go_to_dir(path_to_tx)
        _set_translation(path_to_tx, resource, lang, path_to_file)
    _set_mode(options.resource, options.mode, path_to_tx)
    _set_type(options.resource, options.i18n_type, path_to_tx)
    _set_minimum_perc(options.resource, options.minimum_perc, path_to_tx)
    logger.info('Done.')",options.is_source,43,options.source_file,False,25.40663740773074,N/A
"def subcommand_mapping(path_to_tx, options, from_wizard=False):
    expression = options.expression or options.expression_legacy
    _auto_local(path_to_tx, options.resource, source_language=options.source_language, expression=expression, source_file=options.source_file, execute=options.execute, regex=False)
<mask>:
        _set_minimum_perc(options.resource, options.minimum_perc, path_to_tx)
        _set_mode(options.resource, options.mode, path_to_tx)
        _set_type(options.resource, options.i18n_type, path_to_tx)
    if from_wizard:
        _print_instructions(options.resource, path_to_tx)",options.execute,30,options.minimum_perc,False,21.3643503198117,N/A
"def __init__(self, path_to_tx=None, init=True):
    """"""Initialize the Project attributes.""""""
<mask>:
        self._init(path_to_tx)",init,10,init,True,100.00000000000004,N/A
"def _init(self, path_to_tx=None):
    instructions = ""Run 'tx init' to initialize your project first!""
    try:
        self.root = utils.get_tx_dir_path(path_to_tx)
        self.config_file = utils.get_config_file_path(self.root)
        self.config = utils.read_config_file(self.config_file)
        self.txrc_file = utils.get_transifex_file()
        self.txrc = utils.get_transifex_config(self.txrc_file)
    except ProjectNotInit:
        logger.error(instructions)
        raise
    host = self.config.get('main', 'host')
<mask>:
        self.conn = urllib3.connection_from_url(host, cert_reqs=CERT_REQUIRED, ca_certs=web.certs_file())
    else:
        self.conn = urllib3.connection_from_url(host)",host.lower().startswith('https://'),47,"self.config.get('main', 'use_ca')",False,4.053997537205932,N/A
"def validate_credentials(self, username, password, host=None):
    """"""Check if api credentials are valid.""""""
    try:
        api.Api(username=username, password=password, path_to_tx=self.txrc_file, host=host).get('auth_check')
        return True
    except HTTPError as e:
<mask>:
            return False
        raise
    return True",e.response.status_code == 401,28,e.status_code != 401,False,35.1862973998119,N/A
"def getset_host_credentials(self, host, username=None, password=None, token=None, no_interactive=False, only_token=False, force=False):
    """"""Read .transifexrc and report user,
        pass or a token for a specific host else ask the user for input.
        If the credentials provided are different from the .transifexrc file
        ask for confirmation and update them
        """"""
    save = False
    config_username, config_password = (None, None)
<mask>:
        password = os.environ['TX_TOKEN']
        username = API_USERNAME
        '\n            We need to check if hostname info exists in the .transifexrc file\n            If not, it means that this is the first time this function runs, so\n            we need to create its data.\n            '
        save = self._add_host_to_config_file(host)
        if save:
            logger.info(""Using TX_TOKEN environment variable. Credentials won't be saved to .transifexrc."")
            self.save()
        if token:
            logger.warning('There is a token in the TX_TOKEN env variable and the --token argument. The latter will be ignored and no credentials will be saved to the .transifexrc file.')
        return (username, password)
    try:
        config_username = self.txrc.get(host, 'username')
        config_password = self.txrc.get(host, 'password')
    except (configparser.NoOptionError, configparser.NoSectionError):
        save = True
    if token:
        password = token
        username = API_USERNAME
    if not (username and password) and (not (config_username and config_password)):
        username = API_USERNAME
        if not no_interactive:
            password = self._token_prompt(host)
        save = True
    elif config_username and config_password:
        if username == config_username and password == config_password:
            pass
        elif username and password:
            if force:
                save = True
            elif not no_interactive and utils.confirm(messages.update_txrc):
                save = True
        else:
            username = config_username
            password = config_password
    if only_token and (not self.validate_credentials(username, password)):
        logger.info('You need a valid API token to proceed')
        username = API_USERNAME
        password = self._token_prompt(host)
        save = True
    if save:
        self._add_host_to_config_file(host)
        self.txrc.set(host, 'username', username)
        self.txrc.set(host, 'password', password)
        self.save()
    return (username, password)",'TX_TOKEN' in os.environ,261,not username and (not password),False,0.0,N/A
"def _add_host_to_config_file(self, host):
    """"""Check if a given host exists in .transifexrc, and if not it
        updates the file with the appropriate info.

        :param str host: The hostname to search for and add if needed.
        :return: True if adding a new host, nothing otherwise.
        :rtype: bool
        """"""
<mask>:
        logger.info('Updating %s file...' % self.txrc_file)
        self.txrc.add_section(host)
        self.txrc.set(host, 'hostname', host)
        self.txrc.set(host, 'api_hostname', utils.DEFAULT_HOSTNAMES['api_hostname'])
        return True
    return False",not self.txrc.has_section(host),63,os.path.exists(self.txrc_file),False,15.580105704117443,N/A
"def validate_single_bit_value_byte_count(request, response):
    """""" Check of byte count field contains actual byte count and if byte count
    matches with the amount of requests quantity.
    """"""
    byte_count = struct.unpack('>B', response[8:9])[0]
    quantity = struct.unpack('>H', request[-2:])[0]
    expected_byte_count = quantity // 8
<mask>:
        expected_byte_count = quantity // 8 + 1
    assert byte_count == len(response[9:])
    assert byte_count == expected_byte_count",quantity % 8 != 0,54,byte_count == 0,False,16.233395773754953,N/A
"def pdu_to_function_code_or_raise_error(resp_pdu):
    """""" Parse response PDU and return of :class:`ModbusFunction` or
    raise error.

    :param resp_pdu: PDU of response.
    :return: Subclass of :class:`ModbusFunction` matching the response.
    :raises ModbusError: When response contains error code.
    """"""
    function_code = struct.unpack('>B', resp_pdu[0:1])[0]
<mask>:
        error_code = struct.unpack('>B', resp_pdu[1:2])[0]
        raise error_code_to_exception_map[error_code]
    return function_code",function_code not in function_code_to_function_map.keys(),46,function_code == ModbusFunction.ERROR,False,6.328773820616559,N/A
"def create_function_from_response_pdu(resp_pdu, req_pdu=None):
    """""" Parse response PDU and return instance of :class:`ModbusFunction` or
    raise error.

    :param resp_pdu: PDU of response.
    :param  req_pdu: Request PDU, some functions require more info than in
        response PDU in order to create instance. Default is None.
    :return: Number or list with response data.
    """"""
    function_code = pdu_to_function_code_or_raise_error(resp_pdu)
    function = function_code_to_function_map[function_code]
<mask>:
        return function.create_from_response_pdu(resp_pdu, req_pdu)
    return function.create_from_response_pdu(resp_pdu)",req_pdu is not None and 'req_pdu' in getfullargspec(function.create_from_response_pdu).args,61,req_pdu,False,0.06533919798673804,N/A
"@quantity.setter
def quantity(self, value):
    """""" Set number of coils to read. Quantity must be between 1 and 2000.

        :param value: Quantity.
        :raises: IllegalDataValueError.
        """"""
<mask>:
        raise IllegalDataValueError('Quantify field of request must be a value between 0 and {0}.'.format(2000))
    self._quantity = value",not 1 <= value <= 2000,41,value < 0 or value > 2000,False,12.600736402830258,N/A
"@property
def request_pdu(self):
    """""" Build request PDU to read coils.

        :return: Byte array of 5 bytes with PDU.
        """"""
<mask>:
        raise Exception
    return struct.pack('>BHH', self.function_code, self.starting_address, self.quantity)","None in [self.starting_address, self.quantity]",27,self.function_code is None,False,6.628576403773604,N/A
"def execute(self, slave_id, route_map):
    """""" Execute the Modbus function registered for a route.

        :param slave_id: Slave id.
        :param route_map: Instance of modbus.route.Map.
        :return: Result of call to endpoint.
        """"""
    values = []
    for address in range(self.starting_address, self.starting_address + self.quantity):
        endpoint = route_map.match(slave_id, self.function_code, address)
<mask>:
            raise IllegalDataAddressError()
        values.append(endpoint(slave_id=slave_id, address=address, function_code=self.function_code))
    return values",endpoint is None,52,endpoint is None,True,100.00000000000004,N/A
"def _set_multi_bit_value_format_character(self):
    """""" Set format character for multibit values.

        The format character depends on size of the value and whether values
        are signed or unsigned.

        """"""
    self.MULTI_BIT_VALUE_FORMAT_CHARACTER = self.MULTI_BIT_VALUE_FORMAT_CHARACTER.upper()
<mask>:
        self.MULTI_BIT_VALUE_FORMAT_CHARACTER = self.MULTI_BIT_VALUE_FORMAT_CHARACTER.lower()",self.SIGNED_VALUES,33,"self.MULTI_BIT_VALUE_SIZE_FORMAT_CHARACTER in (1, 2)",False,4.480836160121357,N/A
"def match(self, slave_id, function_code, address):
    for rule in self._rules:
<mask>:
            return rule.endpoint","rule.match(slave_id, function_code, address)",12,rule.slave_id == slave_id and rule.function_code == function_code,False,12.918025833717252,N/A
"def memoize(f):
    """""" Decorator which caches function's return value each it is called.
    If called later with same arguments, the cached value is returned.
    """"""
    cache = {}

    @wraps(f)
    def inner(arg):
<mask>:
            cache[arg] = f(arg)
        return cache[arg]
    return inner",arg not in cache,39,arg not in cache,True,100.00000000000004,N/A
"def recv_exactly(recv_fn, size):
    """""" Use the function to read and return exactly number of bytes desired.

    https://docs.python.org/3/howto/sockets.html#socket-programming-howto for
    more information about why this is necessary.

    :param recv_fn: Function that can return up to given bytes
        (i.e. socket.recv, file.read)
    :param size: Number of bytes to read.
    :return: Byte string with length size.
    :raises ValueError: Could not receive enough data (usually timeout).
    """"""
    recv_bytes = 0
    chunks = []
    while recv_bytes < size:
        chunk = recv_fn(size - recv_bytes)
<mask>:
            break
        recv_bytes += len(chunk)
        chunks.append(chunk)
    response = b''.join(chunks)
    if len(response) != size:
        raise ValueError
    return response",len(chunk) == 0,93,not chunk,False,4.104249931194939,N/A
"def get_char_size(baudrate):
    """""" Get the size of 1 character in seconds.

    From the implementation guide:

        ""The implementation of RTU reception driver may imply the management of
        a lot of interruptions due to the t 1.5  and t 3.5  timers. With high

        communication baud rates, this leads to a heavy CPU load. Consequently
        these two timers must be strictly respected when the baud rate is equal
        or lower than 19200 Bps. For baud rates greater than 19200 Bps, fixed
        values for the 2 timers should be used:  it is recommended to use a
        value of 750us for the inter-character time-out (t 1.5) and a value of
        1.750ms for inter-frame delay (t 3.5).""
    """"""
<mask>:
        return 11 / baudrate
    return 0.0005",baudrate <= 19200,119,baudrate > 0,False,19.716118825581447,N/A
"def serve_once(self):
    """""" Listen and handle 1 request. """"""
    request_adu = self.serial_port.read(256)
    log.debug('<-- {0}'.format(hexlify(request_adu)))
<mask>:
        raise ValueError
    response_adu = self.process(request_adu)
    self.respond(response_adu)",len(request_adu) == 0,21,request_adu != b'OK',False,19.692104496063735,N/A
"def generate_look_up_table():
    """""" Generate look up table.

    :return: List
    """"""
    poly = 40961
    table = []
    for index in range(256):
        data = index << 1
        crc = 0
        for _ in range(8, 0, -1):
            data >>= 1
<mask>:
                crc = crc >> 1 ^ poly
            else:
                crc >>= 1
        table.append(crc)
    return table",(data ^ crc) & 1,52,data & 1,False,16.605579150202516,N/A
"def validate_crc(msg):
    """""" Validate CRC of message.

    :param msg: Byte array with message with CRC.
    :raise: CRCError.
    """"""
<mask>:
        raise CRCError('CRC validation failed.')","not struct.unpack('<H', get_crc(msg[:-2])) == struct.unpack('<H', msg[-2:])",23,not crc32(msg).check(msg.encode('utf-8')),False,2.3603963017637812,N/A
"def parse_response_adu(resp_adu, req_adu=None):
    """""" Parse response ADU and return response data. Some functions require
    request ADU to fully understand request ADU.

    :param resp_adu: Resonse ADU.
    :param req_adu: Request ADU, default None.
    :return: Response data.
    """"""
    resp_pdu = resp_adu[1:-2]
    validate_crc(resp_adu)
    req_pdu = None
<mask>:
        req_pdu = req_adu[1:-2]
    function = create_function_from_response_pdu(resp_pdu, req_pdu)
    return function.data",req_adu is not None,52,req_adu.startswith('#'),False,15.619699684601283,N/A
"def choose_selection(self, allowed_values: Dict[str, str], k_or_v='v') -> str:
<mask>:
        allowed_values = {v: k for k, v in allowed_values.items()}
    while True:
        choice = self.user_io.handle_user_input()
        choice = choice.strip()
        if choice == '/quit':
            raise QuitSession('/quit')
        elif choice in allowed_values.keys():
            return allowed_values[choice]
        elif choice in allowed_values.values():
            return choice
        else:
            self.user_io.handle_basic_output('Please enter a valid selection.')
            continue",k_or_v == 'k',51,k_or_v == 'v',False,84.08964152537145,N/A
"def process_next_action(self):
    user_input = self.user_io.handle_user_input()
<mask>:
        self.stop_session = True
    elif user_input.startswith('/remember'):
        self.process_remember_action(user_input[len('/remember '):])
    else:
        self.process_regular_action(user_input)",user_input == '/quit',15,user_input.startswith('/session'),False,19.64073254502565,N/A
"def login(self):
    auth_token = self.get_auth_token()
<mask>:
        self.api.update_session_access_token(auth_token)
    else:
        creds = self.get_credentials()
        if creds:
            email, password = creds
            self.api.user_login(email, password)
        else:
            self.api.anonymous_login()",auth_token,21,auth_token,True,100.00000000000004,N/A
"def _choose_character_name(self):
    print(""Enter your character's name...\n"")
    character_name = self.user_io.handle_user_input()
<mask>:
        raise QuitSession('/quit')
    self.character_name = character_name",character_name == '/quit',15,character_name == '',False,54.44460596606694,N/A
"def display_splash(self):
    filename = os.path.dirname(os.path.realpath(__file__))
    locale = None
    term = None
<mask>:
        locale = os.environ['LC_ALL']
    if 'TERM' in os.environ:
        term = os.environ['TERM']
    if locale == 'C' or (term and term.startswith('vt')):
        filename += '/../res/opening-ascii.txt'
    else:
        filename += '/../res/opening-utf8.txt'
    with open(filename, 'r', encoding='utf8') as splash_image:
        print(splash_image.read())",'LC_ALL' in os.environ,44,'LC_ALL' in os.environ,True,100.00000000000004,N/A
"def clear(self):
<mask>:
        _ = os.system('cls')
    else:
        _ = os.system('clear')",os.name == 'nt',10,self.cls,False,10.122592925934278,N/A
"@staticmethod
def merged(confs):
    default_conf = Config()
    conf = Config()
    for c in confs:
        for a in ['prompt', 'slow_typing_effect', 'auth_token', 'email', 'password', 'character_name', 'public_adventure_id', 'debug']:
            v = getattr(c, a)
<mask>:
                setattr(conf, a, v)
    return conf","getattr(default_conf, a) != v",34,v is not None,False,2.7757915716335906,N/A
"def load_from_cli_args(self):
    parsed = Config.parse_cli_args()
<mask>:
        self.prompt = parsed.prompt
    if hasattr(parsed, 'slow_typing'):
        self.slow_typing_effect = parsed.slow_typing
    if hasattr(parsed, 'auth_token'):
        self.auth_token = parsed.auth_token
    if hasattr(parsed, 'email'):
        self.email = parsed.email
    if hasattr(parsed, 'password'):
        self.password = parsed.password
    if hasattr(parsed, 'adventure'):
        self.public_adventure_id = parsed.adventure
    if hasattr(parsed, 'name'):
        self.character_name = parsed.name
    if hasattr(parsed, 'debug'):
        self.debug = parsed.debug","hasattr(parsed, 'prompt')",51,"hasattr(parsed, 'prompt')",True,100.00000000000004,N/A
"@staticmethod
def parse_cli_args():
    parser = argparse.ArgumentParser(description='ai-dungeon-cli is a command-line client to play.aidungeon.io')
    parser.add_argument('--prompt', type=str, required=False, default='> ', help='text for user prompt')
    parser.add_argument('--slow-typing', action='store_const', const=True, help='enable slow typing effect for story')
    parser.add_argument('--auth-token', type=str, required=False, help='authentication token')
    parser.add_argument('--email', type=str, required=False, help='email (for authentication)')
    parser.add_argument('--password', type=str, required=False, help='password (for authentication)')
    parser.add_argument('--adventure', type=str, required=False, help='public multi-user adventure id to connect to')
    parser.add_argument('--name', type=str, required=False, help='character name for multi-user adventure')
    parser.add_argument('--debug', action='store_const', const=True, help='enable debug')
    parsed = parser.parse_args()
<mask>:
        parser.error('--name needs to be provided when joining a multi-user adventure (--adventure argument)')
    return parsed",parsed.adventure and (not parsed.name),88,not parsed.name,False,22.31301601484299,N/A
"def load_from_file(self):
    cfg_file = '/config.yml'
    cfg_file_paths = [os.path.dirname(os.path.realpath(__file__)) + cfg_file, os.path.expanduser('~') + '/.config/ai-dungeon-cli' + cfg_file]
    did_read_cfg_file = False
    cfg = {}
    for file in cfg_file_paths:
        try:
            with open(file, 'r') as cfg_raw:
                cfg = yaml.load(cfg_raw, Loader=yaml.FullLoader)
                did_read_cfg_file = True
        except IOError:
            pass
<mask>:
        self.prompt = cfg['prompt']
    if exists(cfg, 'slow_typing_effect'):
        self.slow_typing_effect = cfg['slow_typing_effect']
    if exists(cfg, 'auth_token'):
        self.auth_token = cfg['auth_token']
    if exists(cfg, 'email'):
        self.email = cfg['email']
    if exists(cfg, 'password'):
        self.password = cfg['password']","exists(cfg, 'prompt')",69,"exists(cfg, 'prompt')",True,100.00000000000004,N/A
"def get_options(self, scenario_id):
    prompt = ''
    options = None
    debug_print('query options (variant #1)')
    result = self._execute_query('\n        query ($id: String) {  user {    id    username    __typename  }  content(id: $id) {    id    userId    contentType    contentId    prompt    gameState    options {      id      title      __typename    }    playPublicId    __typename  }}\n        ', {'id': scenario_id})
    debug_print(result)
    prompt = result['content']['prompt']
<mask>:
        options = self.normalize_options(result['content']['options'])
    return [prompt, options]",result['content']['options'],58,result['content']['options'],True,100.00000000000004,N/A
"@staticmethod
def initial_story_from_history_list(history_list):
    pitch = ''
    for entry in history_list:
<mask>:
            break
        pitch += entry['text']
    return pitch","not entry['type'] in ['story', 'continue']",17,entry['type'] == 'story',False,24.549475440235113,N/A
"def create_adventure(self, scenario_id, story_pitch):
    debug_print('create adventure')
    result = self._execute_query('\n        mutation ($id: String, $prompt: String) {  createAdventureFromScenarioId(id: $id, prompt: $prompt) {    id    contentType    contentId    title    description    musicTheme    tags    nsfw    published    createdAt    updatedAt    deletedAt    publicId    historyList    __typename  }}\n        ', {'id': scenario_id, 'prompt': story_pitch})
    debug_print(result)
    adventure_id = result['createAdventureFromScenarioId']['id']
    story_pitch = None
<mask>:
        story_pitch = self.initial_story_from_history_list(result['createAdventureFromScenarioId']['historyList'])
    return [adventure_id, story_pitch]",'historyList' in result['createAdventureFromScenarioId'],55,result['createAdventureFromScenarioId']['historyList'],False,43.47208719449914,N/A
"def init_story_multi_adventure(self, public_adventure_id):
    debug_print('get story multi-user adventure')
    result = self._execute_query('\n        query ($id: String, $playPublicId: String) {  content(id: $id, playPublicId: $playPublicId) {    id    actions {      id      text      __typename    }    quests    newQuests {      id      text      completed      active      __typename    }    playPublicId    userId    __typename  }}\n        ', {'playPublicId': public_adventure_id})
    debug_print(result)
    entries = []
    for entry in result['content']['actions']:
<mask>:
            continue
        entry = entry['text']
        if entry.startswith('\n>'):
            entry = '\n' + entry + '\n'
        entries.append(entry)
    return ''.join(entries)",entry['__typename'] != 'Action',69,entry['id'] != public_adventure_id,False,19.081654556856684,N/A
"def test_auth(self):
    """"""Test get_auth returns port(int) and key(bytes), and when run a second
        time returns those same values from the cache file

        """"""
    port, key = KM.__main__.get_auth()
    self.assertIsInstance(port, int)
<mask>:
        self.assertIsInstance(key, str)
    else:
        self.assertIsInstance(key, bytes)
    port2, key2 = KM.__main__.get_auth()
    self.assertEqual(port2, port)
    self.assertEqual(key2, key)",sys.version_info.major < 3,43,sys.version_info[0] == 2,False,35.08439695638686,N/A
"def hotp(key, counter, digits=6, digest='sha1', steam=False):
    """""" Generates HMAC OTP.  Taken from https://github.com/susam/mintotp

    Args: key - Secret key
          counter - Moving factor
          digits - The number of characters/digits that the otp should have
          digest - Algorithm to use to generate the otp
          steam - whether or not to use steam settings

    Returns: otp

    """"""
    key = base64.b32decode(key.upper() + '=' * ((8 - len(key)) % 8))
    counter = struct.pack('>Q', counter)
    mac = hmac.new(key, counter, digest).digest()
    offset = mac[-1] & 15
    binary = struct.unpack('>L', mac[offset:offset + 4])[0] & 2147483647
    code = ''
<mask>:
        chars = '23456789BCDFGHJKMNPQRTVWXY'
        full_code = int(binary)
        for _ in range(digits):
            code += chars[full_code % len(chars)]
            full_code //= len(chars)
    else:
        code = str(binary)[-digits:].rjust(digits, '0')
    return code",steam,116,steam,True,100.00000000000004,N/A
"def gen_otp(otp_url):
    """""" Generates one time password

    Args: otp_url - KeePassXC url encoding with information on how to generate otp
    Returns: otp

    """"""
    parsed_otp_url = parse.urlparse(otp_url)
<mask>:
        query_string = parse.parse_qs(parsed_otp_url.query)
    else:
        query_string = parse.parse_qs(otp_url)
    params = {}
    if 'secret' in query_string:
        params['key'] = query_string['secret'][0]
        try:
            params['time_step'] = int(query_string['periods'][0])
        except KeyError:
            pass
        try:
            params['digits'] = int(query_string['digits'][0])
        except KeyError:
            pass
        try:
            params['digest'] = query_string['algorithm'][0].lower()
        except KeyError:
            pass
        try:
            params['steam'] = query_string['encoder'][0] == 'steam'
        except KeyError:
            pass
    elif 'key' in query_string:
        params['key'] = query_string['key'][0]
        try:
            params['time_step'] = int(query_string['step'][0])
        except KeyError:
            pass
        try:
            params['digits'] = int(query_string['size'][0])
        except KeyError:
            pass
        try:
            params['digest'] = query_string['otpHashMode'][0].lower()
        except KeyError:
            pass
    else:
        return ''
    return totp(**params)",parsed_otp_url.scheme == 'otpauth',107,parsed_otp_url.query,False,52.70837136273562,N/A
"def get_otp_url(kp_entry):
    """""" Shim to return otp url from KeePass entry
    This is required to fully support pykeepass>=4.0.0
    ""otp"" was upgraded to a reserved property in pykeepass==4.0.3

    Args: kp_entry - KeePassXC entry
    Returns: otp url string or None

    """"""
    otp_url = ''
<mask>:
        otp_url = kp_entry.deref('otp') or ''
    else:
        otp_url = kp_entry.get_custom_property('otp')
    if otp_url:
        return otp_url
    otp_url_format = 'otpauth://totp/Entry?secret={}&period={}&digits={}&algorithm={}'
    digits, period, algorithm = (6, 30, 'sha1')
    seed = kp_entry.get_custom_property('TOTP Seed')
    if seed:
        settings = kp_entry.get_custom_property('TOTP Settings') or ''
        try:
            period, digits = settings.split(';')
        except ValueError:
            pass
        return otp_url_format.format(seed, period, digits, algorithm)
    seed = kp_entry.get_custom_property('TimeOtp-Secret-Base32')
    if seed:
        period = int(kp_entry.get_custom_property('TimeOtp-Period') or period)
        digits = int(kp_entry.get_custom_property('TimeOtp-Length') or digits)
        algorithm = kp_entry.get_custom_property('TimeOtp-Algorithm') or algorithm
        algo_map = {'hmac-sha-1': 'sha1', 'hmac-sha-256': 'sha256', 'hmac-sha-512': 'sha512'}
        algorithm = algo_map.get(algorithm.lower(), 'sha1')
        return otp_url_format.format(seed, period, digits, algorithm)
    return otp_url","hasattr(kp_entry, 'otp')",130,"hasattr(kp_entry, 'deref')",False,70.71067811865478,N/A
"def get_databases():
    """"""Read databases from config

    Returns: [DataBase obj, DataBase obj2,...]
             If not specified in the config, the value will be None
             If database name is None, an error has occurred

    """"""
    dargs = keepmenu.CONF.items('database')
    args_dict = dict(dargs)
    dbases = [i for i in args_dict if i.startswith('database')]
    dbs = []
    for dbase in dbases:
        dbn = args_dict[dbase]
        idx = dbase.rsplit('_', 1)[-1]
        try:
            keyfile = args_dict[f'keyfile_{idx}']
        except KeyError:
            keyfile = None
        try:
            passw = args_dict[f'password_{idx}']
        except KeyError:
            passw = None
        try:
            autotype = args_dict[f'autotype_default_{idx}']
        except KeyError:
            autotype = None
        try:
            cmd = expanduser(args_dict[f'password_cmd_{idx}'])
            res = subprocess.run(shlex.split(cmd), capture_output=True, check=False, encoding=keepmenu.ENC)
<mask>:
                dmenu_err(f'Password command error: {res.stderr}')
                sys.exit()
            else:
                passw = res.stdout.rstrip('\n') if res.stdout else passw
        except KeyError:
            pass
        if dbn:
            dbo = DataBase(dbase=dbn, kfile=keyfile, pword=passw, atype=autotype)
            dbs.append(dbo)
    return dbs",res.stderr,126,res.returncode != 0,False,16.233395773754953,N/A
"def get_initial_db():
    """"""Ask for and set initial database name and keyfile if not entered in
    config file. Create new database if desired.

    """"""
    db_name = dmenu_select(0, 'Enter path to existing Keepass database or to create new database. ~/ for $HOME is ok')
<mask>:
        dmenu_err('No database entered. Try again.')
        return False
    if not isfile(expanduser(db_name)):
        create = dmenu_select(0, f'Create new database {db_name} (y/n)?')
        if create.lower() == 'y':
            kpo = create_db(db_name)
            keyfile_name = kpo.keyfile
        else:
            dmenu_err('Database not created. Try again.')
            return False
    else:
        keyfile_name = dmenu_select(0, 'Enter path to keyfile (optional). ~/ for $HOME is ok')
    with open(keepmenu.CONF_FILE, 'w', encoding=keepmenu.ENC) as conf_file:
        keepmenu.CONF.set('database', 'database_1', db_name)
        if keyfile_name:
            keepmenu.CONF.set('database', 'keyfile_1', keyfile_name)
        keepmenu.CONF.write(conf_file)
    return True",not db_name,110,not db_name,True,100.00000000000004,N/A
"def get_entries(dbo):
    """"""Open keepass database and return the PyKeePass object

        Args: dbo: DataBase object
        Returns: PyKeePass object or None

    """"""
    from pykeepass import PyKeePass
<mask>:
        return None
    try:
        kpo = PyKeePass(dbo.dbase, dbo.pword, keyfile=dbo.kfile)
    except (FileNotFoundError, construct.core.ChecksumError) as err:
        if str(err.args[0]).startswith('wrong checksum'):
            dmenu_err('Invalid Password or keyfile')
            return None
        try:
            if err.errno == errno.ENOENT:
                if not isfile(dbo.dbase):
                    dmenu_err('Database does not exist. Check path and filename.')
                elif not isfile(dbo.kfile):
                    dmenu_err('Keyfile does not exist. Check path and filename.')
        except AttributeError:
            pass
        return None
    except Exception as err:
        dmenu_err(f'Error: {err}')
        return None
    return kpo",dbo.dbase is None,89,not dbo.dbase or not dbo.kfile,False,17.747405280050266,N/A
"def get_passphrase(check=False):
    """"""Get a database password from dmenu or pinentry

    Returns: string

    """"""
    msg = 'Enter Password' if check is False else 'Verify password'
    pinentry = keepmenu.CONF.get('dmenu', 'pinentry', fallback=None)
<mask>:
        password = ''
        res = subprocess.run(pinentry, capture_output=True, check=False, encoding=keepmenu.ENC, input=f'SETDESC {msg}\nGETPIN\n')
        if res.stdout:
            pin = res.stdout.split('\n')[2]
            if pin.startswith('D '):
                password = pin.split('D ')[1]
    else:
        password = dmenu_select(0, f'{msg}')
    return None or password",pinentry,62,pinentry,True,100.00000000000004,N/A
"def reload_config(conf_file=None):
    """"""Reload config file. Primarly for use with tests and the --config flag.

    Args: conf_file - os.path

    """"""
    global CACHE_PERIOD_MIN, CACHE_PERIOD_DEFAULT_MIN, CLIPBOARD_CMD, CONF, MAX_LEN, ENV, ENC, SEQUENCE
    CONF = configparser.ConfigParser()
    conf_file = conf_file if conf_file is not None else CONF_FILE
<mask>:
        try:
            os.mkdir(os.path.dirname(conf_file))
        except OSError:
            pass
        with open(conf_file, 'w', encoding=ENC) as cfile:
            CONF.add_section('dmenu')
            CONF.set('dmenu', 'dmenu_command', 'dmenu')
            CONF.add_section('dmenu_passphrase')
            CONF.set('dmenu_passphrase', 'obscure', 'True')
            CONF.set('dmenu_passphrase', 'obscure_color', '#222222')
            CONF.add_section('database')
            CONF.set('database', 'database_1', '')
            CONF.set('database', 'keyfile_1', '')
            CONF.set('database', 'pw_cache_period_min', str(CACHE_PERIOD_DEFAULT_MIN))
            CONF.set('database', 'autotype_default', SEQUENCE)
            CONF.write(cfile)
    try:
        CONF.read(conf_file)
    except configparser.ParsingError as err:
        dmenu_err(f'Config file error: {err}')
        sys.exit()
    if CONF.has_option('dmenu', 'dmenu_command'):
        command = shlex.split(CONF.get('dmenu', 'dmenu_command'))
    else:
        CONF.set('dmenu', 'dmenu_command', 'dmenu')
        command = 'dmenu'
    if '-l' in command:
        MAX_LEN = int(command[command.index('-l') + 1])
    if CONF.has_option('database', 'pw_cache_period_min'):
        CACHE_PERIOD_MIN = int(CONF.get('database', 'pw_cache_period_min'))
    else:
        CACHE_PERIOD_MIN = CACHE_PERIOD_DEFAULT_MIN
    if CONF.has_option('database', 'autotype_default'):
        SEQUENCE = CONF.get('database', 'autotype_default')
    if CONF.has_option('database', 'type_library'):
        for typ in ['xdotool', 'ydotool', 'wtype', 'dotool']:
            if CONF.get('database', 'type_library') == typ:
                try:
                    _ = run([typ, '--version'], check=False, stdout=DEVNULL, stderr=DEVNULL)
                except OSError:
                    dmenu_err(f'{typ} not installed.\nPlease install or remove that option from config.ini')
                    sys.exit()
    if os.environ.get('WAYLAND_DISPLAY'):
        clips = ['wl-copy -o']
    else:
        clips = ['xsel -b', 'xclip -l 1 -selection clip']
    for clip in clips:
        try:
            _ = run(shlex.split(clip), check=False, stdout=DEVNULL, stderr=DEVNULL, input='')
            CLIPBOARD_CMD = clip
            break
        except OSError:
            continue
    if CLIPBOARD_CMD == 'true':
        dmenu_err(f""{' or '.join([shlex.split(i)[0] for i in clips])} needed for clipboard support"")",not exists(conf_file),216,not os.path.exists(os.path.dirname(conf_file)),False,23.124463019346486,N/A
"def add_entry(kpo):
    """"""Add Keepass entry

    Args: kpo - Keepass object
    Returns: False if not added
             Keepass Entry object on success

    """"""
    group = select_group(kpo)
<mask>:
        return False
    entry = kpo.add_entry(destination_group=group, title='', username='', password='')
    edit = True
    while edit is True:
        edit = edit_entry(kpo, entry)
    return entry",group is False,46,not group,False,30.326532985631665,N/A
"def delete_entry(kpo, kp_entry):
    """"""Delete an entry

    Args: kpo - Keepass object
          kp_entry - keepass entry
    Returns: True if no delete
             'del' if delete

    """"""
    inp = 'NO\nYes - confirm delete\n'
    delete = dmenu_select(2, 'Confirm delete', inp=inp)
<mask>:
        return True
    kpo.delete_entry(kp_entry)
    kpo.save()
    return 'del'",delete != 'Yes - confirm delete',43,delete,False,0.24787521766663595,N/A
"def edit_expiry(kp_entry):
    """"""Edit expiration date

    Args: kp_entry - selected Entry object

    Input can be date and time, date or time

    """"""
    sel = dmenu_select(1, ""Expiration Date (yyyy-mm-dd hh:mm OR yyyy-mm-dd OR HH:MM OR 'None' to unset)"", inp=kp_entry.expiry_time.strftime('%Y-%m-%d %H:%M') if kp_entry.expires is True else '')
<mask>:
        return True
    if sel.lower() == 'none':
        setattr(kp_entry, 'expires', False)
        return True
    dtime = exp_date(sel)
    if dtime:
        setattr(kp_entry, 'expires', True)
        setattr(kp_entry, 'expiry_time', dtime)
    return True",not sel,69,not sel,True,100.00000000000004,N/A
"def edit_totp(kp_entry):
    """"""Edit TOTP generation information

    Args: kp_entry - selected Entry object

    """"""
    otp_url = get_otp_url(kp_entry)
<mask>:
        inputs = ['Enter secret key', 'Type TOTP']
        otp_choice = dmenu_select(len(inputs), 'TOTP', inp='\n'.join(inputs))
    else:
        otp_choice = 'Enter secret key'
    if otp_choice == 'Type TOTP':
        type_text(gen_otp(otp_url))
    elif otp_choice == 'Enter secret key':
        inputs = []
        if otp_url:
            parsed_otp_url = parse.urlparse(otp_url)
            query_string = parse.parse_qs(parsed_otp_url.query)
            inputs = [query_string['secret'][0]]
        secret_key = dmenu_select(1, 'Secret Key?', inp='\n'.join(inputs))
        if not secret_key:
            return
        for char in secret_key:
            if char.upper() not in keepmenu.SECRET_VALID_CHARS:
                dmenu_err(f'Invaild character in secret key, valid characters are {keepmenu.SECRET_VALID_CHARS}')
                return
        inputs = ['Defaut RFC 6238 token settings', 'Steam token settings', 'Use custom settings']
        otp_settings_choice = dmenu_select(len(inputs), 'Settings', inp='\n'.join(inputs))
        if otp_settings_choice == 'Defaut RFC 6238 token settings':
            algorithm_choice = 'sha1'
            time_step_choice = 30
            code_size_choice = 6
        elif otp_settings_choice == 'Steam token settings':
            algorithm_choice = 'sha1'
            time_step_choice = 30
            code_size_choice = 5
        elif otp_settings_choice == 'Use custom settings':
            inputs = ['SHA-1', 'SHA-256', 'SHA-512']
            algorithm_choice = dmenu_select(len(inputs), 'Algorithm', inp='\n'.join(inputs))
            if not algorithm_choice:
                return
            algorithm_choice = algorithm_choice.replace('-', '').lower()
            time_step_choice = dmenu_select(1, 'Time Step (sec)', inp='30\n')
            if not time_step_choice:
                return
            try:
                time_step_choice = int(time_step_choice)
            except ValueError:
                time_step_choice = 30
            code_size_choice = dmenu_select(1, 'Code Size', inp='6\n')
            if not code_size_choice:
                return
            try:
                code_size_choice = int(time_step_choice)
            except ValueError:
                code_size_choice = 6
        otp_url = f'otpauth://totp/Main:none?secret={secret_key}&period={time_step_choice}&digits={code_size_choice}&issuer=Main'
        if algorithm_choice != 'sha1':
            otp_url += '&algorithm=' + algorithm_choice
        if otp_settings_choice == 'Steam token settings':
            otp_url += '&encoder=steam'
        if hasattr(kp_entry, 'otp'):
            setattr(kp_entry, 'otp', otp_url)
        else:
            kp_entry.set_custom_property('otp', otp_url)",otp_url is not None,233,otp_url,False,36.78794411714425,N/A
"def tokenize_autotype(autotype):
    """"""Process the autotype sequence

    Args: autotype - string
    Returns: tokens - generator ((token, if_special_char T/F), ...)

    """"""
    while autotype:
        opening_idx = -1
        for char in '{+^%~@':
            idx = autotype.find(char)
<mask>:
                opening_idx = idx
        if opening_idx == -1:
            yield (autotype, False)
            return
        if opening_idx > 0:
            yield (autotype[:opening_idx], False)
        if autotype[opening_idx] in '+^%~@':
            yield (autotype[opening_idx], True)
            autotype = autotype[opening_idx + 1:]
            continue
        closing_idx = autotype.find('}')
        if closing_idx == -1:
            dmenu_err('Unable to find matching right brace (}) while' + f'tokenizing auto-type string: {autotype}\n')
            return
        if closing_idx == opening_idx + 1 and closing_idx + 1 < len(autotype) and (autotype[closing_idx + 1] == '}'):
            yield ('{}}', True)
            autotype = autotype[closing_idx + 2:]
            continue
        yield (autotype[opening_idx:closing_idx + 1], True)
        autotype = autotype[closing_idx + 1:]",idx != -1 and (opening_idx == -1 or idx < opening_idx),121,idx != -1,False,2.3517745856009116,N/A
"def token_command(token):
    """"""When token denotes a special command, this function provides a callable
    implementing its behaviour.

    """"""
    cmd = None

    def _check_delay():
        match = re.match('{DELAY (\\d+)}', token)
<mask>:
            delay = match.group(1)
            nonlocal cmd
            cmd = lambda _, t=delay: time.sleep(int(t) / 1000)
            return True
        return False

    def _check_additional_attribute():
        match = re.match('{S:(.*)}', token)
        if match:
            attr = match.group(1)
            nonlocal cmd
            cmd = lambda e, a=attr: e.get_custom_property(a)
            return True
        return False
    if _check_delay():
        return cmd
    if _check_additional_attribute():
        return cmd
    return None",match,78,match,True,100.00000000000004,N/A
"def type_entry(entry, db_autotype=None):
    """"""Pick which library to use to type strings

    Defaults to pynput

    Args: entry - The entry to type
          db_autotype - the database specific autotype that overrides 'autotype_default'

    """"""
    sequence = keepmenu.SEQUENCE
<mask>:
        if hasattr(entry, 'password'):
            type_clipboard(entry.password)
        else:
            dmenu_err(""Clipboard is active. 'View/Type Individual entries' and select field to copy"")
        return
    if hasattr(entry, 'autotype_enabled') and entry.autotype_enabled is False:
        dmenu_err('Autotype disabled for this entry')
        return
    if db_autotype is not None and db_autotype != '':
        sequence = db_autotype
    if hasattr(entry, 'autotype_sequence') and entry.autotype_sequence is not None and (entry.autotype_sequence != 'None'):
        sequence = entry.autotype_sequence
    tokens = tokenize_autotype(sequence)
    library = 'pynput'
    libraries = {'pynput': type_entry_pynput, 'xdotool': type_entry_xdotool, 'ydotool': type_entry_ydotool, 'wtype': type_entry_wtype, 'dotool': type_entry_dotool}
    library = keepmenu.CONF.get('database', 'type_library', fallback='pynput')
    libraries.get(library, type_entry_pynput)(entry, tokens)",keepmenu.CLIPBOARD is True,118,entry.is_clipboard,False,12.703318703865365,N/A
"def type_entry_pynput(entry, tokens):
    """"""Use pynput to auto-type the selected entry

    """"""
    try:
        from pynput import keyboard
        from .tokens_pynput import AUTOTYPE_TOKENS
    except ModuleNotFoundError:
        return
    kbd = keyboard.Controller()
    enter_idx = True
    for token, special in tokens:
<mask>:
            cmd = token_command(token)
            if callable(cmd):
                to_type = cmd(entry)
                if to_type is not None:
                    try:
                        kbd.type(to_type)
                    except kbd.InvalidCharacterException:
                        dmenu_err('Unable to type string...bad character.\nTry setting `type_library = xdotool` in config.ini')
            elif token in PLACEHOLDER_AUTOTYPE_TOKENS:
                to_type = PLACEHOLDER_AUTOTYPE_TOKENS[token](entry)
                if to_type:
                    try:
                        kbd.type(to_type)
                    except kbd.InvalidCharacterException:
                        dmenu_err('Unable to type string...bad character.\nTry setting `type_library = xdotool` in config.ini')
                        return
            elif token in STRING_AUTOTYPE_TOKENS:
                to_type = STRING_AUTOTYPE_TOKENS[token]
                try:
                    kbd.type(to_type)
                except kbd.InvalidCharacterException:
                    dmenu_err('Unable to type string...bad character.\nTry setting `type_library = xdotool` in config.ini')
                    return
            elif token in AUTOTYPE_TOKENS:
                to_tap = AUTOTYPE_TOKENS[token]
                kbd.tap(to_tap)
                if enter_idx is True and token in ('{ENTER}', '~'):
                    kbd.tap(to_tap)
                    enter_idx = False
            else:
                dmenu_err(f'Unsupported auto-type token (pynput): ""{token}""')
                return
        else:
            try:
                kbd.type(token)
            except kbd.InvalidCharacterException:
                dmenu_err('Unable to type string...bad character.\nTry setting `type_library = xdotool` in config.ini')
                return",special,156,token in token_command,False,0.0,N/A
"def type_entry_xdotool(entry, tokens):
    """"""Auto-type entry entry using xdotool

    """"""
    enter_idx = True
    from .tokens_xdotool import AUTOTYPE_TOKENS
    for token, special in tokens:
<mask>:
            cmd = token_command(token)
            if callable(cmd):
                to_type = cmd(entry)
                if to_type is not None:
                    call(['xdotool', 'type', '--', to_type])
            elif token in PLACEHOLDER_AUTOTYPE_TOKENS:
                to_type = PLACEHOLDER_AUTOTYPE_TOKENS[token](entry)
                if to_type:
                    call(['xdotool', 'type', '--', to_type])
            elif token in STRING_AUTOTYPE_TOKENS:
                to_type = STRING_AUTOTYPE_TOKENS[token]
                call(['xdotool', 'type', '--', to_type])
            elif token in AUTOTYPE_TOKENS:
                cmd = ['xdotool'] + AUTOTYPE_TOKENS[token]
                call(cmd)
                if enter_idx is True and token in ('{ENTER}', '~'):
                    cmd = ['xdotool'] + AUTOTYPE_TOKENS[token]
                    call(cmd)
                    enter_idx = False
            else:
                dmenu_err(f'Unsupported auto-type token (xdotool): ""{token}""')
                return
        else:
            call(['xdotool', 'type', '--', token])",special,103,token in token_command,False,0.0,N/A
"def dmenu_cmd(num_lines, prompt):
    """"""Parse config.ini for dmenu options

    Args: args - num_lines: number of lines to display
                 prompt: prompt to show
    Returns: command invocation (as a list of strings) for
                [""dmenu"", ""-l"", ""<num_lines>"", ""-p"", ""<prompt>"", ""-i"", ...]

    """"""
    commands = {'bemenu': ['-p', str(prompt), '-l', str(num_lines)], 'dmenu': ['-p', str(prompt), '-l', str(num_lines)], 'wmenu': ['-p', str(prompt), '-l', str(num_lines)], 'rofi': ['-dmenu', '-p', str(prompt), '-l', str(num_lines)], 'tofi': ['--require-match=false', f'--prompt-text={str(prompt)}: ', f'--num-results={str(num_lines)}'], 'wofi': ['--dmenu', '-p', str(prompt), '-L', str(num_lines + 1)], 'yofi': ['-p', str(prompt), 'dialog'], 'fuzzel': ['-p', str(prompt) + ' ', '-l', str(num_lines)]}
    command = shlex.split(keepmenu.CONF.get('dmenu', 'dmenu_command', fallback='dmenu'))
    command.extend(commands.get(basename(command[0]), []))
    pwprompts = ('Password', 'password', 'client_secret', 'Verify password', 'Enter Password')
    obscure = keepmenu.CONF.getboolean('dmenu_passphrase', 'obscure', fallback=True)
<mask>:
        pass_prompts = {'dmenu': dmenu_pass(basename(command[0])), 'wmenu': dmenu_pass(basename(command[0])), 'rofi': ['-password'], 'bemenu': ['-x', 'indicator', '*'], 'tofi': ['--hide-input=true', '--hidden-character=*'], 'wofi': ['-P'], 'yofi': ['--password'], 'fuzzel': ['--password']}
        command.extend(pass_prompts.get(basename(command[0]), []))
    return command",any((i == prompt for i in pwprompts)) and obscure is True,133,command[0],False,0.0,N/A
"def dmenu_pass(command):
    """"""Check if dmenu passphrase patch is applied and return the correct command
    line arg list for wmenu or dmenu

    Args: command - string
    Returns: list or None

    """"""
<mask>:
        return None
    try:
        dm_patch = b'P' in run([command, '-h'], capture_output=True, check=False).stderr
    except FileNotFoundError:
        dm_patch = False
    color = keepmenu.CONF.get('dmenu_passphrase', 'obscure_color', fallback='#222222')
    dargs = {'dmenu': ['-nb', color, '-nf', color], 'wmenu': ['-n', color, '-N', color]}
    return ['-P'] if dm_patch else dargs[command]","command not in ('dmenu', 'wmenu')",70,not command,False,3.520477365831487,N/A
"def view_all_entries(options, kp_entries, dbname):
    """"""Generate numbered list of all Keepass entries and open with dmenu.

    Returns: dmenu selection

    """"""
    num_align = len(str(len(kp_entries)))
    kp_entry_pattern = str('{:>{na}} - {} - {} - {}')
    kps = str('\n').join([kp_entry_pattern.format(j, os.path.join('/'.join(i.path[:-1]), i.deref('title') or ''), i.deref('username') or '', i.deref('url') or '', na=num_align) for j, i in enumerate(kp_entries)])
<mask>:
        options_s = '\n'.join(options) + '\n'
        entries_s = options_s + kps
    else:
        entries_s = kps
    prompt = f'Entries: {dbname}'
    if keepmenu.CONF.has_option('dmenu', 'title_path'):
        try:
            max_length = keepmenu.CONF.getboolean('dmenu', 'title_path')
        except ValueError:
            max_length = keepmenu.CONF.getint('dmenu', 'title_path')
        prompt = generate_prompt(max_length, dbname)
    return dmenu_select(min(keepmenu.MAX_LEN, len(options) + len(kp_entries)), inp=entries_s, prompt=prompt)",options,94,options,True,100.00000000000004,N/A
"def view_entry(kp_entry):
    """"""Show title, username, password, url and notes for an entry.

    Returns: dmenu selection

    """"""
    fields = [os.path.join('/'.join(kp_entry.path[:-1]), kp_entry.deref('title') or '') or 'Title: None', kp_entry.deref('username') or 'Username: None', '**********' if kp_entry.deref('password') else 'Password: None', 'TOTP: ******' if get_otp_url(kp_entry) else 'TOTP: None', kp_entry.deref('url') or 'URL: None', 'Notes: <Enter to view>' if kp_entry.deref('notes') else 'Notes: None', str(f'Expire time: {kp_entry.expiry_time}') if kp_entry.expires is True else 'Expiry date: None']
    attrs = kp_entry.custom_properties
    for attr in attrs:
<mask>:
            val = attrs.get(attr) or ''
            protected = kp_entry.is_custom_property_protected(attr) if hasattr(kp_entry, 'is_custom_property_protected') else False
            value = val or 'None' if len(val.split('\n')) <= 1 and (not protected) else '<Enter to view>'
            fields.append(f'{attr}: {value}')
    sel = dmenu_select(len(fields), inp='\n'.join(fields))
    if sel == 'Notes: <Enter to view>':
        sel = view_notes(kp_entry.deref('notes') or '')
    elif sel == 'Notes: None':
        sel = ''
    elif sel == '**********':
        sel = kp_entry.deref('password') or ''
    elif sel == 'TOTP: ******':
        sel = gen_otp(get_otp_url(kp_entry))
    elif sel == fields[4] and (not keepmenu.CONF.getboolean('database', 'type_url', fallback=False)):
        if sel != 'URL: None':
            webbrowser.open(sel)
        sel = ''
    else:
        for attr in attrs:
            if sel == f""{attr}: {attrs.get(attr) or ''}"":
                sel = attrs.get(attr)
                break
            if sel == f'{attr}: <Enter to view>':
                sel = view_notes(attrs.get(attr) or '')
    return sel if not sel.endswith(': None') else ''",attr not in TOTP_FIELDS,200,attr.startswith('_'),False,6.567274736060395,N/A
"def generate_prompt(max_length, dbname):
    """"""Generate a prompt in the format ""Entries: {}"", with ""{}"" replaced by
    the full path to the database truncated to a certain length

    max_length: an int giving the maximum length for the path, or a bool
    specifying whether to show the entire path (True) or to hide it (False)

    dbname: the full path to the database

    """"""
<mask>:
        return 'Entries'
    if max_length is True or max_length is None:
        return f'Entries: {dbname}'
    filename = os.path.basename(dbname)
    if len(filename) >= max_length - 3:
        return f'Entries: {filename}'
    path = dbname.replace(os.path.expanduser('~'), '~')
    if len(path) <= max_length:
        return f'Entries: {path}'
    path = path[:max_length - len(filename) - 3]
    return f'Entries: {path}...{filename}'",max_length is False or max_length == 0,107,not dbname,False,0.0,N/A
"def get_auth():
    """"""Generate and save port and authkey to ~/.cache/.keepmenu-auth

    Returns: int port, bytestring authkey

    """"""
    auth = configparser.ConfigParser()
<mask>:
        fd_ = os.open(keepmenu.AUTH_FILE, os.O_WRONLY | os.O_CREAT, 384)
        with open(fd_, 'w', encoding=keepmenu.ENC) as a_file:
            auth.set('DEFAULT', 'port', str(find_free_port()))
            auth.set('DEFAULT', 'authkey', random_str())
            auth.write(a_file)
    try:
        auth.read(keepmenu.AUTH_FILE)
        port = auth.get('DEFAULT', 'port')
        authkey = auth.get('DEFAULT', 'authkey').encode()
    except (configparser.NoOptionError, configparser.MissingSectionHeaderError):
        os.remove(keepmenu.AUTH_FILE)
        print('Cache file was corrupted. Stopping all instances. Please try again')
        call(['pkill', 'keepmenu'])
        return (None, None)
    return (int(port), authkey)",not exists(keepmenu.AUTH_FILE),72,os.path.exists(keepmenu.AUTH_FILE),False,61.32297420585347,N/A
"def run(**kwargs):
    """"""Start the background Manager and Dmenu runner processes.

    """"""
    server = Server()
<mask>:
        server.totp_flag.set()
    dmenu = DmenuRunner(server, **kwargs)
    dmenu.daemon = True
    server.start()
    dmenu.start()
    try:
        server.join()
    except KeyboardInterrupt:
        sys.exit()
    finally:
        if exists(expanduser(keepmenu.AUTH_FILE)):
            os.remove(expanduser(keepmenu.AUTH_FILE))",kwargs.get('totp'),34,not server.totp_flag.isSet(),False,5.522397783539471,N/A
"def main():
    """"""Main script entrypoint

    """"""
    parser = argparse.ArgumentParser(description='Dmenu (or compatible launcher) frontend for Keepass databases')
    parser.add_argument('-a', '--autotype', type=str, required=False, help='Override autotype sequence in config.ini')
    parser.add_argument('-c', '--config', type=str, required=False, help='File path to a config file')
    parser.add_argument('-C', '--clipboard', action='store_true', default=False, required=False, help='Copy values to clipboard instead of typing.')
    parser.add_argument('-d', '--database', type=str, required=False, help='File path to a database to open, skipping the database selection menu')
    parser.add_argument('-k', '--keyfile', type=str, required=False, help='File path of the keyfile needed to open the database specified by --database/-d')
    parser.add_argument('-t', '--totp', action='store_true', required=False, help='TOTP mode')
    args = vars(parser.parse_args())
    port, auth = get_auth()
<mask>:
        run(**args)
    try:
        manager = client(port, auth)
        conn = manager.get_pipe()
        if args.get('totp'):
            manager.totp_mode()
        if any(args.values()):
            conn.send(args)
            manager.read_args_from_pipe()
        manager.set_event()
    except ConnectionRefusedError:
        pass",port_in_use(port) is False,114,args.get('database'),False,4.955725306405571,N/A
"def build_shapes(df: pd.DataFrame) -> gpd.GeoDataFrame:
<mask>:
        return gpd.GeoDataFrame({'shape_id': [], 'geometry': []}, crs=DEFAULT_CRS)
    data: Dict[str, List] = {'shape_id': [], 'geometry': []}
    for shape_id, shape in df.sort_values('shape_pt_sequence').groupby('shape_id'):
        data['shape_id'].append(shape_id)
        data['geometry'].append(LineString(list(zip(shape.shape_pt_lon, shape.shape_pt_lat))))
    return gpd.GeoDataFrame(data, crs=DEFAULT_CRS)",df.empty,31,not df.empty,False,59.460355750136046,N/A
"def build_stops(df: pd.DataFrame) -> gpd.GeoDataFrame:
<mask>:
        return gpd.GeoDataFrame(df, geometry=[], crs=DEFAULT_CRS)
    df['geometry'] = df.apply(lambda s: Point(s.stop_lon, s.stop_lat), axis=1)
    df.drop(['stop_lon', 'stop_lat'], axis=1, inplace=True)
    return gpd.GeoDataFrame(df, crs={'init': 'EPSG:4326'})",df.empty,25,not df.shape[0],False,13.134549472120788,N/A
"def load_feed(path: str, view: Optional[View]=None, config: Optional[nx.DiGraph]=None) -> Feed:
    config = default_config() if config is None else config
    view = {} if view is None else view
<mask>:
        raise ValueError('Config must be a DAG')
    if os.path.isdir(path):
        feed = _load_feed(path, view, config)
    elif os.path.isfile(path):
        feed = _unpack_feed(path, view, config)
    else:
        raise ValueError('File or path not found: {}'.format(path))
    return feed",not nx.is_directed_acyclic_graph(config),58,config.get('dag') is None,False,4.420141128732569,N/A
"def _service_ids_by_date(feed: Feed) -> Dict[datetime.date, FrozenSet[str]]:
    results: DefaultDict[datetime.date, Set[str]] = defaultdict(set)
    removals: DefaultDict[datetime.date, Set[str]] = defaultdict(set)
    service_ids = set(feed.trips.service_id)
    calendar = feed.calendar
    caldates = feed.calendar_dates
<mask>:
        calendar = calendar[calendar.service_id.isin(service_ids)].copy()
    if not caldates.empty:
        caldates = caldates[caldates.service_id.isin(service_ids)].copy()
    if not calendar.empty:
        calendar.start_date = vparse_date(calendar.start_date)
        calendar.end_date = vparse_date(calendar.end_date)
        for _, cal in calendar.iterrows():
            start = cal.start_date.toordinal()
            end = cal.end_date.toordinal()
            dow = {i: cal[day] for i, day in enumerate(DAY_NAMES)}
            for ordinal in range(start, end + 1):
                date = datetime.date.fromordinal(ordinal)
                if int(dow[date.weekday()]):
                    results[date].add(cal.service_id)
    if not caldates.empty:
        caldates.date = vparse_date(caldates.date)
        cdadd = caldates[caldates.exception_type == '1']
        cdrem = caldates[caldates.exception_type == '2']
        for _, cd in cdadd.iterrows():
            results[cd.date].add(cd.service_id)
        for _, cd in cdrem.iterrows():
            removals[cd.date].add(cd.service_id)
        for date in removals:
            for service_id in removals[date]:
                if service_id in results[date]:
                    results[date].remove(service_id)
            if len(results[date]) == 0:
                del results[date]
    assert results, 'No service found in feed.'
    return {k: frozenset(v) for k, v in results.items()}",not calendar.empty,139,not calendar.empty,True,100.00000000000004,N/A
"def add_geo_config(g: nx.DiGraph) -> nx.DiGraph:
    from .geo import build_shapes, build_stops
    for node, transform in (('shapes.txt', build_shapes), ('stops.txt', build_stops)):
        assert node in g.nodes, 'Missing config for node: {}'.format(node)
<mask>:
            g.nodes[node]['transformations'] = []
        g.nodes[node]['transformations'].append(transform)
    return g",'transformations' not in g.nodes[node],34,'transformations' not in g.nodes[node],True,100.00000000000004,N/A
"@lru_cache(maxsize=2 ** 17)
def parse_time(val: str) -> np.float64:
<mask>:
        return val
    val = val.strip()
    if val == '':
        return np.nan
    h, m, s = val.split(':')
    ssm = int(h) * 3600 + int(m) * 60 + int(s)
    return np.float64(ssm)",val is np.nan,38,val.startswith('.'),False,6.567274736060395,N/A
"def write_feed_dangerously(feed: Feed, outpath: str, nodes: Optional[Collection[str]]=None) -> str:
    """"""Naively write a feed to a zipfile

    This function provides no sanity checks. Use it at
    your own risk.
    """"""
    nodes = DEFAULT_NODES if nodes is None else nodes
    with tempfile.TemporaryDirectory() as tmpdir:

        def write_node(node):
            df = feed.get(node)
<mask>:
                path = os.path.join(tmpdir, node)
                df.to_csv(path, index=False)
        pool = ThreadPool(len(nodes))
        try:
            pool.map(write_node, nodes)
        finally:
            pool.terminate()
        if outpath.endswith('.zip'):
            outpath, _ = os.path.splitext(outpath)
        outpath = shutil.make_archive(outpath, 'zip', tmpdir)
    return outpath",not df.empty,75,df,False,4.9787068367863965,N/A
"def __init__(self, source: Union[str, 'Feed'], view: Optional[View]=None, config: Optional[nx.DiGraph]=None):
    self._config: nx.DiGraph = default_config() if config is None else config
    self._view: View = {} if view is None else view
    self._cache: Dict[str, pd.DataFrame] = {}
    self._pathmap: Dict[str, str] = {}
    self._delete_after_reading: bool = False
    self._shared_lock = RLock()
    self._locks: Dict[str, RLock] = {}
<mask>:
        self._read = source.get
    elif isinstance(source, str) and os.path.isdir(source):
        self._read = self._read_csv
        self._bootstrap(source)
    else:
        raise ValueError('Invalid source')","isinstance(source, self.__class__)",68,"isinstance(source, Feed)",False,19.765609300943975,N/A
"def get(self, filename: str) -> pd.DataFrame:
    lock = self._locks.get(filename, self._shared_lock)
    with lock:
        df = self._cache.get(filename)
<mask>:
            df = self._read(filename)
            df = self._filter(filename, df)
            df = self._prune(filename, df)
            self._convert_types(filename, df)
            df = df.reset_index(drop=True)
            df = self._transform(filename, df)
            self.set(filename, df)
        return self._cache[filename]",df is None,40,df is None,True,100.00000000000004,N/A
"def _bootstrap(self, path: str) -> None:
    for root, _subdirs, files in os.walk(path):
        for fname in files:
            basename = os.path.basename(fname)
<mask>:
                raise ValueError('More than one {} in folder'.format(basename))
            self._pathmap[basename] = os.path.join(root, fname)
            self._locks[basename] = RLock()",basename in self._pathmap,34,basename in self._pathmap,True,100.00000000000004,N/A
"def _read_csv(self, filename: str) -> pd.DataFrame:
    path = self._pathmap.get(filename)
    columns = self._config.nodes.get(filename, {}).get('required_columns', [])
<mask>:
        return empty_df(columns)
    with open(path, 'rb') as f:
        encoding = detect_encoding(f)
    df = pd.read_csv(path, dtype=str, encoding=encoding, index_col=False)
    df.rename(columns=lambda x: x.strip(), inplace=True)
    if not df.empty:
        for col in df.columns:
            df[col] = df[col].str.strip()
    return df",path is None or os.path.getsize(path) == 0,47,not path,False,0.07517195964887861,N/A
"def _filter(self, filename: str, df: pd.DataFrame) -> pd.DataFrame:
    """"""Apply view filters""""""
    view = self._view.get(filename)
<mask>:
        return df
    for col, values in view.items():
        if col in df.columns:
            df = df[df[col].isin(setwrap(values))]
    return df",view is None,31,view is None,True,100.00000000000004,N/A
"def remove_node_attributes(G: nx.DiGraph, attributes: Union[str, Iterable[str]]):
    """"""
    Return a copy of the graph with the given attributes
    deleted from all nodes.
    """"""
    G = G.copy()
    for _, data in G.nodes(data=True):
        for attribute in setwrap(attributes):
<mask>:
                del data[attribute]
    return G",attribute in data,39,attribute in data,True,100.00000000000004,N/A
"def detect_encoding(f: BinaryIO, limit: int=2500) -> str:
    """"""
    Return encoding of provided input stream.

    Most of the time it's unicode, but if we are unable to decode the input
    natively, use `chardet` to determine the encoding heuristically.
    """"""
    unicode_decodable = True
    for line_no, line in enumerate(f):
        try:
            line.decode('utf-8')
        except UnicodeDecodeError:
            unicode_decodable = False
            break
<mask>:
            break
    if unicode_decodable:
        return 'utf-8'
    f.seek(0)
    return detect(f.read())['encoding']",line_no > limit,63,line_no >= limit,False,53.7284965911771,N/A
"@pytest.mark.parametrize('path,dates,shapes', [(fixture('caltrain-2017-07-24'), [], {'agency.txt': (1, 6), 'calendar.txt': (3, 10), 'calendar_dates.txt': (642, 3), 'fare_attributes.txt': (6, 6), 'fare_rules.txt': (144, 4), 'routes.txt': (4, 7), 'shapes.txt': (3008, 5), 'stops.txt': (64, 12)}), (fixture('caltrain-2017-07-24'), [datetime.date(2017, 8, 6)], {'agency.txt': (1, 6), 'calendar.txt': (1, 10), 'calendar_dates.txt': (6, 3), 'fare_attributes.txt': (4, 6), 'fare_rules.txt': (48, 4), 'routes.txt': (3, 7), 'shapes.txt': (1094, 5), 'stops.txt': (50, 12)}), (fixture('nested'), [datetime.date(2017, 8, 6)], {'agency.txt': (1, 6), 'calendar.txt': (1, 10), 'calendar_dates.txt': (6, 3), 'fare_attributes.txt': (4, 6), 'fare_rules.txt': (48, 4), 'routes.txt': (3, 7), 'shapes.txt': (1094, 5), 'stops.txt': (50, 12)}), (fixture('amazon-2017-08-06'), [], {'agency.txt': (1, 7), 'calendar.txt': (2, 10), 'calendar_dates.txt': (2, 3), 'fare_attributes.txt': (0, 5), 'fare_rules.txt': (0, 1), 'routes.txt': (50, 9), 'shapes.txt': (12032, 5), 'stops.txt': (35, 12)}), (fixture('amazon-2017-08-06'), [datetime.date(2017, 8, 5)], {'agency.txt': (1, 7), 'calendar.txt': (1, 10), 'calendar_dates.txt': (2, 3), 'fare_attributes.txt': (0, 5), 'fare_rules.txt': (0, 1), 'routes.txt': (1, 9), 'shapes.txt': (3, 5), 'stops.txt': (2, 12)}), (fixture('region-nord-v2'), [], {'agency.txt': (1, 7), 'calendar.txt': (0, 10), 'calendar_dates.txt': (333, 3), 'fare_attributes.txt': (0, 5), 'fare_rules.txt': (0, 1), 'routes.txt': (181, 12), 'shapes.txt': (0, 4), 'stops.txt': (3693, 15)})])
def test_read_file(path, dates, shapes):
    service_ids_by_date = ptg.read_service_ids_by_date(path)
    service_ids = {service_id for date in dates if date in service_ids_by_date for service_id in service_ids_by_date[date]}
<mask>:
        feed = Feed(path, view={'trips.txt': {'service_id': service_ids}})
    else:
        feed = Feed(path)
    for filename, shape in shapes.items():
        assert feed.get(filename).shape == shape, '{}/{} dataframe shape was incorrect'.format(path, filename)",service_ids,211,not path.startswith('/'),False,0.0,N/A
"@pytest.mark.parametrize('path', [zip_file('seattle-area-2017-11-16'), fixture('seattle-area-2017-11-16')])
def test_extract_agencies(path):
    fd = ptg.load_feed(path)
    agencies = fd.agency
    assert len(agencies) == 3
    routes = fd.routes
    assert len(routes) == 14
    agency_ids = [agencies.iloc[0].agency_id]
    route_ids = set(fd.routes[fd.routes.agency_id.isin(agency_ids)].route_id)
    trip_ids = set(fd.trips[fd.trips.route_id.isin(route_ids)].trip_id)
    stop_ids = set(fd.stop_times[fd.stop_times.trip_id.isin(trip_ids)].stop_id)
    assert len(route_ids)
    assert len(trip_ids)
    assert len(stop_ids)
    with tempfile.TemporaryDirectory() as tmpdir:
        outfile = os.path.join(tmpdir, 'test.zip')
        result = ptg.extract_feed(path, outfile, {'routes.txt': {'agency_id': agency_ids}})
        assert result == outfile
        new_fd = ptg.load_feed(outfile)
        assert list(new_fd.agency.agency_id) == agency_ids
        assert set(new_fd.routes.route_id) == route_ids
        assert set(new_fd.trips.trip_id) == trip_ids
        assert set(new_fd.stop_times.trip_id) == trip_ids
        assert set(new_fd.stops.stop_id) == stop_ids
        nodes = []
        for node in fd._config.nodes():
            df = fd.get(node)
<mask>:
                nodes.append(node)
        assert len(nodes)
        for node in nodes:
            original_df = fd.get(node)
            new_df = new_fd.get(node)
            assert set(original_df.columns) == set(new_df.columns)",not df.empty,110,df is not None,False,18.99589214128981,N/A
"@pytest.mark.parametrize('path', [zip_file('seattle-area-2017-11-16'), fixture('seattle-area-2017-11-16')])
def test_extract_routes(path):
    fd = ptg.load_feed(path)
    agencies = fd.agency
    assert len(agencies) == 3
    routes = fd.routes
    assert len(routes) == 14
    route_ids = [routes.iloc[0].route_id]
    agency_ids = set(fd.routes[fd.routes.route_id.isin(route_ids)].agency_id)
    trip_ids = set(fd.trips[fd.trips.route_id.isin(route_ids)].trip_id)
    stop_ids = set(fd.stop_times[fd.stop_times.trip_id.isin(trip_ids)].stop_id)
    assert len(agency_ids)
    assert len(trip_ids)
    assert len(stop_ids)
    with tempfile.TemporaryDirectory() as tmpdir:
        outfile = os.path.join(tmpdir, 'test.zip')
        result = ptg.extract_feed(path, outfile, {'trips.txt': {'route_id': route_ids}})
        assert result == outfile
        new_fd = ptg.load_feed(outfile)
        assert list(new_fd.routes.route_id) == route_ids
        assert set(new_fd.agency.agency_id) == agency_ids
        assert set(new_fd.trips.trip_id) == trip_ids
        assert set(new_fd.stop_times.trip_id) == trip_ids
        assert set(new_fd.stops.stop_id) == stop_ids
        nodes = []
        for node in fd._config.nodes():
            df = fd.get(node)
<mask>:
                nodes.append(node)
        assert len(nodes)
        for node in nodes:
            original_df = fd.get(node)
            new_df = new_fd.get(node)
            assert set(original_df.columns) == set(new_df.columns)",not df.empty,110,df is not None,False,18.99589214128981,N/A
"@property
def project(self):
    """"""Return a py.path.local object if no exception occurred.""""""
    warning_message = 'project is deprecated and will be removed in a future release, please use project_path instead.'
    warnings.warn(warning_message, DeprecationWarning, stacklevel=1)
<mask>:
        return py.path.local(self._project_dir)
    return None",self.exception is None,36,self._project_dir is not None,False,11.339582221952005,N/A
"@property
def project_path(self):
    """"""Return a pathlib.Path object if no exception occurred.""""""
<mask>:
        return pathlib.Path(self._project_dir)
    return None",self.exception is None,16,self._project_dir is not None,False,11.339582221952005,N/A
"def bake(self, extra_context=None, template=None):
    exception = None
    exit_code = 0
    project_dir = None
    context = None
<mask>:
        template = self._default_template
    context_file = pathlib.Path(template) / 'cookiecutter.json'
    try:
        context = prompt_for_config(generate_context(context_file=str(context_file), extra_context=extra_context), no_input=True)
        project_dir = cookiecutter(template, no_input=True, extra_context=extra_context, output_dir=str(self._new_output_dir()), config_file=str(self._config_file))
    except SystemExit as e:
        if e.code != 0:
            exception = e
        exit_code = e.code
    except Exception as e:
        exception = e
        exit_code = -1
    return Result(exception=exception, exit_code=exit_code, project_dir=project_dir, context=context)",template is None,67,template is None,True,100.00000000000004,N/A
"@pytest.fixture
def cookies(request, tmpdir, _cookiecutter_config_file):
    """"""Yield an instance of the Cookies helper class that can be used to
    generate a project from a template.

    Run cookiecutter:
        result = cookies.bake(extra_context={
            'variable1': 'value1',
            'variable2': 'value2',
        })
    """"""
    template_dir = request.config.option.template
    output_dir = tmpdir.mkdir('cookies')
    output_factory = output_dir.mkdir
    yield Cookies(template_dir, output_factory, _cookiecutter_config_file)
<mask>:
        output_dir.remove()",not request.config.option.keep_baked_projects,50,output_dir.exists(),False,3.823246852690463,N/A
"def get_contents(self, filename, ref=None):
<mask>:
        return mock_gh_files
    else:
        for f in mock_gh_files:
            if filename in f.name:
                return f
        return None",filename == '',20,ref is None,False,0.0,N/A
"def gl_files_content(self, file_path, ref):
    """"""Returns none if the file is not in the known repo""""""
    for glf in mock_gl_files:
<mask>:
            f = Mock()
            f_content = 'The text of a file'
            f.content = base64.b64encode(f_content.encode('utf-8'))
            return f
    return None",file_path in glf['name'],37,glf.file_path == file_path,False,16.784459625186194,N/A
"def test_get_parameters(self):
    rq, _ = self.loader.getTextForName('test-rq')
    params = gquery.get_parameters(rq, '', {})
    self.assertGreaterEqual(len(params), 7, 'Should find some parameters')
    for paramName, param in params.items():
        self.assertIn('name', param, 'Should have a name')
        self.assertIn('type', param, 'Should have a type')
        self.assertIn('required', param, 'Should have a required')
        orig = param['original']
<mask>:
            self.assertEqual(param['type'], 'string', 'Should be type string')
            self.assertEqual(param['format'], 'iri', 'Should be format iri')
        if '_number' in orig:
            self.assertEqual(param['type'], 'number', 'Should be type number')
        if '_literal' in orig:
            self.assertEqual(param['type'], 'literal', 'Should be type literal')
        if '_en' in orig:
            self.assertEqual(param['type'], 'string', 'Should be type literal')
            self.assertEqual(param['lang'], 'en', 'Should be en language')
        if '_integer' in orig:
            self.assertEqual(param['datatype'], 'xsd:integer', 'Should be type xsd:integer')
        if '_xsd_date' in orig:
            self.assertEqual(param['datatype'], 'xsd:date', 'Should be type xsd:date')
    self.assertEqual(params['o1']['type'], 'string', 'o1 should be a string')
    self.assertEqual(params['o2']['format'], 'iri', 'o2 should be format iri')
    self.assertEqual(params['o3']['type'], 'number', 'o3 should be a number')
    self.assertEqual(params['o4']['type'], 'literal', 'o4 should be a literal')
    self.assertEqual(params['o5']['lang'], 'en', 'o5 should be a English')
    self.assertEqual(params['o6']['datatype'], 'xsd:integer', 'o6 should be a integer')
    self.assertEqual(params['o7']['datatype'], 'xsd:date', 'o7 should be a date')",'_iri' in orig,161,'_string' in orig,False,30.213753973567677,N/A
"def getResponseText(endpoint, query, requestedMimeType):
    """"""Returns the result and mimetype of executing the given query against
    the given endpoint.

    Keyword arguments:
    endpoint - URL of sparql endpoint
    query    - SPARQL query to be executed
    requestedMimeType  Type of content requested. can be:
                'text/csv; q=1.0, */*; q=0.1'
                'application/json'
                etc.
    """"""
    retFormat = _mimeTypeToSparqlFormat(requestedMimeType)
    client = SPARQLWrapper(endpoint)
    client.setQuery(query)
    client.setReturnFormat(retFormat)
    client.setCredentials(static.DEFAULT_ENDPOINT_USER, static.DEFAULT_ENDPOINT_PASSWORD)
    result = client.queryAndConvert()
<mask>:
        result = jsonify(result)
    return (result, MIME_FORMAT[retFormat])",retFormat == JSON,67,retFormat == 'application/json',False,30.213753973567677,N/A
"def get_repo_info(loader, sha, prov_g):
    """"""Generate swagger information from the repo being used.""""""
    user_repo = loader.getFullName()
    repo_title = loader.getRepoTitle()
    repo_desc = loader.getRepoDescription()
    contact_name = loader.getContactName()
    contact_url = loader.getContactUrl()
    commit_list = loader.getCommitList()
    licence_url = loader.getLicenceURL()
<mask>:
        prov_g.add_used_entity(loader.getRepoURI())
    prev_commit = None
    next_commit = None
    version = sha if sha else commit_list[0]
    if commit_list.index(version) < len(commit_list) - 1:
        prev_commit = commit_list[commit_list.index(version) + 1]
    if commit_list.index(version) > 0:
        next_commit = commit_list[commit_list.index(version) - 1]
    info = {'version': version, 'title': repo_title, 'description': repo_desc, 'contact': {'name': contact_name, 'url': contact_url}}
    if licence_url:
        info['license'] = {'name': 'License', 'url': licence_url}
    if type(loader) is GithubLoader:
        basePath = '/api-git/' + user_repo + '/'
        basePath += 'subdir/' + loader.subdir + '/' if loader.subdir else ''
        basePath += 'commit/' + sha + '/' if sha else ''
    if type(loader) is GitlabLoader:
        basePath = '/api-gitlab/' + user_repo + '/query/'
        basePath += 'branch/' + loader.branch + '/' if loader.branch else ''
        basePath += 'subdir/' + loader.subdir.strip('/') + '/' if loader.subdir else ''
        basePath += 'commit/' + sha + '/' if sha else ''
    elif type(loader) is LocalLoader:
        basePath = '/api-local/'
    elif type(loader) is URLLoader:
        basePath = '/api-url/'
    else:
        glogger.error('Cannot set basePath, loader type unkown')
    return (prev_commit, next_commit, info, basePath)",prov_g,192,prov_g,True,100.00000000000004,N/A
"def get_path_for_item(item):
    """"""Builds the swagger definition for a specific path, based on
    the given item.""""""
    query = item['original_query']
<mask>:
        if 'grlc' in query:
            del query['grlc']
        query = '\n' + json.dumps(query, indent=2) + '\n'
    description = item['description']
    description += '\n\n```\n{}\n```'.format(query)
    description += '\n\nSPARQL transformation:\n```json\n{}```'.format(item['transform']) if 'transform' in item else ''
    item_path = {item['method']: {'tags': item['tags'], 'summary': item['summary'], 'description': description, 'produces': ['text/csv', 'application/json', 'text/html'], 'parameters': item['params'] if 'params' in item else None, 'responses': {'200': {'description': 'Query response', 'schema': {'type': 'array', 'items': {'type': 'object', 'properties': item['item_properties'] if 'item_properties' in item else None}}}, 'default': {'description': 'Unexpected error', 'schema': {'$ref': '#/definitions/Message'}}}}}
    return item_path","isinstance(query, dict)",98,query,False,0.673794699908547,N/A
"def build_spec(user, repo, subdir=None, query_url=None, sha=None, prov=None, extraMetadata=[], git_type=None, branch=None):
    """"""Build grlc specification for the given github user / repo.""""""
    loader = grlc.utils.getLoader(user, repo, subdir, query_url, sha=sha, prov=prov, git_type=git_type, branch=branch)
    files = loader.fetchFiles()
    raw_repo_uri = loader.getRawRepoUri()
    items = []
    warnings = []
    allowed_ext = ['rq', 'sparql', 'json', 'tpf']
    for c in files:
        glogger.debug('>>>>>>>>>>>>>>>>>>>>>>>>>c_name: {}'.format(c['name']))
        extension = c['name'].split('.')[-1]
<mask>:
            item, warning = _buildItem(c, extension, query_url, raw_repo_uri, loader, extraMetadata)
            if item:
                items.append(item)
            if warning:
                warnings.append(warning)
    if loader.getLicenceURL() is None:
        warnings.append('Queries behind this API do not have a license. You may not be allowed to use them.')
    return (items, warnings)",extension in allowed_ext or query_url,97,extension in allowed_ext,False,44.932896411722176,N/A
"def _buildItem(c, extension, query_url, raw_repo_uri, loader, extraMetadata):
    """"""Collect all the information required to build an item from a file in a repository.""""""
    item = None
    warning = None
    call_name = c['name'].split('.')[0]
    query_text = loader.getTextFor(c)
<mask>:
        query_text = json.loads(query_text)
        if not grlc.utils.SPARQLTransformer_validJSON(query_text):
            glogger.debug('===================================================================')
            glogger.debug('JSON file not a SPARQL query: {}'.format(c['name']))
            glogger.debug('===================================================================')
            return (item, warning)
    if extension in ['rq', 'sparql', 'json'] or query_url:
        glogger.debug('===================================================================')
        glogger.debug('Processing SPARQL query: {}'.format(c['name']))
        glogger.debug('===================================================================')
        try:
            item = process_sparql_query_text(query_text, loader, call_name, extraMetadata)
        except Exception as e:
            warning = str(e)
    elif 'tpf' == extension:
        glogger.debug('===================================================================')
        glogger.debug('Processing TPF query: {}'.format(c['name']))
        glogger.debug('===================================================================')
        item = process_tpf_query_text(query_text, raw_repo_uri, call_name, extraMetadata)
    else:
        glogger.info('Ignoring unsupported source call name: {}'.format(c['name']))
    return (item, warning)",extension == 'json',107,'json' in query_text,False,10.682175159905848,N/A
"def process_tpf_query_text(query_text, raw_repo_uri, call_name, extraMetadata):
    """"""Generates a swagger specification item based on the given TPF query file.""""""
    query_metadata = gquery.get_yaml_decorators(query_text)
    tags = query_metadata['tags'] if 'tags' in query_metadata else []
    glogger.debug('Read query tags: ' + ', '.join(tags))
    summary = query_metadata['summary'] if 'summary' in query_metadata else ''
    glogger.debug('Read query summary: ' + summary)
    description = query_metadata['description'] if 'description' in query_metadata else ''
    glogger.debug('Read query description: ' + description)
    method = query_metadata['method'].lower() if 'method' in query_metadata else 'get'
<mask>:
        method = 'get'
    pagination = query_metadata['pagination'] if 'pagination' in query_metadata else ''
    glogger.debug('Read query pagination: ' + str(pagination))
    endpoint = query_metadata['endpoint'] if 'endpoint' in query_metadata else ''
    glogger.debug('Read query endpoint: ' + endpoint)
    params = []
    if pagination:
        params.append(pageUtils.getSwaggerPaginationDef(pagination))
    item = packItem('/' + call_name, method, tags, summary, description, params, query_metadata, extraMetadata)
    return item","method not in ['get', 'post', 'head', 'put', 'delete', 'options', 'connect']",129,method == 'get_all',False,1.098475607466579,N/A
"def query(user, repo, query_name, subdir=None, spec_url=None, sha=None, content=None, git_type=None, branch=None):
    """"""Execute SPARQL query for a specific grlc-generated API endpoint""""""
    glogger.info('-----> Executing call name at /{}/{} ({})/{}/{} on commit {}'.format(user, repo, git_type, subdir, query_name, sha))
    glogger.debug('Request accept header: ' + request.headers['Accept'])
    requestArgs = request.args
    acceptHeader = request.headers['Accept']
    requestUrl = request.url
    formData = request.form
    method = request.method
    query_response, status, headers = utils.dispatch_query(user, repo, query_name, subdir, spec_url, sha=sha, content=content, requestArgs=requestArgs, acceptHeader=acceptHeader, requestUrl=requestUrl, formData=formData, method=method, git_type=git_type, branch=branch)
<mask>:
        query_response = jsonify(query_response)
    return make_response(query_response, status, headers)","isinstance(query_response, list) or isinstance(query_response, dict)",81,"isinstance(query_response, dict)",False,32.46524673583499,N/A
"def getTextForName(self, query_name):
    """"""Return the query text and query type for the given query name.
        Note that file extention is not part of the query name. For example,
        for `query_name='query1'` would return the content of file `query1.rq`
        from the loader's source (assuming such file exists).""""""
    candidateNames = [query_name + '.rq', query_name + '.sparql', query_name + '.tpf', query_name + '.json']
    candidates = [(name, guessQueryType(name)) for name in candidateNames]
    for queryFullName, queryType in candidates:
        queryText = self._getText(queryFullName)
<mask>:
            if queryType == qType['JSON']:
                queryText = json.loads(queryText)
                if 'proto' not in queryText and '@graph' not in queryText:
                    continue
            return (queryText, queryType)
    return ('', None)",queryText,100,queryText,True,100.00000000000004,N/A
"def fetchFiles(self):
    """"""Returns a list of file items contained on the github repo.""""""
    contents = self.gh_repo.get_contents(self.subdir.strip('/'), ref=self.sha)
    files = []
    for content_file in contents:
<mask>:
            files.append({'download_url': content_file.download_url, 'name': content_file.name, 'decoded_content': content_file.decoded_content})
    return files",content_file.type == 'file',33,content_file.download_url,False,35.64026463354184,N/A
"def getRawRepoUri(self):
    """"""Returns the root url of the github repo.""""""
    raw_repo_uri = static.GITHUB_RAW_BASE_URL + self.user + '/' + self.repo
<mask>:
        raw_repo_uri += '/master/'
    else:
        raw_repo_uri += '/{}/'.format(self.sha)
    return raw_repo_uri",self.sha is NotSet,29,self.sha == 'master',False,30.213753973567677,N/A
"def getTextFor(self, fileItem):
    """"""Returns the contents of the given file item on the github repo.""""""
    raw_query_uri = fileItem['download_url']
<mask>:
        self.prov.add_used_entity(raw_query_uri)
    return str(fileItem['decoded_content'], 'utf-8')",self.prov is not None,23,raw_query_uri and (not self.prov.has_used_entity(raw_query_uri)),False,6.024757292375468,N/A
"def getLicenceURL(self):
    """"""Returns the URL of the license file in this repository if one exists.""""""
    for f in self.fetchFiles():
<mask>:
            return f['download_url']
    return None",f['name'].lower() == 'license' or f['name'].lower() == 'licence',24,f['name'] == 'license',False,6.0472009666934765,N/A
"def guess_endpoint_uri(rq, loader):
    """"""
    Guesses the endpoint URI from (in this order):
    - An endpoint parameter in URL
    - An #+endpoint decorator
    - A endpoint.txt file in the repo
    Otherwise assigns a default one
    """"""
    auth = (static.DEFAULT_ENDPOINT_USER, static.DEFAULT_ENDPOINT_PASSWORD)
<mask>:
        auth = None
    if has_request_context() and 'endpoint' in request.args:
        endpoint = request.args['endpoint']
        glogger.debug('Endpoint provided in request: ' + endpoint)
        return (endpoint, auth)
    try:
        decorators = get_yaml_decorators(rq)
        endpoint = decorators['endpoint']
        auth = None
        glogger.debug('Decorator guessed endpoint: ' + endpoint)
    except (TypeError, KeyError):
        try:
            endpoint_content = loader.getEndpointText()
            endpoint = endpoint_content.strip().splitlines()[0]
            auth = None
            glogger.debug('File guessed endpoint: ' + endpoint)
        except Exception:
            endpoint = static.DEFAULT_ENDPOINT
            auth = (static.DEFAULT_ENDPOINT_USER, static.DEFAULT_ENDPOINT_PASSWORD)
            if auth == ('none', 'none'):
                auth = None
            glogger.info('No endpoint specified, using default ({})'.format(endpoint))
    return (endpoint, auth)","auth == ('none', 'none')",123,"auth == ('none', 'none')",True,100.00000000000004,N/A
"def _getDictWithKey(key, dict_list):
    """"""Returns the first dictionary in dict_list which contains the given key""""""
    for d in dict_list:
<mask>:
            return d
    return None",key in d,23,d['key'] == key,False,7.809849842300637,N/A
"def get_parameters(query, endpoint, query_metadata, auth=None):
    """"""
    ?_name The variable specifies the API mandatory parameter name. The value is incorporated in the query as plain literal.
    ?__name The parameter name is optional.
    ?_name_iri The variable is substituted with the parameter value as a IRI (also: number or literal).
    ?_name_en The parameter value is considered as literal with the language 'en' (e.g., en,it,es, etc.).
    ?_name_integer The parameter value is considered as literal and the XSD datatype 'integer' is added during substitution.
    ?_name_prefix_datatype The parameter value is considered as literal and the datatype 'prefix:datatype' is added during
        substitution. The prefix must be specified according to the SPARQL syntax.
    """"""
    re1 = '\\?'
    re2 = '(?P<required>[_]{1,2})'
    re3 = '(?P<name>[a-zA-Z0-9]+)'
    re4 = '([_](?P<type>(iri)|(number)|(literal)|(integer)))?'
    re5 = '([_](?P<prefix>[a-zA-Z0-9]+)[_](?P<userdefined>[a-zA-Z0-9]+))?'
    re6 = '([_](?P<lang>[a-zA-Z0-9]+))?'
    variable_matcher = re.compile(re1 + re2 + re3 + re4 + re5 + re6)
    parameters = {}
    for match in variable_matcher.finditer(query):
        p = {}
        vname = match.group('name')
        p['original'] = match.group(0)
        p['required'] = len(match.group('required')) == 1
        p['name'] = vname
        mtype = match.group('type')
<mask>:
            p['type'] = mtype
        elif mtype in ['iri']:
            p['type'] = 'string'
            p['format'] = 'iri'
        else:
            p['type'] = 'string'
            if mtype in static.XSD_DATATYPES:
                p['datatype'] = 'xsd:{}'.format(mtype)
            elif match.group('prefix') and match.group('userdefined'):
                p['datatype'] = '{}:{}'.format(match.group('prefix'), match.group('userdefined'))
        vcodes = get_enumeration(query, vname, endpoint, query_metadata, auth)
        if vcodes is not None:
            p['enum'] = sorted(vcodes)
        vdefault = get_defaults(query, vname, query_metadata)
        if vdefault is not None:
            p['default'] = vdefault
        if match.group('lang') is not None:
            p['lang'] = match.group('lang')
        parameters[vname] = p
    glogger.debug('Finished parsing the following parameters: {}'.format(parameters))
    return parameters","mtype in ['number', 'literal', 'string']",244,mtype in static.XSD_TYPES,False,9.870315683072755,N/A
"def get_defaults(rq, v, metadata):
    """"""
    Returns the default value for a parameter or None
    """"""
    glogger.debug('Metadata with defaults: {}'.format(metadata))
<mask>:
        return None
    defaultsDict = _getDictWithKey(v, metadata['defaults'])
    if defaultsDict:
        return defaultsDict[v]
    return None",'defaults' not in metadata,32,'defaults' not in metadata,True,100.00000000000004,N/A
"def get_enumeration(rq, v, endpoint, metadata={}, auth=None):
    """"""
    Returns a list of enumerated values for variable 'v' in query 'rq'
    """"""
<mask>:
        return None
    enumDict = _getDictWithKey(v, metadata['enumerate'])
    if enumDict:
        return enumDict[v]
    if v in metadata['enumerate']:
        return get_enumeration_sparql(rq, v, endpoint, auth)
    return None",'enumerate' not in metadata,42,'enumerate' not in metadata,True,100.00000000000004,N/A
"def getLoader(user, repo, subdir=None, spec_url=None, sha=None, prov=None, git_type=None, branch=None):
    """"""Build a fileLoader (LocalLoader, GithubLoader, URLLoader) for the given parameters.""""""
<mask>:
        loader = LocalLoader()
    elif spec_url:
        loader = URLLoader(spec_url)
    elif git_type == static.TYPE_GITHUB:
        glogger.debug('Building GithubLoader....')
        loader = GithubLoader(user, repo, subdir, sha, prov)
    else:
        glogger.debug('Building GitlabLoader....')
        loader = GitlabLoader(user, repo, subdir, sha, prov, branch)
    return loader",user is None and repo is None and (not spec_url),54,git_type == static.TYPE_LOCAL,False,2.812739937159535,N/A
"def build_swagger_spec(user, repo, subdir, spec_url, sha, serverName, git_type, branch=None):
    """"""Build grlc specification for the given github user / repo in swagger format.""""""
<mask>:
        prov_g = grlcPROV(user, repo)
    else:
        prov_g = None
    swag = swagger.get_blank_spec()
    swag['host'] = serverName
    try:
        loader = getLoader(user, repo, subdir, spec_url, sha, prov_g, git_type, branch)
    except Exception as e:
        swag['info'] = {'title': 'ERROR!', 'description': str(e)}
        swag['paths'] = {}
        return swag
    prev_commit, next_commit, info, basePath = swagger.get_repo_info(loader, sha, prov_g)
    swag['prev_commit'] = prev_commit
    swag['next_commit'] = next_commit
    swag['info'] = info
    swag['basePath'] = basePath
    spec, warnings = swagger.build_spec(user, repo, subdir, spec_url, sha, prov_g, [], git_type, branch)
    for item in spec:
        swag['paths'][item['call_name']] = swagger.get_path_for_item(item)
    if 'description' not in swag['info'] or swag['info']['description'] is None:
        swag['info']['description'] = ''
    for warn in warnings:
        swag['info']['description'] += swagger.get_warning_div(warn)
    if prov_g:
        prov_g.end_prov_graph()
        swag['prov'] = prov_g.serialize(format='turtle')
    return swag",user and repo,129,git_type == 'github.com',False,0.0,N/A
"def dispatch_query(user, repo, query_name, subdir=None, spec_url=None, sha=None, content=None, requestArgs={}, acceptHeader='application/json', requestUrl='http://', formData={}, method='POST', git_type=None, branch=None):
    """"""Executes the specified SPARQL or TPF query.""""""
    loader = getLoader(user, repo, subdir, spec_url, sha=sha, prov=None, git_type=git_type, branch=branch)
    query, q_type = loader.getTextForName(query_name)
<mask>:
        resp, status, headers = dispatchSPARQLQuery(query, loader, requestArgs, acceptHeader, content, formData, requestUrl, method)
        if acceptHeader == 'application/json':
            pass
        return (resp, status, headers)
    elif q_type == qType['TPF']:
        resp, status, headers = dispatchTPFQuery(query, loader, acceptHeader, content)
        return (resp, status, headers)
    else:
        return (""Couldn't find a SPARQL, RDF dump, or TPF query with the requested name"", 404, {})",q_type == qType['SPARQL'] or q_type == qType['JSON'],91,q_type == qType['SPARQL'],False,32.91929878079057,N/A
"def _dispatchQueryDump(raw_sparql_query, endpoint, mime_type, rewritten_query, acceptHeader, content):
    glogger.debug('Detected {} MIME type, proceeding with locally loading remote dump'.format(mime_type))
    g = Graph()
    try:
        g.parse(endpoint, format=mime_type)
        glogger.debug('Local RDF graph loaded successfully with {} triples'.format(len(g)))
    except Exception as e:
        glogger.error(e)
    results = g.query(rewritten_query, result='sparql')
<mask>:
        resp = results.serialize(format='json')
        code = 200
        glogger.debug('Results of SPARQL query against locally loaded dump: {}'.format(resp))
    elif 'text/csv' in acceptHeader or (content and 'text/csv' in static.mimetypes[content]):
        resp = results.serialize(format='csv')
        code = 200
        glogger.debug('Results of SPARQL query against locally loaded dump: {}'.format(resp))
    else:
        resp = 'Unacceptable requested format'
        code = 415
        headers = {}
    glogger.debug('Finished processing query against RDF dump, end of use case')
    del g
    return (resp, code, headers)",'application/json' in acceptHeader or (content and 'application/json' in static.mimetypes[content]),109,'json' in acceptHeader or (content and 'text/json' in static.mimetypes[content]),False,69.6015973294402,N/A
"def _dispatchQueryInsert(method, rewritten_query, formData, acceptHeader, endpoint, auth, headers):
    glogger.debug('Processing INSERT query')
<mask>:
        glogger.debug('INSERT queries must use POST method')
        return ({'error': 'INSERT queries must use POST method'}, 400, headers)
    rewritten_query = rewritten_query.replace('?_g_iri', '{}'.format(formData.get('g')))
    rewritten_query = rewritten_query.replace('<s> <p> <o>', formData.get('data'))
    glogger.debug('INSERT query rewritten as {}'.format(rewritten_query))
    reqHeaders = {'Accept': acceptHeader, 'Content-Type': 'application/sparql-update'}
    response = requests.post(endpoint, data=rewritten_query, headers=reqHeaders, auth=auth)
    glogger.debug('Response header from endpoint: ' + response.headers['Content-Type'])
    resp = response.text
    code = 200
    headers['Content-Type'] = response.headers['Content-Type']
    return (resp, code, headers)",method != 'POST',75,method != 'POST',True,100.00000000000004,N/A
"def guessQueryType(queryUrl):
    queryUrl = queryUrl.lower()
<mask>:
        return qType['SPARQL']
    elif queryUrl.endswith('.sparql'):
        return qType['SPARQL']
    elif queryUrl.endswith('.tpf'):
        return qType['TPF']
    elif queryUrl.endswith('.json'):
        return qType['JSON']
    else:
        raise Exception('Unknown query type: ' + queryUrl)",queryUrl.endswith('.rq'),28,queryUrl.endswith('.sparql'),False,70.71067811865478,N/A
"def buildPaginationHeader(resultCount, resultsPerPage, pageArg, url):
    """"""Build link header for result pagination""""""
    lastPage = resultCount / resultsPerPage
    url_parts = urlparse(url)
    query = dict(parse_qsl(url_parts.query))
    first_url = _buildNewUrlWithPage(url_parts, query, page=1)
    last_url = _buildNewUrlWithPage(url_parts, query, page=lastPage)
<mask>:
        next_url = _buildNewUrlWithPage(url_parts, query, page=1)
        prev_url = ''
        headerLink = '<{}>; rel=next, <{}>; rel=last'.format(next_url, last_url)
    else:
        page = int(pageArg)
        next_url = _buildNewUrlWithPage(url_parts, query, page + 1)
        prev_url = _buildNewUrlWithPage(url_parts, query, page - 1)
        if page == lastPage:
            headerLink = '<{}>; rel=prev, <{}>; rel=first'.format(prev_url, first_url)
        else:
            headerLink = '<{}>; rel=next, <{}>; rel=prev, <{}>; rel=first, <{}>; rel=last'.format(next_url, prev_url, first_url, last_url)
    return headerLink",not pageArg,93,pageArg == 0,False,15.97357760615681,N/A
"def getGrlcLogger(name):
    """"""Construct a logger for grlc with the logging level specified on `config.ini`.""""""
    glogger = logging.getLogger(name)
<mask>:
        glogger.setLevel(logging.DEBUG)
    return glogger",static.LOG_DEBUG_MODE,21,settings.DEBUG,False,9.138402379955025,N/A
"def build_cython():
    """"""Compile the pyx files if we have them.

    The git repository has the .pyx files but not the .c files, and
    the source distributions that are uploaded to PyPI have the .c
    files and not the .pyx files. (The reason for the latter is that
    some versions of pip discovers the .pyx files and implicitly adds
    a dependency on Cython.) Therefore, if we have the .pyx files,
    compile them.

    """"""
    stems = ['_javabridge', '_javabridge_mac', '_javabridge_nomac']
    pyx_filenames = [in_cwd(s + '.pyx') for s in stems]
    c_filenames = [in_cwd(s + '.c') for s in stems]
    nc_pyx_filenames = [pyx for pyx, c in zip(pyx_filenames, c_filenames) if os.path.exists(pyx) and needs_compilation(c, pyx)]
<mask>:
        cython_cmd = [sys.executable, '-m', 'cython', '-3']
        cmd = cython_cmd + nc_pyx_filenames
        env = dict(os.environ)
        env['PYTHONPATH'] = os.pathsep.join(sys.path)
        try:
            subprocess.check_call(cmd, env=env)
        except FileNotFoundError:
            raise RuntimeError('Failed to find Cython: {}'.format(cython_cmd))",len(nc_pyx_filenames) > 0,138,nc_pyx_filenames,False,36.78794411714425,N/A
"def get_jvm_include_dirs():
    """"""Return a sequence of paths to include directories for JVM defs""""""
    jdk_home = find_jdk()
    java_home = find_javahome()
    include_dirs = []
<mask>:
        if jdk_home is not None:
            jdk_include = os.path.join(jdk_home, 'include')
            jdk_include_plat = os.path.join(jdk_include, sys.platform)
            include_dirs += [jdk_include, jdk_include_plat]
    elif is_mac:
        where_jni_h_is_post_6 = os.path.join(java_home, 'include')
        if os.path.isfile(os.path.join(where_jni_h_is_post_6, 'jni.h')):
            include_dirs += [where_jni_h_is_post_6, os.path.join(java_home, 'include', 'darwin')]
        else:
            include_dirs += ['/System/Library/Frameworks/JavaVM.Framework/Headers']
    elif is_linux:
        include_dirs += [os.path.join(jdk_home, 'include'), os.path.join(jdk_home, 'include', 'linux'), os.path.join(jdk_home, 'default-java', 'include'), os.path.join(jdk_home, 'default-java', 'include', 'linux')]
    return include_dirs",is_win,77,is_windows,False,55.03212081491043,N/A
"def ext_modules():
    extensions = []
    extra_link_args = None
    java_home = find_javahome()
<mask>:
        raise Exception('JVM not found')
    jdk_home = find_jdk()
    include_dirs = get_jvm_include_dirs()
    libraries = None
    library_dirs = None
    javabridge_sources = ['_javabridge.c']
    _, jvm_so = find_jre_bin_jdk_so()
    if is_mac:
        javabridge_sources += ['_javabridge_mac.c']
        extra_link_args = ['-framework', 'CoreFoundation']
    else:
        javabridge_sources += ['_javabridge_nomac.c']
    if is_win:
        jdk_lib = os.path.join(jdk_home, 'lib')
        if is_mingw:
            cmd = ['gendef', os.path.join(jdk_home, 'jre\\bin\\server\\jvm.dll')]
            p = subprocess.Popen(cmd)
            p.communicate()
            cmd = ['dlltool', '--dllname', jvm_so, '--output-lib', 'libjvm.a', '--input-def', 'jvm.def', '--kill-at']
            p = subprocess.Popen(cmd)
            p.communicate()
            library_dirs = [os.path.abspath('.'), jdk_lib]
        else:
            extra_link_args = ['/MANIFEST']
            library_dirs = [jdk_lib]
            javabridge_sources.append('strtoull.c')
        libraries = ['jvm']
    elif is_mac:
        javabridge_sources += ['mac_javabridge_utils.c']
    elif is_linux:
        library_dirs = [os.path.dirname(jvm_so)]
        libraries = ['jvm']
    extension_kwargs = dict(name='javabridge._javabridge', sources=javabridge_sources, libraries=libraries, library_dirs=library_dirs, include_dirs=include_dirs, extra_link_args=extra_link_args)
    if not is_win:
        extension_kwargs['runtime_library_dirs'] = library_dirs
    extensions += [Extension(**extension_kwargs)]
    return extensions",java_home is None,126,not java_home,False,46.30777161991026,N/A
"def needs_compilation(target, *sources):
    try:
        target_date = os.path.getmtime(target)
    except OSError as e:
<mask>:
            raise
        return True
    for source in sources:
        source_date = os.path.getmtime(source)
        if source_date > target_date:
            return True
    return False",e.errno != errno.ENOENT,30,e.errno != errno.ENOENT,True,100.00000000000004,N/A
"def initialize_options(self):
    from numpy import get_include
    _build_ext.initialize_options(self)
<mask>:
        self.include_dirs = get_include()
    else:
        self.include_dirs += get_include()",self.include_dirs is None,15,self.include_dirs is None,True,100.00000000000004,N/A
"def get_version():
    version_file = os.path.join(os.path.dirname(__file__), '..', 'javabridge', '_version.py')
<mask>:
        with open(version_file) as f:
            cached_version_line = f.read().strip()
        try:
            import re
            cached_version = re.search('^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]', cached_version_line, re.M).group(1)
        except:
            raise RuntimeError('Unable to find version in %s' % version_file)
        split_version = cached_version.split('.')
        return ('.'.join(split_version[:2]), cached_version)
    else:
        return ('0.0', '0.0.0')",os.path.exists(version_file),46,os.path.isfile(version_file),False,65.80370064762461,N/A
"def bootstrap(tmpdir=None):
    import pip
    packages = ['pip']
<mask>:
        args = [x for x in sys.argv[1:] if x != '--no-setuptools']
    else:
        args = sys.argv[1:]
        try:
            import setuptools
        except ImportError:
            packages += ['setuptools']
    delete_tmpdir = False
    try:
        if tmpdir is None:
            tmpdir = tempfile.mkdtemp()
            delete_tmpdir = True
        cert_path = os.path.join(tmpdir, 'cacert.pem')
        with open(cert_path, 'wb') as cert:
            cert.write(pkgutil.get_data('pip._vendor.requests', 'cacert.pem'))
        os.environ.setdefault('PIP_CERT', cert_path)
        sys.exit(pip.main(['install', '--upgrade'] + packages + args))
    finally:
        if delete_tmpdir and tmpdir:
            shutil.rmtree(tmpdir, ignore_errors=True)",'--no-setuptools' in sys.argv or os.environ.get('PIP_NO_SETUPTOOLS'),71,sys.platform == 'win32',False,2.1969512149331583,N/A
"def main():
    tmpdir = None
    try:
        tmpdir = tempfile.mkdtemp()
        pip_zip = os.path.join(tmpdir, 'pip.zip')
        with open(pip_zip, 'wb') as fp:
            fp.write(base64.decodestring(ZIPFILE))
        sys.path = [pip_zip] + sys.path
        bootstrap(tmpdir=tmpdir)
    finally:
<mask>:
            shutil.rmtree(tmpdir, ignore_errors=True)",tmpdir,29,tmpdir,True,100.00000000000004,N/A
"def __init__(self, throwable):
    """"""Initialize by calling exception_occurred""""""
    env = get_env()
    env.exception_describe()
    self.throwable = throwable
    try:
<mask>:
            raise ValueError('Tried to create a JavaException but there was no current exception')
        klass = env.get_object_class(self.throwable)
        method_id = env.get_method_id(klass, 'getMessage', '()Ljava/lang/String;')
        if method_id is not None:
            message = env.call_method(self.throwable, method_id)
            if message is not None:
                message = env.get_string_utf(message)
                super(JavaException, self).__init__(message)
    finally:
        env.exception_clear()",self.throwable is None,57,self.throwable is None,True,100.00000000000004,N/A
"def _find_jvm_windows():
    java_home = find_javahome()
    jvm_dir = None
<mask>:
        found_jvm = False
        for jre_home in (java_home, os.path.join(java_home, 'jre')):
            jre_bin = os.path.join(jre_home, 'bin')
            for place_to_look in ('client', 'server'):
                jvm_dir = os.path.join(jre_bin, place_to_look)
                if os.path.isfile(os.path.join(jvm_dir, 'jvm.dll')):
                    new_path = ';'.join((os.environ['PATH'], jvm_dir, jre_bin))
                    if isinstance(os.environ['PATH'], str) and isinstance(new_path, unicode) and (sys.version_info < (3, 0, 0)):
                        new_path = new_path.encode('utf-8')
                    os.environ['PATH'] = new_path
                    found_jvm = True
                    break
            if found_jvm:
                break
        if not found_jvm:
            jvm_dir = None
    return jvm_dir",java_home is not None,72,java_home,False,36.78794411714425,N/A
"def _find_mac_lib(library):
    jvm_dir = find_javahome()
    for extension in ('.dylib', '.jnilib'):
        try:
            cmd = ['find', os.path.dirname(jvm_dir), '-name', library + extension]
            result = subprocess.check_output(cmd)
<mask>:
                lines = result.decode('utf-8').split('\n')
            else:
                lines = result.split('\n')
            if len(lines) > 0 and len(lines[0]) > 0:
                library_path = lines[0].strip()
                return library_path
        except Exception as e:
            logger.error('Failed to execute ""%s"" when searching for %s' % (cmd, library), exc_info=1)
    logger.error('Failed to find %s (jvmdir: %s' % (library, jvm_dir))
    return",type(result) == bytes,69,six.PY3,False,0.0,N/A
"def _find_jvm():
    jvm_dir = None
<mask>:
        jvm_dir = _find_jvm_windows()
        if jvm_dir is None:
            raise JVMNotFoundError()
    elif sys.platform == 'darwin':
        jvm_dir = _find_jvm_mac()
    return jvm_dir",sys.platform.startswith('win'),24,sys.platform == 'win32',False,21.64910073203448,N/A
"def __init__(self, fn):
    self.fn = fn
    stack = inspect.stack()
    for f, filename, lineno, module_name, code, index in stack:
<mask>:
            f.f_locals['X' + uuid.uuid4().hex] = self
            break",module_name == '<module>' and f.f_locals.get('__name__') == '__main__',25,'X' not in f.f_locals,False,1.5611236549089973,N/A
"def is_mingw():
<mask>:
        return False
    if os.getenv('DISTUTILS_USE_SDK') != None or os.getenv('MSSdk') != None:
        return False
    mingw32 = ''
    mingw64 = ''
    if os.getenv('MINGW32_PREFIX'):
        mingw32 = os.getenv('MINGW32_PREFIX')
    if os.getenv('MINGW64_PREFIX'):
        mingw64 = os.getenv('MINGW64_PREFIX')
    test = 'gcc --version > NUL 2>&1'
    if os.system(test) == 0 or os.system(mingw32 + test) == 0 or os.system(mingw64 + test) == 0:
        return True
    return False",os.name != 'nt',58,os.getenv('DISTUTILS_USE_NO_MINGW') != None,False,7.432998184513635,N/A
"def find_jdk():
    """"""Find the JDK under Windows""""""
<mask>:
        return os.environ['JDK_HOME']
    if is_linux:
        jdk_home = find_javahome()
        if jdk_home.endswith('jre') or jdk_home.endswith('jre/'):
            jdk_home = jdk_home[:jdk_home.rfind('jre')]
        return jdk_home
    if is_mac:
        return find_javahome()
    if is_win:
        jdk_key_paths = ('SOFTWARE\\JavaSoft\\JDK', 'SOFTWARE\\JavaSoft\\Java Development Kit')
        for jdk_key_path in jdk_key_paths:
            try:
                kjdk = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, jdk_key_path)
                kjdk_values = dict([winreg.EnumValue(kjdk, i)[:2] for i in range(winreg.QueryInfoKey(kjdk)[1])])
                current_version = kjdk_values['CurrentVersion']
                kjdk_current = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, jdk_key_path + '\\' + current_version)
                kjdk_current_values = dict([winreg.EnumValue(kjdk_current, i)[:2] for i in range(winreg.QueryInfoKey(kjdk_current)[1])])
                return kjdk_current_values['JavaHome']
            except WindowsError as e:
                if e.errno == 2:
                    continue
                else:
                    raise
        raise RuntimeError('Failed to find the Java Development Kit. Please download and install the Oracle JDK 1.6 or later')",'JDK_HOME' in os.environ,103,'JDK_HOME' in os.environ,True,100.00000000000004,N/A
"def find_javac_cmd():
    """"""Find the javac executable""""""
<mask>:
        jdk_base = find_jdk()
        javac = os.path.join(jdk_base, 'bin', 'javac.exe')
        if os.path.isfile(javac):
            return javac
        raise RuntimeError('Failed to find javac.exe in its usual location under the JDK (%s)' % javac)
    else:
        return 'javac'",is_win,37,os.path.isdir('javac.exe'),False,0.0,N/A
"def find_jar_cmd():
    """"""Find the javac executable""""""
<mask>:
        jdk_base = find_jdk()
        javac = os.path.join(jdk_base, 'bin', 'jar.exe')
        if os.path.isfile(javac):
            return javac
        raise RuntimeError('Failed to find jar.exe in its usual location under the JDK (%s)' % javac)
    else:
        return 'jar'",is_win,37,os.path.isdir('jdk'),False,0.0,N/A
"def __init__(self, o):
    """"""Initialize the JWrapper with a Java object
        
        :param o: a Java object (class = JB_Object)
        """"""
    STATIC = J.get_static_field('java/lang/reflect/Modifier', 'STATIC', 'I')
    self.o = o
    self.class_wrapper = J.get_class_wrapper(o)
    env = J.get_env()
    jmethods = env.get_object_array_elements(self.class_wrapper.getMethods())
    methods = {}
    for jmethod in jmethods:
<mask>:
            continue
        method = J.get_method_wrapper(jmethod)
        name = method.getName()
        if name not in methods:
            methods[name] = []
            fn = lambda naame=name: lambda *args: self.__call(naame, *args)
            fn = fn()
            fn.__doc__ = J.to_string(jmethod)
            setattr(self, name, fn)
        else:
            fn = getattr(self, name)
            fn.__doc__ = fn.__doc__ + '\n' + J.to_string(jmethod)
        methods[name].append(method)
    jfields = env.get_object_array_elements(self.class_wrapper.getFields(self))
    field_class = env.find_class('java/lang/reflect/Field')
    method_id = env.get_method_id(field_class, 'getName', '()Ljava/lang/String;')
    self.field_names = [env.get_string_utf(env.call_method(o, method_id)) for o in jfields]
    self.methods = methods","J.call(jmethod, 'getModifiers', '()I') & STATIC == STATIC",111,jmethod.getModifiers() != STATIC,False,4.952533158709239,N/A
"def __getattr__(self, name):
<mask>:
        raise AttributeError()
    if not hasattr(self, 'methods') or not hasattr(self, 'field_names'):
        raise AttributeError()
    if name not in self.field_names:
        raise AttributeError()
    try:
        jfield = self.class_wrapper.getField(name)
    except:
        raise AttributeError()
    STATIC = J.get_static_field('java/lang/reflect/Modifier', 'STATIC', 'I')
    if J.call(jfield, 'getModifiers', '()I') & STATIC == STATIC:
        raise AttributeError()
    klass = J.call(jfield, 'getType', '()Ljava/lang/Class;')
    result = J.get_field(self.o, name, sig(klass))
    if isinstance(result, J.JB_Object):
        result = JWrapper(result)
    return result","name in ('o', 'class_wrapper', 'methods', 'field_names')",63,name.startswith('_'),False,3.255629778647324,N/A
"def __setattr__(self, name, value):
<mask>:
        object.__setattr__(self, name, value)
        return
    try:
        jfield = self.class_wrapper.getField(name)
    except:
        object.__setattr__(self, name, value)
        return
    STATIC = J.get_static_field('java/lang/reflect/Modifier', 'STATIC', 'I')
    if J.call(jfield, 'getModifiers', '()I') & STATIC == STATIC:
        raise AttributeError()
    klass = J.call(jfield, 'getType', '()Ljava/lang/Class;')
    result = J.set_field(self.o, name, sig(klass), value)","name in ('o', 'class_wrapper', 'methods', 'field_names') or not hasattr(self, 'methods')",44,name.startswith('_'),False,1.197679263629884,N/A
"def __call(self, method_name, *args):
    """"""Call the appropriate overloaded method with the given name
        
        :param method_name: the name of the method to call
        :param *args: the arguments to the method, which are used to
                      disambiguate between similarly named methods
        """"""
    env = J.get_env()
    last_e = None
    for method in self.methods[method_name]:
        params = env.get_object_array_elements(method.getParameterTypes())
        is_var_args = J.call(method.o, 'isVarArgs', '()Z')
<mask>:
            continue
        if len(args) > len(params) and (not is_var_args):
            continue
        if is_var_args:
            pm1 = len(params) - 1
            args1 = list(args[:pm1]) + [args[pm1:]]
        else:
            args1 = args
        try:
            cargs = [cast(o, klass) for o, klass in zip(args1, params)]
        except:
            last_e = sys.exc_info()[1]
            continue
        rtype = J.call(method.o, 'getReturnType', '()Ljava/lang/Class;')
        args_sig = ''.join(map(sig, params))
        rsig = sig(rtype)
        msig = '(%s)%s' % (args_sig, rsig)
        result = J.call(self.o, method_name, msig, *cargs)
        if isinstance(result, J.JB_Object):
            result = JWrapper(result)
        return result
    raise TypeError('No matching method found for %s' % method_name)",len(args) < len(params) - (1 if is_var_args else 0),140,last_e is not None and (not is_var_args),False,18.967105403203064,N/A
"def __len__(self):
<mask>:
        raise TypeError('%s is not a Collection and does not support __len__' % self)
    return self.size()","not J.is_instance_of(self.o, 'java/util/Collection')",18,"not isinstance(self, collections.Collection)",False,4.131145058582203,N/A
"def begin(self):
    import javabridge
    javabridge.start_vm(self.extra_jvm_args, class_path=self.class_path.split(os.pathsep), run_headless=self.headless, max_heap_size=self.max_heap_size)
<mask>:
        javabridge.activate_awt()",not self.headless,10,self.awt,False,39.43223765116288,N/A
"def configure(self, options, conf):
    import javabridge
    super(JavabridgePlugin, self).configure(options, conf)
    self.class_path = os.pathsep.join(javabridge.JARS)
<mask>:
        self.class_path = os.pathsep.join([options.classpath, self.class_path])
    self.headless = not options.no_headless
    self.max_heap_size = options.max_heap_size",options.classpath,24,options.classpath,True,100.00000000000004,N/A
"def prepareTestRunner(self, testRunner):
    """"""Need to make the test runner call finalize if in Wing
        
        Wing IDE's XML test runner fails to call finalize, so we
        wrap it and add that function here
        """"""
<mask>:
        outer_self = self

        class TestRunnerProxy(object):

            def run(self, test):
                result = testRunner.run(test)
                outer_self.finalize(testRunner.result)
                return result

            @property
            def result(self):
                return testRunner.result
        return TestRunnerProxy()","getattr(testRunner, '__module__', 'unknown') == 'wingtest_common'",55,self.finalize,False,0.0,N/A
"def test_01_05_03_set_static_field(self):
    class_name = 'org/cellprofiler/javabridge/test/RealRect'
    test_cases = (('fs_char', 'C', 'A'), ('fs_byte', 'B', 3), ('fs_short', 'S', 15), ('fs_int', 'I', 392), ('fs_long', 'J', -14), ('fs_float', 'F', 1.03), ('fs_double', 'D', -889.1), ('fs_object', 'Ljava/lang/Object;', javabridge.make_instance('java/lang/Integer', '(I)V', 15)), ('fs_object', 'Ljava/lang/Object;', None))
    for field_name, signature, value in test_cases:
        javabridge.set_static_field(class_name, field_name, signature, value)
        v = javabridge.get_static_field(class_name, field_name, signature)
<mask>:
            self.assertAlmostEqual(v, value)
        elif isinstance(value, javabridge.JB_Object):
            self.assertTrue(javabridge.call(value, 'equals', '(Ljava/lang/Object;)Z', v))
        else:
            self.assertEqual(v, value)","isinstance(value, float)",64,"isinstance(value, javabridge.JB_Integer)",False,27.77619034011791,N/A
"def test_01_06_get_enumeration_wrapper(self):
    properties = javabridge.static_call('java/lang/System', 'getProperties', '()Ljava/util/Properties;')
    keys = javabridge.call(properties, 'keys', '()Ljava/util/Enumeration;')
    enum = javabridge.get_enumeration_wrapper(keys)
    has_java_vm_name = False
    while enum.hasMoreElements():
        key = javabridge.to_string(enum.nextElement())
<mask>:
            has_java_vm_name = True
    self.assertTrue(has_java_vm_name)",key == 'java.vm.name',28,key == 'vm',False,21.874242445215206,N/A
"def test_09_03_set_field(self):
    class_name = 'org/cellprofiler/javabridge/test/RealRect'
    o = javabridge.make_instance(class_name, '()V')
    test_cases = (('f_char', 'C', 'A'), ('f_byte', 'B', 3), ('f_short', 'S', 15), ('f_int', 'I', 392), ('f_long', 'J', -14), ('f_float', 'F', 1.03), ('f_double', 'D', -889.1), ('f_object', 'Ljava/lang/Object;', javabridge.make_instance('java/lang/Integer', '(I)V', 15)), ('f_object', 'Ljava/lang/Object;', None))
    for field_name, signature, value in test_cases:
        javabridge.set_field(o, field_name, signature, value)
        v = javabridge.get_field(o, field_name, signature)
<mask>:
            self.assertAlmostEqual(v, value)
        elif isinstance(value, javabridge.JB_Object):
            self.assertTrue(javabridge.call(value, 'equals', '(Ljava/lang/Object;)Z', v))
        else:
            self.assertEqual(v, value)","isinstance(value, float)",68,"isinstance(value, javabridge.JB_Integer)",False,27.77619034011791,N/A
"def test_13_01_unicode_arg(self):
    s = u'Hola niños'
    s1, s2 = s.split(' ')
<mask>:
        s2 = s2.encode('utf-8')
    env = javabridge.get_env()
    js1 = env.new_string(s1 + ' ')
    result = javabridge.call(js1, 'concat', '(Ljava/lang/String;)Ljava/lang/String;', s2)
    self.assertEqual(s, result)",sys.version_info.major == 2,32,s2,False,0.0,N/A
"def __setitem__(self, name, size):
<mask>:
        raise RuntimeError('H5NetCDF: Write to read only')
    if name in self._objects:
        raise ValueError(f'dimension {name:!r} already exists')
    self._objects[name] = Dimension(self._group, name, size, create_h5ds=True)",not self._group._root._writable,26,self._group is None,False,22.0833582031774,N/A
"def __repr__(self):
<mask>:
        return '<Closed h5netcdf.Dimensions>'
    dims = ', '.join((f'{k}={v!r}' for k, v in self._objects.items()))
    return f'<h5netcdf.Dimensions: {dims}>'",self._group._root._closed,18,self._closed,False,18.762935180380186,N/A
"def __init__(self, parent, name, size=None, create_h5ds=False, phony=False):
    """"""NetCDF4 Dimension constructor.

        Parameters
        ----------
        parent: h5netcdf.Group
            Parent group.
        name: str
            Name of the dimension.
        size : int
            Size of the Netcdf4 Dimension. Defaults to None (unlimited).
        create_h5ds : bool
            For internal use only.
        phony : bool
            For internal use only.
        """"""
    self._parent_ref = weakref.ref(parent)
    self._phony = phony
    self._root_ref = weakref.ref(parent._root)
    self._h5path = _join_h5paths(parent.name, name)
    self._name = name
    self._size = 0 if size is None else size
<mask>:
        self._root._phony_dim_count += 1
    else:
        self._root._max_dim_id += 1
    self._dimensionid = self._root._max_dim_id
    if parent._root._writable and create_h5ds and (not self._phony):
        self._create_scale()
    self._initialized = True",self._phony,96,self._phony,True,100.00000000000004,N/A
"@property
def name(self):
    """"""Return dimension name.""""""
<mask>:
        return self._name
    return self._h5ds.name.split('/')[-1]",self._phony,11,self._name is not None,False,24.446151121745054,N/A
"@property
def size(self):
    """"""Return dimension size.""""""
    size = len(self)
<mask>:
        reflist = self._h5ds.attrs.get('REFERENCE_LIST', None)
        if reflist is not None:
            for ref, axis in reflist:
                var = self._parent._h5group['/'][ref]
                size = max(var.shape[axis], size)
    return size",self.isunlimited(),33,self._parent is not None,False,13.134549472120788,N/A
"def _invalid_netcdf_feature(feature, allow):
<mask>:
        msg = f'{feature} are not a supported NetCDF feature, and are not allowed by h5netcdf unless invalid_netcdf=True.'
        raise CompatibilityError(msg)",not allow,23,feature not in ALLOW_NETCDF_FEATURES,False,5.522397783539471,N/A
"def _expanded_indexer(key, ndim):
    """"""Expand indexing key to tuple with length equal the number of dimensions.""""""
<mask>:
        return key
    key = np.index_exp[key]
    len_key = len(key)
    ellipsis = [i for i, k in enumerate(key) if k is Ellipsis]
    if len(ellipsis) > 1:
        raise IndexError(f""an index can only have a single ellipsis ('...'), {len(ellipsis)} given"")
    else:
        len_key -= len(ellipsis)
        res_dim_cnt = ndim - len_key
        res_dims = res_dim_cnt * (slice(None),)
        ellipsis = ellipsis[0] if ellipsis else None
    if ndim and res_dim_cnt < 0:
        raise IndexError(f'too many indices for array: array is {ndim}-dimensional, but {len_key} were indexed')
    key = tuple([slice(k, k + 1) if isinstance(k, int) else k for k in key])
    k1 = slice(ellipsis)
    k2 = slice(len_key, None) if ellipsis is None else slice(ellipsis + 1, None)
    return key[k1] + res_dims + key[k2]","key is tuple and all((isinstance(k, slice) for k in key))",129,key is None,False,0.26569232734887793,N/A
"def __repr__(self):
<mask>:
        return f'<Closed {self._cls_name!r}>'
    header = f'<class {self._cls_name!r}: name = {self.name!r}, numpy dtype = {self.dtype!r}'
    return header",self._parent._root._closed,19,self.closed,False,6.108851178104657,N/A
"@property
def _h5datatype(self):
    """"""Returns comparable h5type.

        - DatatypeID for h5py
        - (dtype, dtype.metadata) for h5pyd
        """"""
<mask>:
        return self._h5ds.id
    else:
        return (self.dtype, self.dtype.metadata)",self._root._h5py.__name__ == 'h5py',23,self._h5ds,False,2.9603567969095352,N/A
"def _string_to_char_array_dtype(dtype):
    """"""Converts fixed string to char array dtype.""""""
<mask>:
        return None
    return np.dtype({name: (np.dtype(('S1', fmt.itemsize)) if fmt.kind == 'S' else fmt, offset) for name, (fmt, offset) in dtype.fields.items()})",dtype.kind == 'c',29,not dtype,False,6.7667641618306344,N/A
"def _get_default_fillvalue(dtype):
    kind = np.dtype(dtype).kind
    fillvalue = None
<mask>:
        size = np.dtype(dtype).itemsize
        fillvalue = default_fillvals[f'{kind}{size}']
    return fillvalue","kind in ['u', 'i', 'f']",17,kind in default_fillvals,False,9.599621398238423,N/A
"def _check_return_dtype_endianess(endian='native'):
    little_endian = sys.byteorder == 'little'
    endianess = '='
<mask>:
        endianess = little_endian and endianess or '<'
    elif endian == 'big':
        endianess = not little_endian and endianess or '>'
    elif endian == 'native':
        pass
    else:
        raise ValueError(f""'endian' keyword argument must be 'little','big' or 'native', got '{endian}'"")
    return endianess",endian == 'little',49,endian == 'little',True,100.00000000000004,N/A
"def __setattr__(self, name, value):
<mask>:
        self.attrs[name] = value
    else:
        object.__setattr__(self, name, value)",self._initialized and name not in self.__dict__,12,name in self.attrs,False,5.78270080339587,N/A
"def chunking(self):
    """"""Return variable chunking information.

        The chunksize is returned as a sequence with the size for each dimension.
        If the dataset is defined to be contiguous (no chunking) the word 'contiguous'
        is returned.
        """"""
    chunks = self._h5ds.chunks
<mask>:
        return 'contiguous'
    else:
        return chunks",chunks is None,44,self.is_contiguous,False,10.682175159905848,N/A
"@property
def dtype(self):
    """"""Return netCDF4.Variable numpy dtype.""""""
    dt = self._h5ds.dtype
<mask>:
        return str
    return dt",h5py.check_dtype(vlen=dt) is str,15,dt.ndim == 1,False,3.9297526283216277,N/A
"def __getitem__(self, key):
<mask>:
        raise KeyError(key)
    if self._h5py.__name__ == 'h5py':
        attr = self._h5attrs.get_id(key)
    else:
        attr = self._h5attrs[key]
    if isinstance(self._h5attrs[key], self._h5py.Empty):
        string_info = self._h5py.check_string_dtype(self._h5attrs[key].dtype)
        if string_info and string_info.length == 1:
            return b''
        else:
            return np.array([], dtype=attr.dtype)
    output = self._h5attrs[key]
    if self._h5py.__name__ == 'h5py':
        string_info = self._h5py.check_string_dtype(attr.dtype)
        if string_info is not None:
            if string_info.length is not None and string_info.length > 1:
                encoding = string_info.encoding
                if np.isscalar(output):
                    output = output.decode(encoding, 'surrogateescape')
                else:
                    output = [b.decode(encoding, 'surrogateescape') for b in output.flat]
            elif not np.isscalar(output):
                output = output.tolist()
        if not np.isscalar(output) and len(output) == 1:
            return output[0]
    return output",key in _HIDDEN_ATTRS,94,key not in self._h5attrs,False,8.643019616048525,N/A
"def __setitem__(self, key, value):
<mask>:
        raise AttributeError(f'cannot write attribute with reserved name {key!r}')
    if hasattr(value, 'dtype'):
        dtype = value.dtype
    else:
        dtype = np.asarray(value).dtype
    self._check_dtype(dtype)
    self._h5attrs[key] = value",key in _HIDDEN_ATTRS,27,key in self._h5attrs,False,17.965205598154213,N/A
"@pytest.fixture(params=['testfile.nc', 'hdf5://testfile'])
def tmp_local_or_remote_netcdf(request, tmpdir, hsds_up):
<mask>:
        if without_h5pyd:
            pytest.skip('h5pyd package not available')
        elif not hsds_up:
            pytest.skip('HSDS service not running')
        rnd = ''.join((random.choice(string.ascii_uppercase) for _ in range(5)))
        return 'hdf5://' + 'home' + '/' + env['HS_USERNAME'] + '/' + 'testfile' + rnd + '.nc'
    else:
        return str(tmpdir.join(request.param))",request.param.startswith(remote_h5),46,request.param == 'hdf5://testfile',False,15.619699684601283,N/A
"def get_hdf5_module(resource):
    """"""Return the correct h5py module based on the input resource.""""""
<mask>:
        return h5pyd
    else:
        return h5py","isinstance(resource, str) and resource.startswith(remote_h5)",18,resource == 'darwin',False,1.0211566521809647,N/A
"def string_to_char(arr):
    """"""Like nc4.stringtochar, but faster and more flexible.""""""
    arr = np.array(arr, copy=False, order='C')
    kind = arr.dtype.kind
<mask>:
        raise ValueError('argument must be a string')
    return arr.reshape(arr.shape + (1,)).view(kind + '1')","kind not in ['U', 'S']",30,"kind not in ('C', 'C')",False,22.089591134157878,N/A
"def array_equal(a, b):
    a, b = map(np.array, (a[...], b[...]))
<mask>:
        return False
    try:
        return np.allclose(a, b)
    except TypeError:
        return (a == b).all()",a.shape != b.shape,22,len(a) != len(b),False,9.980099403873663,N/A
"def is_h5py_char_working(tmp_netcdf, name):
<mask>:
        h5 = get_hdf5_module(tmp_netcdf)
        with h5.File(tmp_netcdf, 'r') as ds:
            return is_h5py_char_working(ds, name)
    v = tmp_netcdf[name]
    try:
        assert array_equal(v, _char_array)
        return True
    except Exception as e:
        if re.match(""^Can't read data"", e.args[0]):
            return False
        else:
            raise","not isinstance(tmp_netcdf, h5py.File) and (without_h5pyd or not isinstance(tmp_netcdf, h5pyd.File))",37,name in tmp_netcdf,False,0.3272438466621321,N/A
"@pytest.fixture(scope='session')
def hsds_up():
    """"""Provide HDF Highly Scalabale Data Service (HSDS) for h5pyd testing.""""""
<mask>:
        root_dir = Path(tempfile.mkdtemp(prefix='tmp-hsds-root-'))
        bucket_name = 'pytest'
        os.environ['BUCKET_NAME'] = bucket_name
        os.mkdir(f'{root_dir}/{bucket_name}')
        hs_username = 'h5netcdf-pytest'
        hs_password = 'TestEarlyTestEverything'
        kwargs = {}
        kwargs['username'] = hs_username
        kwargs['password'] = hs_password
        kwargs['root_dir'] = str(root_dir)
        kwargs['logfile'] = f'{root_dir}/hsds.log'
        kwargs['log_level'] = 'DEBUG'
        kwargs['host'] = 'localhost'
        kwargs['sn_port'] = 5101
        try:
            hsds = HsdsApp(**kwargs)
            hsds.run()
            is_up = hsds.ready
            if is_up:
                os.environ['HS_ENDPOINT'] = hsds.endpoint
                os.environ['HS_USERNAME'] = hs_username
                os.environ['HS_PASSWORD'] = hs_password
                Folder('/home/', mode='w')
                Folder('/home/h5netcdf-pytest/', mode='w')
        except Exception:
            is_up = False
        yield is_up
        hsds.check_processes()
        hsds.stop()
        rmtree(root_dir, ignore_errors=True)
    else:
        yield False",with_reqd_pkgs,91,os.path.exists(f'{hsds_root_dir}/hsds.log'),False,2.4074859035470344,N/A
"@pytest.mark.parametrize('extension', ['', '.gz', '.bz2', '.xz'])
@pytest.mark.parametrize('pathlike', [False, True])
def test_read_taxrank_path(extension: str, pathlike: bool) -> None:
    """"""
    Test reading the taxrank ontology OBO file from paths. Includes reading
    compressed paths.
    """"""
    path = os.path.join(directory, 'data', 'taxrank.obo' + extension)
<mask>:
        path = pathlib.Path(path)
    taxrank = obonet.read_obo(path)
    assert len(taxrank) == 61",pathlike,48,pathlike,True,100.00000000000004,N/A
"def open_read_file(path: PathType, encoding: str | None=None) -> TextIO:
    """"""
    Return a file object from the path. Automatically detects and supports
    URLs and compression. If path is pathlike, it's converted to a string.
    If path is not a string nor pathlike, it's passed through without
    modification. Use encoding to set the text character set encoding.
    Use `encoding=None` to use the platform-dependent default locale encoding.
    """"""
<mask>:
        path = os.fspath(path)
    if not isinstance(path, str):
        return path
    opener = get_opener(path)
    if re.match('^(http|ftp)s?://', path):
        with urlopen(path) as response:
            content = response.read()
        if opener == io.open:
            if not encoding:
                encoding = response.headers.get_content_charset(failobj='utf-8')
            logging.info(f'Will decode content from {path} using {encoding} charset.')
            text = content.decode(encoding)
            return io.StringIO(text)
        else:
            compressed_bytes = io.BytesIO(content)
            return opener(compressed_bytes, 'rt', encoding=encoding)
    return opener(path, 'rt', encoding=encoding)","hasattr(path, '__fspath__')",123,"isinstance(path, PathType)",False,11.943865131127652,N/A
"def get_opener(filename: str) -> Callable[..., TextIO]:
    """"""
    Automatically detect compression and return the file opening function.
    """"""
    _type, compression = mimetypes.guess_type(filename)
<mask>:
        opener = io.open
    else:
        module = compression_to_module[compression]
        opener = importlib.import_module(module).open
    return opener",compression is None,34,compression == 'io',False,15.97357760615681,N/A
"def read_obo(path_or_file: PathType, ignore_obsolete: bool=True, encoding: str | None='utf-8') -> networkx.MultiDiGraph[str]:
    """"""
    Return a networkx.MultiDiGraph of the ontology serialized by the
    specified path or file.

    This function attempts to follow the specifications provided at:
    http://owlcollab.github.io/oboformat/doc/obo-syntax.html

    Parameters
    ==========
    path_or_file : str or file
        Path, URL, or open file object. If path or URL, compression is
        inferred from the file extension.
    ignore_obsolete : boolean
        When true (default), terms that are marked 'is_obsolete' will
        not be added to the graph.
    encoding : str or None
        The character set encoding to use for path_or_file when path_or_file
        is a path/URL. Set to None for platform-dependent locale default.
    """"""
    with open_read_file(path_or_file, encoding=encoding) as obo_file:
        typedefs, terms, instances, header = get_sections(obo_file)
<mask>:
        header['name'] = header.get('ontology')
    if 'name' not in header:
        logging.warning('name and ontology keys are both missing')
    graph = networkx.MultiDiGraph(typedefs=typedefs, instances=instances, **header)
    edge_tuples = []
    for term in terms:
        is_obsolete = term.get('is_obsolete', 'false') == 'true'
        if ignore_obsolete and is_obsolete:
            continue
        term_id = term.pop('id')
        graph.add_node(term_id, **term)
        for target_term in term.pop('is_a', []):
            edge_tuple = (term_id, 'is_a', target_term)
            edge_tuples.append(edge_tuple)
        for relationship in term.pop('relationship', []):
            typedef, target_term = relationship.split(' ')
            edge_tuple = (term_id, typedef, target_term)
            edge_tuples.append(edge_tuple)
    for term0, typedef, term1 in edge_tuples:
        graph.add_edge(term0, term1, key=typedef)
    return graph",'ontology' in header,196,'ontology' in header,True,100.00000000000004,N/A
"def get_sections(lines: Iterator[str]) -> tuple[list[dict[str, Any]], list[dict[str, Any]], list[dict[str, Any]], dict[str, Any]]:
    """"""
    Separates an obo file into stanzas and process.
    Returns (typedefs, terms, instances, header) tuples
    where `typedefs`, `terms`, and `instances` are lists of
    dictionaries and `header` is a dictionary.
    """"""
    typedefs, terms, instances = ([], [], [])
    header = None
    groups = itertools.groupby(lines, lambda line: line.strip() == '')
    for is_blank, stanza_lines_iter in groups:
<mask>:
            continue
        stanza_type_line, *stanza_lines = stanza_lines_iter
        if stanza_type_line.startswith('[Typedef]'):
            typedef = parse_stanza(stanza_lines, typedef_tag_singularity)
            typedefs.append(typedef)
        elif stanza_type_line.startswith('[Term]'):
            term = parse_stanza(stanza_lines, term_tag_singularity)
            terms.append(term)
        elif stanza_type_line.startswith('[Instance]'):
            instance = parse_stanza(stanza_lines, instance_tag_singularity)
            instances.append(instance)
        else:
            stanza_lines = [stanza_type_line] + stanza_lines
            header = parse_stanza(stanza_lines, header_tag_singularity)
    if header is None:
        logger.warning('got no header information')
        header = {}
    return (typedefs, terms, instances, header)",is_blank,118,is_blank,True,100.00000000000004,N/A
"def parse_tag_line(line: str) -> tuple[str, str | None, str | None, str | None]:
    """"""
    Take a line representing a single tag-value pair and parse
    the line into (tag, value, trailing_modifier, comment).
    """"""
    match = re.match(tag_line_pattern, line)
<mask>:
        message = f'Tag-value pair parsing failed for:\n{line}'
        raise ValueError(message)
    tag = match.group('tag')
    value = match.group('value')
    trailing_modifier = match.group('trailing_modifier')
    if trailing_modifier:
        trailing_modifier = trailing_modifier.strip('{}')
    comment = match.group('comment')
    if comment:
        comment = comment.lstrip('! ')
    return (tag, value, trailing_modifier, comment)",match is None,75,not match,False,30.326532985631665,N/A
"def parse_stanza(lines: list[str], tag_singularity: dict[str, bool]) -> dict[str, Any]:
    """"""
    Returns a dictionary representation of a stanza.
    """"""
    stanza: dict[str, Any] = {}
    for line in lines:
<mask>:
            continue
        tag, value, trailing_modifier, comment = parse_tag_line(line)
        if tag_singularity.get(tag, False):
            stanza[tag] = value
        else:
            stanza.setdefault(tag, []).append(value)
    return stanza",line.startswith('!'),46,not line,False,2.489353418393197,N/A
"def pytest_collection_modifyitems(config, items):
<mask>:
        return
    skip_slow = pytest.mark.skip(reason='Specified --skip-slow')
    for item in items:
        if 'slow' in item.keywords:
            item.add_marker(skip_slow)",not config.getoption('--skip-slow'),18,config.getoption('--skip-slow'),False,84.64817248906144,N/A
"def unquote(s):
    """"""
    >>> unquote('""foo""')
    'foo'
    >>> unquote('""foo""bar')
    '""foo""bar'
    """"""
    for quote in ('""', ""'""):
<mask>:
            return s[1:-1]
    return s",s.startswith(quote) and s.endswith(quote),20,s.startswith(quote),False,31.140322391459787,N/A
"@pytest.fixture
def dummy_global_config():
    XDG_CONFIG_HOME = Path.home() / '.config'
    pdbr_dir = XDG_CONFIG_HOME / 'pdbr'
    pdbr_dir.mkdir(exist_ok=True, parents=True)
    setup_file = pdbr_dir / 'setup.cfg'
    backup_file = pdbr_dir / (setup_file.stem + '.cfg.bak')
<mask>:
        setup_file.rename(backup_file)
    with open(setup_file, 'wt') as f:
        f.writelines(['[pdbr]\n', 'theme = ansi_light'])
    yield setup_file
    setup_file.unlink()
    if backup_file.exists():
        backup_file.rename(setup_file)",setup_file.exists(),44,backup_file.exists(),False,80.91067115702207,N/A
"def __exit__(self, _, exc_value, exc_traceback):
<mask>:
        post_mortem(exc_traceback, exc_value)
        return self.suppress_exc
    return False",exc_traceback and self.debug,12,self.post_mortem,False,15.848738972120703,N/A
"def shell():
    import getopt
    _, args = getopt.getopt(sys.argv[1:], 'mhc:', ['command='])
<mask>:
        run_ipython_shell()
    else:
        from pdbr.__main__ import main
        main()",not args,18,args,False,36.78794411714425,N/A
"def telnet():
    from pdbr.__main__ import RichPdb
    pdb_cls = RichPdb()
<mask>:
        pdb_cls.error('Usage : pdbr_telnet hostname port')
        sys.exit()

    class MyTelnet(Telnet):

        def fill_rawq(self):
            """"""
            exactly the same with Telnet.fill_rawq,
            buffer size is just changed from 50 to 1024.
            """"""
            if self.irawq >= len(self.rawq):
                self.rawq = b''
                self.irawq = 0
            buf = self.sock.recv(1024)
            self.msg('recv %r', buf)
            self.eof = not buf
            self.rawq = self.rawq + buf
    console = pdb_cls.console
    sys.stdout = FileProxy(console, sys.stdout)
    sys.stderr = FileProxy(console, sys.stderr)
    try:
        host = sys.argv[1]
        port = int(sys.argv[2])
        with MyTelnet(host, port) as tn:
            tn.interact()
    except BaseException as e:
        pdb_cls.error(e)
        sys.exit()",len(sys.argv) < 3,91,len(sys.argv) < 3,True,100.00000000000004,N/A
"def set_history_file(history_file):
    """"""
    This is just for Pdb,
    For Ipython, look at RichPdb.pt_init
    """"""
<mask>:
        return
    try:
        readline.read_history_file(history_file)
        readline.set_history_length(1000)
    except FileNotFoundError:
        pass
    except OSError:
        pass
    atexit.register(readline.write_history_file, history_file)",readline is None,27,not history_file,False,0.0,N/A
"def read_config():
    style = None
    theme = None
    store_history = '.pdbr_history'
    context = None
    config = configparser.ConfigParser()
    config.sections()
    setup_filename = 'setup.cfg'
    xdg_config_home = Path(os.getenv('XDG_CONFIG_HOME', Path.home() / '.config'))
    global_config_path = xdg_config_home / 'pdbr' / setup_filename
    cwd_config_path = Path.cwd() / setup_filename
    config_path = cwd_config_path.exists() and cwd_config_path or global_config_path
    config.read(config_path)
<mask>:
        if 'style' in config['pdbr']:
            style = config['pdbr']['style']
        if 'theme' in config['pdbr']:
            theme = config['pdbr']['theme']
        if 'use_traceback' in config['pdbr']:
            if config['pdbr']['use_traceback'].lower() == 'true':
                set_traceback(theme)
        else:
            set_traceback(theme)
        if 'store_history' in config['pdbr']:
            store_history = config['pdbr']['store_history']
        if 'context' in config['pdbr']:
            context = config['pdbr']['context']
    history_file = str(Path.home() / store_history)
    set_history_file(history_file)
    ipython_history_file = f'{history_file}_ipython'
    return (style, theme, history_file, ipython_history_file, context)",'pdbr' in config,102,config['pdbr'].exists(),False,6.567274736060395,N/A
"def debugger_cls(klass=None, console=None, context=None, is_celery=False, show_layouts=True):
<mask>:
        try:
            from IPython.terminal.debugger import TerminalPdb
            klass = TerminalPdb
        except ImportError:
            from pdb import Pdb
            klass = Pdb
    style, theme, history_file, ipython_history_file, context = read_config()
    RichPdb = rich_pdb_klass(klass, console=console, context=context, is_celery=is_celery, show_layouts=show_layouts)
    RichPdb._style = style
    RichPdb._theme = theme
    RichPdb._history_file = history_file
    RichPdb._ipython_history_file = ipython_history_file
    return RichPdb",klass is None,52,klass is None,True,100.00000000000004,N/A
"def _pdbr_cls(console=None, context=None, return_instance=True, show_layouts=True):
    klass = debugger_cls(console=console, context=context, show_layouts=show_layouts)
<mask>:
        return klass()
    return klass",return_instance,15,return_instance,True,100.00000000000004,N/A
"def _rdbr_cls(return_instance=True):
    try:
        from celery.contrib import rdb
        rdb.BANNER = '{self.ident}: Type `pdbr_telnet {self.host} {self.port}` to connect\n\n{self.ident}: Waiting for client...\n'
    except ModuleNotFoundError as error:
        raise type(error)('In order to install celery, use pdbr[celery]') from error
    klass = debugger_cls(klass=rdb.Rdb, is_celery=True, show_layouts=False)
<mask>:
        return klass()
    return klass",return_instance,43,return_instance,True,100.00000000000004,N/A
"def __call__(cls, *args, **kwargs):
<mask>:
        instance = super().__call__(*args, **kwargs)
        cls._instances[cls] = instance
    return cls._instances[cls]",cls not in cls._instances,14,cls not in cls._instances,True,100.00000000000004,N/A
"def run_ipython_shell():
    try:
        from IPython.terminal.interactiveshell import TerminalInteractiveShell
        from IPython.terminal.ipapp import TerminalIPythonApp
        from prompt_toolkit.history import FileHistory
        from traitlets import Type
        TerminalInteractiveShell.simple_prompt = False
    except ModuleNotFoundError as error:
        raise type(error)('In order to use pdbr shell, install IPython with pdbr[ipython]') from error

    class PdbrTerminalInteractiveShell(TerminalInteractiveShell):

        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)
<mask>:
                self.debugger_history = FileHistory(RichPdb._ipython_history_file)

        @property
        def debugger_cls(self):
            return RichPdb

    class PdbrTerminalIPythonApp(TerminalIPythonApp):
        interactive_shell_class = Type(klass=object, default_value=PdbrTerminalInteractiveShell, help='Class to use to instantiate the TerminalInteractiveShell object. Useful for custom Frontends').tag(config=True)
    app = PdbrTerminalIPythonApp.instance()
    app.initialize()
    sys.exit(app.start())",RichPdb._ipython_history_file,79,RichPdb._ipython_history_file,True,100.00000000000004,N/A
"def set_trace(*, console=None, header=None, context=None, show_layouts=False):
    pdb_cls = _pdbr_cls(console=console, context=context, show_layouts=show_layouts)
<mask>:
        pdb_cls.message(header)
    pdb_cls.set_trace(sys._getframe().f_back)",header is not None,14,header,False,4.9787068367863965,N/A
"def post_mortem(traceback=None, value=None):
    _, sys_value, sys_traceback = sys.exc_info()
    value = value or sys_value
    traceback = traceback or sys_traceback
<mask>:
        raise ValueError('A valid traceback must be passed if no exception is being handled')
    p = RichPdb()
    p.reset()
    if value:
        p.error(value)
    p.interaction(None, traceback)",traceback is None,41,not traceback,False,30.326532985631665,N/A
"def celery_set_trace(frame=None):
    pdb_cls = _rdbr_cls()
<mask>:
        frame = sys._getframe().f_back
    return pdb_cls.set_trace(frame)",frame is None,11,frame is None,True,100.00000000000004,N/A
"def grafana_switch_organisation(self, headers):
    try:
        r = open_url('%s/api/user/using/%s' % (self.grafana_url, self.grafana_org_id), headers=headers, method='POST', validate_certs=self.validate_certs, ca_path=self.ca_path)
    except HTTPError as e:
        raise GrafanaAPIException('Unable to switch to organization %s : %s' % (self.grafana_org_id, to_native(e)))
    except SSLValidationError as e:
        raise GrafanaAPIException(""Unable to validate server's certificate with %s: %s"" % (self.ca_path, to_native(e)))
<mask>:
        raise GrafanaAPIException('Unable to switch to organization %s : %s' % (self.grafana_org_id, str(r.getcode())))",r.getcode() != 200,59,r.getcode() != 200,True,100.00000000000004,N/A
"def grafana_headers(self):
    headers = {'content-type': 'application/json; charset=utf8'}
<mask>:
        headers['Authorization'] = 'Bearer %s' % self.grafana_api_key
    else:
        headers['Authorization'] = basic_auth_header(self.grafana_user, self.grafana_password)
        self.grafana_switch_organisation(headers)
    return headers",self.grafana_api_key,22,self.grafana_api_key,True,100.00000000000004,N/A
"def grafana_list_dashboards(self):
    headers = self.grafana_headers()
    dashboard_list = []
    try:
<mask>:
            r = open_url('%s/api/search?query=%s' % (self.grafana_url, self.search), headers=headers, method='GET', validate_certs=self.validate_certs, ca_path=self.ca_path)
        else:
            r = open_url('%s/api/search/' % self.grafana_url, headers=headers, method='GET', validate_certs=self.validate_certs, ca_path=self.ca_path)
    except HTTPError as e:
        raise GrafanaAPIException('Unable to search dashboards : %s' % to_native(e))
    except SSLValidationError as e:
        raise GrafanaAPIException(""Unable to validate server's certificate with %s: %s"" % (self.ca_path, to_native(e)))
    if r.getcode() == 200:
        try:
            dashboard_list = json.loads(r.read())
        except Exception as e:
            raise GrafanaAPIException('Unable to parse json list %s' % to_native(e))
    else:
        raise GrafanaAPIException('Unable to list grafana dashboards : %s' % str(r.getcode()))
    return dashboard_list",self.search,93,self.search,True,100.00000000000004,N/A
"def grafana_argument_spec():
    argument_spec = url_argument_spec()
    del argument_spec['force']
    del argument_spec['force_basic_auth']
    del argument_spec['http_agent']
<mask>:
        del argument_spec['use_gssapi']
    argument_spec.update(state=dict(choices=['present', 'absent'], default='present'), url=dict(aliases=['grafana_url'], type='str', required=True), grafana_api_key=dict(type='str', no_log=True), url_username=dict(aliases=['grafana_user'], default='admin'), url_password=dict(aliases=['grafana_password'], default='admin', no_log=True))
    return argument_spec",'use_gssapi' in argument_spec,29,'use_gssapi' in argument_spec,True,100.00000000000004,N/A
"def set_options(self, task_keys=None, var_options=None, direct=None):
    super(CallbackModule, self).set_options(task_keys=task_keys, var_options=var_options, direct=direct)
    self.grafana_api_key = self.get_option('grafana_api_key')
    self.grafana_url = self.get_option('grafana_url')
    self.validate_grafana_certs = self.get_option('validate_certs')
    self.http_agent = self.get_option('http_agent')
    self.grafana_user = self.get_option('grafana_user')
    self.grafana_password = self.get_option('grafana_password')
    self.dashboard_id = self.get_option('grafana_dashboard_id')
    self.panel_ids = self.get_option('grafana_panel_ids')
<mask>:
        self.headers['Authorization'] = 'Bearer %s' % self.grafana_api_key
    else:
        self.force_basic_auth = True
    if self.grafana_url is None:
        self.disabled = True
        self._display.warning('Grafana URL was not provided. The Grafana URL can be provided using the `GRAFANA_URL` environment variable.')
    self._display.debug('Grafana URL: %s' % self.grafana_url)",self.grafana_api_key,72,self.grafana_api_key,True,100.00000000000004,N/A
"def v2_playbook_on_stats(self, stats):
    end_time = datetime.now()
    duration = end_time - self.start_time
    summarize_stat = {}
    for host in stats.processed.keys():
        summarize_stat[host] = stats.summarize(host)
    status = 'FAILED'
<mask>:
        status = 'OK'
    text = PLAYBOOK_STATS_TXT.format(playbook=self.playbook, hostname=self.hostname, duration=duration.total_seconds(), status=status, username=self.username, summary=json.dumps(summarize_stat))
    data = {'time': to_millis(self.start_time), 'timeEnd': to_millis(end_time), 'isRegion': True, 'text': text, 'tags': ['ansible', 'ansible_report', self.playbook, self.hostname]}
    self._send_annotations(data)",self.errors == 0,52,not summarize_stat,False,0.0,N/A
"def v2_runner_on_failed(self, result, ignore_errors=False, **kwargs):
    text = PLAYBOOK_ERROR_TXT.format(playbook=self.playbook, hostname=self.hostname, username=self.username, task=result._task, host=result._host.name, result=self._dump_results(result._result))
<mask>:
        return
    data = {'time': to_millis(datetime.now()), 'text': text, 'tags': ['ansible', 'ansible_event_failure', self.playbook, self.hostname]}
    self.errors += 1
    self._send_annotations(data)",ignore_errors,30,ignore_errors,True,100.00000000000004,N/A
"def _send_annotations(self, data):
<mask>:
        data['dashboardId'] = int(self.dashboard_id)
    if self.panel_ids:
        for panel_id in self.panel_ids:
            data['panelId'] = int(panel_id)
            self._send_annotation(data)
    else:
        self._send_annotation(data)",self.dashboard_id,19,self.dashboard_id,True,100.00000000000004,N/A
"def grafana_organization_id_by_name(module, grafana_url, org_name, headers):
    r, info = fetch_url(module, '%s/api/user/orgs' % grafana_url, headers=headers, method='GET')
<mask>:
        raise GrafanaAPIException('Unable to retrieve users organizations: %s' % info)
    organizations = json.loads(to_text(r.read()))
    for org in organizations:
        if org['name'] == org_name:
            return org['orgId']
    raise GrafanaAPIException(""Current user isn't member of organization: %s"" % org_name)",info['status'] != 200,47,r.status_code != 200,False,20.556680845025987,N/A
"def grafana_switch_organization(module, grafana_url, org_id, headers):
    r, info = fetch_url(module, '%s/api/user/using/%s' % (grafana_url, org_id), headers=headers, method='POST')
<mask>:
        raise GrafanaAPIException('Unable to switch to organization %s : %s' % (org_id, info))",info['status'] != 200,28,r.status_code != 200,False,20.556680845025987,N/A
"def grafana_headers(module, data):
    headers = {'content-type': 'application/json; charset=utf8'}
<mask>:
        headers['Authorization'] = 'Bearer %s' % data['grafana_api_key']
    else:
        module.params['force_basic_auth'] = True
        if module.params['org_name']:
            org_name = module.params['org_name']
            data['org_id'] = grafana_organization_id_by_name(module, data['url'], org_name, headers)
        grafana_switch_organization(module, data['url'], data['org_id'], headers)
    return headers",'grafana_api_key' in data and data['grafana_api_key'],36,data['grafana_api_key'],False,36.78794411714425,N/A
"def get_grafana_version(module, grafana_url, headers):
    grafana_version = None
    r, info = fetch_url(module, '%s/api/frontend/settings' % grafana_url, headers=headers, method='GET')
<mask>:
        try:
            settings = json.loads(to_text(r.read()))
            grafana_version = settings['buildInfo']['version'].split('.')[0]
        except UnicodeError:
            raise GrafanaAPIException('Unable to decode version string to Unicode')
        except Exception as e:
            raise GrafanaAPIException(e)
    else:
        raise GrafanaAPIException('Unable to get grafana version: %s' % info)
    return int(grafana_version)",info['status'] == 200,52,r.status_code == 200,False,20.556680845025987,N/A
"def grafana_folder_exists(module, grafana_url, folder_name, headers):
<mask>:
        return (True, 0)
    try:
        r, info = fetch_url(module, '%s/api/folders' % grafana_url, headers=headers, method='GET')
        if info['status'] != 200:
            raise GrafanaAPIException('Unable to query Grafana API for folders (name: %s): %d' % (folder_name, info['status']))
        folders = json.loads(r.read())
        for folder in folders:
            if folder['title'] == folder_name:
                return (True, folder['id'])
    except Exception as e:
        raise GrafanaAPIException(e)
    return (False, 0)",folder_name == 'General',60,folder_name == '',False,75.98356856515926,N/A
"def compare_datasources(new, current, compareSecureData=True):
<mask>:
        new.pop('uid', None)
        current.pop('uid', None)
    for field in ['apiVersion', 'basicAuthPassword', 'id', 'password', 'readOnly', 'typeLogoUrl', 'version']:
        current.pop(field, None)
    if not current.get('basicAuth', True):
        current.pop('basicAuthUser', None)
    if current.get('type') == 'grafana-postgresql-datasource' and new.get('type') == 'postgres':
        new.pop('type', None)
        current.pop('type', None)
    if not compareSecureData:
        new.pop('secureJsonData', None)
        new.pop('secureJsonFields', None)
        current.pop('secureJsonData', None)
        current.pop('secureJsonFields', None)
    elif not new.get('secureJsonData'):
        new.pop('secureJsonData', None)
        current.pop('secureJsonFields', None)
    else:
        current['secureJsonData'] = current.pop('secureJsonFields')
    return dict(before=current, after=new)",new.get('uid') is None,64,not compareUID,False,0.0,N/A
"def get_datasource_payload(data, org_id=None):
    payload = {'orgId': data['org_id'] if org_id is None else org_id, 'name': data['name'], 'uid': data['uid'], 'type': data['ds_type'], 'access': data['access'], 'url': data['ds_url'], 'database': data['database'], 'withCredentials': data['with_credentials'], 'isDefault': data['is_default'], 'user': data['user'], 'jsonData': data['additional_json_data'], 'secureJsonData': data['additional_secure_json_data']}
    json_data = payload['jsonData']
    secure_json_data = payload['secureJsonData']
<mask>:
        secure_json_data['password'] = data['password']
    if 'basic_auth_user' in data and data['basic_auth_user'] and ('basic_auth_password' in data) and data['basic_auth_password']:
        payload['basicAuth'] = True
        payload['basicAuthUser'] = data['basic_auth_user']
        secure_json_data['basicAuthPassword'] = data['basic_auth_password']
    else:
        payload['basicAuth'] = False
    if data.get('tls_client_cert') and data.get('tls_client_key'):
        json_data['tlsAuth'] = True
        if data.get('tls_ca_cert'):
            secure_json_data['tlsCACert'] = data['tls_ca_cert']
            json_data['tlsAuthWithCACert'] = True
        json_data['serverName'] = data['tls_servername']
        secure_json_data['tlsClientCert'] = data['tls_client_cert']
        secure_json_data['tlsClientKey'] = data['tls_client_key']
    else:
        json_data['tlsAuth'] = False
        json_data['tlsAuthWithCACert'] = False
        if data.get('tls_ca_cert'):
            json_data['tlsAuthWithCACert'] = True
            secure_json_data['tlsCACert'] = data['tls_ca_cert']
    if data.get('tls_skip_verify'):
        json_data['tlsSkipVerify'] = True
    if data['ds_type'] == 'alertmanager':
        json_data['implementation'] = data['alertmanager_implementation']
        json_data['handleGrafanaManagedAlerts'] = data['alertmanager_handle_grafana_alerts']
    if data['ds_type'] == 'elasticsearch':
        json_data['maxConcurrentShardRequests'] = data['max_concurrent_shard_requests']
        json_data['timeField'] = data['time_field']
        if data.get('interval'):
            json_data['interval'] = data['interval']
        try:
            es_version = int(data['es_version'])
            if es_version < 56:
                json_data.pop('maxConcurrentShardRequests')
        except ValueError:
            es_version = ES_VERSION_MAPPING.get(data['es_version'])
        json_data['esVersion'] = es_version
    if data['ds_type'] in ['elasticsearch', 'influxdb', 'prometheus']:
        if data.get('time_interval'):
            json_data['timeInterval'] = data['time_interval']
    if data['ds_type'] == 'opentsdb':
        json_data['tsdbVersion'] = data['tsdb_version']
        if data['tsdb_resolution'] == 'second':
            json_data['tsdbResolution'] = 1
        else:
            json_data['tsdbResolution'] = 2
    if data['ds_type'] == 'postgres':
        json_data['sslmode'] = data['sslmode']
    if data['ds_type'] == 'alexanderzobnin-zabbix-datasource':
        if data.get('trends'):
            json_data['trends'] = True
        json_data['username'] = data['zabbix_user']
        json_data['password'] = data['zabbix_password']
    if data['ds_type'] == 'grafana-azure-monitor-datasource':
        json_data['tenantId'] = data['azure_tenant']
        json_data['clientId'] = data['azure_client']
        json_data['cloudName'] = data['azure_cloud']
        json_data['clientsecret'] = 'clientsecret'
        if data.get('azure_secret'):
            secure_json_data['clientSecret'] = data['azure_secret']
    if data['ds_type'] == 'cloudwatch':
        if data.get('aws_credentials_profile'):
            payload['database'] = data.get('aws_credentials_profile')
        json_data['authType'] = data['aws_auth_type']
        json_data['defaultRegion'] = data['aws_default_region']
        if data.get('aws_custom_metrics_namespaces'):
            json_data['customMetricsNamespaces'] = data.get('aws_custom_metrics_namespaces')
        if data.get('aws_assume_role_arn'):
            json_data['assumeRoleArn'] = data.get('aws_assume_role_arn')
        if data.get('aws_access_key') and data.get('aws_secret_key'):
            secure_json_data['accessKey'] = data.get('aws_access_key')
            secure_json_data['secretKey'] = data.get('aws_secret_key')
    payload['jsonData'] = json_data
    payload['secureJsonData'] = secure_json_data
    return payload",data.get('password'),271,'password' in data,False,12.753667906901528,N/A
"def __init__(self, module):
    self._module = module
    self.grafana_url = base.clean_url(module.params.get('url'))
    self.org_id = None
    self.headers = {'Content-Type': 'application/json'}
<mask>:
        self.headers['Authorization'] = 'Bearer %s' % module.params['grafana_api_key']
    else:
        self.headers['Authorization'] = basic_auth_header(module.params['url_username'], module.params['url_password'])
        self.org_id = self.organization_by_name(module.params['org_name']) if module.params['org_name'] else module.params['org_id']
        self.switch_organization(self.org_id)","module.params.get('grafana_api_key', None)",36,module.params['grafana_api_key'],False,35.21740722723756,N/A
"def _send_request(self, url, data=None, headers=None, method='GET'):
<mask>:
        data = json.dumps(data, sort_keys=True)
    if not headers:
        headers = []
    full_url = '{grafana_url}{path}'.format(grafana_url=self.grafana_url, path=url)
    resp, info = fetch_url(self._module, full_url, data=data, headers=headers, method=method)
    status_code = info['status']
    if status_code == 404:
        return None
    elif status_code == 401:
        self._module.fail_json(failed=True, msg=""Unauthorized to perform action '%s' on '%s'"" % (method, full_url))
    elif status_code == 403:
        self._module.fail_json(failed=True, msg='Permission Denied')
    elif status_code == 200:
        return self._module.from_json(resp.read())
    self._module.fail_json(failed=True, msg='Grafana API answered with HTTP %d for url %s and data %s' % (status_code, url, data))",data is not None,83,data,False,4.9787068367863965,N/A
"def organization_by_name(self, org_name):
    url = '/api/user/orgs'
    organizations = self._send_request(url, headers=self.headers, method='GET')
    orga = next((org for org in organizations if org['name'] == org_name))
<mask>:
        return orga['orgId']
    return self._module.fail_json(failed=True, msg=""Current user isn't member of organization: %s"" % org_name)",orga,36,orga,True,100.00000000000004,N/A
"def _api_call(self, method, path, payload):
    data = None
<mask>:
        data = json.dumps(payload)
    return fetch_url(self._module, self.grafana_url + '/api/' + path, headers=self.headers, method=method, data=data)",payload,22,payload,True,100.00000000000004,N/A
"def _organization_by_name(self, org_name):
    r, info = self._api_call('GET', 'orgs/name/%s' % org_name, None)
<mask>:
        raise GrafanaAPIException('Unable to retrieve organization: %s' % info)
    return json.loads(to_text(r.read()))",info['status'] != 200,22,r is None,False,0.0,N/A
"def _organization_users(self, org_id):
    r, info = self._api_call('GET', 'orgs/%d/users' % org_id, None)
<mask>:
        raise GrafanaAPIException('Unable to retrieve organization users: %s' % info)
    return json.loads(to_text(r.read()))",info['status'] != 200,23,r.status_code != 200,False,20.556680845025987,N/A
"def _organization_user_by_login(self, org_id, login):
    for user in self._organization_users(org_id):
<mask>:
            return user","login in (user['login'], user['email'])",11,user.login == login,False,3.005799339448764,N/A
"def create_or_update_user(self, org_id, login, role):
    r, info = self._create_organization_user(org_id, login, role)
<mask>:
        return {'state': 'present', 'changed': True, 'user': self._organization_user_by_login(org_id, login)}
    if info['status'] == 409:
        user = self._organization_user_by_login(org_id, login)
        if not user:
            raise Exception('[BUG] User not found in organization')
        if user['role'] == role:
            return {'changed': False}
        r, info = self._update_organization_user_role(org_id, user['userId'], role)
        if info['status'] == 200:
            return {'changed': True, 'user': self._organization_user_by_login(org_id, login)}
        else:
            raise GrafanaAPIException('Unable to update organization user: %s' % info)
    else:
        raise GrafanaAPIException('Unable to add user to organization: %s' % info)",info['status'] == 200,82,info['status'] == 409,False,80.91067115702207,N/A
"def __init__(self, module):
    self._module = module
    self.grafana_url = base.clean_url(module.params.get('url'))
    self.org_id = None
    self.headers = {'Content-Type': 'application/json'}
<mask>:
        self.headers['Authorization'] = 'Bearer %s' % module.params['grafana_api_key']
    else:
        self.headers['Authorization'] = basic_auth_header(module.params['url_username'], module.params['url_password'])
        self.org_id = self.organization_by_name(module.params['org_name']) if module.params['org_name'] else module.params['org_id']
        self.switch_organization(self.org_id)
    if module.params.get('skip_version_check') is False:
        try:
            grafana_version = self.get_version()
        except GrafanaError as e:
            self._module.fail_json(failed=True, msg=to_text(e))
        if grafana_version['major'] < 5:
            self._module.fail_json(failed=True, msg='folders API is available starting Grafana v5')
        if grafana_version['major'] < 11 and module.params['parent_uid']:
            self._module.fail_json(failed=True, msg='Subfolder API is available starting Grafana v11')","module.params.get('grafana_api_key', None)",76,module.params.get('grafana_api_key'),False,78.17032396134894,N/A
"def _send_request(self, url, data=None, headers=None, method='GET'):
<mask>:
        data = json.dumps(data, sort_keys=True)
    if not headers:
        headers = []
    full_url = '{grafana_url}{path}'.format(grafana_url=self.grafana_url, path=url)
    resp, info = fetch_url(self._module, full_url, data=data, headers=headers, method=method)
    status_code = info['status']
    if status_code == 404:
        return None
    elif status_code == 401:
        self._module.fail_json(failed=True, msg=""Unauthorized to perform action '%s' on '%s'"" % (method, full_url))
    elif status_code == 403:
        self._module.fail_json(failed=True, msg='Permission Denied')
    elif status_code == 412:
        error_msg = resp.read()['message']
        self._module.fail_json(failed=True, msg=error_msg)
    elif status_code == 200:
        response = resp.read() or '{}'
        return self._module.from_json(response)
    self._module.fail_json(failed=True, msg='Grafana folders API answered with HTTP %d' % status_code)",data is not None,90,data,False,4.9787068367863965,N/A
"def organization_by_name(self, org_name):
    url = '/api/user/orgs'
    organizations = self._send_request(url, headers=self.headers, method='GET')
    orga = next((org for org in organizations if org['name'] == org_name))
<mask>:
        return orga['orgId']
    self._module.fail_json(failed=True, msg=""Current user isn't member of organization: %s"" % org_name)",orga,35,orga,True,100.00000000000004,N/A
"def get_version(self):
    url = '/api/health'
    response = self._send_request(url, data=None, headers=self.headers, method='GET')
    version = response.get('version')
<mask>:
        major, minor, rev = version.split('.')
        return {'major': int(major), 'minor': int(minor), 'rev': int(rev)}
    raise GrafanaError(""Failed to retrieve version from '%s'"" % url)",version is not None,36,version,False,4.9787068367863965,N/A
"def get_folder(self, title, uid=None, parent_uid=None):
    url = '/api/folders%s' % ('?parentUid=%s' % parent_uid if parent_uid else '')
    response = self._send_request(url, headers=self.headers, method='GET')
<mask>:
        if uid:
            folders = [item for item in response if item.get('uid') == uid]
        else:
            folders = [item for item in response if item.get('title') == to_text(title)]
        if folders:
            return folders[0]
    return None",response,53,response,True,100.00000000000004,N/A
"def grafana_cli_bin(params):
    """"""
    Get the grafana-cli binary path with global options.
    Raise a GrafanaCliException if the grafana-cli is not present or not in PATH

    :param params: ansible module params. Used to fill grafana-cli global params.
    """"""
    program = 'grafana-cli'
    grafana_cli = None

    def is_exe(fpath):
        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)
    fpath, fname = os.path.split(program)
<mask>:
        if is_exe(program):
            grafana_cli = program
    else:
        for path in os.environ['PATH'].split(os.pathsep):
            path = path.strip('""')
            exe_file = os.path.join(path, program)
            if is_exe(exe_file):
                grafana_cli = exe_file
                break
    if grafana_cli is None:
        raise GrafanaCliException('grafana-cli binary is not present or not in PATH')
    else:
        if 'grafana_plugin_url' in params and params['grafana_plugin_url']:
            grafana_cli = '{0} {1} {2}'.format(grafana_cli, '--pluginUrl', params['grafana_plugin_url'])
        if 'grafana_plugins_dir' in params and params['grafana_plugins_dir']:
            grafana_cli = '{0} {1} {2}'.format(grafana_cli, '--pluginsDir', params['grafana_plugins_dir'])
        if 'grafana_repo' in params and params['grafana_repo']:
            grafana_cli = '{0} {1} {2}'.format(grafana_cli, '--repo', params['grafana_repo'])
        if 'validate_certs' in params and params['validate_certs'] is False:
            grafana_cli = '{0} {1}'.format(grafana_cli, '--insecure')
        return '{0} {1}'.format(grafana_cli, 'plugins')",fpath,148,fpath,True,100.00000000000004,N/A
"def get_grafana_plugin_version(module, params):
    """"""
    Fetch grafana installed plugin version. Return None if plugin is not installed.

    :param module: ansible module object. used to run system commands.
    :param params: ansible module params.
    """"""
    grafana_cli = grafana_cli_bin(params)
    rc, stdout, stderr = module.run_command('{0} ls'.format(grafana_cli))
    stdout_lines = stdout.split('\n')
    for line in stdout_lines:
<mask>:
            line = line.rstrip()
            plugin_name, plugin_version = parse_version(line)
            if plugin_name == params['name']:
                return plugin_version
    return None",line.find(' @ ') != -1,64,line.startswith('#'),False,13.147944990735287,N/A
"def get_grafana_plugin_version_latest(module, params):
    """"""
    Fetch the latest version available from grafana-cli.
    Return the newest version number or None not found.

    :param module: ansible module object. used to run system commands.
    :param params: ansible module params.
    """"""
    grafana_cli = grafana_cli_bin(params)
    rc, stdout, stderr = module.run_command('{0} list-versions {1}'.format(grafana_cli, params['name']))
    stdout_lines = stdout.split('\n')
<mask>:
        return stdout_lines[0].rstrip()
    return None",stdout_lines[0],55,len(stdout_lines) == 1,False,17.747405280050266,N/A
"def grafana_plugin(module, params):
    """"""
    Install update or remove grafana plugin

    :param module: ansible module object. used to run system commands.
    :param params: ansible module params.
    """"""
    grafana_cli = grafana_cli_bin(params)
<mask>:
        grafana_plugin_version = get_grafana_plugin_version(module, params)
        if grafana_plugin_version is not None:
            if 'version' in params and params['version']:
                if params['version'] == grafana_plugin_version:
                    return {'msg': 'Grafana plugin already installed', 'changed': False, 'version': grafana_plugin_version}
                elif params['version'] == 'latest' or params['version'] is None:
                    latest_version = get_grafana_plugin_version_latest(module, params)
                    if latest_version == grafana_plugin_version:
                        return {'msg': 'Grafana plugin already installed', 'changed': False, 'version': grafana_plugin_version}
                    cmd = '{0} update {1}'.format(grafana_cli, params['name'])
                else:
                    cmd = '{0} install {1} {2}'.format(grafana_cli, params['name'], params['version'])
            else:
                return {'msg': 'Grafana plugin already installed', 'changed': False, 'version': grafana_plugin_version}
        elif 'version' in params:
            if params['version'] == 'latest' or params['version'] is None:
                cmd = '{0} install {1}'.format(grafana_cli, params['name'])
            else:
                cmd = '{0} install {1} {2}'.format(grafana_cli, params['name'], params['version'])
        else:
            cmd = '{0} install {1}'.format(grafana_cli, params['name'])
    else:
        cmd = '{0} uninstall {1}'.format(grafana_cli, params['name'])
    rc, stdout, stderr = module.run_command(cmd, umask=18)
    if rc == 0:
        stdout_lines = stdout.split('\n')
        for line in stdout_lines:
            if line.find(params['name']):
                if line.find(' @ ') != -1:
                    line = line.rstrip()
                    plugin_name, plugin_version = parse_version(line)
                else:
                    plugin_version = None
                if params['state'] == 'present':
                    return {'msg': 'Grafana plugin {0} installed : {1}'.format(params['name'], cmd), 'changed': True, 'version': plugin_version}
                else:
                    return {'msg': 'Grafana plugin {0} uninstalled : {1}'.format(params['name'], cmd), 'changed': True}
    else:
        if params['state'] == 'absent' and stdout.find('plugin does not exist'):
            return {'msg': 'Grafana plugin {0} already uninstalled : {1}'.format(params['name'], cmd), 'changed': False}
        raise GrafanaCliException(""'{0}' execution returned an error : [{1}] {2} {3}"".format(cmd, rc, stdout, stderr))",params['state'] == 'present',252,params['name'] != 'grafana',False,15.619699684601283,N/A
"def __init__(self, module):
    self._module = module
    self.headers = {'Content-Type': 'application/json'}
<mask>:
        self.headers['Authorization'] = 'Bearer %s' % module.params['grafana_api_key']
    else:
        self.headers['Authorization'] = basic_auth_header(module.params['url_username'], module.params['url_password'])
    self.grafana_url = base.clean_url(module.params.get('url'))
    if module.params.get('skip_version_check') is False:
        try:
            grafana_version = self.get_version()
        except GrafanaError as e:
            self._module.fail_json(failed=True, msg=to_text(e))
        if grafana_version['major'] < 5:
            self._module.fail_json(failed=True, msg='Teams API is available starting Grafana v5')","module.params.get('grafana_api_key', None)",51,module.params.get('grafana_api_key') is not None,False,73.11104457090251,N/A
"def _send_request(self, url, data=None, headers=None, method='GET'):
<mask>:
        data = json.dumps(data, sort_keys=True)
    if not headers:
        headers = []
    full_url = '{grafana_url}{path}'.format(grafana_url=self.grafana_url, path=url)
    resp, info = fetch_url(self._module, full_url, data=data, headers=headers, method=method)
    status_code = info['status']
    if status_code == 404:
        return None
    elif status_code == 401:
        self._module.fail_json(failed=True, msg=""Unauthorized to perform action '%s' on '%s'"" % (method, full_url))
    elif status_code == 403:
        self._module.fail_json(failed=True, msg='Permission Denied')
    elif status_code == 409:
        self._module.fail_json(failed=True, msg='Team name is taken')
    elif status_code == 200:
        return self._module.from_json(resp.read())
    self._module.fail_json(failed=True, msg='Grafana Teams API answered with HTTP %d' % status_code)",data is not None,85,data,False,4.9787068367863965,N/A
"def get_team(self, name):
    url = '/api/teams/search?name={team}'.format(team=quote(name))
    response = self._send_request(url, headers=self.headers, method='GET')
<mask>:
        raise AssertionError('Expected 1 team, got %d' % response['totalCount'])
    if len(response.get('teams')) == 0:
        return None
    return response.get('teams')[0]",not response.get('totalCount') <= 1,28,response['totalCount'] != 1,False,10.175282441454787,N/A
"def get_user_id_from_mail(self, email):
    url = '/api/users/lookup?loginOrEmail={email}'.format(email=quote(email))
    user = self._send_request(url, headers=self.headers, method='GET')
<mask>:
        self._module.fail_json(failed=True, msg=""User '%s' does not exists"" % email)
    return user.get('id')",user is None,22,not user,False,30.326532985631665,N/A
"def _send_request(self, url, data=None, headers=None, method='GET'):
<mask>:
        data = json.dumps(data, sort_keys=True)
    if not headers:
        headers = []
    full_url = '{grafana_url}{path}'.format(grafana_url=self.grafana_url, path=url)
    resp, info = fetch_url(self._module, full_url, data=data, headers=headers, method=method)
    status_code = info['status']
    if status_code == 404:
        return None
    elif status_code == 401:
        self._module.fail_json(failed=True, msg=""Unauthorized to perform action '%s' on '%s' header: %s"" % (method, full_url, self.headers))
    elif status_code == 403:
        self._module.fail_json(failed=True, msg='Permission Denied')
    elif status_code == 200:
        return self._module.from_json(resp.read())
    if resp is None:
        self._module.fail_json(failed=True, msg='Cannot connect to API Grafana %s' % info['msg'], status=status_code, url=info['url'])
    else:
        self._module.fail_json(failed=True, msg='Grafana Org API answered with HTTP %d' % status_code, body=self._module.from_json(resp.read()))",data is not None,96,data,False,4.9787068367863965,N/A
"def main():
    module = setup_module_object()
    state = module.params['state']
    name = module.params['name']
    grafana_iface = GrafanaOrgInterface(module)
    actual_org = grafana_iface.get_actual_org(name)
<mask>:
        has_changed = False
        if actual_org is None:
            actual_org = grafana_iface.create_org(name)
            has_changed = True
            module.exit_json(changed=has_changed, msg='Organization %s created.' % name, org=actual_org)
        else:
            module.exit_json(changed=has_changed, msg='Organization %s already created.' % name, org=actual_org)
    elif state == 'absent':
        if actual_org is None:
            module.exit_json(msg='No org found, nothing to do')
        result = grafana_iface.delete_org(actual_org.get('id'))
        module.exit_json(changed=True, msg=result.get('message'))",state == 'present',66,state == 'created',False,59.460355750136046,N/A
"def __init__(self, module):
    self._module = module
    self.org_id = None
    self.headers = {'Content-Type': 'application/json'}
<mask>:
        self.headers['Authorization'] = 'Bearer %s' % module.params['grafana_api_key']
    else:
        self.headers['Authorization'] = basic_auth_header(module.params['url_username'], module.params['url_password'])
        self.org_id = self.grafana_organization_by_name(module.params, module.params['org_name']) if module.params['org_name'] else module.params['org_id']
        self.grafana_switch_organisation(module.params, self.org_id)
    self.contact_point = self.grafana_check_contact_point_match(module.params)","module.params.get('grafana_api_key', None)",38,module.params['grafana_api_key'],False,35.21740722723756,N/A
"def grafana_handle_api_provisioning(self, data):
<mask>:
        self.headers['X-Disable-Provenance'] = 'true'
    elif self.contact_point.get('provenance') and (not data.get('provisioning')):
        self._module.fail_json(msg=""Unable to update contact point '%s': provisioning cannot be disabled if it's already enabled"" % data['uid'])
    else:
        pass",not self.contact_point or (not self.contact_point.get('provenance') and (not data.get('provisioning'))),30,self.contact_point.get('provenance') and (not data.get('provisioning')),False,60.653065971263366,N/A
"def grafana_organization_by_name(self, data, org_name):
    r, info = fetch_url(self._module, '%s/api/user/orgs' % data['url'], headers=self.headers, method='GET')
    organizations = json.loads(to_text(r.read()))
    orga = next((org for org in organizations if org['name'] == org_name))
<mask>:
        return orga['orgId']
    raise GrafanaAPIException(""Current user isn't member of organization: %s"" % org_name)",orga,40,orga,True,100.00000000000004,N/A
"def grafana_switch_organisation(self, data, org_id):
    r, info = fetch_url(self._module, '%s/api/user/using/%s' % (data['url'], org_id), headers=self.headers, method='POST')
<mask>:
        raise GrafanaAPIException(""Unable to switch to organization '%s': %s"" % (org_id, info))",info['status'] != 200,26,r.status_code != 200,False,20.556680845025987,N/A
"def dingding_channel_payload(data, payload):
    payload['settings']['url'] = data['dingding_url']
<mask>:
        payload['settings']['msgType'] = {'link': 'link', 'action_card': 'actionCard'}[data['dingding_message_type']]",data.get('dingding_message_type'),13,data['dingding_message_type'],False,42.13952948452608,N/A
"def discord_channel_payload(data, payload):
    payload['settings']['url'] = data['discord_url']
<mask>:
        payload['settings']['content'] = data['discord_message_content']",data.get('discord_message_content'),10,data['discord_message_content'],False,42.13952948452608,N/A
"def email_channel_payload(data, payload):
    payload['settings']['addresses'] = ';'.join(data['email_addresses'])
<mask>:
        payload['settings']['singleEmail'] = data['email_single']",data.get('email_single'),10,data['email_single'],False,23.263472697663296,N/A
"def hipchat_channel_payload(data, payload):
    payload['settings']['url'] = data['hipchat_url']
<mask>:
        payload['settings']['apiKey'] = data['hipchat_api_key']
    if data.get('hipchat_room_id'):
        payload['settings']['roomid'] = data['hipchat_room_id']",data.get('hipchat_api_key'),15,data.get('hipchat_api_key'),True,100.00000000000004,N/A
"def pagerduty_channel_payload(data, payload):
    payload['settings']['integrationKey'] = data['pagerduty_integration_key']
<mask>:
        payload['settings']['severity'] = data['pagerduty_severity']
    if data.get('pagerduty_auto_resolve'):
        payload['settings']['autoResolve'] = data['pagerduty_auto_resolve']
    if data.get('pagerduty_message_in_details'):
        payload['settings']['messageInDetails'] = data['pagerduty_message_in_details']",data.get('pagerduty_severity'),20,data.get('pagerduty_severity'),True,100.00000000000004,N/A
"def __init__(self, module):
    self._module = module
    self.grafana_url = base.clean_url(module.params.get('url'))
    self.org_id = None
    self.headers = {'Content-Type': 'application/json'}
<mask>:
        self.headers['Authorization'] = 'Bearer %s' % module.params['grafana_api_key']
    else:
        self.headers['Authorization'] = basic_auth_header(module.params['url_username'], module.params['url_password'])
        self.org_id = self.organization_by_name(module.params['org_name']) if module.params['org_name'] else module.params['org_id']
        self.switch_organization(self.org_id)
    if module.params.get('skip_version_check') is False:
        try:
            grafana_version = self.get_version()
        except GrafanaError as e:
            self._module.fail_json(failed=True, msg=to_text(e))
        if grafana_version['major'] < 8:
            self._module.fail_json(failed=True, msg='Silences API is available starting with Grafana v8')","module.params.get('grafana_api_key', None)",63,module.params.get('grafana_api_key'),False,78.17032396134894,N/A
"def _send_request(self, url, data=None, headers=None, method='GET'):
<mask>:
        data = json.dumps(data)
    if not headers:
        headers = []
    full_url = '{grafana_url}{path}'.format(grafana_url=self.grafana_url, path=url)
    resp, info = fetch_url(self._module, full_url, data=data, headers=headers, method=method)
    status_code = info['status']
    if status_code == 404:
        return None
    elif status_code == 401:
        self._module.fail_json(failed=True, msg=""Unauthorized to perform action '%s' on '%s'"" % (method, full_url))
    elif status_code == 403:
        self._module.fail_json(failed=True, msg='Permission Denied')
    elif status_code in [200, 202]:
        return self._module.from_json(resp.read())
    elif status_code == 400:
        self._module.fail_json(failed=True, msg=info)
    self._module.fail_json(failed=True, msg='Grafana Silences API answered with HTTP %d' % status_code)",data is not None,82,data,False,4.9787068367863965,N/A
"def create_silence(self, comment, created_by, starts_at, ends_at, matchers):
    url = '/api/alertmanager/grafana/api/v2/silences'
    silence = dict(comment=comment, createdBy=created_by, endsAt=ends_at, matchers=matchers, startsAt=starts_at)
    response = self._send_request(url, data=silence, headers=self.headers, method='POST')
<mask>:
        response['silenceID'] = response['id']
        response.pop('id', None)
    return response",self.get_version()['major'] == 8,31,response.get('id'),False,6.011598678897526,N/A
"def _send_request(self, url, data=None, headers=None, method='GET'):
<mask>:
        data = json.dumps(data, sort_keys=True)
    if not headers:
        headers = []
    full_url = '{grafana_url}{path}'.format(grafana_url=self.grafana_url, path=url)
    resp, info = fetch_url(self._module, full_url, data=data, headers=headers, method=method)
    status_code = info['status']
    if status_code == 404:
        return None
    elif status_code == 401:
        self._module.fail_json(failed=True, msg=""Unauthorized to perform action '%s' on '%s' header: %s"" % (method, full_url, self.headers))
    elif status_code == 403:
        self._module.fail_json(failed=True, msg='Permission Denied')
    elif status_code == 200:
        return self._module.from_json(resp.read())
    self._module.fail_json(failed=True, msg='Grafana Users API answered with HTTP %d' % status_code, body=self._module.from_json(resp.read()))",data is not None,80,data,False,4.9787068367863965,N/A
"def create_user(self, name, email, login, password):
<mask>:
        self._module.fail_json(failed=True, msg='missing required arguments: password')
    url = '/api/admin/users'
    user = dict(name=name, email=email, login=login, password=password)
    self._send_request(url, data=user, headers=self.headers, method='POST')
    return self.get_user_from_login(login)",not password,27,password is None,False,27.516060407455225,N/A
"def main():
    module = setup_module_object()
    state = module.params['state']
    name = module.params['name']
    email = module.params['email']
    login = module.params['login']
    password = module.params['password']
    is_admin = module.params['is_admin']
    grafana_iface = GrafanaUserInterface(module)
    actual_grafana_user = grafana_iface.get_user_from_login(login)
<mask>:
        has_changed = False
        if actual_grafana_user is None:
            actual_grafana_user = grafana_iface.create_user(name, email, login, password)
            has_changed = True
        if is_user_update_required(actual_grafana_user, email, name, login, is_admin):
            actual_grafana_user_id = actual_grafana_user.get('id')
            if is_admin != actual_grafana_user.get('isGrafanaAdmin'):
                grafana_iface.update_user_permissions(actual_grafana_user_id, is_admin)
            actual_grafana_user = grafana_iface.update_user(actual_grafana_user_id, email, name, login)
            has_changed = True
        module.exit_json(changed=has_changed, user=actual_grafana_user)
    elif state == 'absent':
        if actual_grafana_user is None:
            module.exit_json(message='No user found, nothing to do')
        result = grafana_iface.delete_user(actual_grafana_user.get('id'))
        module.exit_json(changed=True, message=result.get('message'))",state == 'present',91,state == 'created',False,59.460355750136046,N/A
"def exit_json(*args, **kwargs):
    """"""function to patch over exit_json; package return data into an exception""""""
<mask>:
        kwargs['changed'] = False
    raise AnsibleExitJson(kwargs)",'changed' not in kwargs,20,"kwargs.get('changed', False)",False,6.567274736060395,N/A
"def get_by_major(version):
<mask>:
        version = version[1:]
    return (int(version.split('.')[0]), version, tuple(map(int, version.split('.'))))",version.startswith('v'),11,version.startswith('.'),False,36.55552228545123,N/A
"def get_grafana_releases():
    r = requests.get('https://api.github.com/repos/grafana/grafana/releases?per_page=100', headers={'Accept': 'application/vnd.github.v3+json'})
<mask>:
        raise Exception('Failed to get releases from GitHub')
    return r.json()",r.status_code != 200,17,r.status_code != 200,True,100.00000000000004,N/A
"@property
def table(self):
    """"""Return a nicely formatted table with results.""""""
    table = Table(show_header=True, header_style='bold magenta', show_footer=False)
<mask>:
        keys = self[0].asdict().keys()
        for key in keys:
            table.add_column(key.capitalize())
        for res in self:
            res = res.asdict()
            table.add_row(*[res[k] for k in keys])
    return table",self,39,self,True,100.00000000000004,N/A
"def __init__(self, data, monitor_data):
    self.data = data
    for data in monitor_data:
<mask>:
            self.data[data['mode']] = data",data['original_interface'] == self.data['interface'],15,'mode' in data,False,0.70335269181743,N/A
"def __eq__(self, other):
<mask>:
        return other.interface == self.interface
    return self.interface == other","isinstance(other, Interface)",12,"isinstance(other, Interface)",True,100.00000000000004,N/A
"def __init__(self, data):
<mask>:
        raise Exception('Pyrcrack must be run as root')
    pos = data.index(b'PHY\tInterface\tDriver\t\tChipset')
    ifaces_data = self.parse(b'\n'.join([a for a in data[pos:] if a and (not a.startswith(b'\t\t'))]))
    monitor_data = filter(lambda x: MONITOR_RE.match(x.decode()), data[pos + len(ifaces_data):])

    def groups(data):
        return MONITOR_RE.match(data.decode()).groups()
    keys = ['driver', 'mode', 'status', 'original_interface', 'interface']
    monitor_data = [dict(zip(keys, groups(a))) for a in monitor_data]
    self.extend([Interface(a, monitor_data) for a in ifaces_data])",data == [b'Run it as root'],59,not self.root,False,0.0,N/A
"@property
def clients(self):
    """"""List of connected clients.

        Returns:

            List of Client instance.
        """"""
<mask>:
        return [Client(d) for d in self.data['wireless-client']]
    else:
        return [Client(self.data['wireless-client'])]","isinstance(self.data['wireless-client'], list)",23,"isinstance(self.data['wireless-client'], list)",True,100.00000000000004,N/A
"@property
@functools.lru_cache
def formatted(self):
    """"""Format given option acording to definition.""""""
    result = Option.short(self.word) if self.is_short else Option.long(self.word)
<mask>:
        return result
    sword = self.word.replace('_', '-')
    return Option.short(sword) if self.is_short else Option.long(sword)",self.usage.get(result),30,result,False,0.09118819655545167,N/A
"@property
def parsed(self):
    """"""Returns key, value if value is required.""""""
<mask>:
        return (self.formatted, str(self.value))
    return (self.formatted,)",self.expects_args,16,self.value is not None,False,16.233395773754953,N/A
"def check():
    """"""Check if aircrack-ng is compatible.""""""
    ver_check = subprocess.check_output(['aircrack-ng', '--help'])
<mask>:
        if b'Aircrack-ng 1.7' not in ver_check:
            raise Exception('Unsupported Aircrack-ng detected')
        else:
            warnings.warn('Aircrack-ng 1.7 detected, some features may not work')",b'Aircrack-ng 1.6' not in ver_check,31,ver_check,False,26.359713811572682,N/A
"def __init__(self):
    """"""Set docstring.""""""
<mask>:
        self.__doc__ = self.helpstr
    self.uuid = uuid.uuid4().hex
    self.called = False
    self.execn = 0
    self.logger = logging.getLogger(self.__class__.__name__)
    self.proc = None
    self.meta = {}
    self.debug = os.getenv('PYRCRACK_DEBUG', '') == 1
    self.tempfile = None
    self.tempdir = None
    if self.requires_tempfile:
        self.tempfile = tempfile.NamedTemporaryFile()
    elif self.requires_tempdir:
        self.tempdir = tempfile.TemporaryDirectory()
    if self.requires_root and (not os.getenv('SKIP_ROOT_CHECK')):
        if os.geteuid() != 0:
            raise Exception('Must be run as root')
    if not os.getenv('SKIP_VERSION_CHECK'):
        check()",not self.__doc__,67,self.helpstr,False,10.394224994345743,N/A
"def _run(self, *args, **kwargs):
    """"""Check command usage and execute it.

        If self.sync is defined, it will return process call output,
        and launch it blockingly.

        Otherwise it will call asyncio.create_subprocess_exec()
        """"""
<mask>:
        raise Exception('Subclassing error, please specify a base cmd')
    self.logger.debug('Parsing options: %s', kwargs)
    options = [Option(self.usage, a, v, self.logger) for a, v in kwargs.items()]
    self.logger.debug('Got options: %s', options)
    opts = [self.command, *list(args), *list(itertools.chain(*(o.parsed for o in options)))]
    self.logger.debug('Running command: %s', opts)
    return opts",not self.command,73,self.sync is None,False,21.3643503198117,N/A
"def validate_schema(data: Any, *, is_closed: bool=False) -> None:
    """"""Validate the schema of the file-under-test.""""""
<mask>:
        raise TypeError(f'Test file has to be YAML list, got {type(data)!r}.')
    schema = json.loads((pathlib.Path(__file__).parent / 'schema.json').read_text('utf8'))
    schema['items']['properties']['__line__'] = {'type': 'integer', 'description': 'Line number where the test starts (`pytest-mypy-plugins` internal)'}
    schema['items']['additionalProperties'] = not is_closed
    jsonschema.validate(instance=data, schema=schema)","not isinstance(data, list)",49,"not isinstance(data, yaml.Mapping)",False,46.713797772819994,N/A
"def parse_parametrized(params: List[Mapping[str, Any]]) -> List[Mapping[str, Any]]:
<mask>:
        return [{}]
    parsed_params: List[Mapping[str, Any]] = []
    known_params: Optional[Set[str]] = None
    for idx, param in enumerate(params):
        param_keys = set(sorted(param.keys()))
        if not known_params:
            known_params = param_keys
        elif known_params.intersection(param_keys) != known_params:
            raise ValueError(f""All parametrized entries must have same keys.First entry is {', '.join(known_params)} but {', '.join(param_keys)} was spotted at {{idx}} position"")
        parsed_params.append({k: v for k, v in param.items() if not k.startswith('__')})
    return parsed_params",not params,69,not params,True,100.00000000000004,N/A
"def construct_mapping(self, node: yaml.MappingNode, deep: bool=False) -> Dict[Hashable, Any]:
    mapping = super().construct_mapping(node, deep=deep)
    starting_line = node.start_mark.line + 1
    for title_node, _contents_node in node.value:
<mask>:
            starting_line = title_node.start_mark.line + 1
    mapping['__line__'] = starting_line
    return mapping",title_node.value == 'main',34,"isinstance(title_node, yaml.TitleNode) and title_node.start_mark.line < starting_line",False,9.84934946888872,N/A
"def collect(self) -> Iterator['YamlTestItem']:
    from pytest_mypy_plugins.item import YamlTestItem
    parsed_file = yaml.load(stream=self.path.read_text('utf8'), Loader=SafeLineLoader)
<mask>:
        return
    validate_schema(parsed_file, is_closed=self.config.option.mypy_closed_schema)
    if not isinstance(parsed_file, list):
        raise ValueError(f'Test file has to be YAML list, got {type(parsed_file)!r}.')
    for raw_test in parsed_file:
        test_name_prefix = raw_test['case']
        if ' ' in test_name_prefix:
            raise ValueError(f""Invalid test name {test_name_prefix!r}, only '[a-zA-Z0-9_]' is allowed."")
        else:
            parametrized = parse_parametrized(raw_test.get('parametrized', []))
        for params in parametrized:
            if params:
                test_name_suffix = ','.join((f'{k}={v}' for k, v in params.items()))
                test_name_suffix = f'[{test_name_suffix}]'
            else:
                test_name_suffix = ''
            test_name = f'{test_name_prefix}{test_name_suffix}'
            main_content = utils.render_template(template=raw_test['main'], data=params)
            main_file = File(path='main.py', content=main_content)
            test_files = [main_file] + parse_test_files(raw_test.get('files', []))
            expect_fail = raw_test.get('expect_fail', False)
            regex = raw_test.get('regex', False)
            expected_output = []
            for test_file in test_files:
                output_lines = utils.extract_output_matchers_from_comments(test_file.path, test_file.content.split('\n'), regex=regex)
                expected_output.extend(output_lines)
            starting_lineno = raw_test['__line__']
            extra_environment_variables = parse_environment_variables(raw_test.get('env', []))
            disable_cache = raw_test.get('disable_cache', False)
            expected_output.extend(utils.extract_output_matchers_from_out(raw_test.get('out', ''), params, regex=regex))
            additional_mypy_config = utils.render_template(template=raw_test.get('mypy_config', ''), data=params)
            skip = self._eval_skip(str(raw_test.get('skip', 'False')))
            if not skip:
                yield YamlTestItem.from_parent(self, name=test_name, files=test_files, starting_lineno=starting_lineno, environment_variables=extra_environment_variables, disable_cache=disable_cache, expected_output=expected_output, parsed_test_data=raw_test, mypy_config=additional_mypy_config, expect_fail=expect_fail)",parsed_file is None,153,not parsed_file,False,46.30777161991026,N/A
"def pytest_collect_file(file_path: pathlib.Path, parent: Node) -> Optional[YamlTestFile]:
<mask>:
        return YamlTestFile.from_parent(parent, path=file_path, fspath=None)
    return None","file_path.suffix in {'.yaml', '.yml'} and file_path.name.startswith(('test-', 'test_'))",14,file_path.is_file(),False,2.756139445230646,N/A
"def join_ini_configs(base_ini_fpath: Optional[str], additional_mypy_config: str, execution_path: Path) -> Optional[str]:
    mypy_ini_config = ConfigParser()
<mask>:
        mypy_ini_config.read(base_ini_fpath)
    if additional_mypy_config:
        if '[mypy]' not in additional_mypy_config:
            additional_mypy_config = f'[mypy]\n{additional_mypy_config}'
        mypy_ini_config.read_string(additional_mypy_config)
    if mypy_ini_config.sections():
        mypy_config_file_path = execution_path / 'mypy.ini'
        with mypy_config_file_path.open('w') as f:
            mypy_ini_config.write(f)
        return str(mypy_config_file_path)
    return None",base_ini_fpath,41,base_ini_fpath,True,100.00000000000004,N/A
"def join_toml_configs(base_pyproject_toml_fpath: str, additional_mypy_config: str, execution_path: Path) -> Optional[str]:
<mask>:
        with open(base_pyproject_toml_fpath) as f:
            toml_config = tomlkit.parse(f.read())
    else:
        toml_config = tomlkit.document()
    if 'tool' not in toml_config or 'mypy' not in toml_config['tool']:
        tool = tomlkit.table(is_super_table=True)
        tool.append('mypy', tomlkit.table())
        toml_config.append('tool', tool)
    if additional_mypy_config:
        if _TOML_TABLE_NAME not in additional_mypy_config:
            additional_mypy_config = f'{_TOML_TABLE_NAME}\n{dedent(additional_mypy_config)}'
        additional_data = tomlkit.parse(additional_mypy_config)
        toml_config['tool']['mypy'].update(additional_data['tool']['mypy'].value.items())
    mypy_config_file_path = execution_path / 'pyproject.toml'
    with mypy_config_file_path.open('w') as f:
        tool_mypy = toml_config['tool']['mypy']
        min_toml = tomlkit.document()
        min_tool = tomlkit.table(is_super_table=True)
        min_toml.append('tool', min_tool)
        min_tool.append('mypy', tool_mypy)
        f.write(min_toml.as_string())
    return str(mypy_config_file_path)",base_pyproject_toml_fpath,77,base_pyproject_toml_fpath is not None,False,63.894310424627285,N/A
"def matches(self, actual: str) -> bool:
<mask>:
        pattern = regex.escape(f'{self.fname}:{self.lnum}: {self.severity}: ' if self.col is None else f'{self.fname}:{self.lnum}:{self.col}: {self.severity}: ') + self.message
        return bool(regex.match(pattern, actual))
    else:
        return str(self) == actual",self.regex,30,self.message is not None,False,16.233395773754953,N/A
"def __str__(self) -> str:
<mask>:
        return f'{self.fname}:{self.lnum}: {self.severity}: {self.message}'
    else:
        return f'{self.fname}:{self.lnum}:{self.col}: {self.severity}: {self.message}'",self.col is None,14,self.col is None,True,100.00000000000004,N/A
"def _add_aligned_message(s1: str, s2: str, error_message: str) -> str:
    """"""Align s1 and s2 so that the their first difference is highlighted.

    For example, if s1 is 'foobar' and s2 is 'fobar', display the
    following lines:

      E: foobar
      A: fobar
           ^

    If s1 and s2 are long, only display a fragment of the strings around the
    first difference. If s1 is very short, do nothing.
    """"""
<mask>:
        return error_message
    maxw = 72
    error_message += 'Alignment of first line difference:\n'
    assert s1 != s2
    trunc = False
    while s1[:30] == s2[:30]:
        s1 = s1[10:]
        s2 = s2[10:]
        trunc = True
    if trunc:
        s1 = '...' + s1
        s2 = '...' + s2
    max_len = max(len(s1), len(s2))
    extra = ''
    if max_len > maxw:
        extra = '...'
    error_message += f'  E: {s1[:maxw]}{extra}\n'
    error_message += f'  A: {s2[:maxw]}{extra}\n'
    error_message += '     '
    for j in range(min(maxw, max(len(s1), len(s2)))):
        if s1[j:j + 1] != s2[j:j + 1]:
            error_message += '^'
            break
        else:
            error_message += ' '
    error_message += '\n'
    return error_message",len(s1) < 4,166,len(s1) == 0 and len(s2) == 0,False,16.451929399933107,N/A
"def remove_empty_lines(lines: List[str]) -> List[str]:
    filtered_lines = []
    for line in lines:
<mask>:
            filtered_lines.append(line)
    return filtered_lines",line,16,not line.strip(),False,8.116697886877475,N/A
"def sorted_by_file_and_line(lines: List[str]) -> List[str]:

    def extract_parts_as_tuple(line: str) -> Tuple[str, int]:
<mask>:
            return ('', 0)
        fname, line_number, _ = line.split(':', maxsplit=2)
        try:
            return (fname, int(line_number))
        except ValueError:
            return ('', 0)
    return sorted(lines, key=extract_parts_as_tuple)","len(line.split(':', maxsplit=2)) < 3",33,':' not in line,False,5.190782388638235,N/A
"def toterminal(self, tw: TerminalWriter) -> None:
<mask>:
        return
    self.reprfileloc.toterminal(tw)
    for line in self.lines:
        red = line.startswith('E   ')
        tw.line(line, bold=True, red=red)
    return",not self.reprfileloc,21,self.lines is None,False,21.3643503198117,N/A
"def make_files(rootdir: Path, files_to_create: Dict[str, str]) -> List[str]:
    created_modules = []
    for rel_fpath, file_contents in files_to_create.items():
        fpath = rootdir / rel_fpath
        fpath.parent.mkdir(parents=True, exist_ok=True)
        fpath.write_text(file_contents)
        created_module = fname_to_module(fpath, root_path=rootdir)
<mask>:
            created_modules.append(created_module)
    return created_modules",created_module,32,created_module,True,100.00000000000004,N/A
"def replace_fpath_with_module_name(line: str, rootdir: Path) -> str:
<mask>:
        return line
    out_fpath, res_line = line.split(':', 1)
    line = os.path.relpath(out_fpath, start=rootdir) + ':' + res_line
    return line.strip().replace('.py:', ':')",':' not in line,26,line.startswith('#'),False,7.267884212102741,N/A
"def maybe_to_abspath(rel_or_abs: str, rootdir: Optional[Path]) -> str:
    rel_or_abs = os.path.expandvars(rel_or_abs)
<mask>:
        return rel_or_abs
    return str(rootdir / rel_or_abs)",rootdir is None or os.path.isabs(rel_or_abs),17,rootdir is None,False,1.3123728736940974,N/A
"def run_mypy_typechecking(cmd_options: List[str], stdout: TextIO, stderr: TextIO) -> int:
    fscache = FileSystemCache()
    sources, options = process_options(cmd_options, fscache=fscache)
    error_messages = []

    def flush_errors(*args: Any) -> None:
        new_messages: List[str]
        serious: bool
        *_, new_messages, serious = args
        error_messages.extend(new_messages)
        f = stderr if serious else stdout
        try:
            for msg in new_messages:
                f.write(msg + '\n')
            f.flush()
        except BrokenPipeError:
            sys.exit(ReturnCodes.FATAL_ERROR)
    try:
        build.build(sources, options, flush_errors=flush_errors, fscache=fscache, stdout=stdout, stderr=stderr)
    except SystemExit as sysexit:
        code = sysexit.code
<mask>:
            code = 0
        elif not isinstance(code, int):
            code = 1
        return code
    finally:
        fscache.flush()
    if error_messages:
        return ReturnCodes.FAIL
    return ReturnCodes.SUCCESS",code is None,89,code is None,True,100.00000000000004,N/A
"def get_all_yaml_files(dir_path: pathlib.Path) -> Sequence[pathlib.Path]:
    yaml_files = []
    for file in dir_path.rglob('*'):
<mask>:
            yaml_files.append(file)
    return yaml_files","file.suffix in ('.yml', '.yaml')",16,file.is_file(),False,6.628576403773604,N/A
"def hook(item: YamlTestItem) -> None:
    parsed_test_data = item.parsed_test_data
    obj_to_reveal = parsed_test_data.get('reveal_type')
<mask>:
        for file in item.files:
            if file.path.endswith('main.py'):
                file.content = f'reveal_type({obj_to_reveal})'",obj_to_reveal,21,obj_to_reveal,True,100.00000000000004,N/A
"def authorize_endpoint(self, scope=None, redirect_uri=None, **kwargs):
    """"""
        Build the authorization url for a user to click.

        :param scope: Scopes to request from the user. Defaults to self.SCOPE
        :type scope: str
        :param redirect_uri: Where to redirect after user grants access.
        :type redirect_uri: str
        """"""
    self.session.scope = scope or self.SCOPE
<mask>:
        self.session.redirect_uri = redirect_uri
    return self.session.authorization_url(self.AUTHORIZE_BASE_URL, **kwargs)",redirect_uri,54,redirect_uri,True,100.00000000000004,N/A
"def __init__(self, client_id, client_secret=None, access_token=None, refresh_token=None, refresh_callback=None):
    self.client_id = client_id
    self.client_secret = client_secret
    token = {}
<mask>:
        token['access_token'] = access_token
    if refresh_token:
        token['refresh_token'] = refresh_token
    self._session = OAuth2Session(client_id, token=token, auto_refresh_url=self.TOKEN_BASE_URL, token_updater=refresh_callback)",access_token,31,access_token,True,100.00000000000004,N/A
"def make_request(self, url, method='GET'):
    response = self._session.request(method, url)
<mask>:
        self._refresh_token()
        response = self._session.request(method, url)
    return response",response.status_code == 401,16,response.status_code == 401,True,100.00000000000004,N/A
"def _refresh_token(self):
    token = self._session.refresh_token(self.TOKEN_BASE_URL, client_id=self.client_id, client_secret=self.client_secret)
<mask>:
        self._session.token_updater(token)
    return token",self._session.token_updater,11,self._session.token_updater,True,100.00000000000004,N/A
"def __init__(self, client_id=None, client_secret=None, access_token=None, refresh_token=None, refresh_callback=None, personal_access_token=None):
    """"""
        :param client_id: The client id - identifies your application.
        :type client_id: str

        :param client_secret: The client secret. Required for auto refresh.
        :type client_secret: str

        :param access_token: Access token.
        :type access_token: str

        :param refresh_token: Use this to renew tokens when they expire
        :type refresh_token: str

        :param refresh_callback: Callback to handle token response
        :type refresh_callback: callable

        :param personal_access_token: Token used for accessing personal data
        :type personal_access_token: str

        """"""
<mask>:
        self._auth_handler = OAuthRequestHandler(client_id, client_secret, access_token, refresh_token, refresh_callback)
    if personal_access_token is not None:
        self._auth_handler = PersonalRequestHandler(personal_access_token)",client_id is not None,91,client_id is not None,True,100.00000000000004,N/A
"def _build_summary_url(self, start, end, summary_type):
    url = '{0}/v1/{1}'.format(self.API_ENDPOINT, summary_type)
    params = {}
<mask>:
        if not isinstance(start, str):
            raise TypeError('start date must be of type str')
        params['start'] = start
    if end is not None:
        if not isinstance(end, str):
            raise TypeError('end date must be of type str')
        params['end'] = end
    qs = '&'.join([f'{k}={v}' for k, v in params.items()])
    url = f'{url}?{qs}'
    return url",start is not None,61,start is not None,True,100.00000000000004,N/A
"def to_pandas(summary, metrics=None, date_key='summary_date'):
    """"""
    Creates a dataframe from a summary object

    :param summary: A summary object returned from API
    :type summary: list of dictionaries. See https://cloud.ouraring.com/docs/readiness for an example

    :param metrics: The metrics to include in the DF. None includes all metrics
    :type metrics: A list of metric names, or alternatively a string for one metric name

    :param date_key: Column to set as the index
    :type date_key: str
    """"""
<mask>:
        summary = [summary]
    df = pd.DataFrame(summary)
    if df.size == 0:
        return df
    if metrics is not None:
        if isinstance(metrics, str):
            metrics = [metrics]
        else:
            metrics = metrics.copy()
        metrics = [m for m in metrics if m in df.columns]
        if date_key not in metrics:
            metrics.insert(0, date_key)
        df = df[metrics]
    df[date_key] = pd.to_datetime(df[date_key]).dt.date
    df = df.set_index(date_key)
    return df","isinstance(summary, dict)",127,"isinstance(summary, str)",False,53.7284965911771,N/A
"def sleep_df(self, start=None, end=None, metrics=None, convert=True, convert_cols=None):
    """"""
        Create a dataframe from sleep summary dict object.

        :param start: Beginning of date range
        :type start: string representation of a date i.e. '2020-10-31'

        :param end: End of date range, or None if you want the current day.
        :type end: string representation of a date i.e. '2020-10-31'

        :param metrics: Metrics to include in the df.
        :type metrics: A list of strings, or a string

        :param convert: Whether to convert datetime columns to pandas types
        :type convert: bool

        :param convert_cols: If convert is True, a set of columns to convert,
            or None for the default. Currently supported column types include
            datetime, timespan, and hypnogram
        :type convert_cols: list
        """"""
    sleep_summary = super().sleep_summary(start, end)['sleep']
    df = to_pandas(sleep_summary, metrics)
<mask>:
        return SleepConverter(convert_cols).convert_metrics(df)
    return df",convert,127,convert,True,100.00000000000004,N/A
"def activity_df(self, start=None, end=None, metrics=None, convert=True, convert_cols=None):
    """"""
        Create a dataframe from activity summary dict object.

        :param start: Beginning of date range
        :type start: string representation of a date i.e. '2020-10-31'

        :param end: End of date range, or None if you want the current day.
        :type end: string representation of a date i.e. '2020-10-31'

        :param metrics: Metrics to include in the df.
        :type metrics: A list of strings, or a string

        :param convert: Whether to convert datetime columns to pandas types
        :type convert: bool

        :param convert_cols: If convert is True, a set of columns to convert,
            or None for the default. Currently supported column types include
            datetime.
        :type convert_cols: list
        """"""
    activity_summary = super().activity_summary(start, end)['activity']
    df = to_pandas(activity_summary, metrics)
<mask>:
        return ActivityConverter(convert_cols).convert_metrics(df)
    return df",convert,124,convert,True,100.00000000000004,N/A
"def combined_df_edited(self, start=None, end=None, metrics=None):
    """"""
        Combines sleep, activity, and summary into one DF
        Some cols are unit converted for easier use or readability.

        If user specifies a metric that appears in all 3 summaries,
        i.e. 'score', then all 3 metrics will be returned.

        Each summary's column is prepended with the summary name.
        i.e. sleep summary 'total' metric will be re-named 'SLEEP.total'

        :param start: Beginning of date range
        :type start: string representation of a date i.e. '2020-10-31'

        :param end: End of date range, or None if you want the current day.
        :type end: string representation of a date i.e. '2020-10-31'

        :param metrics: Metrics to include in the df.
        :type metrics: A list of strings, or a string
        """"""

    def prefix_cols(df, prefix):
        d_to_rename = {}
        for col in df.columns:
<mask>:
                d_to_rename[col] = prefix + ':' + col
        return df.rename(columns=d_to_rename)
    sleep_df = self.sleep_df(start, end, metrics)
    sleep_df = prefix_cols(sleep_df, 'SLEEP')
    readiness_df = self.readiness_df(start, end, metrics)
    readiness_df = prefix_cols(readiness_df, 'READY')
    activity_df = self.activity_df(start, end, metrics)
    activity_df = prefix_cols(activity_df, 'ACTIVITY')
    combined_df = sleep_df.merge(readiness_df, on='summary_date').merge(activity_df, on='summary_date')
    return combined_df",col != 'summary_date',172,col not in prefix,False,9.688464563433238,N/A
"def __init__(self, response, *args, **kwargs):
    try:
        errors = json.loads(response.content.decode('utf8'))['errors']
        message = '\n'.join([error['message'] for error in errors])
    except Exception:
<mask>:
            message = response.content.decode('utf8')
        else:
            message = response
    super(HTTPException, self).__init__(message, *args, **kwargs)","hasattr(response, 'status_code') and response.status_code == 401",30,PY3,False,0.0,N/A
"def detect_and_raise_error(response):
<mask>:
        raise HTTPUnauthorized(response)
    elif response.status_code == 403:
        raise HTTPForbidden(response)
    elif response.status_code == 404:
        raise HTTPNotFound(response)
    elif response.status_code == 409:
        raise HTTPConflict(response)
    elif response.status_code == 426:
        raise HTTPUpgradeRequired(response)
    elif response.status_code == 429:
        exc = HTTPTooManyRequests(response)
        exc.retry_after_secs = int(response.headers['Retry-After'])
        raise exc
    elif response.status_code >= 500:
        raise HTTPServerError(response)
    elif response.status_code >= 400:
        raise HTTPBadRequest(response)",response.status_code == 401,53,response.status_code == 401,True,100.00000000000004,N/A
"def __init__(self, convert_cols=None):
<mask>:
        convert_cols = set(convert_cols)
        defaults = set(self.all_metrics)
        invalid = convert_cols - defaults
        if any(invalid):
            print(f'Ignoring metrics with no conversion: {invalid}')
        self.convert_cols = list(convert_cols & defaults)
    else:
        self.convert_cols = self.all_metrics",convert_cols is not None,32,convert_cols,False,36.78794411714425,N/A
"def convert_metrics(self, df):
    df = super().convert_metrics(df)
<mask>:
        df = self.convert_hypnogram(df)
    return df",'hypnogram_5min' in self.convert_cols,12,self.hypnogram_type == 'hypnogram',False,10.786826322527466,N/A
"def __init__(self, client_id=None, client_secret=None, access_token=None, refresh_token=None, refresh_callback=None, personal_access_token=None):
    """"""
        :param client_id: The client id - identifies your application.
        :type client_id: str

        :param client_secret: The client secret. Required for auto refresh.
        :type client_secret: str

        :param access_token: Access token.
        :type access_token: str

        :param refresh_token: Use this to renew tokens when they expire
        :type refresh_token: str

        :param refresh_callback: Callback to handle token response
        :type refresh_callback: callable

        :param personal_access_token: Token used for accessing personal data
        :type personal_access_token: str
        """"""
<mask>:
        self._auth_handler = OAuthRequestHandler(client_id, client_secret, access_token, refresh_token, refresh_callback)
    if personal_access_token is not None:
        self._auth_handler = PersonalRequestHandler(personal_access_token)",client_id is not None,91,client_id is not None,True,100.00000000000004,N/A
"def _build_summary_url(self, start_date, end_date, next_token, summary_type):
    url = f'{self.API_ENDPOINT}/{summary_type}'
    params = {}
<mask>:
        if not isinstance(start_date, str):
            raise TypeError('start date must be of type str')
        key = 'start_datetime' if summary_type == 'heartrate' else 'start_date'
        params[key] = start_date
    if end_date is not None:
        if not isinstance(end_date, str):
            raise TypeError('end date must be of type str')
        key = 'end_datetime' if summary_type == 'heartrate' else 'end_date'
        params[key] = end_date
    if next_token is not None:
        params['next_token'] = next_token
    qs = urlencode(params)
    url = f'{url}?{qs}' if qs != '' else url
    return url",start_date is not None,88,start_date is not None,True,100.00000000000004,N/A
"def to_pandas(summary, metrics=None, date_key='timestamp'):
    """"""
    Creates a dataframe from a summary object

    :param summary: A summary object returned from API
    :type summary: list of dictionaries. See https://cloud.ouraring.com/v2/docs#tag/Daily-Activity for example

    :param metrics: The metrics to include in the DF. None includes all metrics
    :type metrics: A list of metric names, or alternatively a string for one metric name

    :param date_key: Column to set as the index
    :type date_key: str
    """"""
<mask>:
        summary = [summary]
    df = pd.DataFrame(summary)
    if df.size == 0:
        return df
    if metrics is not None:
        if isinstance(metrics, str):
            metrics = [metrics]
        else:
            metrics = metrics.copy()
        metrics = [m for m in metrics if m in df.columns]
        if date_key not in metrics:
            metrics.insert(0, date_key)
        df = df[metrics]
    df[date_key] = pd.to_datetime(df[date_key]).dt.date
    df = df.set_index(date_key)
    return df","isinstance(summary, dict)",126,"isinstance(summary, str)",False,53.7284965911771,N/A
"def tableize(df, tablefmt='pretty', is_print=True):
    """"""
    Converts dataframe to a formatted table
    For more details, see https://pypi.org/project/tabulate/

    :param df: dataframe to save
    :type df: df object

    :param tablefmt: format of table
    :type tablefmt: string

    :param is_print: print to standard output?
    :type is_print: boolean
    """"""
    from tabulate import tabulate
    table = tabulate(df, headers='keys', tablefmt=tablefmt, showindex=True, stralign='center', numalign='center')
<mask>:
        print(table)
    return table",is_print,59,is_print,True,100.00000000000004,N/A
"def __setitem__(self, key: str, item: Any) -> None:
    """"""Set configuration value.""""""
<mask>:
        warn(f""Parameter '{key}' not known. Skipping."")
    else:
        self.data[key] = item",key not in self._valid,21,key not in self.data,False,64.31870218238025,N/A
"def load(self, source: os.PathLike[str] | str) -> None:
    """"""
        Load parameters from file.

        Parameters
        ----------
        source : path
            Location of file to load parameters from.

        Raises
        ------
        FileNotFoundError
            If the path does not exist.
        """"""
<mask>:
        raise FileNotFoundError(f""File '{source}' does not exist."")
    conn = sqlite3.connect(Path(source))
    with conn as c:
        ser = c.execute('SELECT rowid, * FROM params ORDER BY rowid DESC LIMIT 1').fetchone()[1]
    params = json.loads(ser)
    self.update(params)
    msg.info(f""Updated global parameters with values loaded from '{source}'."")",not Path(source).exists(),74,not Path(source).exists(),True,100.00000000000004,N/A
"@property
def clusters(self) -> ig.VertexClustering:
    """"""
        Return graph partition.

        The partition is detected by the Leiden algorithm, unless a different
        partition that was supplied to the setter.
        """"""
<mask>:
        self._partition = self._partition_graph(resolution=tn.params['resolution_parameter'], seed=tn.params['seed'])
    return self._partition",self._partition is None,35,self._partition is None,True,100.00000000000004,N/A
"@clusters.setter
def clusters(self, value: ig.VertexClustering | ig.VertexDendrogram | dict[int, list[int]]) -> None:
<mask>:
        self._partition = value
    elif isinstance(value, ig.VertexDendrogram):
        self._partition = value.as_clustering()
    elif isinstance(value, dict):
        sorted_node_community_map = dict(sorted(value.items()))
        part = ig.VertexClustering(self.graph, membership=[i[0] for i in sorted_node_community_map.values()])
        self._partition = part
    elif isinstance(value, list):
        part = ig.VertexClustering(self.graph, membership=value)
        self._partition = part
    else:
        raise ValueError('No valid clusters supplied.')","isinstance(value, ig.VertexClustering)",55,"isinstance(value, ig.VertexClustering)",True,100.00000000000004,N/A
"def __init__(self, data: TidyText | BiadjacencyMatrix | pd.DataFrame, min_docs: int=2, max_docs: int | None=None, connected: bool=False, remove_weak_edges: bool=False, doc_attrs: dict[str, dict[str, Any]] | None=None) -> None:
    self._connected = connected
    self._doc_attrs = doc_attrs
<mask>:
        raise ValueError('Data is empty.')
    if isinstance(data, BiadjacencyMatrix):
        self._matrix = data
    elif isinstance(data, (TidyText, pd.DataFrame)):
        self._matrix = _matrix_from_tidy_text(data, min_docs, max_docs)
    if remove_weak_edges:
        pairs: pd.Series = self._matrix.stack()
        edge_weights: pd.Series = pairs[pairs > 0]
        iqr: float = edge_weights.quantile(0.75) - edge_weights.quantile(0.25)
        cutoff: float = edge_weights.median() - 1.5 * iqr
        if cutoff > 0:
            self._matrix = BiadjacencyMatrix(self._matrix[self._matrix > cutoff].dropna(how='all').fillna(0))",data.empty,87,data is None,False,27.516060407455225,N/A
"@cached_property
def graph(self) -> ig.Graph:
    """"""Direct access to the underlying igraph object.""""""
    g = _graph_from_matrix(self._matrix)
<mask>:
        for name, attr in self._doc_attrs.items():
            g.vs[name] = [attr.get(doc) for doc in g.vs['id']]
    if self._connected:
        return giant_component(g)
    return g",self._doc_attrs is not None,34,self._doc_attrs,False,60.653065971263366,N/A
"@cached_property
def m(self) -> BiadjacencyMatrix:
    """"""Weighted bipartite adjacency matrix of the bipartite graph.""""""
<mask>:
        return self._matrix
    a = np.array(self.graph.get_biadjacency(types=self.node_types)[0]).astype('float64')
    a[a == 1] = self.edges['weight']
    doc_count, _ = a.shape
    m = BiadjacencyMatrix(a, index=self.nodes['id'][:doc_count], columns=self.nodes['id'][doc_count:])
    m.T.index.name = 'term'
    return m",not self._connected,38,self._matrix is not None,False,26.269098944241588,N/A
"def __init__(self, n: int):
    base_colors = [color_name_to_rgba(c) for c in BASE_COLORS]
    num_base_colors = len(base_colors)
    colors = base_colors[:]
    blocks_to_add = ceil((n - num_base_colors) / num_base_colors)
    ratio_increment = 1.0 / (ceil(blocks_to_add / 2.0) + 1)
    adding_darker = False
    ratio = ratio_increment
    while len(colors) < n:
<mask>:
            new_block = [darken(color, ratio) for color in base_colors]
        else:
            new_block = [lighten(color, ratio) for color in base_colors]
            ratio += ratio_increment
        colors.extend(new_block)
        adding_darker = not adding_darker
    colors = colors[:n]
    super().__init__(colors)",adding_darker,73,adding_darker,True,100.00000000000004,N/A
"def __init__(self, data: pd.Series, lang: str | None=None) -> None:
<mask>:
        raise ValueError('Corpus data is empty.')
    documents: pd.Series = data.copy()
    if (missings := documents.isna().sum()):
        warn(f'Dropping {missings} empty document(s).')
        documents = documents[~documents.isna()]
    if (duplicated := documents.index.duplicated().sum()):
        warn(f'There are {duplicated} duplicate labels. Concatenating documents.')
        documents = documents.groupby(level=0).agg('\n\n'.join)
    documents.index = documents.index.set_names(['label'])
    self.documents = documents
    if lang is None:
        lang = tn.params['lang']
    self.lang = LANGS.get(lang, lang)
    if self.lang not in _INSTALLED_MODELS:
        warn(f""Language model '{self.lang}' is not yet installed."")",data.empty,74,not data.empty,False,59.460355750136046,N/A
"@property
@memoize
def _nlp_pipeline(self) -> spacy.Language:
    model_opts: dict[str, dict | list] = {'exclude': ['ner', 'textcat']}
<mask>:
        model_opts['config'] = {'nlp': {'tokenizer': {'segmenter': 'jieba'}}}
    try:
        return spacy.load(self.lang, **model_opts)
    except OSError as err:
        if tn.params['autodownload']:
            try:
                spacy.cli.download(self.lang)
                _INSTALLED_MODELS.append(self.lang)
                return spacy.load(self.lang, **model_opts)
            except (KeyError, OSError):
                pass
        elif self.lang in LANGS.values():
            raise err
        warn(f""Using basic '{self.lang}' language model."")
        return spacy.blank(self.lang)",self.lang.startswith('zh'),55,self.lang == 'textcat',False,21.64910073203448,N/A
"@memoize
def _run_pipeline(self, lang: str) -> pd.Series:
    norm_docs: pd.Series = self.documents.map(_normalize_whitespace)
    max_length = max(map(len, norm_docs))
<mask>:
        warn('Corpus contains very long documents. Memory usage will be high.')
        self._nlp_pipeline.max_length = max_length
    tqdm_args = dict(disable=not tn.params['progress_bar'] or None, unit='docs')
    cores = cpu_count() or 1
    if cores > 1 and len(self.documents) >= cores:
        nlp_ufunc = np.frompyfunc(self._nlp_pipeline, 1, 1)
        doc_chunks = df_split(norm_docs, cores)
        return pd.concat(thread_map(nlp_ufunc, doc_chunks, **tqdm_args))
    tqdm.pandas(**tqdm_args)
    return norm_docs.progress_map(self._nlp_pipeline)",max_length > 1000000,65,max_length > self._nlp_pipeline.max_length,False,17.77835117834348,N/A
"@classmethod
def from_df(cls, data: pd.DataFrame, doc_col: str | None=None, lang: str | None=None) -> Corpus:
    """"""
        Create corpus from data frame.

        Parameters
        ----------
        data : DataFrame
            DataFrame containing documents. The index must contain document
            labels.
        doc_col : str, optional
            Indicates which column of ``data`` contains the document texts. If
            none is specified, the first column with strings is used.
        lang : str, optional
            The langugage model to use (default set by ""lang"" parameter).

        Raises
        ------
        NoDocumentColumnException
            If no document column can be detected.

        Returns
        -------
        `Corpus`
        """"""
    object_cols = data.select_dtypes(include='object').columns
<mask>:
        raise NoDocumentColumnException('No suitable document column.')
    if doc_col is None:
        doc_col = str(object_cols[0])
    return cls(data.copy()[doc_col], lang=lang)",doc_col is None and object_cols.empty,106,len(object_cols) == 0,False,14.211011212459495,N/A
"@classmethod
def from_files(cls, files: str | list[str] | list[Path], doc_labels: list[str] | None=None, lang: str | None=None) -> Corpus:
    """"""Construct corpus from files.

        Parameters
        ----------
        files : str or list of str or list of Path
            Path to files (with globbing pattern) or list of file paths.
        doc_labels : list of str, optional
            Labels for documents (default: file name without suffix).
        lang : str, optional
            The langugage model to use (default set by ""lang"" parameter).

        Raises
        ------
        IsADirectoryError
            If the provided path is a directory. (Use globbing.)
        FileNotFoundError
            If the provided path does not exist.

        Returns
        -------
        `Corpus`
        """"""
<mask>:
        files = glob(os.path.expanduser(files))
    files = [Path(f) for f in files]
    for file in files:
        if file.expanduser().is_file():
            pass
        elif file.expanduser().exists():
            raise IsADirectoryError(file.name)
        else:
            raise FileNotFoundError(file.name)
    if not doc_labels:
        doc_labels = [file.stem for file in files]
    data = pd.DataFrame({'path': files}, index=doc_labels)
    data['raw'] = data['path'].map(_read_file)
    return cls.from_df(data, doc_col='raw', lang=lang)","isinstance(files, str)",146,"isinstance(files, str)",True,100.00000000000004,N/A
"def validate_acs_format(accession):
<mask>:
        logging.warning('Error provided accession number is not a string: {}'.format(accession))
        return False
    if not re.search('[A-Z][A-Z]\\d\\d\\d', accession):
        logging.debug('Error provided accession number is not in the correct format ->[A-Z][A-Z][0-9][0-9][0-9]<-: {}'.format(accession))
        return False
    else:
        return True","not isinstance(accession, str)",35,"not isinstance(accession, str)",True,100.00000000000004,N/A
"def acs_to_int(accession):
    is_valid = validate_acs_format(accession)
<mask>:
        logging.error('Error cannot continue due to invalid accession number {}'.format(accession))
        return -1
    return ACS_LETTER_VALUES[accession[0]] * ACS_FORMAT_VALUES[0] + ACS_LETTER_VALUES[accession[1]] * ACS_FORMAT_VALUES[1] + int(accession[2:5]) * ACS_FORMAT_VALUES[2]",not is_valid,29,not is_valid,True,100.00000000000004,N/A
"def validate_int(number):
<mask>:
        logging.error('Error provided numerical id is not a valid integer: {}'.format(number))
        return False
    if number < 0:
        logging.error('Error provided a negative number which is not a valid id: {}'.format(number))
        return False
    if number > MAX_ACS_VALUE:
        logging.error('Error provided a number greater than what the existing ACS format can accommodate: {}'.format(number))
        return False
    else:
        return True","not isinstance(number, int)",56,"not isinstance(number, int)",True,100.00000000000004,N/A
"def int_to_acs(numerical_id):
    is_valid = validate_int(numerical_id)
<mask>:
        logging.error('Error cannot continue due to invalid id number {}'.format(numerical_id))
        return ''
    acs = [0, 0, 0]
    remainder = numerical_id
    for i in range(0, len(ACS_FORMAT_VALUES)):
        x = int(remainder / ACS_FORMAT_VALUES[i])
        remainder = remainder - x * ACS_FORMAT_VALUES[i]
        acs[i] = x
    return '{}{}{}'.format(ACS_VALUES_TO_LETTERS[acs[0]], ACS_VALUES_TO_LETTERS[acs[1]], str(acs[2]).zfill(3))",not is_valid,49,not is_valid,True,100.00000000000004,N/A
"def read_user_new_sequence_info(file):
<mask>:
        return dict()
    data = pd.read_csv(file, sep='\t', header=0, names=MOB_TYPER_REPORT_HEADER, index_col=0)
    sequences = dict()
    header = list(data.head())
    for index, row in data.iterrows():
        if not index in sequences:
            sequences[index] = {}
        for i in range(0, len(header)):
            if row[header[i]] == 'nan':
                sequences[index][header[i]] = ''
            else:
                sequences[index][header[i]] = row[header[i]]
    return sequences",os.path.getsize(file) == 0,49,not os.path.isfile(file),False,34.56543232731582,N/A
"def filter_sequences(input_fasta, blastdb, min_ident, min_cov, evalue, min_length, out_dir, blast_results_file, seq_filterfile=None, num_threads=1, max_length=400000):
    blastn(input_fasta, blastdb, min_ident, min_cov, evalue, min_length, out_dir, blast_results_file, seq_filterfile, num_threads, max_length)
<mask>:
        os.remove(blast_results_file)
        return pd.DataFrame()
    blast_df = BlastReader(blast_results_file).df
    if seq_filterfile:
        blast_df = filter_blastdf_by_seqs(blast_df, seq_filterfile)
        blast_df = blast_df.reset_index(drop=True)
    return blast_df",os.path.getsize(blast_results_file) == 0,41,os.path.exists(blast_results_file),False,57.23320664346175,N/A
"def find_mash_genomes(reference_mash_sketch, fasta_query, outfile, cutoff_distance, num_threads=1):
    cutoff_distance = float(cutoff_distance)
<mask>:
        sys.exit(-1)
    if not os.path.isfile(reference_mash_sketch):
        sys.exit(-1)
    m = mash()
    distances = parseMash(m.run_mash(reference_mash_sketch, fasta_query, num_threads=num_threads))
    genomes = []
    for query in distances:
        for ref in distances[query]:
            score = distances[query][ref]
            if score < cutoff_distance:
                genomes.append(ref)
    return genomes",not os.path.isfile(fasta_query),44,not os.path.isfile(reference_mash_sketch),False,50.08718428920986,N/A
"def calc_hit_coverage(blast_df, overlap_threshold, reference_sequence_meta):
    blast_df = blast_df.sort_values(['sseqid', 'sstart', 'send', 'bitscore'], ascending=[True, True, True, False])
    hit_scores = {}
    size = str(len(blast_df))
    prev_size = 0
    while size != prev_size:
        blast_df = filter_overlaping_records(blast_df, overlap_threshold, 'sseqid', 'sstart', 'send', 'bitscore')
        prev_size = size
        size = str(len(blast_df))
    blast_df['qseqid'].apply(str)
    blast_df['sseqid'].apply(str)
    for index, row in blast_df.iterrows():
        query = str(row['qseqid'])
        pID = str(row['sseqid'])
        score = float(row['bitscore'])
        aln_length = int(row['length'])
        total_len = int(row['slen'])
<mask>:
            logging.warning('Seqid {} in blast results but not cluster file'.format(pID))
            continue
        else:
            clust_id = reference_sequence_meta[pID]['primary_cluster_id']
        if pID not in hit_scores:
            hit_scores[pID] = {'score': 0, 'length': total_len, 'covered_bases': 0, 'clust_id': clust_id, 'contigs': []}
        hit_scores[pID]['covered_bases'] += aln_length
        hit_scores[pID]['score'] += score
        hit_scores[pID]['contigs'].append(query)
        hit_scores[pID]['contigs'] = list(set(hit_scores[pID]['contigs']))
    return hit_scores",pID not in reference_sequence_meta,107,not reference_sequence_meta.get(pID),False,38.16330911371339,N/A
"def calc_contig_reference_cov(blast_df, overlap_threshold, reference_sequence_meta):
    blast_df = blast_df.sort_values(['qseqid', 'sseqid', 'qstart', 'qend', 'bitscore'], ascending=[True, True, True, True, False])
    contig_scores = {}
    size = str(len(blast_df))
    prev_size = 0
    for index, row in blast_df.iterrows():
        query = str(row['qseqid'])
        pID = str(row['sseqid'])
        score = float(row['bitscore'])
<mask>:
            logging.warning('Seqid {} in blast results but not cluster file'.format(pID))
            continue
        else:
            clust_id = reference_sequence_meta[pID]['primary_cluster_id']
        if query not in contig_scores:
            contig_scores[query] = {}
        if not pID in contig_scores[query]:
            contig_scores[query][pID] = 0
        contig_scores[query][pID] += score
    for contig_id in contig_scores:
        contig_scores[contig_id] = OrderedDict(sorted(iter(list(contig_scores[contig_id].items())), key=lambda x: x[1], reverse=True))
    return contig_scores",pID not in reference_sequence_meta,86,pID in reference_sequence_meta,False,72.89545183625967,N/A
"def extract(fname, outdir):
    """"""
    Decompress a zip or gzip archive. Decompression method is selected based
    on file extension. Following extraction, the original archive is deleted.

    :param fname:  Path to the archive to be extracted
    :param outdir: Directory into which the results are placed
    :return: None
    """"""
    logger.info(f'Decompressing {fname}')
    shutil.unpack_archive(fname, outdir)
    dir_name = os.path.join(outdir, os.path.basename(fname))
    for ext in ['.tar.gz', '.zip', '.gz']:
        dir_name = dir_name.replace(ext, '')
    src_files = os.listdir(dir_name)
    for file_name in src_files:
        full_file_name = os.path.join(dir_name, file_name)
<mask>:
            shutil.copyfile(full_file_name, os.path.join(outdir, file_name))
    shutil.rmtree(dir_name)
    os.remove(fname)",os.path.isfile(full_file_name),81,not os.path.isfile(full_file_name),False,91.21679090703874,N/A
"def parseMashScreen(mash_results):
    mash_results = mash_results.decode('utf-8').split('\n')
    hits = {}
    for line in mash_results:
<mask>:
            continue
        row = line.strip('\n').split('\t')
        ref_id = str(row[4])
        score = float(row[0])
        if ref_id not in hits:
            hits[ref_id] = {}
        hits[ref_id] = score
    return hits",len(line) < 4,36,line.startswith('#') or line.startswith('#'),False,3.0098043843528286,N/A
"def parseMash(mash_results):
    mash_results = mash_results.decode('utf-8').split('\n')
    hits = {}
    for line in mash_results:
        row = line.strip('\n').split('\t')
<mask>:
            continue
        ref_id = str(row[0])
        query_id = str(row[1])
        score = float(row[2])
        if query_id not in hits:
            hits[query_id] = {}
        hits[query_id][ref_id] = score
    return hits",len(row) < 4,39,len(row) < 3,False,75.98356856515926,N/A
"def filter_invalid_taxids(taxids):
    filtered = []
    for i in taxids:
<mask>:
            continue
        try:
            filtered.append(int(i))
        except ValueError:
            continue
    return filtered",i == 'NaN' or i == 'nan' or i == None or (i == '-'),18,"i in ['0', '0']",False,0.8887839864973845,N/A
"def getHeirarchy(taxid, ETE3DBTAXAFILE, database_directory):
<mask>:
        logging.info('Did not find taxa.sqlite in {}. Initializaing ete3 taxonomy database'.format(ETE3DBTAXAFILE))
        initETE3Database(database_directory, ETE3DBTAXAFILE)
    ncbi = NCBITaxa(dbfile=ETE3DBTAXAFILE)
    if not isETE3DBTAXAFILEexists(ETE3DBTAXAFILE):
        logging.error('Tried ete3 init, but still was not able to find taxa.sqlite file for ete3 lib in {}. Aborting'.format(ETE3DBTAXAFILE))
        return ['-', '-']
    if not isinstance(taxid, int):
        return {'names': [], 'ranks': []}
    lineage = ncbi.get_lineage(taxid)
    names = ncbi.get_taxid_translator(lineage)
    ranks = []
    for id in lineage:
        ranks.append(ncbi.get_rank(id))
    return {'names': names, 'ranks': names}",not isETE3DBTAXAFILEexists(ETE3DBTAXAFILE),72,not isETE3DBTAXAFILEexists(database_directory),False,26.269098944241588,N/A
"def getTaxid(taxon, ETE3DBTAXAFILE, database_directory):
<mask>:
        logging.info('Did not find taxa.sqlite in {}. Initializaing ete3 taxonomy database'.format(ETE3DBTAXAFILE))
        initETE3Database(database_directory, ETE3DBTAXAFILE)
    ncbi = NCBITaxa(dbfile=ETE3DBTAXAFILE)
    if not isETE3DBTAXAFILEexists(ETE3DBTAXAFILE):
        logging.error('Tried ete3 init, but still was not able to find taxa.sqlite file for ete3 lib in {}. Aborting'.format(ETE3DBTAXAFILE))
        return ['-', '-']
    taxid = ncbi.get_name_translator(taxon)
    return taxid",not isETE3DBTAXAFILEexists(ETE3DBTAXAFILE),49,not isETE3DBTAXAFILEexists(database_directory),False,26.269098944241588,N/A
"def run_mash(self, reference_db, input_fasta, table=False, num_threads=1):
<mask>:
        p = Popen(['mash', 'dist', '-t', '-p', str(num_threads), reference_db, input_fasta], stdout=PIPE, stderr=PIPE)
        stdout, stderr = p.communicate()
    else:
        p = Popen(['mash', 'dist', '-p', str(num_threads), reference_db, input_fasta], stdout=PIPE, stderr=PIPE)
        stdout, stderr = p.communicate()
    if len(str(stderr)) > 0 and str(stderr) != ""b''"":
        logging.info('{}'.format(stderr))
    return stdout",table,48,table,True,100.00000000000004,N/A
"def run_mash_screen(self, reference_db, input_fasta, winner_take_all=True, num_threads=1):
<mask>:
        p = Popen(['mash', 'screen', '-p', str(num_threads), '-w', '-i', '0', reference_db, input_fasta], stdout=PIPE, stderr=PIPE)
        stdout, stderr = p.communicate()
    else:
        p = Popen(['mash', 'screen', '-p', str(num_threads), '-i', '0', reference_db, input_fasta], stdout=PIPE, stderr=PIPE)
        stdout, stderr = p.communicate()
    logging.info('{}'.format(stderr))
    return stdout",winner_take_all,44,winner_take_all,True,100.00000000000004,N/A
"def mashsketch(self, input_fasta, output_path, sketch_ind=True, num_threads=1, kmer_size=21, sketch_size=1000):
<mask>:
        os.path.dirname(input_fasta)
    if sketch_ind:
        p = Popen(['mash', 'sketch', '-p', str(num_threads), '-i', '-o', output_path, '-k', str(kmer_size), '-s', str(sketch_size), input_fasta], stdout=PIPE, stderr=PIPE)
    else:
        p = Popen(['mash', 'sketch', '-p', str(num_threads), '-o', output_path, '-k', str(kmer_size), '-s', str(sketch_size), input_fasta], stdout=PIPE, stderr=PIPE)
    p.wait()
    stdout = p.stdout.read()
    stderr = p.stderr.read()",output_path == '',51,not os.path.isdir(input_fasta),False,4.456882760699063,N/A
"def run_blast(self, input_fasta, output_path, blast_results_file, logging, min_cov=1, min_ident=1, evalue=1, num_threads=1, min_length=25):
    blast_runner = BlastRunner(input_fasta, output_path)
    blast_runner.makeblastdb(input_fasta, 'nucl', logging)
    blast_runner.run_blast(query_fasta_path=input_fasta, blast_task='megablast', db_path=input_fasta, db_type='nucl', min_cov=min_cov, min_ident=min_ident, evalue=evalue, blast_outfile=blast_results_file, num_threads=num_threads, word_size=11, logging=logging)
<mask>:
        fh = open(blast_results_file, 'w', encoding='utf-8')
        fh.write('')
        fh.close()
        return dict()
    blast_df = BlastReader(blast_results_file, logging).df
    blast_df = blast_df.loc[blast_df['length'] >= min_length]
    blast_df = blast_df.reset_index(drop=True)
    blast_df.to_csv(blast_results_file, sep='\t', header=False, line_terminator='\n', index=False)",os.path.getsize(blast_results_file) == 0,56,not os.path.exists(blast_results_file),False,57.475393483140245,N/A
"def overhangDetection(self, blast_results_file, logging, min_length=25):
<mask>:
        return dict()
    blast_df = BlastReader(blast_results_file, logging).df.sort_values(['qseqid', 'qstart', 'qend', 'bitscore'], ascending=[True, True, True, False])
    circular_contigs = {}
    for index, row in blast_df.iterrows():
        contig_id_query = row['qseqid']
        contig_id_subject = row['sseqid']
        contig_start_subject = int(row['sstart'])
        contig_end_subject = int(row['send'])
        contig_start_query = int(row['qstart'])
        contig_end_query = int(row['qend'])
        contig_length = int(row['qlen'])
        length = int(row['length'])
        if contig_id_query != contig_id_subject and contig_id_subject != 'ref|{}|'.format(contig_id_query):
            continue
        if contig_start_query != 1 or length < min_length:
            continue
        if contig_start_query == contig_start_subject and contig_end_query == contig_end_subject:
            continue
        if contig_start_query == 1 and contig_end_subject == contig_length:
            circular_contigs[contig_id_query] = 'Circular: Overlap {} bp'.format(length)
    return circular_contigs",os.path.getsize(blast_results_file) == 0,94,not blast_results_file,False,16.954225822593184,N/A
"def test_download_databases_with_input_dir():
<mask>:
        os.mkdir(TEST_ROOT + '/run_test')
    database_dir = TEST_ROOT + '/run_test/databases_custom_dir'
    args = ['-d', database_dir]
    sys.argv[1:] = args
    mob_suite.mob_init.main()",os.path.exists(TEST_ROOT + '/run_test') == False,19,not os.path.exists(TEST_ROOT + '/run_test'),False,83.13427988970655,N/A
"def makeblastdb(self, fasta_path, dbtype, logging, parse_seqids=False):
<mask>:
        p = Popen(['makeblastdb', '-in', Path(fasta_path), '-parse_seqids', '-dbtype', dbtype], stdout=PIPE, stderr=PIPE)
    else:
        p = Popen(['makeblastdb', '-in', Path(fasta_path), '-dbtype', dbtype], stdout=PIPE, stderr=PIPE)
    p.wait()
    stdout = str(p.stdout.read())
    stderr = str(p.stderr.read())
    if stderr is not None and stderr != '' and (stderr != ""b''""):
        logging.error('makeblastdb on {} had the following messages STDERR: {}'.format(fasta_path, stderr))
        return False
    return True",parse_seqids,61,parse_seqids,True,100.00000000000004,N/A
"def run_tblastn(self, query_fasta_path, blast_task, db_path, db_type, min_cov, min_ident, evalue, blast_outfile, logging, num_threads=1, max_target_seqs=100000000, seq_id_file=None):
<mask>:
        p = Popen(['tblastn', '-query', Path(query_fasta_path), '-seqidlist', '{}'.format(seq_id_file), '-num_threads', '{}'.format(num_threads), '-db', '{}'.format(db_path), '-evalue', '{}'.format(evalue), '-out', blast_outfile, '-max_target_seqs', '{}'.format(max_target_seqs), '-outfmt', '6 {}'.format(' '.join(BLAST_TABLE_COLS))], stdout=PIPE, stderr=PIPE)
    else:
        p = Popen(['tblastn', '-query', Path(query_fasta_path), '-num_threads', '{}'.format(num_threads), '-db', '{}'.format(Path(db_path)), '-evalue', '{}'.format(evalue), '-out', blast_outfile, '-max_target_seqs', '{}'.format(max_target_seqs), '-outfmt', '6 {}'.format(' '.join(BLAST_TABLE_COLS))], stdout=PIPE, stderr=PIPE)
    p.wait()
    stdout = str(p.stdout.read())
    stderr = str(p.stderr.read())
    if stdout is not None and stdout != '':
        logging.debug('blastn on db {} and query {} STDOUT: {}'.format(query_fasta_path, db_path, stdout))
    if stderr is None or str(stderr) != '' or str(stderr) != ""b''"":
        if os.path.exists(blast_outfile):
            return blast_outfile
    if stderr is not None and (str(stderr) != '' and str(stderr) != ""b''""):
        logging.debug('blastn on db {} and query {} STDERR: {}'.format(query_fasta_path, db_path, stderr))
        if os.path.exists(blast_outfile):
            return blast_outfile
        else:
            ex_msg = 'tblastn on db {} and query {} did not produce expected output file at {}'.format(query_fasta_path, db_path, blast_outfile)
            logging.error(ex_msg)
            raise Exception(ex_msg)",seq_id_file,154,seq_id_file,True,100.00000000000004,N/A
"def run_blast(self, query_fasta_path, blast_task, db_path, db_type, min_cov, min_ident, evalue, blast_outfile, logging, num_threads=1, word_size=11, max_target_seqs=100000000, seq_id_file=None):
<mask>:
        p = Popen(['blastn', '-task', blast_task, '-query', Path(query_fasta_path), '-db', '{}'.format(Path(db_path)), '-seqidlist', '{}'.format(seq_id_file), '-num_threads', '{}'.format(num_threads), '-evalue', '{}'.format(evalue), '-dust', 'yes', '-perc_identity', '{}'.format(min_ident), '-max_target_seqs', '{}'.format(max_target_seqs), '-out', Path(blast_outfile), '-outfmt', '6 {}'.format(' '.join(BLAST_TABLE_COLS))], stdout=PIPE, stderr=PIPE)
    else:
        p = Popen(['blastn', '-task', blast_task, '-query', '{}'.format(Path(query_fasta_path)), '-db', '{}'.format(Path(db_path)), '-num_threads', '{}'.format(num_threads), '-evalue', '{}'.format(evalue), '-dust', 'yes', '-perc_identity', '{}'.format(min_ident), '-max_target_seqs', '{}'.format(max_target_seqs), '-out', '{}'.format(Path(blast_outfile)), '-outfmt', '6 {}'.format(' '.join(BLAST_TABLE_COLS))], stdout=PIPE, stderr=PIPE)
    p.wait()
    stdout = str(p.stdout.read())
    stderr = str(p.stderr.read())
    if stdout is not None and stdout != '' and (str(stdout) != ""b''""):
        logging.info('blastn on db {} and query {} STDOUT: {}'.format(query_fasta_path, db_path, stdout))
    if stderr is None and stderr != '' and (str(stderr) != ""b''""):
        if os.path.exists(blast_outfile):
            return blast_outfile
    if stderr is not None and stderr != '' and (stderr != ""b''""):
        logging.error('blastn on db {} and query {} STDERR: {}'.format(query_fasta_path, db_path, stderr))",seq_id_file,144,seq_id_file,True,100.00000000000004,N/A
"def __init__(self, blast_outfile, logging):
    """"""Read BLASTN output file into a pandas DataFrame
        Sort the DataFrame by BLAST bitscore.
        If there are no BLASTN results, then no results can be returned.
        Args:
            blast_outfile (str): `blastn` output file path
        Raises:
            EmptyDataError: No data could be parsed from the `blastn` output file
        """"""
    self.blast_outfile = blast_outfile
    try:
<mask>:
            logging.warning('No BLASTN results to parse from file %s', blast_outfile)
            self.df = pd.DataFrame()
            return
        if os.path.getsize(blast_outfile) == 0:
            logging.warning('No BLASTN results to parse from file %s', blast_outfile)
            self.df = pd.DataFrame()
            return
        self.df = pd.read_csv(self.blast_outfile, sep='\t', header=None)
        self.df.columns = BLAST_TABLE_COLS
        logger.debug(self.df.head())
        self.is_missing = False
    except EmptyDataError as exc:
        logging.warning('No BLASTN results to parse from file %s', blast_outfile)
        self.is_missing = True
        self.df = pd.DataFrame(index=['A'], columns='A')",not os.path.isfile(blast_outfile),117,not os.path.exists(self.blast_outfile),False,49.73567356124543,N/A
"@pytest.mark.parametrize('option, value, expected', [('provider', 'maxmind', {'maxmind'}), ('firewall', utils.Firewall.IP_TABLES.value, {utils.Firewall.IP_TABLES.value}), ('address-family', utils.AddressFamily.IPV6.value, {utils.AddressFamily.IPV6.value}), ('no-checksum', 'unused', False), ('countries', 'RU,CN', {'ru', 'cn'}), ('output-dir', '/var/local', '/var/local')])
def test_single_cli_opts_no_config_file(option, value, expected):
    """"""
    Do single value CLI options correctly override defaults?
    Note: specifying 'maxmind' without license key will generate a RuntimeError during real execution
    """"""
<mask>:
        config = __main__.get_config(['--' + option])
        assert not config.get('checksum')
    else:
        config = __main__.get_config(['--' + option, value])
        assert config.get(option) == expected",option == 'no-checksum',69,expected is None,False,0.0,N/A
"@pytest.mark.parametrize('option, value', [('provider', 'maxmind'), ('firewall', utils.Firewall.IP_TABLES.value), ('address-family', utils.AddressFamily.IPV6.value), ('no-checksum', 'unused'), ('countries', 'ru')])
def test_config_file_cli_args_precedence(option, value, monkeypatch):
    """"""
    Do CLI args take precedence over config-file options?
    """"""

    def mockreturn(path):
        cp = ConfigParser(allow_no_value=True)
        cp.read_string('\n            [general]\n            provider=dbip\n            firewall=nftables\n            address-family=ipv4\n            checksum=True\n            [countries]\n            CA\n            ')
        return cp
    monkeypatch.setattr(__main__, 'get_config_parser', mockreturn, raising=True)
<mask>:
        config = __main__.get_config(['--' + option, '-c', '/tmp/dummy.conf'])
        assert not config.get('checksum')
    else:
        config = __main__.get_config(['--' + option, value, '-c', '/tmp/dummy.conf'])
        assert config.get(option) == {value}",option == 'no-checksum',70,option == 'no-checksum',True,100.00000000000004,N/A
"def __init__(self, firewall: set, address_family: set, checksum: bool, countries: set, output_dir: str, provider_options: dict):
    super().__init__(firewall, address_family, checksum, countries, output_dir)
<mask>:
        raise SystemExit('ERROR: Account ID cannot be empty')
    if not (license_key := provider_options.get('license-key')):
        raise SystemExit('ERROR: License key cannot be empty')
    self.auth = HTTPBasicAuth(account_id, license_key)
    self.base_url = 'https://download.maxmind.com/geoip/databases/GeoLite2-Country-CSV/download'",not (account_id := provider_options.get('account-id')),46,not (account_id := provider_options.get('account-id')),True,100.00000000000004,N/A
"def generate(self):
    zip_file = self.download()
<mask>:
        self.check_checksum(zip_file)
    with ZipFile(Path(zip_file.name), 'r') as zip_ref:
        zip_dir_prefix = os.path.commonprefix(zip_ref.namelist())
        id_cc_map = self.build_id_cc_map(zip_ref, zip_dir_prefix)
        if self.ipv4:
            self.build_sets(id_cc_map, zip_ref, zip_dir_prefix, utils.AddressFamily.IPV4)
        if self.ipv6:
            self.build_sets(id_cc_map, zip_ref, zip_dir_prefix, utils.AddressFamily.IPV6)",self.checksum,31,not self.checksum_check,False,30.213753973567677,N/A
"def build_id_cc_map(self, zip_ref: ZipFile, dir_prefix: str):
    locations = 'GeoLite2-Country-Locations-en.csv'
    id_country_code_map = dict()
    with ZipFile(Path(zip_ref.filename), 'r') as zip_file:
        with zip_file.open(dir_prefix + locations, 'r') as csv_file_bytes:
            rows = DictReader(TextIOWrapper(csv_file_bytes))
            for r in rows:
<mask>:
                    if self.countries == 'all' or cc.lower() in self.countries:
                        id_country_code_map[r['geoname_id']] = cc
    return id_country_code_map",(cc := r['country_iso_code']),45,(cc := r['geoname_id']).lower() in self.countries,False,28.889830842564407,N/A
"def build_sets(self, id_country_code_map: dict, zip_ref: ZipFile, dir_prefix: str, addr_fam: utils.AddressFamily):
    ipset_dir = self.base_dir / 'maxmind/ipset' / addr_fam.value
    nftset_dir = self.base_dir / 'maxmind/nftset' / addr_fam.value
<mask>:
        ip_blocks = 'GeoLite2-Country-Blocks-IPv4.csv'
        inet_family = 'family inet'
    else:
        ip_blocks = 'GeoLite2-Country-Blocks-IPv6.csv'
        inet_family = 'family inet6'
    country_subnets = dict()
    with ZipFile(Path(zip_ref.filename), 'r') as zip_file:
        with zip_file.open(dir_prefix + ip_blocks, 'r') as csv_file_bytes:
            rows = DictReader(TextIOWrapper(csv_file_bytes))
            for r in rows:
                geo_id = r['geoname_id']
                if not geo_id:
                    geo_id = r['registered_country_geoname_id']
                if not geo_id:
                    continue
                try:
                    cc = id_country_code_map[geo_id]
                except KeyError:
                    continue
                net = r['network']
                filename_key = cc + '.' + addr_fam.value
                if filename_key in country_subnets:
                    country_subnets[filename_key].append(net)
                else:
                    country_subnets[filename_key] = [net]
    if self.ip_tables:
        if ipset_dir.is_dir():
            shutil.rmtree(ipset_dir)
        ipset_dir.mkdir(parents=True)
    if self.nf_tables:
        if nftset_dir.is_dir():
            shutil.rmtree(nftset_dir)
        nftset_dir.mkdir(parents=True)
    for set_name, subnets in country_subnets.items():
        set_name_parts = set_name.split('.')
        country_code = set_name_parts[0]
        if self.ip_tables:
            ipset_file = open(ipset_dir / set_name, 'w')
            maxelem = max(131072, 1 if len(subnets) == 0 else 1 << (len(subnets) - 1).bit_length())
            ipset_file.write('create {0} hash:net {1} maxelem {2} comment\n'.format(set_name, inet_family, maxelem))
        if self.nf_tables:
            nftset_file = open(nftset_dir / set_name, 'w')
            nftset_file.write('define ' + set_name + ' = {\n')
        for subnet in subnets:
            if self.ip_tables:
                ipset_file.write('add ' + set_name + ' ' + subnet + ' comment ' + country_code + '\n')
            if self.nf_tables:
                nftset_file.write(subnet + ',\n')
        if self.ip_tables:
            ipset_file.close()
        if self.nf_tables:
            nftset_file.write('}\n')
            nftset_file.close()",addr_fam == utils.AddressFamily.IPV4,206,addr_fam.value == utils.AddressFamily.IPv4,False,53.107253497886994,N/A
"def check_checksum(self, zip_ref):
    expected_sha256sum = self.download_checksum()
    with open(zip_ref.name, 'rb') as raw_zip_file:
        sha256_hash = hashlib.sha256()
        while (chunk := raw_zip_file.read(8192)):
            sha256_hash.update(chunk)
        computed_sha256sum = sha256_hash.hexdigest()
<mask>:
        raise SystemExit(""ERROR: Computed zip file digest '{0}' does not match expected value '{1}'"".format(computed_sha256sum, expected_sha256sum))",expected_sha256sum != computed_sha256sum,37,computed_sha256sum != expected_sha256sum,False,58.14307369682194,N/A
"def generate(self):
    """"""
        While nftables' set facility accepts both IPv4 and IPv6 IP ranges, ipset only accepts IPv4 IP ranges.
        So, for simplicity we convert all ranges into subnets.

        ip_start, ip_end, country
        """"""
    gzip_ref = self.download()
    country_subnets = dict()
    with gzip.GzipFile(gzip_ref, 'rb') as csv_file_bytes:
<mask>:
            self.check_checksum(csv_file_bytes)
        rows = DictReader(TextIOWrapper(csv_file_bytes), fieldnames=('ip_start', 'ip_end', 'country'))
        for r in rows:
            cc = r['country']
            if cc != 'ZZ' and (self.countries == 'all' or cc.lower() in self.countries):
                ip_start = ip_address(r['ip_start'])
                ip_version = ip_start.version
                if ip_version == 4 and self.ipv4 or (ip_version == 6 and self.ipv6):
                    inet_suffix = 'ipv' + str(ip_version)
                    filename_key = cc + '.' + inet_suffix
                    ip_end = ip_address(r['ip_end'])
                    if self.ip_tables:
                        subnets = [nets.with_prefixlen for nets in summarize_address_range(ip_start, ip_end)]
                        if filename_key in country_subnets:
                            country_subnets[filename_key].extend(subnets)
                        else:
                            country_subnets[filename_key] = subnets
                    else:
                        if ip_start == ip_end:
                            ip_range = r['ip_start']
                        else:
                            ip_range = r['ip_start'] + '-' + r['ip_end']
                        if filename_key in country_subnets:
                            country_subnets[filename_key].append(ip_range)
                        else:
                            country_subnets[filename_key] = [ip_range]
    self.build_sets(country_subnets)",self.checksum,149,self.check_checksum,False,23.643540225079384,N/A
"def build_sets(self, dict_of_lists):
    ipset_dir = self.base_dir / 'dbip/ipset' / utils.AddressFamily.IPV4.value
    nftset_dir = self.base_dir / 'dbip/nftset' / utils.AddressFamily.IPV4.value
    ip6set_dir = self.base_dir / 'dbip/ipset' / utils.AddressFamily.IPV6.value
    nft6set_dir = self.base_dir / 'dbip/nftset' / utils.AddressFamily.IPV6.value
<mask>:
        if self.ipv4:
            if ipset_dir.is_dir():
                shutil.rmtree(ipset_dir)
            ipset_dir.mkdir(parents=True)
        if self.ipv6:
            if ip6set_dir.is_dir():
                shutil.rmtree(ip6set_dir)
            ip6set_dir.mkdir(parents=True)
    if self.nf_tables:
        if self.ipv4:
            if nftset_dir.is_dir():
                shutil.rmtree(nftset_dir)
            nftset_dir.mkdir(parents=True)
        if self.ipv6:
            if nft6set_dir.is_dir():
                shutil.rmtree(nft6set_dir)
            nft6set_dir.mkdir(parents=True)
    for set_name, subnets in dict_of_lists.items():
        set_name_parts = set_name.split('.')
        country_code = set_name_parts[0]
        ip_version = set_name_parts[1]
        if ip_version == utils.AddressFamily.IPV4.value:
            inet_family = 'family inet'
        else:
            inet_family = 'family inet6'
        if self.ip_tables:
            ipset_path = self.base_dir / 'dbip/ipset' / ip_version / set_name
            ipset_file = open(ipset_path, 'w')
            maxelem = max(131072, 1 if len(subnets) == 0 else 1 << (len(subnets) - 1).bit_length())
            ipset_file.write('create {0} hash:net {1} maxelem {2} comment\n'.format(set_name, inet_family, maxelem))
        if self.nf_tables:
            nftset_path = self.base_dir / 'dbip/nftset' / ip_version / set_name
            nftset_file = open(nftset_path, 'w')
            nftset_file.write('define ' + set_name + ' = {\n')
        for subnet in subnets:
            if self.ip_tables:
                ipset_file.write('add ' + set_name + ' ' + subnet + ' comment ' + country_code + '\n')
            if self.nf_tables:
                nftset_file.write(subnet + ',\n')
        if self.ip_tables:
            ipset_file.close()
        if self.nf_tables:
            nftset_file.write('}\n')
            nftset_file.close()",self.ip_tables,181,self.ip_tables,True,100.00000000000004,N/A
"def check_checksum(self, csv_file_bytes):
    expected_sha1sum = self.download_checksum()
    sha1_hash = hashlib.sha1()
    while (chunk := csv_file_bytes.read(8192)):
        sha1_hash.update(chunk)
    computed_sha1sum = sha1_hash.hexdigest()
    csv_file_bytes.seek(0)
<mask>:
        raise SystemExit(""ERROR: Computed CSV file digest '{0}' does not match expected value '{1}'"".format(computed_sha1sum, expected_sha1sum))",expected_sha1sum != computed_sha1sum,33,computed_sha1sum != expected_sha1sum,False,58.14307369682194,N/A
"def main():
    opts = get_config()
    providers = opts.get('provider')
    print('Building geoipsets...')
<mask>:
        mmp = maxmind.MaxMindProvider(opts.get('firewall'), opts.get('address-family'), opts.get('checksum'), opts.get('countries'), opts.get('output-dir'), opts.get('maxmind'))
        mmp.generate()
    if 'dbip' in providers:
        dbipp = dbip.DbIpProvider(opts.get('firewall'), opts.get('address-family'), opts.get('checksum'), opts.get('countries'), opts.get('output-dir'))
        dbipp.generate()",'maxmind' in providers,32,'mind' in providers,False,55.03212081491043,N/A
"def read_version():
    with open(os.path.join('freezegun', '__init__.py')) as f:
        m = re.search('__version__\\s*=\\s*[\'""]([^\'""]*)[\'""]', f.read())
<mask>:
            return m.group(1)
        raise ValueError(""couldn't find version"")",m,18,m,True,100.00000000000004,N/A
"def create_tag():
    from subprocess import call
    version = read_version()
    errno = call(['git', 'tag', '--annotate', version, '--message', 'Version %s' % version])
<mask>:
        print('Added tag for version %s' % version)",errno == 0,28,errno == 0,True,100.00000000000004,N/A
"@utils.cpython_only_mark
@pytest.mark.parametrize('func_name', ('monotonic', 'monotonic_ns', 'perf_counter', 'perf_counter_ns'))
def test_ticking_monotonic(func_name: str) -> None:
<mask>:
        assert hasattr(time, func_name)
    elif not hasattr(time, func_name):
        pytest.skip('time.%s does not exist in the current Python version' % func_name)
    func = getattr(time, func_name)
    with freeze_time('Jan 14th, 2012, 23:59:59', tick=True):
        initial = func()
        time.sleep(0.001)
        assert func() > initial","sys.version_info[0:2] >= (3, 7)",48,sys.version_info[0] == 2,False,36.15980809040258,N/A
"def cpython_only(func: 'Callable[P, T]') -> 'Callable[P, T]':

    @wraps(func)
    def wrapper(*args: 'P.args', **kwargs: 'P.kwargs') -> T:
<mask>:
            raise SkipTest('Requires CPython')
        return func(*args)
    return wrapper",not _is_cpython,23,not cpython,False,15.777684932819515,N/A
"@pytest.mark.parametrize('func_name, has_func, tick_size', (('monotonic', True, 1.0), ('monotonic_ns', HAS_MONOTONIC_NS, int(1000000000.0)), ('perf_counter', True, 1.0), ('perf_counter_ns', HAS_PERF_COUNTER_NS, int(1000000000.0))))
def test_time_monotonic(func_name: str, has_func: bool, tick_size: int) -> None:
    initial_datetime = datetime.datetime(year=1, month=7, day=12, hour=15, minute=6, second=3)
<mask>:
        pytest.skip('%s does not exist in current version' % func_name)
    with freeze_time(initial_datetime) as frozen_datetime:
        func = getattr(time, func_name)
        t0 = func()
        frozen_datetime.tick()
        t1 = func()
        assert t1 == t0 + tick_size
        frozen_datetime.tick(10)
        t11 = func()
        assert t11 == t1 + 10 * tick_size",not has_func,75,"not hasattr(time, func_name)",False,6.27465531099474,N/A
"def test_maya_datetimes() -> None:
<mask>:
        raise SkipTest(""maya is optional since it's not supported for enough python versions"")
    with freeze_time(maya.when('October 2nd, 1997')):
        assert datetime.datetime.now() == datetime.datetime(year=1997, month=10, day=2)",not maya,27,maya is None,False,27.516060407455225,N/A
"def test_should_use_real_time() -> None:
    frozen = datetime.datetime(2015, 3, 5)
    expected_frozen = 1425513600.0
    expected_frozen_gmt = (2015, 3, 5, 0, 0, 0, 3, 64, -1)
    expected_clock = 0
    from freezegun import api
    api.call_stack_inspection_limit = 100
    timestamp_to_convert = 1579602312
    time_tuple = time.gmtime(timestamp_to_convert)
    with freeze_time(frozen):
        assert time.time() == expected_frozen
        assert time.gmtime() == expected_frozen_gmt
<mask>:
            assert time.clock() == expected_clock
        if HAS_TIME_NS:
            assert time.time_ns() == expected_frozen * 1000000000.0
        assert calendar.timegm(time.gmtime()) == expected_frozen
        assert calendar.timegm(time_tuple) == timestamp_to_convert
    with freeze_time(frozen, ignore=['_pytest']):
        assert time.time() != expected_frozen
        assert time.gmtime() != expected_frozen_gmt
        if HAS_CLOCK:
            assert time.clock() != expected_clock
        if HAS_TIME_NS:
            assert time.time_ns() != expected_frozen * 1000000000.0
        assert calendar.timegm(time.gmtime()) != expected_frozen
        assert calendar.timegm(time_tuple) == timestamp_to_convert",HAS_CLOCK,103,HAS_CLOCK,True,100.00000000000004,N/A
"def configure(default_ignore_list: Optional[List[str]]=None, extend_ignore_list: Optional[List[str]]=None) -> None:
<mask>:
        raise ConfigurationError('Either default_ignore_list or extend_ignore_list might be given, not both')
    if default_ignore_list is not None:
        settings.default_ignore_list = default_ignore_list
    if extend_ignore_list:
        settings.default_ignore_list = list(dict.fromkeys([*settings.default_ignore_list, *extend_ignore_list]))",default_ignore_list is not None and extend_ignore_list is not None,32,default_ignore_list is None and extend_ignore_list is None,False,71.26047597394759,N/A
"def _setup_module_cache(module: types.ModuleType) -> None:
    date_attrs = []
    all_module_attributes = _get_module_attributes(module)
    for attribute_name, attribute_value in all_module_attributes:
<mask>:
            date_attrs.append((attribute_name, attribute_value))
    _GLOBAL_MODULES_CACHE[module.__name__] = (_get_module_attributes_hash(module), date_attrs)",id(attribute_value) in _real_time_object_ids,23,"isinstance(attribute_value, datetime.date)",False,16.847111051295393,N/A
"def _get_cached_module_attributes(module: types.ModuleType) -> List[Tuple[str, Any]]:
    module_hash, cached_attrs = _GLOBAL_MODULES_CACHE.get(module.__name__, ('0', []))
<mask>:
        return cached_attrs
    _setup_module_cache(module)
    module_hash, cached_attrs = _GLOBAL_MODULES_CACHE[module.__name__]
    return cached_attrs",_get_module_attributes_hash(module) == module_hash,22,module_hash,False,1.3123728736940974,N/A
"def _should_use_real_time() -> bool:
<mask>:
        return False
    if not ignore_lists:
        return True
    if not ignore_lists[-1]:
        return False
    frame = inspect.currentframe().f_back.f_back
    for _ in range(call_stack_inspection_limit):
        module_name = frame.f_globals.get('__name__')
        if module_name and module_name.startswith(ignore_lists[-1]):
            return True
        frame = frame.f_back
        if frame is None:
            break
    return False",not call_stack_inspection_limit,43,call_stack_inspection_limit == 0,False,63.894310424627285,N/A
"def fake_time() -> float:
<mask>:
        return real_time()
    current_time = get_current_time()
    return calendar.timegm(current_time.timetuple()) + current_time.microsecond / 1000000.0",_should_use_real_time(),16,not settings.USE_REAL_TIME,False,5.11459870708889,N/A
"def fake_time_ns() -> int:
<mask>:
        return real_time_ns()
    return int(fake_time() * 1000000000.0)",_should_use_real_time(),11,sys.version_info[0] == 2,False,3.7477767366779213,N/A
"def wrap_coroutine(api: Any, coroutine: _CallableT) -> _CallableT:

    @functools.wraps(coroutine)
    async def wrapper(*args: Any, **kwargs: Any) -> Any:
        with api as time_factory:
<mask>:
                result = await coroutine(time_factory, *args, **kwargs)
            else:
                result = await coroutine(*args, **kwargs)
        return result
    return cast(_CallableT, wrapper)",api.as_arg,38,"isinstance(time_factory, _TimeFactory)",False,4.767707020457095,N/A
"def __init__(self, pretrained=True):
    super().__init__()
    self.conv1 = nn.Conv2d(3, 10, kernel_size=3)
    self.prelu1 = nn.PReLU(10)
    self.pool1 = nn.MaxPool2d(2, 2, ceil_mode=True)
    self.conv2 = nn.Conv2d(10, 16, kernel_size=3)
    self.prelu2 = nn.PReLU(16)
    self.conv3 = nn.Conv2d(16, 32, kernel_size=3)
    self.prelu3 = nn.PReLU(32)
    self.conv4_1 = nn.Conv2d(32, 2, kernel_size=1)
    self.softmax4_1 = nn.Softmax(dim=1)
    self.conv4_2 = nn.Conv2d(32, 4, kernel_size=1)
    self.training = False
<mask>:
        state_dict_path = os.path.join(os.path.dirname(__file__), '../data/pnet.pt')
        state_dict = torch.load(state_dict_path)
        self.load_state_dict(state_dict)",pretrained,58,pretrained,True,100.00000000000004,N/A
"def __init__(self, pretrained=True):
    super().__init__()
    self.conv1 = nn.Conv2d(3, 28, kernel_size=3)
    self.prelu1 = nn.PReLU(28)
    self.pool1 = nn.MaxPool2d(3, 2, ceil_mode=True)
    self.conv2 = nn.Conv2d(28, 48, kernel_size=3)
    self.prelu2 = nn.PReLU(48)
    self.pool2 = nn.MaxPool2d(3, 2, ceil_mode=True)
    self.conv3 = nn.Conv2d(48, 64, kernel_size=2)
    self.prelu3 = nn.PReLU(64)
    self.dense4 = nn.Linear(576, 128)
    self.prelu4 = nn.PReLU(128)
    self.dense5_1 = nn.Linear(128, 2)
    self.softmax5_1 = nn.Softmax(dim=1)
    self.dense5_2 = nn.Linear(128, 4)
    self.training = False
<mask>:
        state_dict_path = os.path.join(os.path.dirname(__file__), '../data/rnet.pt')
        state_dict = torch.load(state_dict_path)
        self.load_state_dict(state_dict)",pretrained,68,pretrained,True,100.00000000000004,N/A
"def __init__(self, pretrained=True):
    super().__init__()
    self.conv1 = nn.Conv2d(3, 32, kernel_size=3)
    self.prelu1 = nn.PReLU(32)
    self.pool1 = nn.MaxPool2d(3, 2, ceil_mode=True)
    self.conv2 = nn.Conv2d(32, 64, kernel_size=3)
    self.prelu2 = nn.PReLU(64)
    self.pool2 = nn.MaxPool2d(3, 2, ceil_mode=True)
    self.conv3 = nn.Conv2d(64, 64, kernel_size=3)
    self.prelu3 = nn.PReLU(64)
    self.pool3 = nn.MaxPool2d(2, 2, ceil_mode=True)
    self.conv4 = nn.Conv2d(64, 128, kernel_size=2)
    self.prelu4 = nn.PReLU(128)
    self.dense5 = nn.Linear(1152, 256)
    self.prelu5 = nn.PReLU(256)
    self.dense6_1 = nn.Linear(256, 2)
    self.softmax6_1 = nn.Softmax(dim=1)
    self.dense6_2 = nn.Linear(256, 4)
    self.dense6_3 = nn.Linear(256, 10)
    self.training = False
<mask>:
        state_dict_path = os.path.join(os.path.dirname(__file__), '../data/onet.pt')
        state_dict = torch.load(state_dict_path)
        self.load_state_dict(state_dict)",pretrained,85,pretrained,True,100.00000000000004,N/A
"def __init__(self, image_size=160, margin=0, min_face_size=20, thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True, select_largest=True, selection_method=None, keep_all=False, device=None):
    super().__init__()
    self.image_size = image_size
    self.margin = margin
    self.min_face_size = min_face_size
    self.thresholds = thresholds
    self.factor = factor
    self.post_process = post_process
    self.select_largest = select_largest
    self.keep_all = keep_all
    self.selection_method = selection_method
    self.pnet = PNet()
    self.rnet = RNet()
    self.onet = ONet()
    self.device = torch.device('cpu')
<mask>:
        self.device = device
        self.to(device)
    if not self.selection_method:
        self.selection_method = 'largest' if self.select_largest else 'probability'",device is not None,69,device is not None,True,100.00000000000004,N/A
"def forward(self, img, save_path=None, return_prob=False):
    """"""Run MTCNN face detection on a PIL image or numpy array. This method performs both
        detection and extraction of faces, returning tensors representing detected faces rather
        than the bounding boxes. To access bounding boxes, see the MTCNN.detect() method below.
        
        Arguments:
            img {PIL.Image, np.ndarray, or list} -- A PIL image, np.ndarray, torch.Tensor, or list.
        
        Keyword Arguments:
            save_path {str} -- An optional save path for the cropped image. Note that when
                self.post_process=True, although the returned tensor is post processed, the saved
                face image is not, so it is a true representation of the face in the input image.
                If `img` is a list of images, `save_path` should be a list of equal length.
                (default: {None})
            return_prob {bool} -- Whether or not to return the detection probability.
                (default: {False})
        
        Returns:
            Union[torch.Tensor, tuple(torch.tensor, float)] -- If detected, cropped image of a face
                with dimensions 3 x image_size x image_size. Optionally, the probability that a
                face was detected. If self.keep_all is True, n detected faces are returned in an
                n x 3 x image_size x image_size tensor with an optional list of detection
                probabilities. If `img` is a list of images, the item(s) returned have an extra 
                dimension (batch) as the first dimension.

        Example:
        >>> from facenet_pytorch import MTCNN
        >>> mtcnn = MTCNN()
        >>> face_tensor, prob = mtcnn(img, save_path='face.png', return_prob=True)
        """"""
    batch_boxes, batch_probs, batch_points = self.detect(img, landmarks=True)
<mask>:
        batch_boxes, batch_probs, batch_points = self.select_boxes(batch_boxes, batch_probs, batch_points, img, method=self.selection_method)
    faces = self.extract(img, batch_boxes, save_path)
    if return_prob:
        return (faces, batch_probs)
    else:
        return faces",not self.keep_all,250,self.keep_all,False,81.87307530779823,N/A
"def __init__(self, scale=1.0, noReLU=False):
    super().__init__()
    self.scale = scale
    self.noReLU = noReLU
    self.branch0 = BasicConv2d(1792, 192, kernel_size=1, stride=1)
    self.branch1 = nn.Sequential(BasicConv2d(1792, 192, kernel_size=1, stride=1), BasicConv2d(192, 192, kernel_size=(1, 3), stride=1, padding=(0, 1)), BasicConv2d(192, 192, kernel_size=(3, 1), stride=1, padding=(1, 0)))
    self.conv2d = nn.Conv2d(384, 1792, kernel_size=1, stride=1)
<mask>:
        self.relu = nn.ReLU(inplace=False)",not self.noReLU,47,noReLU,False,4.9787068367863965,N/A
"def forward(self, x):
    x0 = self.branch0(x)
    x1 = self.branch1(x)
    out = torch.cat((x0, x1), 1)
    out = self.conv2d(out)
    out = out * self.scale + x
<mask>:
        out = self.relu(out)
    return out",not self.noReLU,30,self.relu is not None,False,17.965205598154213,N/A
"def __init__(self, pretrained=None, classify=False, num_classes=None, dropout_prob=0.6, device=None):
    super().__init__()
    self.pretrained = pretrained
    self.classify = classify
    self.num_classes = num_classes
<mask>:
        tmp_classes = 8631
    elif pretrained == 'casia-webface':
        tmp_classes = 10575
    elif pretrained is None and self.classify and (self.num_classes is None):
        raise Exception('If ""pretrained"" is not specified and ""classify"" is True, ""num_classes"" must be specified')
    self.conv2d_1a = BasicConv2d(3, 32, kernel_size=3, stride=2)
    self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1)
    self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)
    self.maxpool_3a = nn.MaxPool2d(3, stride=2)
    self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)
    self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1)
    self.conv2d_4b = BasicConv2d(192, 256, kernel_size=3, stride=2)
    self.repeat_1 = nn.Sequential(Block35(scale=0.17), Block35(scale=0.17), Block35(scale=0.17), Block35(scale=0.17), Block35(scale=0.17))
    self.mixed_6a = Mixed_6a()
    self.repeat_2 = nn.Sequential(Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1), Block17(scale=0.1))
    self.mixed_7a = Mixed_7a()
    self.repeat_3 = nn.Sequential(Block8(scale=0.2), Block8(scale=0.2), Block8(scale=0.2), Block8(scale=0.2), Block8(scale=0.2))
    self.block8 = Block8(noReLU=True)
    self.avgpool_1a = nn.AdaptiveAvgPool2d(1)
    self.dropout = nn.Dropout(dropout_prob)
    self.last_linear = nn.Linear(1792, 512, bias=False)
    self.last_bn = nn.BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True)
    if pretrained is not None:
        self.logits = nn.Linear(512, tmp_classes)
        load_weights(self, pretrained)
    if self.classify and self.num_classes is not None:
        self.logits = nn.Linear(512, self.num_classes)
    self.device = torch.device('cpu')
    if device is not None:
        self.device = device
        self.to(device)",pretrained == 'vggface2',179,pretrained == 'casia-webface',False,59.460355750136046,N/A
"def forward(self, x):
    """"""Calculate embeddings or logits given a batch of input image tensors.

        Arguments:
            x {torch.tensor} -- Batch of image tensors representing faces.

        Returns:
            torch.tensor -- Batch of embedding vectors or multinomial logits.
        """"""
    x = self.conv2d_1a(x)
    x = self.conv2d_2a(x)
    x = self.conv2d_2b(x)
    x = self.maxpool_3a(x)
    x = self.conv2d_3b(x)
    x = self.conv2d_4a(x)
    x = self.conv2d_4b(x)
    x = self.repeat_1(x)
    x = self.mixed_6a(x)
    x = self.repeat_2(x)
    x = self.mixed_7a(x)
    x = self.repeat_3(x)
    x = self.block8(x)
    x = self.avgpool_1a(x)
    x = self.dropout(x)
    x = self.last_linear(x.view(x.shape[0], -1))
    x = self.last_bn(x)
<mask>:
        x = self.logits(x)
    else:
        x = F.normalize(x, p=2, dim=1)
    return x",self.classify,99,self.logits is not None,False,16.233395773754953,N/A
"def load_weights(mdl, name):
    """"""Download pretrained state_dict and load into model.

    Arguments:
        mdl {torch.nn.Module} -- Pytorch model.
        name {str} -- Name of dataset that was used to generate pretrained state_dict.

    Raises:
        ValueError: If 'pretrained' not equal to 'vggface2' or 'casia-webface'.
    """"""
<mask>:
        path = 'https://github.com/timesler/facenet-pytorch/releases/download/v2.2.9/20180402-114759-vggface2.pt'
    elif name == 'casia-webface':
        path = 'https://github.com/timesler/facenet-pytorch/releases/download/v2.2.9/20180408-102900-casia-webface.pt'
    else:
        raise ValueError('Pretrained models only exist for ""vggface2"" and ""casia-webface""')
    model_dir = os.path.join(get_torch_home(), 'checkpoints')
    os.makedirs(model_dir, exist_ok=True)
    cached_file = os.path.join(model_dir, os.path.basename(path))
    if not os.path.exists(cached_file):
        download_url_to_file(path, cached_file)
    state_dict = torch.load(cached_file)
    mdl.load_state_dict(state_dict)",name == 'vggface2',80,name == 'vggface2',True,100.00000000000004,N/A
"def import_tf_params(tf_mdl_dir, sess):
    """"""Import tensorflow model from save directory.
    
    Arguments:
        tf_mdl_dir {str} -- Location of protobuf, checkpoint, meta files.
        sess {tensorflow.Session} -- Tensorflow session object.
    
    Returns:
        (list, list, list) -- Tuple of lists containing the layer names,
            parameter arrays as numpy ndarrays, parameter shapes.
    """"""
    print('\nLoading tensorflow model\n')
<mask>:
        tf_mdl_dir(sess)
    else:
        facenet.load_model(tf_mdl_dir)
    print('\nGetting model weights\n')
    tf_layers = tf.trainable_variables()
    tf_params = sess.run(tf_layers)
    tf_shapes = [p.shape for p in tf_params]
    tf_layers = [l.name for l in tf_layers]
    if not callable(tf_mdl_dir):
        path = os.path.join(tf_mdl_dir, 'layer_description.json')
    else:
        path = 'data/layer_description.json'
    with open(path, 'w') as f:
        json.dump({l: s for l, s in zip(tf_layers, tf_shapes)}, f)
    return (tf_layers, tf_params, tf_shapes)",callable(tf_mdl_dir),104,not callable(tf_mdl_dir),False,86.33400213704509,N/A
"def load_tf_conv2d(weights, layer, transpose=False):
    """"""Load tensorflow weights into nn.Conv2d object.
    
    Arguments:
        weights {list} -- Tensorflow parameters.
        layer {torch.nn.Module} -- nn.Conv2d.
    """"""
<mask>:
        if len(weights) == 2:
            layer.bias.data = torch.tensor(weights[1]).view(layer.bias.data.shape)
        weights = weights[0]
    if transpose:
        dim_order = (3, 2, 1, 0)
    else:
        dim_order = (3, 2, 0, 1)
    layer.weight.data = torch.tensor(weights).permute(dim_order).view(layer.weight.data.shape)","isinstance(weights, list)",50,"isinstance(weights, list)",True,100.00000000000004,N/A
"def load_tf_linear(weights, layer):
    """"""Load tensorflow weights into nn.Linear object.
    
    Arguments:
        weights {list} -- Tensorflow parameters.
        layer {torch.nn.Module} -- nn.Linear.
    """"""
<mask>:
        if len(weights) == 2:
            layer.bias.data = torch.tensor(weights[1]).view(layer.bias.data.shape)
        weights = weights[0]
    layer.weight.data = torch.tensor(weights).transpose(-1, 0).view(layer.weight.data.shape)","isinstance(weights, list)",35,len(weights) > 1,False,17.965205598154213,N/A
"def load_tf_mixed6a(weights, layer):
<mask>:
        raise ValueError(f'Number of weight arrays ({len(weights)}) not equal to 16')
    load_tf_basicConv2d(weights[:4], layer.branch0)
    load_tf_basicConv2d(weights[4:8], layer.branch1[0])
    load_tf_basicConv2d(weights[8:12], layer.branch1[1])
    load_tf_basicConv2d(weights[12:16], layer.branch1[2])",len(weights) != 16,22,len(weights) != 16,True,100.00000000000004,N/A
"def load_tf_mixed7a(weights, layer):
<mask>:
        raise ValueError(f'Number of weight arrays ({len(weights)}) not equal to 28')
    load_tf_basicConv2d(weights[:4], layer.branch0[0])
    load_tf_basicConv2d(weights[4:8], layer.branch0[1])
    load_tf_basicConv2d(weights[8:12], layer.branch1[0])
    load_tf_basicConv2d(weights[12:16], layer.branch1[1])
    load_tf_basicConv2d(weights[16:20], layer.branch2[0])
    load_tf_basicConv2d(weights[20:24], layer.branch2[1])
    load_tf_basicConv2d(weights[24:28], layer.branch2[2])",len(weights) != 28,28,len(weights) != 28,True,100.00000000000004,N/A
"def download_url_to_file(url, dst, hash_prefix=None, progress=True):
    """"""Download object at the given URL to a local path.
    Args:
        url (string): URL of the object to download
        dst (string): Full path where object will be saved, e.g. `/tmp/temporary_file`
        hash_prefix (string, optional): If not None, the SHA256 downloaded file should start with `hash_prefix`.
            Default: None
        progress (bool, optional): whether or not to display a progress bar to stderr
            Default: True
    Example:
        >>> torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')
    """"""
    file_size = None
    req = Request(url, headers={'User-Agent': 'torch.hub'})
    u = urlopen(req)
    meta = u.info()
<mask>:
        content_length = meta.getheaders('Content-Length')
    else:
        content_length = meta.get_all('Content-Length')
    if content_length is not None and len(content_length) > 0:
        file_size = int(content_length[0])
    dst = os.path.expanduser(dst)
    dst_dir = os.path.dirname(dst)
    f = tempfile.NamedTemporaryFile(delete=False, dir=dst_dir)
    try:
        if hash_prefix is not None:
            sha256 = hashlib.sha256()
        with tqdm(total=file_size, disable=not progress, unit='B', unit_scale=True, unit_divisor=1024) as pbar:
            while True:
                buffer = u.read(8192)
                if len(buffer) == 0:
                    break
                f.write(buffer)
                if hash_prefix is not None:
                    sha256.update(buffer)
                pbar.update(len(buffer))
        f.close()
        if hash_prefix is not None:
            digest = sha256.hexdigest()
            if digest[:len(hash_prefix)] != hash_prefix:
                raise RuntimeError('invalid hash value (expected ""{}"", got ""{}"")'.format(hash_prefix, digest))
        shutil.move(f.name, dst)
    finally:
        f.close()
        if os.path.exists(f.name):
            os.remove(f.name)","hasattr(meta, 'getheaders')",180,torch.version.major >= 3,False,0.0,N/A
"def bbreg(boundingbox, reg):
<mask>:
        reg = torch.reshape(reg, (reg.shape[2], reg.shape[3]))
    w = boundingbox[:, 2] - boundingbox[:, 0] + 1
    h = boundingbox[:, 3] - boundingbox[:, 1] + 1
    b1 = boundingbox[:, 0] + reg[:, 0] * w
    b2 = boundingbox[:, 1] + reg[:, 1] * h
    b3 = boundingbox[:, 2] + reg[:, 2] * w
    b4 = boundingbox[:, 3] + reg[:, 3] * h
    boundingbox[:, :4] = torch.stack([b1, b2, b3, b4]).permute(1, 0)
    return boundingbox",reg.shape[1] == 1,73,"not torch.allclose(reg.shape[2], reg.shape[3])",False,13.400825781778892,N/A
"def nms_numpy(boxes, scores, threshold, method):
<mask>:
        return np.empty((0, 3))
    x1 = boxes[:, 0].copy()
    y1 = boxes[:, 1].copy()
    x2 = boxes[:, 2].copy()
    y2 = boxes[:, 3].copy()
    s = scores
    area = (x2 - x1 + 1) * (y2 - y1 + 1)
    I = np.argsort(s)
    pick = np.zeros_like(s, dtype=np.int16)
    counter = 0
    while I.size > 0:
        i = I[-1]
        pick[counter] = i
        counter += 1
        idx = I[0:-1]
        xx1 = np.maximum(x1[i], x1[idx]).copy()
        yy1 = np.maximum(y1[i], y1[idx]).copy()
        xx2 = np.minimum(x2[i], x2[idx]).copy()
        yy2 = np.minimum(y2[i], y2[idx]).copy()
        w = np.maximum(0.0, xx2 - xx1 + 1).copy()
        h = np.maximum(0.0, yy2 - yy1 + 1).copy()
        inter = w * h
        if method == 'Min':
            o = inter / np.minimum(area[i], area[idx])
        else:
            o = inter / (area[i] + area[idx] - inter)
        I = I[np.where(o <= threshold)]
    pick = pick[:counter].copy()
    return pick",boxes.size == 0,134,boxes.size == 0,True,100.00000000000004,N/A
"def batched_nms_numpy(boxes, scores, idxs, threshold, method):
    device = boxes.device
<mask>:
        return torch.empty((0,), dtype=torch.int64, device=device)
    max_coordinate = boxes.max()
    offsets = idxs.to(boxes) * (max_coordinate + 1)
    boxes_for_nms = boxes + offsets[:, None]
    boxes_for_nms = boxes_for_nms.cpu().numpy()
    scores = scores.cpu().numpy()
    keep = nms_numpy(boxes_for_nms, scores, threshold, method)
    return torch.as_tensor(keep, dtype=torch.long, device=device)",boxes.numel() == 0,46,idxs.size() == 0,False,54.10822690539397,N/A
"def crop_resize(img, box, image_size):
<mask>:
        img = img[box[1]:box[3], box[0]:box[2]]
        out = cv2.resize(img, (image_size, image_size), interpolation=cv2.INTER_AREA).copy()
    elif isinstance(img, torch.Tensor):
        img = img[box[1]:box[3], box[0]:box[2]]
        out = imresample(img.permute(2, 0, 1).unsqueeze(0).float(), (image_size, image_size)).byte().squeeze(0).permute(1, 2, 0)
    else:
        out = img.crop(box).copy().resize((image_size, image_size), Image.BILINEAR)
    return out","isinstance(img, np.ndarray)",39,"isinstance(img, cv2.Tensor)",False,38.260294162784454,N/A
"def __init__(self, mode, length, calculate_mean=False):
    self.mode = mode
    self.length = length
    self.calculate_mean = calculate_mean
<mask>:
        self.fn = lambda x, i: x / (i + 1)
    else:
        self.fn = lambda x, i: x",self.calculate_mean,32,self.mode == 'F',False,16.233395773754953,N/A
"def __call__(self, loss, metrics, i):
    track_str = '\r{} | {:5d}/{:<5d}| '.format(self.mode, i + 1, self.length)
    loss_str = 'loss: {:9.4f} | '.format(self.fn(loss, i))
    metric_str = ' | '.join(('{}: {:9.4f}'.format(k, self.fn(v, i)) for k, v in metrics.items()))
    print(track_str + loss_str + metric_str + '   ', end='')
<mask>:
        print('')",i + 1 == self.length,46,self.verbose,False,10.394224994345743,N/A
"def __call__(self, y_pred, y):
    self.end = time.time()
    elapsed = self.end - self.start
    self.start = self.end
    self.end = None
<mask>:
        elapsed /= len(y_pred)
    if self.rate:
        elapsed = 1 / elapsed
    return torch.tensor(elapsed)",self.per_sample,31,y_pred,False,14.127216461522432,N/A
"def decode_audio_url(string, user_id):
    vals = string.split('?extra=', 1)[1].split('#')
    tstr = vk_o(vals[0])
    ops_list = vk_o(vals[1]).split('\t')[::-1]
    for op_data in ops_list:
        split_op_data = op_data.split('\x0b')
        cmd = split_op_data[0]
        arg = split_op_data[1] if len(split_op_data) > 1 else None
<mask>:
            tstr = tstr[::-1]
        elif cmd == 'r':
            tstr = vk_r(tstr, arg)
        elif cmd == 'x':
            tstr = vk_xor(tstr, arg)
        elif cmd == 's':
            tstr = vk_s(tstr, arg)
        elif cmd == 'i':
            tstr = vk_i(tstr, arg, user_id)
        else:
            raise VkAudioUrlDecodeError(f'Unknown decode cmd: ""{cmd}""; Please send bugreport')
    return tstr",cmd == 'v',80,cmd == 'n',False,59.460355750136046,N/A
"def vk_o(string):
    result = []
    index2 = 0
    for s in string:
        sym_index = VK_STR.find(s)
<mask>:
            if index2 % 4 != 0:
                index2 += 1
                i = (i << 6) + sym_index
                result += [chr(255 & i >> (-2 * index2 & 6))]
            else:
                i = sym_index
                index2 += 1
    return ''.join(result)",sym_index != -1,52,sym_index != -1,True,100.00000000000004,N/A
"def vk_r(string, i):
    vk_str2 = VK_STR + VK_STR
    vk_str2_len = len(vk_str2)
    result = []
    for s in string:
        index = vk_str2.find(s)
<mask>:
            offset = index - int(i)
            if offset < 0:
                offset += vk_str2_len
            result += [vk_str2[offset]]
        else:
            result += [s]
    return ''.join(result)",index != -1,43,index >= 0,False,18.99589214128981,N/A
"def vk_s_child(t, e):
    i = len(t)
<mask>:
        return []
    o = []
    e = int(e)
    for a in range(i - 1, -1, -1):
        e = (i * (a + 1) ^ e + a) % i
        o.append(e)
    return o[::-1]",not i,39,i == 0,False,15.97357760615681,N/A
"def vk_s(t, e):
    i = len(t)
<mask>:
        return t
    o = vk_s_child(t, e)
    t = list(t)
    for a in range(1, i):
        t, y = splice(t, o[i - 1 - a], 1, t[a])
        t[a] = y[0]
    return ''.join(t)",not i,37,i == 0,False,15.97357760615681,N/A
"def __init__(self, vk):
<mask>:
        raise TypeError('The arg should be VkApi or VkApiMethod instance')
    self.vk = vk if isinstance(vk, VkApiMethod) else vk.get_api()
    self.http = requests.Session()
    self.http.headers.pop('user-agent')","not isinstance(vk, (VkApi, VkApiMethod))",25,"not isinstance(vk, VkApiMethod)",False,44.910379243707105,N/A
"def photo(self, photos, album_id, latitude=None, longitude=None, caption=None, description=None, group_id=None):
    """""" Загрузка изображений в альбом пользователя

        :param photos: путь к изображению(ям) или file-like объект(ы)
        :type photos: str or list

        :param album_id: идентификатор альбома
        :param latitude: географическая широта, заданная в градусах
                            (от -90 до 90)
        :param longitude: географическая долгота, заданная в градусах
                            (от -180 до 180)
        :param caption: текст описания изображения
        :param description: текст описания альбома
        :param group_id: идентификатор сообщества (если загрузка идет в группу)
        """"""
    values = {'album_id': album_id}
<mask>:
        values['group_id'] = group_id
    url = self.vk.photos.getUploadServer(**values)['upload_url']
    with FilesOpener(photos) as photo_files:
        response = self.http.post(url, files=photo_files).json()
    if 'album_id' not in response:
        response['album_id'] = response['aid']
    response.update({'latitude': latitude, 'longitude': longitude, 'caption': caption, 'description': description})
    values.update(response)
    return self.vk.photos.save(**values)",group_id,112,album_id is None,False,21.3643503198117,N/A
"def photo_profile(self, photo, owner_id=None, crop_x=None, crop_y=None, crop_width=None):
    """""" Загрузка изображения профиля

        :param photo: путь к изображению или file-like объект
        :param owner_id: идентификатор сообщества или текущего пользователя.
                По умолчанию загрузка идет в профиль текущего пользователя.
                При отрицательном значении загрузка идет в группу.
        :param crop_x: координата X верхнего правого угла миниатюры.
        :param crop_y: координата Y верхнего правого угла миниатюры.
        :param crop_width: сторона квадрата миниатюры.
                При передаче всех crop_* для фотографии также будет
                подготовлена квадратная миниатюра.
        """"""
    values = {}
<mask>:
        values['owner_id'] = owner_id
    crop_params = {}
    if crop_x is not None and crop_y is not None and (crop_width is not None):
        crop_params['_square_crop'] = f'{crop_x},{crop_y},{crop_width}'
    response = self.vk.photos.getOwnerPhotoUploadServer(**values)
    url = response['upload_url']
    with FilesOpener(photo, key_format='file') as photo_files:
        response = self.http.post(url, data=crop_params, files=photo_files)
    return self.vk.photos.saveOwnerPhoto(**response.json())",owner_id,120,photo.strip(),False,0.0,N/A
"def photo_wall(self, photos, user_id=None, group_id=None, caption=None):
    """""" Загрузка изображений на стену пользователя или в группу

        :param photos: путь к изображению(ям) или file-like объект(ы)
        :type photos: str or list

        :param user_id: идентификатор пользователя
        :param group_id: идентификатор сообщества (если загрузка идет в группу)
        :param caption: текст описания фотографии.
        """"""
    values = {}
<mask>:
        values['user_id'] = user_id
    elif group_id:
        values['group_id'] = group_id
    if caption:
        values['caption'] = caption
    response = self.vk.photos.getWallUploadServer(**values)
    url = response['upload_url']
    with FilesOpener(photos) as photos_files:
        response = self.http.post(url, files=photos_files)
    values.update(response.json())
    return self.vk.photos.saveWallPhoto(**values)",user_id,81,user_id,True,100.00000000000004,N/A
"def photo_market(self, photo, group_id, main_photo=False, crop_x=None, crop_y=None, crop_width=None):
    """""" Загрузка изображений для товаров в магазине

        :param photo: путь к изображению(ям) или file-like объект(ы)
        :type photo: str or list

        :param group_id: идентификатор сообщества, для которого необходимо загрузить фотографию товара
        :type group_id: int
        :param main_photo: является ли фотография обложкой товара
        :type main_photo: bool
        :param crop_x: координата x для обрезки фотографии (верхний правый угол)
        :type crop_x: int
        :param crop_y: координата y для обрезки фотографии (верхний правый угол)
        :type crop_y: int
        :param crop_width: ширина фотографии после обрезки в px
        :type crop_width: int
        """"""
<mask>:
        group_id = abs(group_id)
    values = {'main_photo': main_photo, 'group_id': group_id}
    if crop_x is not None:
        values['crop_x'] = crop_x
    if crop_y is not None:
        values['crop_y'] = crop_y
    if crop_width is not None:
        values['crop_width'] = crop_width
    response = self.vk.photos.getMarketUploadServer(**values)
    url = response['upload_url']
    with FilesOpener(photo) as photos_files:
        response = self.http.post(url, files=photos_files)
    values.update(response.json())
    return self.vk.photos.saveMarketPhoto(**values)",group_id < 0,140,main_photo,False,14.127216461522432,N/A
"def __init__(self, session: requests.Session, api_version: str='5.207') -> None:
    set_cookies_from_list(session.cookies, self.DEFAULT_COOKIES)
    response = session.get(self.SIGNIN_URL)
    pattern = re.compile('window\\.init\\s*=\\s*({.*?});', re.DOTALL)
    json_config = search_re(pattern, response.text)
<mask>:
        raise AuthError('Failed to get the value of variable window.init.')
    self._config = json.loads(json_config)
    self.sid = ''
    self.can_skip_password = False
    self.device_id = generate_device_id()
    self.api_version = api_version",json_config is None,46,json_config is None,True,100.00000000000004,N/A
"def __init__(self, login=None, password=None, token=None, auth_handler=None, captcha_handler=None, config=jconfig.Config, config_filename='vk_config.v2.json', api_version='5.92', app_id=6222115, scope=DEFAULT_USER_SCOPE, client_secret=None, session=None):
    self.login = login
    self.password = password
    self.token = {'access_token': token}
    self.api_version = api_version
    self.app_id = app_id
    self.scope = scope
    self.client_secret = client_secret
    self.storage = config(self.login, filename=config_filename)
    self.http = session or requests.Session()
<mask>:
        self.http.headers['User-agent'] = DEFAULT_USERAGENT
    self.last_request = 0.0
    self.error_handlers = {NEED_VALIDATION_CODE: self.need_validation_handler, CAPTCHA_ERROR_CODE: captcha_handler or self.captcha_handler, TOO_MANY_RPS_CODE: self.too_many_rps_handler, TWOFACTOR_CODE: auth_handler or self.auth_handler}
    self.lock = threading.Lock()
    self.logger = logging.getLogger('vk_api')",not session,72,not self.http.headers.get('User-agent'),False,3.7477767366779213,N/A
"def auth(self, reauth=False, token_only=False):
    """""" Аутентификация

        :param reauth: Позволяет переавторизоваться, игнорируя сохраненные
            куки и токен

        :param token_only: Включает оптимальную стратегию аутентификации, если
            необходим только access_token

            Например если сохраненные куки не валидны,
            но токен валиден, то аутентификация пройдет успешно

            При token_only=False, сначала проверяется
            валидность куки. Если кука не будет валидна, то
            будет произведена попытка аутетификации с паролем.
            Тогда если пароль не верен или пароль не передан,
            то аутентификация закончится с ошибкой.

            Если вы не делаете запросы к веб версии сайта
            используя куки, то лучше использовать
            token_only=True
        """"""
<mask>:
        raise LoginRequired('Login is required to auth')
    self.logger.info(f'Auth with login: {self.login}')
    set_cookies_from_list(self.http.cookies, self.storage.setdefault('cookies', []))
    self.token = self.storage.setdefault('token', {}).setdefault(f'app{str(self.app_id)}', {}).get(f'scope_{str(self.scope)}')
    if token_only:
        self._auth_token(reauth=reauth)
    else:
        self._auth_cookies(reauth=reauth)",not self.login,110,token_only,False,0.0,N/A
"def _auth_cookies(self, reauth=False):
<mask>:
        self.logger.info('Auth forced')
        self.storage.clear_section()
        self._vk_login()
        self._api_login()
        return
    if not self.check_sid():
        self.logger.info(f'remixsid from config is not valid: {self._sid}')
        self._vk_login()
    else:
        self._pass_security_check()
    if not self._check_token():
        self.logger.info(f'access_token from config is not valid: {self.token}')
        self._api_login()
    else:
        self.logger.info('access_token from config is valid')",reauth,40,reauth,True,100.00000000000004,N/A
"def _auth_token(self, reauth=False):
<mask>:
        self.logger.info('access_token from config is valid')
        return
    if reauth:
        self.logger.info('Auth (API) forced')
    if self.check_sid():
        self._pass_security_check()
        self._api_login()
    elif self.password:
        self._vk_login()
        self._api_login()",not reauth and self._check_token(),23,not self.config.get('access_token'),False,9.864703138979419,N/A
"def _check_challenge(self, response):
<mask>:
        return response
    hash429 = urllib.parse.parse_qs(response.url.split('?', 1)[-1])['hash429'][0]
    salt = re.search(""salt\\s*=\\s*'(.*)'"", response.text).group(1)
    hash429_md5 = md5(hash429.encode('ascii') + b':' + salt.encode('ascii')).hexdigest()
    response = self.http.get(f'{response.url}&key={hash429_md5}')
    return response",not response.url.startswith('https://vk.com/challenge.html?'),26,response.status_code != 200,False,2.1748491974013704,N/A
"def get_all_iter(self, method, max_count, values=None, key='items', limit=None, stop_fn=None, negative_offset=False):
    """""" Получить все элементы.

        Работает в методах, где в ответе есть count и items или users.
        За один запрос получает max_count * 25 элементов

        :param method: имя метода
        :type method: str

        :param max_count: максимальное количество элементов, которое можно
                          получить за один запрос
        :type max_count: int

        :param values: параметры
        :type values: dict

        :param key: ключ элементов, которые нужно получить
        :type key: str

        :param limit: ограничение на количество получаемых элементов,
                            но может прийти больше
        :type limit: int

        :param stop_fn: функция, отвечающая за выход из цикла
        :type stop_fn: func

        :param negative_offset: True если offset должен быть отрицательный
        :type negative_offset: bool
        """"""
    values = values.copy() if values else {}
    values['count'] = max_count
    offset = max_count if negative_offset else 0
    items_count = 0
    count = None
    while True:
        response = vk_get_all_items(self.vk, method, key, values, count, offset, offset_mul=-1 if negative_offset else 1)
<mask>:
            raise VkToolsException(f""Could not load items: {response['execute_errors']}"", response=response)
        response = response['response']
        items = response['items']
        items_count += len(items)
        yield from items
        if not response['more']:
            break
        if limit and items_count >= limit:
            break
        if stop_fn and stop_fn(items):
            break
        count = response['count']
        offset = response['offset']",'execute_errors' in response,187,not self.is_iter_initialized(),False,4.196114906296549,N/A
"def get_all_slow_iter(self, method, max_count, values=None, key='items', limit=None, stop_fn=None, negative_offset=False):
    """""" Получить все элементы (без использования execute)

        Работает в методах, где в ответе есть count и items или users

        :param method: имя метода
        :type method: str

        :param max_count: максимальное количество элементов, которое можно
                          получить за один запрос
        :type max_count: int

        :param values: параметры
        :type values: dict

        :param key: ключ элементов, которые нужно получить
        :type key: str

        :param limit: ограничение на количество получаемых элементов,
                            но может прийти больше
        :type limit: int

        :param stop_fn: функция, отвечающая за выход из цикла
        :type stop_fn: func

        :param negative_offset: True если offset должен быть отрицательный
        :type negative_offset: bool
        """"""
    values = values.copy() if values else {}
    values['count'] = max_count
    offset_mul = -1 if negative_offset else 1
    offset = max_count if negative_offset else 0
    count = None
    items_count = 0
    while count is None or offset < count:
        values['offset'] = offset * offset_mul
        response = self.vk.method(method, values)
        new_count = response['count']
        count_diff = new_count - count if count is not None else 0
<mask>:
            offset += count_diff
            count = new_count
            continue
        response_items = response[key]
        items = response_items[count_diff:]
        items_count += len(items)
        yield from items
        if len(response_items) < max_count - count_diff:
            break
        if limit and items_count >= limit:
            break
        if stop_fn and stop_fn(items):
            break
        offset += max_count
        count = new_count",count_diff < 0,209,not self.is_slow,False,8.116697886877475,N/A
"def compile(self, args):
    compiled_args = {}
    for key, value in args.items():
<mask>:
            compiled_args[key] = str(value)
        else:
            compiled_args[key] = sjson_dumps(value)
    return self._minified_code % compiled_args",key in self.clean_args,23,"isinstance(value, str)",False,0.0,N/A
"def __call__(self, vk, *args, **kwargs):
    """"""
        :param vk: VkApi или VkApiMethod
        :param *args:
        :param **kwargs:
        """"""
<mask>:
        raise TypeError('The first arg should be VkApi or VkApiMethod instance')
    if isinstance(vk, VkApiMethod):
        vk = vk._vk
    args = parse_args(self.args, args, kwargs)
    return vk.method('execute', {'code': self.compile(args)}, raw=self.return_raw)","not isinstance(vk, (VkApi, VkApiMethod))",43,"not isinstance(vk, VkApi)",False,37.764976913718144,N/A
"def parse_args(function_args, args, kwargs):
    parsed_args = {}
    for arg_name in kwargs.keys():
<mask>:
            parsed_args[arg_name] = kwargs[arg_name]
        else:
            raise VkFunctionException(f""function got an unexpected keyword argument '{arg_name}'"")
    args_count = len(args) + len(kwargs)
    func_args_count = len(function_args)
    if args_count != func_args_count:
        raise VkFunctionException(f""function takes exactly {func_args_count} argument{('s' if func_args_count > 1 else '')} ({args_count} given)"")
    for arg_name, arg_value in zip(function_args, args):
        parsed_args[arg_name] = arg_value
    return parsed_args",arg_name in function_args,61,arg_name in args and kwargs[arg_name],False,23.462350320527996,N/A
"@property
def result(self):
    """"""Результат запроса, если он прошёл успешно.""""""
<mask>:
        raise RuntimeError('Result is not available in `with` context')
    if self._error:
        raise VkRequestsPoolException(self._error, f""Got error while executing request: [{self.error['error_code']}] {self.error['error_msg']}"")
    return self._result",not self.ready,31,self._result is None,False,16.233395773754953,N/A
"def method(self, method, values=None):
    """""" Добавляет запрос в пул.
            Возвращаемое значение будет содержать результат после закрытия пула.

        :param method: метод
        :type method: str

        :param values: параметры
        :type values: dict

        :rtype: RequestResult
        """"""
<mask>:
        values = {}
    result = RequestResult()
    self.pool.append(PoolRequest(method, values, result))
    return result",values is None,44,values is None,True,100.00000000000004,N/A
"def execute(self):
    """"""
        Выполняет все находящиеся в пуле запросы и отчищает пул.
        Необходим для использования пула-объекта.
        Для пула менеджера контекста вызывается автоматически.
        """"""
    for i in range(0, len(self.pool), 25):
        cur_pool = self.pool[i:i + 25]
        one_method = check_one_method(cur_pool)
<mask>:
            value_list = [i.values for i in cur_pool]
            response_raw = vk_one_method(self.vk_session, one_method, value_list)
        else:
            response_raw = vk_many_methods(self.vk_session, cur_pool)
        response = response_raw['response']
        response_errors = response_raw.get('execute_errors', [])
        response_errors_iter = iter(response_errors)
        for x, current_response in enumerate(response):
            current_result = cur_pool[x].result
            if current_response is not False:
                current_result.result = current_response
            else:
                current_result.error = next(response_errors_iter)
    self.pool = []",one_method,88,one_method,True,100.00000000000004,N/A
"def check_one_method(pool):
    """""" Возвращает True, если все запросы в пуле к одному методу """"""
<mask>:
        return False
    first_method = pool[0].method
    if all((req.method == first_method for req in pool[1:])):
        return first_method
    return False",not pool,32,len(pool) == 0,False,6.567274736060395,N/A
"def vk_request_one_param_pool(vk_session, method, key, values, default_values=None):
    """""" Использовать, если изменяется значение только одного параметра.
        Возвращаемое значение содержит tuple из dict с результатами и
        dict с ошибками при выполнении

    :param vk_session: объект VkApi
    :type vk_session: vk_api.VkAPi

    :param method: метод
    :type method: str

    :param default_values: одинаковые значения для запросов
    :type default_values: dict

    :param key: ключ изменяющегося параметра
    :type key: str

    :param values: список значений изменяющегося параметра (max: 25)
    :type values: list

    :rtype: (dict, dict)
    """"""
    result = {}
    errors = {}
<mask>:
        default_values = {}
    for i in range(0, len(values), 25):
        current_values = values[i:i + 25]
        response_raw = vk_one_param(vk_session, method, current_values, default_values, key)
        response = response_raw['response']
        response_errors = response_raw.get('execute_errors', [])
        response_errors_iter = iter(response_errors)
        for x, r in enumerate(response):
            if r is not False:
                result[current_values[x]] = r
            else:
                errors[current_values[x]] = next(response_errors_iter)
    return (result, errors)",default_values is None,131,values is None,False,51.341711903259224,N/A
"def search_re(reg, string):
    """""" Поиск по регулярке """"""
    s = reg.search(string)
<mask>:
        groups = s.groups()
        return groups[0]",s,17,s,True,100.00000000000004,N/A
"def code_from_number(prefix, postfix, number):
    prefix_len = len(prefix)
    postfix_len = len(postfix)
<mask>:
        number = number[1:]
    if prefix_len + postfix_len >= len(number):
        return
    if number[:prefix_len] != prefix:
        return
    if number[-postfix_len:] != postfix:
        return
    return number[prefix_len:-postfix_len]",number[0] == '+',33,number[0] == '.',False,75.06238537503395,N/A
"def enable_debug_mode(vk_session, print_content=False):
    """""" Включает режим отладки:
        - Вывод сообщений лога
        - Вывод http запросов

    :param vk_session: объект VkApi
    :param print_content: печатать ответ http запросов
    """"""
    import logging
    import sys
    import time
    import requests
    from . import __version__
    pypi_version = requests.get('https://pypi.org/pypi/vk_api/json').json()['info']['version']
<mask>:
        print()
        print('######### MODULE IS NOT UPDATED!!1 ##########')
        print()
        print('Installed vk_api version is:', __version__)
        print('PyPI vk_api version is:', pypi_version)
        print()
        print('######### MODULE IS NOT UPDATED!!1 ##########')
        print()

    class DebugHTTPAdapter(requests.adapters.HTTPAdapter):

        def send(self, request, **kwargs):
            start = time.time()
            response = super(DebugHTTPAdapter, self).send(request, **kwargs)
            end = time.time()
            total = end - start
            body = request.body
            if body and len(body) > 1024:
                body = f'{body[:1024]}[STRIPPED]'
            print('{:0.2f} {} {} {} {} {} {}'.format(total, request.method, request.url, request.headers, repr(body), response.status_code, response.history))
            if print_content:
                print(response.text)
            return response
    vk_session.http.mount('http://', DebugHTTPAdapter())
    vk_session.http.mount('https://', DebugHTTPAdapter())
    vk_session.logger.setLevel(logging.INFO)
    vk_session.logger.addHandler(logging.StreamHandler(sys.stdout))",__version__ != pypi_version,126,print_content,False,2.6682865255867765,N/A
"def __init__(self, raw):
    self.raw = raw
    self.from_user = False
    self.from_chat = False
    self.from_group = False
    self.from_me = False
    self.to_me = False
    self.attachments = {}
    self.message_data = None
    self.message_id = None
    self.timestamp = None
    self.peer_id = None
    self.flags = None
    self.extra = None
    self.extra_values = None
    self.type_id = None
    try:
        self.type = VkEventType(self.raw[0])
        self._list_to_attr(self.raw[1:], EVENT_ATTRS_MAPPING[self.type])
    except ValueError:
        self.type = self.raw[0]
<mask>:
        self._dict_to_attr(self.extra_values)
    if self.type in PARSE_PEER_ID_EVENTS:
        self._parse_peer_id()
    if self.type in PARSE_MESSAGE_FLAGS_EVENTS:
        self._parse_message_flags()
    if self.type is VkEventType.CHAT_UPDATE:
        self._parse_chat_info()
        try:
            self.update_type = VkChatEventType(self.type_id)
        except ValueError:
            self.update_type = self.type_id
    elif self.type is VkEventType.NOTIFICATION_SETTINGS_UPDATE:
        self._dict_to_attr(self.values)
        self._parse_peer_id()
    elif self.type is VkEventType.PEER_FLAGS_REPLACE:
        self._parse_peer_flags()
    elif self.type in [VkEventType.MESSAGE_NEW, VkEventType.MESSAGE_EDIT]:
        self._parse_message()
    elif self.type in [VkEventType.USER_ONLINE, VkEventType.USER_OFFLINE]:
        self.user_id = abs(self.user_id)
        self._parse_online_status()
    elif self.type is VkEventType.USER_RECORDING_VOICE:
        if isinstance(self.user_id, list):
            self.user_id = self.user_id[0]
    if self.timestamp:
        self.datetime = datetime.utcfromtimestamp(self.timestamp)",self.extra_values,126,self.type in PARSE_EXTRA_VALUES,False,10.552670315936318,N/A
"def _parse_peer_id(self):
<mask>:
        self.from_group = True
        self.group_id = abs(self.peer_id)
    elif self.peer_id > CHAT_START_ID:
        self.from_chat = True
        self.chat_id = self.peer_id - CHAT_START_ID
        if self.extra_values and 'from' in self.extra_values:
            self.user_id = int(self.extra_values['from'])
    else:
        self.from_user = True
        self.user_id = self.peer_id",self.peer_id < 0,37,self.peer_id < CHAT_START_ID,False,46.17366309441026,N/A
"def _parse_message(self):
<mask>:
        if self.flags & VkMessageFlag.OUTBOX:
            self.from_me = True
        else:
            self.to_me = True
    self.text = self.text.replace('<br>', '\n')
    self.message = self.text.replace('&lt;', '<').replace('&gt;', '>').replace('&quot;', '""').replace('&amp;', '&')",self.type is VkEventType.MESSAGE_NEW,25,self.flags & VkMessageFlag.INBOX,False,10.923299908191149,N/A
"def _parse_online_status(self):
    try:
<mask>:
            self.platform = VkPlatform(self.extra & 255)
        elif self.type is VkEventType.USER_OFFLINE:
            self.offline_type = VkOfflineType(self.flags)
    except ValueError:
        pass",self.type is VkEventType.USER_ONLINE,19,self.type is VkEventType.USER_PLATFORM,False,86.33400213704509,N/A
"def _parse_chat_info(self):
<mask>:
        self.info = {'admin_id': self.info}
    elif self.type_id == VkChatEventType.MESSAGE_PINNED.value:
        self.info = {'conversation_message_id': self.info}
    elif self.type_id in [VkChatEventType.USER_JOINED.value, VkChatEventType.USER_LEFT.value, VkChatEventType.USER_KICKED.value, VkChatEventType.ADMIN_REMOVED.value]:
        self.info = {'user_id': self.info}",self.type_id == VkChatEventType.ADMIN_ADDED.value,26,self.type_id == VkChatEventType.ADMIN_ID.value,False,80.91067115702207,N/A
"def __init__(self, raw):
    super(VkBotMessageEvent, self).__init__(raw)
    self.from_user = False
    self.from_chat = False
    self.from_group = False
    self.chat_id = None
    peer_id = self.obj.peer_id or self.message.peer_id
<mask>:
        self.from_group = True
    elif peer_id < CHAT_START_ID:
        self.from_user = True
    else:
        self.from_chat = True
        self.chat_id = peer_id - CHAT_START_ID",peer_id < 0,42,peer_id > CHAT_START_ID,False,17.747405280050266,N/A
"def update_longpoll_server(self, update_ts=True):
    values = {'group_id': self.group_id}
    response = self.vk.method('groups.getLongPollServer', values)
    self.key = response['key']
    self.server = response['server']
    self.url = self.server
<mask>:
        self.ts = response['ts']",update_ts,24,update_ts,True,100.00000000000004,N/A
"def check(self):
    """""" Получить события от сервера один раз

        :returns: `list` of :class:`Event`
        """"""
    values = {'act': 'a_check', 'key': self.key, 'ts': self.ts, 'wait': self.wait}
    response = self.session.get(self.url, params=values, timeout=self.wait + 10).json()
<mask>:
        self.ts = response['ts']
        return [self._parse_event(raw_event) for raw_event in response['updates']]
    elif response['failed'] == 1:
        self.ts = response['ts']
    elif response['failed'] == 2:
        self.update_longpoll_server(update_ts=False)
    elif response['failed'] == 3:
        self.update_longpoll_server()
    return []",'failed' not in response,60,response['failed'] == 0,False,7.809849842300637,N/A
"def __str__(self):
<mask>:
        return f'Security check. Enter number: {self.phone_prefix} ... {self.phone_postfix}'
    else:
        return 'Security check. Phone prefix and postfix are not detected. Please send bugreport (response in self.response)'",self.phone_prefix and self.phone_postfix,28,self.enter,False,3.8238216823301503,N/A
"def get_url(self):
    """""" Получить ссылку на изображение капчи """"""
<mask>:
        self.url = f'https://api.vk.com/captcha.php?sid={self.sid}'
    return self.url",not self.url,15,not self.url,True,100.00000000000004,N/A
"def get_image(self):
    """""" Получить изображение капчи (jpg) """"""
<mask>:
        self.image = self.vk.http.get(self.get_url()).content
    return self.image",not self.image,14,not self.image,True,100.00000000000004,N/A
"def try_again(self, key=None):
    """""" Отправить запрос заново с ответом капчи

        :param key: ответ капчи
        """"""
<mask>:
        self.key = key
        self.kwargs.update({'captcha_sid': self.sid, 'captcha_key': self.key})
    return self.func(*self.args, **self.kwargs)",key,26,key,True,100.00000000000004,N/A
"def get_iter(self, owner_id=None, album_id=None, access_hash=None):
    """""" Получить список аудиозаписей пользователя (по частям)

        :param owner_id: ID владельца (отрицательные значения для групп)
        :param album_id: ID альбома
        :param access_hash: ACCESS_HASH альбома
        """"""
<mask>:
        owner_id = self.user_id
    if album_id is not None:
        offset_diff = TRACKS_PER_ALBUM_PAGE
    else:
        offset_diff = TRACKS_PER_USER_PAGE
    offset = 0
    while True:
        response = self._vk.http.post('https://m.vk.com/audio', data={'act': 'load_section', 'owner_id': owner_id, 'playlist_id': album_id if album_id else -1, 'offset': offset, 'type': 'playlist', 'access_hash': access_hash, 'is_loading_all': 1}, headers={'X-Requested-With': 'XMLHttpRequest'}, allow_redirects=False).json()
        if not response['data'][0]:
            raise AccessDenied(f""You don't have permissions to browse {owner_id}'s albums"")
        ids = scrap_ids(response['data'][0]['list'])
        if not ids:
            break
        yield from scrap_tracks(ids, self.user_id, self._vk.http, convert_m3u8_links=self.convert_m3u8_links)
        if response['data'][0]['hasMore']:
            offset += offset_diff
        else:
            break",owner_id is None,106,owner_id is None,True,100.00000000000004,N/A
"def get_albums_iter(self, owner_id=None):
    """""" Получить список альбомов пользователя (по частям)

        :param owner_id: ID владельца (отрицательные значения для групп)
        """"""
<mask>:
        owner_id = self.user_id
    offset = 0
    while True:
        response = self._vk.http.get(f'https://m.vk.com/audio?act=audio_playlists{owner_id}', params={'offset': offset}, allow_redirects=False)
        if not response.text:
            raise AccessDenied(f""You don't have permissions to browse {owner_id}'s albums"")
        albums = scrap_albums(response.text)
        if not albums:
            break
        yield from albums
        offset += ALBUMS_PER_USER_PAGE",owner_id is None,59,owner_id is None,True,100.00000000000004,N/A
"def search_user(self, owner_id=None, q=''):
    """""" Искать по аудиозаписям пользователя

        :param owner_id: ID владельца (отрицательные значения для групп)
        :param q: запрос
        """"""
<mask>:
        owner_id = self.user_id
    response = self._vk.http.post('https://vk.com/al_audio.php', data={'al': 1, 'act': 'section', 'claim': 0, 'is_layer': 0, 'owner_id': owner_id, 'section': 'search', 'q': q})
    json_response = json.loads(response.text.replace('<!--', ''))
    if not json_response['payload'][1]:
        raise AccessDenied(f""You don't have permissions to browse {owner_id}'s audio"")
    if json_response['payload'][1][1]['playlists']:
        ids = scrap_ids(json_response['payload'][1][1]['playlists'][0]['list'])
        tracks = scrap_tracks(ids, self.user_id, self._vk.http, convert_m3u8_links=self.convert_m3u8_links)
        return list(tracks)
    else:
        return []",owner_id is None,74,owner_id is None,True,100.00000000000004,N/A
"def search_iter(self, q, offset=0):
    """""" Искать аудиозаписи (генератор)

        :param q: запрос
        :param offset: смещение
        """"""
    offset_left = 0
    response = self._vk.http.post('https://vk.com/al_audio.php', data={'al': 1, 'act': 'section', 'claim': 0, 'is_layer': 0, 'owner_id': self.user_id, 'section': 'search', 'q': q})
    json_response = json.loads(response.text.replace('<!--', ''))
    while json_response['payload'][1][1]['playlist']:
        ids = scrap_ids(json_response['payload'][1][1]['playlist']['list'])
<mask>:
            break
        if offset_left + len(ids) >= offset:
            if offset_left < offset:
                ids = ids[offset - offset_left:]
            yield from scrap_tracks(ids, self.user_id, convert_m3u8_links=self.convert_m3u8_links, http=self._vk.http)
        offset_left += len(ids)
        response = self._vk.http.post('https://vk.com/al_audio.php', data={'al': 1, 'act': 'load_catalog_section', 'section_id': json_response['payload'][1][1]['sectionId'], 'start_from': json_response['payload'][1][1]['nextFrom']})
        json_response = json.loads(response.text.replace('<!--', ''))",not ids,85,not ids,True,100.00000000000004,N/A
"def get_updates_iter(self):
    """""" Искать обновления друзей (генератор) """"""
    response = self._vk.http.post('https://vk.com/al_audio.php', data={'al': 1, 'act': 'section', 'claim': 0, 'is_layer': 0, 'owner_id': self.user_id, 'section': 'updates'})
    json_response = json.loads(response.text.replace('<!--', ''))
    while True:
        updates = [i['list'] for i in json_response['payload'][1][1]['playlists']]
        ids = scrap_ids([i[0] for i in updates if i])
<mask>:
            break
        yield from scrap_tracks(ids, self.user_id, convert_m3u8_links=self.convert_m3u8_links, http=self._vk.http)
        if len(updates) < 11:
            break
        response = self._vk.http.post('https://vk.com/al_audio.php', data={'al': 1, 'act': 'load_catalog_section', 'section_id': json_response['payload'][1][1]['sectionId'], 'start_from': json_response['payload'][1][1]['nextFrom']})
        json_response = json.loads(response.text.replace('<!--', ''))",not ids,73,not ids,True,100.00000000000004,N/A
"def get_rules(self):
    """""" Получить список добавленных правил """"""
    response = self.vk.http.get(self.URL_TEMPLATE.format(schema='https', server=self.server, method='rules', key=self.key)).json()
<mask>:
        return response['rules'] or []
    elif response['code'] == 400:
        raise VkStreamingError(response['error'])",response['code'] == 200,25,response['code'] == 200,True,100.00000000000004,N/A
"def add_rule(self, value, tag):
    """""" Добавить правило

        :param value: Строковое представление правила
        :type value: str

        :param tag: Тег правила
        :type tag: str
        """"""
    response = self.vk.http.post(self.URL_TEMPLATE.format(schema='https', server=self.server, method='rules', key=self.key), json={'rule': {'value': value, 'tag': tag}}).json()
<mask>:
        return True
    elif response['code'] == 400:
        raise VkStreamingError(response['error'])",response['code'] == 200,43,response['code'] == 200,True,100.00000000000004,N/A
"def delete_rule(self, tag):
    """""" Удалить правило

        :param tag: Тег правила
        :type tag: str
        """"""
    response = self.vk.http.delete(self.URL_TEMPLATE.format(schema='https', server=self.server, method='rules', key=self.key), json={'tag': tag}).json()
<mask>:
        return True
    elif response['code'] == 400:
        raise VkStreamingError(response['error'])",response['code'] == 200,31,response['code'] == 200,True,100.00000000000004,N/A
"def listen(self):
    """""" Слушать сервер """"""
    ws = websocket.create_connection(self.URL_TEMPLATE.format(schema='wss', server=self.server, method='stream', key=self.key))
    while True:
        response = json.loads(ws.recv())
<mask>:
            yield response['event']
        elif response['code'] == 300:
            raise VkStreamingServiceMessage(response['service_message'])",response['code'] == 100,26,response['code'] == 200,False,80.91067115702207,N/A
"def add_button(self, label, color=VkKeyboardColor.SECONDARY, payload=None):
    """""" Добавить кнопку с текстом.
            Максимальное количество кнопок на строке - MAX_BUTTONS_ON_LINE

        :param label: Надпись на кнопке и текст, отправляющийся при её нажатии.
        :type label: str
        :param color: цвет кнопки.
        :type color: VkKeyboardColor or str
        :param payload: Параметр для callback api
        :type payload: str or list or dict
        """"""
    current_line = self.lines[-1]
<mask>:
        raise ValueError(f'Max {MAX_BUTTONS_ON_LINE} buttons on a line')
    color_value = color
    if isinstance(color_value, VkKeyboardColor):
        color_value = color_value.value
    if payload is not None and (not isinstance(payload, str)):
        payload = sjson_dumps(payload)
    button_type = VkKeyboardButton.TEXT.value
    current_line.append({'color': color_value, 'action': {'type': button_type, 'payload': payload, 'label': label}})",len(current_line) >= MAX_BUTTONS_ON_LINE,98,len(current_line) > MAX_BUTTONS_ON_LINE,False,80.52253761904356,N/A
"def add_callback_button(self, label, color=VkKeyboardColor.SECONDARY, payload=None):
    """""" Добавить callback-кнопку с текстом.
            Максимальное количество кнопок на строке - MAX_BUTTONS_ON_LINE

        :param label: Надпись на кнопке и текст, отправляющийся при её нажатии.
        :type label: str
        :param color: цвет кнопки.
        :type color: VkKeyboardColor or str
        :param payload: Параметр для callback api
        :type payload: str or list or dict
        """"""
    current_line = self.lines[-1]
<mask>:
        raise ValueError(f'Max {MAX_BUTTONS_ON_LINE} buttons on a line')
    color_value = color
    if isinstance(color_value, VkKeyboardColor):
        color_value = color_value.value
    if payload is not None and (not isinstance(payload, str)):
        payload = sjson_dumps(payload)
    button_type = VkKeyboardButton.CALLBACK.value
    current_line.append({'color': color_value, 'action': {'type': button_type, 'payload': payload, 'label': label}})",len(current_line) >= MAX_BUTTONS_ON_LINE,98,len(current_line) > MAX_BUTTONS_ON_LINE,False,80.52253761904356,N/A
"def add_location_button(self, payload=None):
    """""" Добавить кнопку с местоположением.
            Всегда занимает всю ширину линии.

        :param payload: Параметр для callback api
        :type payload: str or list or dict
        """"""
    current_line = self.lines[-1]
<mask>:
        raise ValueError('This type of button takes the entire width of the line')
    if payload is not None and (not isinstance(payload, str)):
        payload = sjson_dumps(payload)
    button_type = VkKeyboardButton.LOCATION.value
    current_line.append({'action': {'type': button_type, 'payload': payload}})",len(current_line) != 0,63,len(current_line) != 2,False,86.33400213704509,N/A
"def add_vkpay_button(self, hash, payload=None):
    """""" Добавить кнопку с оплатой с помощью VKPay.
            Всегда занимает всю ширину линии.

        :param hash: Параметры платежа VKPay и ID приложения
        (в поле aid) разделённые &
        :type hash: str
        :param payload: Параметр для совместимости со старыми клиентами
        :type payload: str or list or dict
        """"""
    current_line = self.lines[-1]
<mask>:
        raise ValueError('This type of button takes the entire width of the line')
    if payload is not None and (not isinstance(payload, str)):
        payload = sjson_dumps(payload)
    button_type = VkKeyboardButton.VKPAY.value
    current_line.append({'action': {'type': button_type, 'payload': payload, 'hash': hash}})",len(current_line) != 0,87,len(current_line) != 2,False,86.33400213704509,N/A
"def add_vkapps_button(self, app_id, owner_id, label, hash, payload=None):
    """""" Добавить кнопку с приложением VK Apps.
            Всегда занимает всю ширину линии.

        :param app_id: Идентификатор вызываемого приложения с типом VK Apps
        :type app_id: int
        :param owner_id: Идентификатор сообщества, в котором установлено
        приложение, если требуется открыть в контексте сообщества
        :type owner_id: int
        :param label: Название приложения, указанное на кнопке
        :type label: str
        :param hash: хэш для навигации в приложении, будет передан в строке
        параметров запуска после символа #
        :type hash: str
        :param payload: Параметр для совместимости со старыми клиентами
        :type payload: str or list or dict
        """"""
    current_line = self.lines[-1]
<mask>:
        raise ValueError('This type of button takes the entire width of the line')
    if payload is not None and (not isinstance(payload, str)):
        payload = sjson_dumps(payload)
    button_type = VkKeyboardButton.VKAPPS.value
    current_line.append({'action': {'type': button_type, 'app_id': app_id, 'owner_id': owner_id, 'label': label, 'payload': payload, 'hash': hash}})",len(current_line) != 0,137,payload is None,False,0.0,N/A
"def main():
    """""" Пример получения всех постов со стены """"""
    login, password = ('python@vk.com', 'mypassword')
    vk_session = vk_api.VkApi(login, password)
    try:
        vk_session.auth(token_only=True)
    except vk_api.AuthError as error_msg:
        print(error_msg)
        return
    tools = vk_api.VkTools(vk_session)
    ' VkTools.get_all позволяет получить все объекты со всех страниц.\n        Соответственно get_all используется только если метод принимает\n        параметры: count и offset.\n\n        Например может использоваться для получения всех постов стены,\n        всех диалогов, всех сообщений, etc.\n\n        При использовании get_all сокращается количество запросов к API\n        за счет метода execute в 25 раз.\n\n        Например за раз со стены можно получить 100 * 25 = 2500, где\n        100 - максимальное количество постов, которое можно получить за один\n        запрос (обычно написано на странице с описанием метода)\n    '
    wall = tools.get_all('wall.get', 100, {'owner_id': 1})
    print('Posts count:', wall['count'])
<mask>:
        print('First post:', wall['items'][0], '\n')
    if wall['count'] > 1:
        print('Last post:', wall['items'][-1])",wall['count'],131,tools.get_all(token_only=True),False,0.0,N/A
"@app.route('/my_bot', methods=['POST'])
def bot():
    data = request.get_json(force=True, silent=True)
<mask>:
        return 'not ok'
    if data['type'] == 'confirmation':
        return confirmation_code
    elif data['type'] == 'message_new':
        from_id = data['object']['message']['from_id']
        vk.messages.send(message='Hello World!', random_id=get_random_id(), peer_id=from_id)
        return 'ok'
    return 'ok'",not data or 'type' not in data,33,data is None,False,7.253154775624655,N/A
"def main():
    """""" Пример получения последнего сообщения со стены """"""
    login, password = ('python@vk.com', 'mypassword')
    vk_session = vk_api.VkApi(login, password)
    try:
        vk_session.auth(token_only=True)
    except vk_api.AuthError as error_msg:
        print(error_msg)
        return
    vk = vk_session.get_api()
    ' VkApi.method позволяет выполнять запросы к API. В этом примере\n        используется метод wall.get (https://vk.com/dev/wall.get) с параметром\n        count = 1, т.е. мы получаем один последний пост со стены текущего\n        пользователя.\n    '
    response = vk.wall.get(count=1)
<mask>:
        print(response['items'][0])",response['items'],65,response['count'] == 0,False,14.535768424205482,N/A
"def main():
    vk_session = VkApi(token=GROUP_TOKEN, api_version=API_VERSION)
    vk = vk_session.get_api()
    longpoll = VkBotLongPoll(vk_session, group_id=GROUP_ID)
    keyboard_1 = VkKeyboard(one_time=False, inline=True)
    keyboard_1.add_callback_button(label='Покажи pop-up сообщение', color=VkKeyboardColor.SECONDARY, payload={'type': 'show_snackbar', 'text': 'Это исчезающее сообщение на экране'})
    keyboard_1.add_line()
    keyboard_1.add_callback_button(label='Откртыть Url', color=VkKeyboardColor.POSITIVE, payload={'type': 'open_link', 'link': 'https://vk.com/dev/bots_docs_5'})
    keyboard_1.add_line()
    keyboard_1.add_callback_button(label='Открыть приложение', color=VkKeyboardColor.NEGATIVE, payload={'type': 'open_app', 'app_id': APP_ID, 'owner_id': OWNER_ID, 'hash': 'anything_data_100500'})
    keyboard_1.add_line()
    keyboard_1.add_callback_button(label='Добавить красного ', color=VkKeyboardColor.PRIMARY, payload={'type': 'my_own_100500_type_edit'})
    keyboard_2 = VkKeyboard(one_time=False, inline=True)
    keyboard_2.add_callback_button('Назад', color=VkKeyboardColor.NEGATIVE, payload={'type': 'my_own_100500_type_edit'})
    f_toggle: bool = False
    for event in longpoll.listen():
<mask>:
            if event.obj.message['text'] != '':
                if event.from_user:
                    if 'callback' not in event.obj.client_info['button_actions']:
                        print(f""Клиент user_id{event.obj.message['from_id']} не поддерживает callback-кнопки."")
                    vk.messages.send(user_id=event.obj.message['from_id'], random_id=get_random_id(), peer_id=event.obj.message['from_id'], keyboard=keyboard_1.get_keyboard(), message='Меню #1')
        elif event.type == VkBotEventType.MESSAGE_EVENT:
            if event.object.payload.get('type') in CALLBACK_TYPES:
                r = vk.messages.sendMessageEventAnswer(event_id=event.object.event_id, user_id=event.object.user_id, peer_id=event.object.peer_id, event_data=json.dumps(event.object.payload))
            elif event.object.payload.get('type') == 'my_own_100500_type_edit':
                last_id = vk.messages.edit(peer_id=event.obj.peer_id, message='Меню #2', conversation_message_id=event.obj.conversation_message_id, keyboard=(keyboard_1 if f_toggle else keyboard_2).get_keyboard())
                f_toggle = not f_toggle",event.type == VkBotEventType.MESSAGE_NEW,128,vk.version.split('-')[0] == 2,False,6.754312828675707,N/A
"def main():
    """""" Пример использования longpoll

        https://vk.com/dev/using_longpoll
        https://vk.com/dev/using_longpoll_2
    """"""
    login, password = ('python@vk.com', 'mypassword')
    vk_session = vk_api.VkApi(login, password)
    try:
        vk_session.auth(token_only=True)
    except vk_api.AuthError as error_msg:
        print(error_msg)
        return
    longpoll = VkLongPoll(vk_session)
    for event in longpoll.listen():
<mask>:
            print('Новое сообщение:')
            if event.from_me:
                print('От меня для: ', end='')
            elif event.to_me:
                print('Для меня от: ', end='')
            if event.from_user:
                print(event.user_id)
            elif event.from_chat:
                print(event.user_id, 'в беседе', event.chat_id)
            elif event.from_group:
                print('группы', event.group_id)
            print('Текст: ', event.text)
            print()
        elif event.type == VkEventType.USER_TYPING:
            print('Печатает ', end='')
            if event.from_user:
                print(event.user_id)
            elif event.from_group:
                print('администратор группы', event.group_id)
        elif event.type == VkEventType.USER_TYPING_IN_CHAT:
            print('Печатает ', event.user_id, 'в беседе', event.chat_id)
        elif event.type == VkEventType.USER_ONLINE:
            print('Пользователь', event.user_id, 'онлайн', event.platform)
        elif event.type == VkEventType.USER_OFFLINE:
            print('Пользователь', event.user_id, 'оффлайн', event.offline_type)
        else:
            print(event.type, event.raw[1:])",event.type == VkEventType.MESSAGE_NEW,111,event.type == VkEventType.EVENT_TEXT,False,66.06328636027612,N/A
"def main():
    """""" Пример использования bots longpoll

        https://vk.com/dev/bots_longpoll
    """"""
    vk_session = vk_api.VkApi(token='your_group_token')
    longpoll = VkBotLongPoll(vk_session, 'your_group_id')
    for event in longpoll.listen():
<mask>:
            print('Новое сообщение:')
            print('Для меня от: ', end='')
            print(event.obj.from_id)
            print('Текст:', event.obj.text)
        elif event.type == VkBotEventType.MESSAGE_REPLY:
            print('Новое сообщение:')
            print('От меня для: ', end='')
            print(event.obj.peer_id)
            print('Текст:', event.obj.text)
        elif event.type == VkBotEventType.MESSAGE_TYPING_STATE:
            print('Печатает ', end='')
            print(event.obj.from_id, end=' ')
            print('для ', end='')
            print(event.obj.to_id)
        elif event.type == VkBotEventType.GROUP_JOIN:
            print(event.obj.user_id, end=' ')
            print('Вступил в группу!')
        elif event.type == VkBotEventType.GROUP_LEAVE:
            print(event.obj.user_id, end=' ')
            print('Покинул группу!')
        else:
            print(event.type)
        print()",event.type == VkBotEventType.MESSAGE_NEW,81,event.type == VkBotEventType.MESSAGE_REQUEST,False,88.01117367933934,N/A
"def main():
    session = requests.Session()
    ""\n    login, password = 'python@vk.com', 'mypassword'\n    vk_session = vk_api.VkApi(login, password)\n\n    try:\n        vk_session.auth(token_only=True)\n    except vk_api.AuthError as error_msg:\n        print(error_msg)\n        return\n    ""
    ""\n    vk_session = vk_api.VkApi(token='токен с доступом к сообщениям и фото')\n    ""
    vk = vk_session.get_api()
    upload = VkUpload(vk_session)
    longpoll = VkLongPoll(vk_session)
    for event in longpoll.listen():
<mask>:
            print(f'id{event.user_id}: ""{event.text}""', end=' ')
            response = session.get('http://api.duckduckgo.com/', params={'q': event.text, 'format': 'json'}).json()
            text = response.get('AbstractText')
            image_url = response.get('Image')
            if not text:
                vk.messages.send(user_id=event.user_id, random_id=get_random_id(), message='No results')
                print('no results')
                continue
            attachments = []
            if image_url:
                image = session.get(image_url, stream=True)
                photo = upload.photo_messages(photos=image.raw)[0]
                attachments.append(f""photo{photo['owner_id']}_{photo['id']}"")
            vk.messages.send(user_id=event.user_id, attachment=','.join(attachments), random_id=get_random_id(), message=text)
            print('ok')",event.type == VkEventType.MESSAGE_NEW and event.to_me and event.text,94,event.event == 'duckduckgo_event',False,3.8570367021898537,N/A
"def __init__(self, fileepub):
    self.path = os.path.abspath(fileepub)
    self.file = zipfile.ZipFile(fileepub, 'r')
    cont = ET.parse(self.file.open('META-INF/container.xml'))
    self.rootfile = cont.find('CONT:rootfiles/CONT:rootfile', self.NS).attrib['full-path']
    self.rootdir = os.path.dirname(self.rootfile) + '/' if os.path.dirname(self.rootfile) != '' else ''
    cont = ET.parse(self.file.open(self.rootfile))
    self.version = cont.getroot().get('version')
<mask>:
        self.toc = self.rootdir + cont.find(""OPF:manifest/*[@media-type='application/x-dtbncx+xml']"", self.NS).get('href')
    elif self.version == '3.0':
        self.toc = self.rootdir + cont.find(""OPF:manifest/*[@properties='nav']"", self.NS).get('href')
    self.contents = []
    self.toc_entries = []",self.version == '2.0',57,self.version == '2.1',False,75.98356856515926,N/A
"def get_meta(self):
    meta = []
    cont = ET.fromstring(self.file.open(self.rootfile).read())
    for i in cont.findall('OPF:metadata/*', self.NS):
<mask>:
            meta.append([re.sub('{.*?}', '', i.tag), i.text])
    return meta",i.text is not None,20,i.tag,False,20.24518585186855,N/A
"def initialize(self):
    cont = ET.parse(self.file.open(self.rootfile)).getroot()
    manifest = []
    for i in cont.findall('OPF:manifest/*', self.NS):
<mask>:
            manifest.append([i.get('id'), i.get('href')])
    spine, contents = ([], [])
    for i in cont.findall('OPF:spine/*', self.NS):
        spine.append(i.get('idref'))
    for i in spine:
        for j in manifest:
            if i == j[0]:
                self.contents.append(self.rootdir + unquote(j[1]))
                contents.append(unquote(j[1]))
                manifest.remove(j)
                break
    toc = ET.parse(self.file.open(self.toc)).getroot()
    if self.version == '2.0':
        navPoints = toc.findall('DAISY:navMap//DAISY:navPoint', self.NS)
    elif self.version == '3.0':
        navPoints = toc.findall(""XHTML:body//XHTML:nav[@EPUB:type='toc']//XHTML:a"", self.NS)
    for i in contents:
        name = '-'
        for j in navPoints:
            if self.version == '2.0':
                if re.search(i, unquote(j.find('DAISY:content', self.NS).get('src'))) is not None:
                    name = j.find('DAISY:navLabel/DAISY:text', self.NS).text
                    break
            elif self.version == '3.0':
                if re.search(i, unquote(j.get('href'))) is not None:
                    name = ''.join(list(j.itertext()))
                    break
        self.toc_entries.append(name)",i.get('media-type') != 'application/x-dtbncx+xml' and i.get('properties') != 'nav',106,i.get('idref') is not None,False,6.661402457069713,N/A
"def handle_starttag(self, tag, attrs):
<mask>:
        self.ishead = True
    elif tag in self.inde:
        self.isinde = True
    elif tag in self.pref:
        self.ispref = True
    elif tag in self.bull:
        self.isbull = True
    elif tag in self.hide:
        self.ishidden = True
    elif tag == 'sup':
        self.text[-1] += '^{'
    elif tag == 'sub':
        self.text[-1] += '_{'
    elif tag in {'img', 'image'}:
        for i in attrs:
            if tag == 'img' and i[0] == 'src' or (tag == 'image' and i[0].endswith('href')):
                self.text.append('[IMG:{}]'.format(len(self.imgs)))
                self.imgs.append(unquote(i[1]))","re.match('h[1-6]', tag) is not None",75,tag in self.head,False,1.1524190727977786,N/A
"def handle_startendtag(self, tag, attrs):
<mask>:
        self.text += ['']
    elif tag in {'img', 'image'}:
        for i in attrs:
            if tag == 'img' and i[0] == 'src' or (tag == 'image' and i[0].endswith('href')):
                self.text.append('[IMG:{}]'.format(len(self.imgs)))
                self.imgs.append(unquote(i[1]))
                self.text.append('')",tag == 'br',34,tag == 'head',False,59.460355750136046,N/A
"@pytest.fixture(scope='module')
def string_json_attribute_person_model(base):
    """"""
    This approach to faking JSON support for testing with sqlite is borrowed from:
    https://avacariu.me/articles/2016/compiling-json-as-text-for-sqlite-with-sqlalchemy
    """"""
    import sqlalchemy.types as types
    import json

    class StringyJSON(types.TypeDecorator):
        """"""Stores and retrieves JSON as TEXT.""""""
        impl = types.TEXT

        def process_bind_param(self, value, dialect):
<mask>:
                value = json.dumps(value)
            return value

        def process_result_value(self, value, dialect):
            if value is not None:
                value = json.loads(value)
            return value
    MagicJSON = types.JSON().with_variant(StringyJSON, 'sqlite')

    class StringJsonAttributePerson(base):
        __tablename__ = 'string_json_attribute_person'
        person_id = Column(Integer, primary_key=True)
        name = Column(String, nullable=False)
        birth_date = Column(DateTime)
        address = Column(MagicJSON)
    yield StringJsonAttributePerson",value is not None,85,value is not None,True,100.00000000000004,N/A
"@pytest.fixture(scope='module')
def query():

    def query_(self, view_kwargs):
<mask>:
            return self.session.query(computer_model).join(person_model).filter_by(person_id=view_kwargs['person_id'])
        return self.session.query(computer_model)
    yield query_",view_kwargs.get('person_id') is not None,13,view_kwargs.get('person_id'),False,74.08182206817182,N/A
"def before_get_object(self, view_kwargs):
<mask>:
        try:
            computer = self.session.query(Computer).filter_by(id=view_kwargs['computer_id']).one()
        except NoResultFound:
            raise ObjectNotFound({'parameter': 'computer_id'}, 'Computer: {} not found'.format(view_kwargs['computer_id']))
        else:
            if computer.person is not None:
                view_kwargs['id'] = computer.person.id
            else:
                view_kwargs['id'] = None",view_kwargs.get('computer_id') is not None,30,'computer_id' in view_kwargs,False,17.447394295753046,N/A
"def query(self, view_kwargs):
    query_ = self.session.query(Computer)
<mask>:
        try:
            self.session.query(Person).filter_by(id=view_kwargs['id']).one()
        except NoResultFound:
            raise ObjectNotFound({'parameter': 'id'}, 'Person: {} not found'.format(view_kwargs['id']))
        else:
            query_ = query_.join(Person).filter(Person.id == view_kwargs['id'])
    return query_",view_kwargs.get('id') is not None,26,query_.count() == 0,False,5.3990167242108145,N/A
"def before_create_object(self, data, view_kwargs):
<mask>:
        person = self.session.query(Person).filter_by(id=view_kwargs['id']).one()
        data['person_id'] = person.id",view_kwargs.get('id') is not None,11,data.get('person_id') != None,False,16.59038701421971,N/A
"def __new__(cls, name, bases, d):
    """"""Constructor of a resource class""""""
    rv = super(ResourceMeta, cls).__new__(cls, name, bases, d)
<mask>:
        if not isinstance(d['data_layer'], dict):
            raise Exception('You must provide a data layer information as dict in {}'.format(cls.__name__))
        if d['data_layer'].get('class') is not None and BaseDataLayer not in inspect.getmro(d['data_layer']['class']):
            raise Exception('You must provide a data layer class inherited from BaseDataLayer in {}'.format(cls.__name__))
        data_layer_cls = d['data_layer'].get('class', SqlalchemyDataLayer)
        data_layer_kwargs = d['data_layer']
        rv._data_layer = data_layer_cls(data_layer_kwargs)
    rv.decorators = (check_headers,)
    if 'decorators' in d:
        rv.decorators += d['decorators']
    return rv",'data_layer' in d,79,'data_layer' in d,True,100.00000000000004,N/A
"def __new__(cls, *args, **kwargs):
    """"""Constructor of a resource instance""""""
<mask>:
        cls._data_layer.resource = cls
    return super(Resource, cls).__new__(cls)","hasattr(cls, '_data_layer')",16,cls._data_layer,False,16.669006580554246,N/A
"@jsonapi_exception_formatter
def dispatch_request(self, *args, **kwargs):
    """"""Logic of how to handle a request""""""
    method = getattr(self, request.method.lower(), None)
<mask>:
        method = getattr(self, 'get', None)
    assert method is not None, 'Unimplemented method {}'.format(request.method)
    headers = {'Content-Type': 'application/vnd.api+json'}
    response = method(*args, **kwargs)
    if isinstance(response, Response):
        response.headers.add('Content-Type', 'application/vnd.api+json')
        return response
    if not isinstance(response, tuple):
        if isinstance(response, dict):
            response.update({'jsonapi': {'version': '1.0'}})
        return make_response(json.dumps(response, cls=JSONEncoder), 200, headers)
    try:
        data, status_code, headers = response
        headers.update({'Content-Type': 'application/vnd.api+json'})
    except ValueError:
        pass
    try:
        data, status_code = response
    except ValueError:
        pass
    if isinstance(data, dict):
        data.update({'jsonapi': {'version': '1.0'}})
    if isinstance(data, FlaskResponse):
        data.headers.add('Content-Type', 'application/vnd.api+json')
        data.status_code = status_code
        return data
    elif isinstance(data, str):
        json_reponse = data
    else:
        json_reponse = json.dumps(data, cls=JSONEncoder)
    return make_response(json_reponse, status_code, headers)",method is None and request.method == 'HEAD',111,method is None,False,9.697196786440509,N/A
"@check_method_requirements
def post(self, *args, **kwargs):
    """"""Create an object""""""
    json_data = request.get_json() or {}
    qs = QSManager(request.args, self.schema)
    self.before_marshmallow(args, kwargs)
    schema = compute_schema(self.schema, getattr(self, 'post_schema_kwargs', dict()), qs, qs.include)
    try:
        data = schema.load(json_data)
    except IncorrectTypeError as e:
        errors = e.messages
        for error in errors['errors']:
            error['status'] = '409'
            error['title'] = 'Incorrect type'
        return (errors, 409)
    except ValidationError as e:
        errors = e.messages
        for message in errors['errors']:
            message['status'] = '422'
            message['title'] = 'Validation error'
        return (errors, 422)
    self.before_post(args, kwargs, data=data)
    obj = self.create_object(data, kwargs)
    result = schema.dump(obj)
<mask>:
        final_result = (result, 201, {'Location': result['data']['links']['self']})
    else:
        final_result = (result, 201)
    result = self.after_post(final_result)
    return result","result['data'].get('links', {}).get('self')",100,'links' in result,False,0.32600330570703107,N/A
"def _get_parent_filter(self, url, kwargs):
    """"""
        Returns a dictionary of filters that should be applied to ensure only resources
        belonging to the parent resource are returned
        """"""
    url_segments = url.split('/')
    parent_segment = url_segments[-3]
    parent_id = url_segments[-2]
    for key, value in self.schema._declared_fields.items():
<mask>:
            if value.type_ == parent_segment:
                return {value.id_field: parent_id}
    return {}","isinstance(value, BaseRelationship)",50,key == 'parent_segment',False,0.0,N/A
"def __init__(self, app=None, blueprint=None, decorators=None):
    """"""Initialize an instance of the Api

        :param app: the flask application
        :param blueprint: a flask blueprint
        :param tuple decorators: a tuple of decorators plugged to each resource methods
        """"""
    self.app = app
    self.blueprint = blueprint
    self.resources = []
    self.resource_registry = []
    self.decorators = decorators or tuple()
<mask>:
        self.init_app(app, blueprint)",app is not None,54,app is not None,True,100.00000000000004,N/A
"def init_app(self, app=None, blueprint=None, additional_blueprints=None):
    """"""Update flask application with our api

        :param Application app: a flask application
        """"""
<mask>:
        self.app = app
    if blueprint is not None:
        self.blueprint = blueprint
    for resource in self.resources:
        self.route(resource['resource'], resource['view'], *resource['urls'], url_rule_options=resource['url_rule_options'])
    if self.blueprint is not None:
        self.app.register_blueprint(self.blueprint)
    if additional_blueprints is not None:
        for blueprint in additional_blueprints:
            self.app.register_blueprint(blueprint)
    self.app.config.setdefault('PAGE_SIZE', 30)",app is not None,56,app is not None,True,100.00000000000004,N/A
"def route(self, resource, view, *urls, **kwargs):
    """"""Create an api view.

        :param Resource resource: a resource class inherited from flask_rest_jsonapi.resource.Resource
        :param str view: the view name
        :param list urls: the urls of the view
        :param dict kwargs: additional options of the route
        """"""
    resource.view = view
    url_rule_options = kwargs.get('url_rule_options') or dict()
    resource_args = kwargs.get('resource_args', [])
    resource_kwargs = kwargs.get('resource_kwargs', {})
    view_func = resource.as_view(view, *resource_args, **resource_kwargs)
<mask>:
        resource.view = '.'.join([kwargs['blueprint'].name, resource.view])
        for url in urls:
            kwargs['blueprint'].add_url_rule(url, view_func=view_func, **url_rule_options)
    elif self.blueprint is not None:
        resource.view = '.'.join([self.blueprint.name, resource.view])
        for url in urls:
            self.blueprint.add_url_rule(url, view_func=view_func, **url_rule_options)
    elif self.app is not None:
        for url in urls:
            self.app.add_url_rule(url, view_func=view_func, **url_rule_options)
    else:
        self.resources.append({'resource': resource, 'view': view, 'urls': urls, 'url_rule_options': url_rule_options})
    self.resource_registry.append(resource)",'blueprint' in kwargs,113,kwargs.get('blueprint') is not None,False,5.669791110976001,N/A
"def oauth_manager(self, oauth_manager):
    """"""Use the oauth manager to enable oauth for API

        :param oauth_manager: the oauth manager
        """"""

    @self.app.before_request
    @jsonapi_exception_formatter
    def before_request():
        endpoint = request.endpoint
        resource = None
<mask>:
            resource = getattr(self.app.view_functions[endpoint], 'view_class', None)
        if resource and (not getattr(resource, 'disable_oauth', None)):
            scopes = request.args.get('scopes')
            if getattr(resource, 'schema'):
                scopes = [self.build_scope(resource, request.method)]
            elif scopes:
                scopes = scopes.split(',')
                if scopes:
                    scopes = scopes.split(',')
            valid, req = oauth_manager.verify_request(scopes)
            for func in oauth_manager._after_request_funcs:
                valid, req = func(valid, req)
            if not valid:
                if oauth_manager._invalid_response:
                    return oauth_manager._invalid_response(req)
                return abort(401)
            request.oauth = req",endpoint,86,endpoint in self.app.view_functions,False,4.767707020457095,N/A
"@staticmethod
def build_scope(resource, method):
    """"""Compute the name of the scope for oauth

        :param Resource resource: the resource manager
        :param str method: an http method
        :return str: the name of the scope
        """"""
<mask>:
        prefix = 'list'
    else:
        method_to_prefix = {'GET': 'get', 'POST': 'create', 'PATCH': 'update', 'DELETE': 'delete'}
        prefix = method_to_prefix[method]
        if ResourceRelationship in inspect.getmro(resource):
            prefix = '_'.join([prefix, 'relationship'])
    return '_'.join([prefix, resource.schema.opts.type_])",ResourceList in inspect.getmro(resource) and method == 'GET',61,method == 'list',False,6.26707538823693,N/A
"def default(self, obj):
<mask>:
        return obj.isoformat()
    elif isinstance(obj, UUID):
        return str(obj)
    elif isinstance(obj, Decimal):
        return str(obj)
    return json.JSONEncoder.default(self, obj)","isinstance(obj, datetime)",19,"isinstance(obj, datetime.datetime)",False,57.21248424548516,N/A
"def __init__(self, detail, source=None, title=None, status=None, code=None, id_=None, links=None, meta=None):
    """"""Initialize a jsonapi exception

        :param dict source: the source of the error
        :param str detail: the detail of the error
        """"""
    self.detail = detail
    self.source = source
    self.code = code
    self.id = id_
    self.links = links or {}
    self.meta = meta or {}
<mask>:
        self.title = title
    if status is not None:
        self.status = status",title is not None,65,title is not None,True,100.00000000000004,N/A
"def to_dict(self):
    """"""Return values of each fields of an jsonapi error""""""
    error_dict = {}
    for field in ('status', 'source', 'title', 'detail', 'id', 'code', 'links', 'meta'):
<mask>:
            error_dict.update({field: getattr(self, field)})
    return error_dict","getattr(self, field, None)",31,"hasattr(self, field)",False,38.49815007763549,N/A
"def add_pagination_links(data, object_count, querystring, base_url):
    """"""Add pagination links to result

    :param dict data: the result of the view
    :param int object_count: number of objects in result
    :param QueryStringManager querystring: the managed querystring fields and values
    :param str base_url: the base url for pagination
    """"""
    links = {}
    all_qs_args = copy(querystring.querystring)
    links['self'] = base_url
<mask>:
        links['self'] += '?' + urlencode(all_qs_args)
    if querystring.pagination.get('size') != '0' and object_count > 1:
        page_size = int(querystring.pagination.get('size', 0)) or current_app.config['PAGE_SIZE']
        last_page = int(ceil(object_count / page_size))
        if last_page > 1:
            links['first'] = links['last'] = base_url
            all_qs_args.pop('page[number]', None)
            if all_qs_args:
                links['first'] += '?' + urlencode(all_qs_args)
            all_qs_args.update({'page[number]': last_page})
            links['last'] += '?' + urlencode(all_qs_args)
            current_page = int(querystring.pagination.get('number', 0)) or 1
            if current_page > 1:
                all_qs_args.update({'page[number]': current_page - 1})
                links['prev'] = '?'.join((base_url, urlencode(all_qs_args)))
            if current_page < last_page:
                all_qs_args.update({'page[number]': current_page + 1})
                links['next'] = '?'.join((base_url, urlencode(all_qs_args)))
    data['links'] = links",all_qs_args,136,all_qs_args,True,100.00000000000004,N/A
"def __init__(self, querystring, schema):
    """"""Initialization instance

        :param dict querystring: query string dict from request.args
        """"""
<mask>:
        raise ValueError('QueryStringManager require a dict-like object querystring parameter')
    self.qs = querystring
    self.schema = schema","not isinstance(querystring, dict)",30,"not isinstance(querystring, dict)",True,100.00000000000004,N/A
"def _get_key_values(self, name):
    """"""Return a dict containing key / values items for a given key, used for items like filters, page, etc.

        :param str name: name of the querystring parameter
        :return dict: a dict of key / values items
        """"""
    results = {}
    for key, value in self.qs.items():
        try:
<mask>:
                continue
            key_start = key.index('[') + 1
            key_end = key.index(']')
            item_key = key[key_start:key_end]
            if ',' in value:
                item_value = value.split(',')
            else:
                item_value = value
            results.update({item_key: item_value})
        except Exception:
            raise BadRequest('Parse error', source={'parameter': key})
    return results",not key.startswith(name),84,'[' in key,False,7.16047614494885,N/A
"def _simple_filters(self, dict_):
    """"""Return filter list

        :return list: list of dict for filter parameters. Includes support for 'in' for list values
        """"""
    filter_list = []
    for key, value in dict_.items():
        operator = 'eq'
<mask>:
            operator = 'in'
        filter_list.append({'name': key, 'op': operator, 'val': value})
    return filter_list","isinstance(value, list)",45,key in self.filter_operators,False,0.0,N/A
"@property
def filters(self):
    """"""Return filters from query string.

        :return list: filter information
        """"""
    results = []
    filters = self.qs.get('filter')
<mask>:
        try:
            results.extend(json.loads(filters))
        except (ValueError, TypeError):
            raise InvalidFilters('Parse error')
    if self._get_key_values('filter['):
        results.extend(self._simple_filters(self._get_key_values('filter[')))
    return results",filters is not None,33,filters,False,4.9787068367863965,N/A
"@property
def pagination(self):
    """"""Return all page parameters as a dict.

        :return dict: a dict of pagination information

        To allow multiples strategies, all parameters starting with `page` will be included. e.g::

            {
                ""number"": '25',
                ""size"": '150',
            }

        Example with number strategy::

            >>> query_string = {'page[number]': '25', 'page[size]': '10'}
            >>> parsed_query.pagination
            {'number': '25', 'size': '10'}
        """"""
    result = self._get_key_values('page')
    for key, value in result.items():
<mask>:
            raise BadRequest('{} is not a valid parameter of pagination'.format(key), source={'parameter': 'page'})
        try:
            int(value)
        except ValueError:
            raise BadRequest('Parse error', source={'parameter': 'page[{}]'.format(key)})
    if current_app.config.get('ALLOW_DISABLE_PAGINATION', True) is False and int(result.get('size', 1)) == 0:
        raise BadRequest('You are not allowed to disable pagination', source={'parameter': 'page[size]'})
    if current_app.config.get('MAX_PAGE_SIZE') is not None and 'size' in result:
        if int(result['size']) > current_app.config['MAX_PAGE_SIZE']:
            raise BadRequest('Maximum page size is {}'.format(current_app.config['MAX_PAGE_SIZE']), source={'parameter': 'page[size]'})
    return result","key not in ('number', 'size')",126,key not in current_app.config['ALLOW_DISABLE_PAGINATION'],False,9.78237574896145,N/A
"def compute_schema(schema_cls, default_kwargs, qs, include):
    """"""Compute a schema around compound documents and sparse fieldsets

    :param Schema schema_cls: the schema class
    :param dict default_kwargs: the schema default kwargs
    :param QueryStringManager qs: qs
    :param list include: the relation field to include data from

    :return Schema schema: the schema computed
    """"""
    schema_kwargs = default_kwargs
    schema_kwargs['include_data'] = tuple()
    related_includes = {}
<mask>:
        for include_path in include:
            field = include_path.split('.')[0]
            if field not in schema_cls._declared_fields:
                raise InvalidInclude('{} has no attribute {}'.format(schema_cls.__name__, field))
            elif not isinstance(schema_cls._declared_fields[field], Relationship):
                raise InvalidInclude('{} is not a relationship attribute of {}'.format(field, schema_cls.__name__))
            schema_kwargs['include_data'] += (field,)
            if field not in related_includes:
                related_includes[field] = []
            if '.' in include_path:
                related_includes[field] += ['.'.join(include_path.split('.')[1:])]
    if schema_kwargs.get('only') is not None and 'id' not in schema_kwargs['only']:
        schema_kwargs['only'] += ('id',)
    schema = schema_cls(**schema_kwargs)
    if schema.opts.type_ in qs.fields:
        tmp_only = set(schema.declared_fields.keys()) & set(qs.fields[schema.opts.type_])
        if schema.only:
            tmp_only &= set(schema.only)
        schema.only = tuple(tmp_only)
        if schema.only is not None and 'id' not in schema.only:
            schema.only += ('id',)
    if include:
        for include_path in include:
            field = include_path.split('.')[0]
            relation_field = schema.declared_fields[field]
            related_schema_cls = schema.declared_fields[field].__dict__['_Relationship__schema']
            related_schema_kwargs = {}
            if 'context' in default_kwargs:
                related_schema_kwargs['context'] = default_kwargs['context']
            if isinstance(related_schema_cls, SchemaABC):
                related_schema_kwargs['many'] = related_schema_cls.many
                related_schema_cls = related_schema_cls.__class__
            if isinstance(related_schema_cls, str):
                related_schema_cls = class_registry.get_class(related_schema_cls)
            related_schema = compute_schema(related_schema_cls, related_schema_kwargs, qs, related_includes[field] or None)
            relation_field.__dict__['_Relationship__schema'] = related_schema
    return schema",include,208,include,True,100.00000000000004,N/A
"def get_model_field(schema, field):
    """"""Get the model field of a schema field

    :param Schema schema: a marshmallow schema
    :param str field: the name of the schema field
    :return str: the name of the field in the model
    """"""
<mask>:
        raise Exception('{} has no attribute {}'.format(schema.__name__, field))
    if schema._declared_fields[field].attribute is not None:
        return schema._declared_fields[field].attribute
    return field",schema._declared_fields.get(field) is None,54,field not in schema._declared_fields,False,39.14236894465539,N/A
"def get_nested_fields(schema, model_field=False):
    """"""Return nested fields of a schema to support a join

    :param Schema schema: a marshmallow schema
    :param boolean model_field: whether to extract the model field for the nested fields
    :return list: list of nested fields of the schema
    """"""
    nested_fields = []
    for key, value in schema._declared_fields.items():
<mask>:
            nested_fields.append(key)
        elif isinstance(value, Nested):
            nested_fields.append(key)
    if model_field is True:
        nested_fields = [get_model_field(schema, key) for key in nested_fields]
    return nested_fields","isinstance(value, List) and isinstance(value.container, Nested)",70,"isinstance(value, Join)",False,11.988448048923711,N/A
"def get_relationships(schema, model_field=False):
    """"""Return relationship fields of a schema

    :param Schema schema: a marshmallow schema
    :param list: list of relationship fields of a schema
    """"""
    relationships = [key for key, value in schema._declared_fields.items() if isinstance(value, Relationship)]
<mask>:
        relationships = [get_model_field(schema, key) for key in relationships]
    return relationships",model_field is True,47,model_field,False,51.341711903259224,N/A
"def get_schema_from_type(resource_type):
    """"""Retrieve a schema from the registry by his type

    :param str type_: the type of the resource
    :return Schema: the schema class
    """"""
    for cls_name, cls in class_registry._registry.items():
        try:
<mask>:
                return cls[0]
        except Exception:
            pass
    raise Exception(""Couldn't find schema for type: {}"".format(resource_type))",cls[0].opts.type_ == resource_type,44,cls[0] == resource_type,False,42.89807392106132,N/A
"def check_headers(func):
    """"""Check headers according to jsonapi reference

    :param callable func: the function to decorate
    :return callable: the wrapped function
    """"""

    @wraps(func)
    def wrapper(*args, **kwargs):
<mask>:
            if 'Content-Type' not in request.headers or 'application/vnd.api+json' not in request.headers['Content-Type'] or request.headers['Content-Type'] != 'application/vnd.api+json':
                error = json.dumps(jsonapi_errors([{'source': '', 'detail': 'Content-Type header must be application/vnd.api+json', 'title': 'Invalid request header', 'status': '415'}]), cls=JSONEncoder)
                return make_response(error, 415, {'Content-Type': 'application/vnd.api+json'})
        if 'Accept' in request.headers:
            flag = False
            for accept in request.headers['Accept'].split(','):
                if accept.strip() == 'application/vnd.api+json':
                    flag = False
                    break
                if 'application/vnd.api+json' in accept and accept.strip() != 'application/vnd.api+json':
                    flag = True
            if flag is True:
                error = json.dumps(jsonapi_errors([{'source': '', 'detail': 'Accept header must be application/vnd.api+json withoutmedia type parameters', 'title': 'Invalid request header', 'status': '406'}]), cls=JSONEncoder)
                return make_response(error, 406, {'Content-Type': 'application/vnd.api+json'})
        return func(*args, **kwargs)
    return wrapper","request.method in ('POST', 'PATCH')",126,'Accept' not in request.headers,False,10.89644800332157,N/A
"def check_method_requirements(func):
    """"""Check methods requirements

    :param callable func: the function to decorate
    :return callable: the wrapped function
    """"""

    @wraps(func)
    def wrapper(*args, **kwargs):
        error_message = 'You must provide {error_field} in {cls} to get access to the default {method} method'
        error_data = {'cls': args[0].__class__.__name__, 'method': request.method.lower()}
<mask>:
            if not hasattr(args[0], 'schema'):
                error_data.update({'error_field': 'a schema class'})
                raise Exception(error_message.format(**error_data))
        return func(*args, **kwargs)
    return wrapper",request.method != 'DELETE',60,"request.method.lower() in ('GET', 'POST')",False,11.498759556447217,N/A
"def jsonapi_exception_formatter(func):

    @wraps(func)
    def wrapper(*args, **kwargs):
        headers = {'Content-Type': 'application/vnd.api+json'}
        try:
            return func(*args, **kwargs)
        except JsonApiException as e:
            return make_response(jsonify(jsonapi_errors([e.to_dict()])), e.status, headers)
        except Exception as e:
<mask>:
                raise
            if 'sentry' in current_app.extensions:
                current_app.extensions['sentry'].captureException()
            exc = JsonApiException(getattr(e, 'detail', current_app.config.get('GLOBAL_ERROR_MESSAGE') or str(e)), source=getattr(e, 'source', ''), title=getattr(e, 'title', None), status=getattr(e, 'status', None), code=getattr(e, 'code', None), id_=getattr(e, 'id', None), links=getattr(e, 'links', None), meta=getattr(e, 'meta', None))
            return make_response(jsonify(jsonapi_errors([exc.to_dict()])), exc.status, headers)
    return wrapper",current_app.config['DEBUG'] is True or current_app.config.get('PROPAGATE_EXCEPTIONS') is True,67,e.status == 404,False,0.34206884555663863,N/A
"def __init__(self, kwargs):
    """"""Initialize an instance of SqlalchemyDataLayer

        :param dict kwargs: initialization parameters of an SqlalchemyDataLayer instance
        """"""
    super(SqlalchemyDataLayer, self).__init__(kwargs)
<mask>:
        raise Exception('You must provide a session in data_layer_kwargs to use sqlalchemy data layer in {}'.format(self.resource.__name__))
    if not hasattr(self, 'model'):
        raise Exception('You must provide a model in data_layer_kwargs to use sqlalchemy data layer in {}'.format(self.resource.__name__))","not hasattr(self, 'session')",55,"not hasattr(self, 'session')",True,100.00000000000004,N/A
"def get_object(self, view_kwargs, qs=None):
    """"""Retrieve an object through sqlalchemy

        :params dict view_kwargs: kwargs from the resource view
        :return DeclarativeMeta: an object from sqlalchemy
        """"""
    self.before_get_object(view_kwargs)
    id_field = getattr(self, 'id_field', inspect(self.model).primary_key[0].key)
    try:
        filter_field = getattr(self.model, id_field)
    except Exception:
        raise Exception('{} has no attribute {}'.format(self.model.__name__, id_field))
    url_field = getattr(self, 'url_field', 'id')
    filter_value = view_kwargs[url_field]
    query = self.retrieve_object_query(view_kwargs, filter_field, filter_value)
<mask>:
        query = self.eagerload_includes(query, qs)
    try:
        obj = query.one()
    except NoResultFound:
        obj = None
    self.after_get_object(obj, view_kwargs)
    return obj","qs is not None and getattr(self, 'eagerload_includes', True)",75,qs,False,8.315287191035683e-05,N/A
"def get_collection(self, qs, view_kwargs, filters=None):
    """"""Retrieve a collection of objects through sqlalchemy

        :param QueryStringManager qs: a querystring manager to retrieve information from url
        :param dict view_kwargs: kwargs from the resource view
        :param dict filters: A dictionary of key/value filters to apply to the eventual query
        :return tuple: the number of object and the list of objects
        """"""
    self.before_get_collection(qs, view_kwargs)
    query = self.query(view_kwargs)
<mask>:
        query = query.filter_by(**filters)
    if qs.filters:
        query = self.filter_query(query, qs.filters, self.model)
    if qs.sorting:
        query = self.sort_query(query, qs.sorting)
    object_count = query.count()
    if getattr(self, 'eagerload_includes', True):
        query = self.eagerload_includes(query, qs)
    query = self.paginate_query(query, qs.pagination)
    collection = query.all()
    collection = self.after_get_collection(collection, qs, view_kwargs)
    return (object_count, collection)",filters,105,filters,True,100.00000000000004,N/A
"def update_object(self, obj, data, view_kwargs):
    """"""Update an object through sqlalchemy

        :param DeclarativeMeta obj: an object from sqlalchemy
        :param dict data: the data validated by marshmallow
        :param dict view_kwargs: kwargs from the resource view
        :return boolean: True if object have changed else False
        """"""
<mask>:
        url_field = getattr(self, 'url_field', 'id')
        filter_value = view_kwargs[url_field]
        raise ObjectNotFound('{}: {} not found'.format(self.model.__name__, filter_value), source={'parameter': url_field})
    self.before_update_object(obj, data, view_kwargs)
    relationship_fields = get_relationships(self.resource.schema, model_field=True)
    nested_fields = get_nested_fields(self.resource.schema, model_field=True)
    join_fields = relationship_fields + nested_fields
    for key, value in data.items():
        if hasattr(obj, key) and key not in join_fields:
            setattr(obj, key, value)
    self.apply_relationships(data, obj)
    self.apply_nested_fields(data, obj)
    try:
        self.session.commit()
    except JsonApiException as e:
        self.session.rollback()
        raise e
    except Exception as e:
        self.session.rollback()
        raise JsonApiException('Update object error: ' + str(e), source={'pointer': '/data'})
    self.after_update_object(obj, data, view_kwargs)",obj is None,122,not obj,False,30.326532985631665,N/A
"def delete_object(self, obj, view_kwargs):
    """"""Delete an object through sqlalchemy

        :param DeclarativeMeta item: an item from sqlalchemy
        :param dict view_kwargs: kwargs from the resource view
        """"""
<mask>:
        url_field = getattr(self, 'url_field', 'id')
        filter_value = view_kwargs[url_field]
        raise ObjectNotFound('{}: {} not found'.format(self.model.__name__, filter_value), source={'parameter': url_field})
    self.before_delete_object(obj, view_kwargs)
    self.session.delete(obj)
    try:
        self.session.commit()
    except JsonApiException as e:
        self.session.rollback()
        raise e
    except Exception as e:
        self.session.rollback()
        raise JsonApiException('Delete object error: ' + str(e))
    self.after_delete_object(obj, view_kwargs)",obj is None,68,not obj,False,30.326532985631665,N/A
"def __init__(self, kwargs):
    """"""Intialize an data layer instance with kwargs

        :param dict kwargs: information about data layer instance
        """"""
<mask>:
        self.bound_rewritable_methods(kwargs['methods'])
        kwargs.pop('methods')
    kwargs.pop('class', None)
    for key, value in kwargs.items():
        setattr(self, key, value)",kwargs.get('methods') is not None,32,'methods' in kwargs,False,4.691812222477093,N/A
"def bound_rewritable_methods(self, methods):
    """"""Bound additional methods to current instance

        :param class meta: information from Meta class used to configure the data layer instance
        """"""
    for key, value in methods.items():
<mask>:
            setattr(self, key, types.MethodType(value, self))",key in self.REWRITABLE_METHODS,34,"not hasattr(self, key)",False,7.809849842300637,N/A
"def resolve(self):
    """"""Create filter for a particular node of the filter tree""""""
<mask>:
        value = self.value
        if isinstance(value, dict):
            value = Node(self.related_model, value, self.resource, self.related_schema).resolve()
        if '__' in self.filter_.get('name', ''):
            value = {self.filter_['name'].split('__')[1]: value}
        if isinstance(value, dict):
            return getattr(self.column, self.operator)(**value)
        else:
            return getattr(self.column, self.operator)(value)
    if 'or' in self.filter_:
        return or_((Node(self.model, filt, self.resource, self.schema).resolve() for filt in self.filter_['or']))
    if 'and' in self.filter_:
        return and_((Node(self.model, filt, self.resource, self.schema).resolve() for filt in self.filter_['and']))
    if 'not' in self.filter_:
        return not_(Node(self.model, self.filter_['not'], self.resource, self.schema).resolve())",'or' not in self.filter_ and 'and' not in self.filter_ and ('not' not in self.filter_),79,self.value is not None,False,0.7571228133405872,N/A
"@property
def name(self):
    """"""Return the name of the node or raise a BadRequest exception

        :return str: the name of the field to filter on
        """"""
    name = self.filter_.get('name')
<mask>:
        raise InvalidFilters(""Can't find name of a filter"")
    if '__' in name:
        name = name.split('__')[0]
    if name not in self.schema._declared_fields:
        raise InvalidFilters('{} has no attribute {}'.format(self.schema.__name__, name))
    return name",name is None,57,not name,False,30.326532985631665,N/A
"@property
def operator(self):
    """"""Get the function operator from his name

        :return callable: a callable to make operation on a column
        """"""
    operators = (self.op, self.op + '_', '__' + self.op + '__')
    for op in operators:
<mask>:
            return op
    raise InvalidFilters('{} has no operator {}'.format(self.column.key, self.op))","hasattr(self.column, op)",46,callable(op),False,13.006502375572222,N/A
"@property
def value(self):
    """"""Get the value to filter on

        :return: the value to filter on
        """"""
<mask>:
        try:
            result = getattr(self.model, self.filter_['field'])
        except AttributeError:
            raise InvalidFilters('{} has no attribute {}'.format(self.model.__name__, self.filter_['field']))
        else:
            return result
    else:
        if 'val' not in self.filter_:
            raise InvalidFilters(""Can't find value or field in a filter"")
        return self.filter_['val']",self.filter_.get('field') is not None,51,'field' in self.filter_,False,19.765609300943975,N/A
"@property
def related_model(self):
    """"""Get the related model of a related (relationship or nested) field

        :return DeclarativeMeta: the related model
        """"""
    related_field_name = self.name
    related_fields = get_relationships(self.schema) + get_nested_fields(self.schema)
<mask>:
        raise InvalidFilters('{} has no relationship or nested attribute {}'.format(self.schema.__name__, related_field_name))
    return getattr(self.model, get_model_field(self.schema, related_field_name)).property.mapper.class_",related_field_name not in related_fields,43,related_field_name not in related_fields,True,100.00000000000004,N/A
"def color(self):
    """"""Returns the color in the line, if any.""""""
    textual_colors = self.settings.get('textual_colors')
<mask>:
        return self.hex_color()
    if self.rgb_color():
        return self.rgb_color()
    if self.rgba_color():
        return self.rgba_color()
    if self.hsl_color():
        return self.hsl_color()
    if self.hsla_color():
        return self.hsla_color()
    if textual_colors and self.web_color():
        return self.web_color()
    if not self.settings.get('custom_colors') == None:
        return self.custom_color()",self.hex_color(),45,not self.hex_color(),False,84.08964152537145,N/A
"def web_color(self):
    """"""Returns the color in the line, if any CSS color name is found.""""""
    matches = re.search(self.WEB_COLORS_REGEX, self.text)
<mask>:
        return matches.group(1)",matches,22,matches,True,100.00000000000004,N/A
"def hex_color(self):
    """"""Returns the color in the line, if any hex is found.""""""
    matches = re.search(Line.HEX_REGEXP, self.text)
<mask>:
        return matches.group(0)",matches,20,matches,True,100.00000000000004,N/A
"def rgb_color(self):
    """"""Returns the color in the line, if any rgb is found.""""""
    matches = re.search(Line.RGB_REGEXP, self.text)
<mask>:
        return matches.group(0)",matches,20,matches,True,100.00000000000004,N/A
"def rgba_color(self):
    """"""Returns the color in the line, if any rgba is found.""""""
    matches = re.search(Line.RGBA_REGEXP, self.text)
<mask>:
        if self.transparency_settings()[0]:
            return matches.group(0)
        else:
            return 'rgb(' + matches.group(1) + ')'",matches,29,matches,True,100.00000000000004,N/A
"def clear_cache(force=False):
    """"""
  If the folder exists, and has more than 5MB of icons in the cache, delete
  it to clear all the icons then recreate it.
  """"""
    from os.path import getsize, join, isfile, exists
    from os import makedirs, listdir
    from sublime import cache_path
    from shutil import rmtree
    icon_path = join(cache_path(), 'GutterColor')
    limit = 5242880
<mask>:
        size = sum((getsize(join(icon_path, f)) for f in listdir(icon_path) if isfile(join(icon_path, f))))
        if force or size > limit:
            rmtree(icon_path)
    if not exists(icon_path):
        makedirs(icon_path)",exists(icon_path),78,exists(icon_path),True,100.00000000000004,N/A
"def on_activated_async(self, view):
    """"""Scan file when it gets focus""""""
<mask>:
        fix_scheme_in_view(view)
        File(view)",syntax(view) in settings().get('supported_syntax'),12,not self.is_focus_in_view(view),False,11.724489878110314,N/A
"def on_modified(self, view):
    """"""Scan file when it is modified""""""
<mask>:
        File(view, 'update')",syntax(view) in settings().get('supported_syntax'),12,self.is_modified(),False,4.981224652850502,N/A
"def on_pre_save_async(self, view):
    """"""Scan file before it is saved""""""
<mask>:
        File(view, 'update')",syntax(view) in settings().get('supported_syntax'),12,"File(view, 'scan')",False,4.008579202215618,N/A
"def current_directory(full=False):
    """"""Return the name of the directory containing this plugin""""""
    from os.path import dirname, realpath, split
<mask>:
        return dirname(realpath(__file__))
    else:
        return split(dirname(realpath(__file__)))[1]",full,23,full,True,100.00000000000004,N/A
"def scan(self):
    """"""Scan the file for colours and add/remove regions appropriately""""""
<mask>:
        regions = [self.view.line(s) for s in self.view.sel()]
    else:
        regions = self.view.lines(Region(0, self.view.size()))
    for region in regions:
        line = Line(self.view, region, self.id)
        if line.has_color():
            line.add_region()
        else:
            try:
                self.view.erase_regions('gutter_color_%s' % region.a)
            except:
                pass",self.action == 'update',43,self.id == 0,False,22.957488466614336,N/A
"@pytest.fixture(autouse=True)
def _docdir(request):
    doctest_plugin = request.config.pluginmanager.getplugin('doctest')
<mask>:
        tmpdir = request.getfixturevalue('tmpdir')
        with tmpdir.as_cwd():
            yield
    else:
        yield","isinstance(request.node, doctest_plugin.DoctestItem)",15,doctest_plugin,False,4.9787068367863965,N/A
"def __init__(self, name, parent=None, children=None):
    super(MyNode, self).__init__()
    self.name = name
    self.parent = parent
<mask>:
        self.children = children",children,17,children is not None,False,15.97357760615681,N/A
"def __init__(self, name, parent=None, children=None):
    super(MyMapping, self).__init__()
    self.name = name
    self.parent = parent
<mask>:
        self.children = children",children,17,children is not None,False,15.97357760615681,N/A
"def __getitem__(self, name):
    for child in self:
<mask>:
            return child
    raise KeyError(name)",child.name == name,12,child.name == name,True,100.00000000000004,N/A
"def assert_gen(genpath, refpath):
    """"""Compare Generated Files Versus Reference.""""""
    genpath.mkdir(parents=True, exist_ok=True)
    refpath.mkdir(parents=True, exist_ok=True)
<mask>:
        shutil.rmtree(refpath, ignore_errors=True)
        shutil.copytree(genpath, refpath)
    gens = [path for path in sorted(genpath.glob('**/*')) if path.is_file() and (not '__pycache__' in path.parts)]
    refs = [path for path in sorted(refpath.glob('**/*')) if path.is_file() and (not '__pycache__' in path.parts)]
    genfiles = [path.relative_to(genpath) for path in gens]
    reffiles = [path.relative_to(refpath) for path in refs]
    assert reffiles == genfiles, f'{reffiles} != {genfiles}'
    for gen, ref in zip(gens, refs):
        reftext = ref.read_text(encoding='utf-8')
        gentext = gen.read_text(encoding='utf-8')
        assert reftext == gentext, f'{reftext} != {gentext}'",LEARN,85,os.path.exists(refpath),False,0.0,N/A
"def __init__(self, name, parent=None, children=None):
    self.name = name
    self.parent = parent
<mask>:
        self.children = children",children,15,children is not None,False,15.97357760615681,N/A
"def test_readonly_pre():
    """"""Read Only Use case, where Exceptions in _pre_{attach,detach} avoid modifications.""""""

    class ReadonlyError(RuntimeError):
        pass

    class ReadonlyNode(Node):
        _is_readonly = False

        def _pre_attach(self, parent):
<mask>:
                raise ReadonlyError()

        def _pre_detach(self, parent):
            if self._is_readonly:
                raise ReadonlyError()
    root = ReadonlyNode('root')
    s0 = ReadonlyNode('sub0', parent=root)
    s0b = ReadonlyNode('sub0B', parent=s0)
    s0a = ReadonlyNode('sub0A', parent=s0)
    s1 = ReadonlyNode('sub1', parent=root)
    s1a = ReadonlyNode('sub1A', parent=s1)
    s1b = ReadonlyNode('sub1B', parent=s1)
    s1c = ReadonlyNode('sub1C', parent=s1)
    s1ca = ReadonlyNode('sub1Ca', parent=s1c)
    ReadonlyNode._is_readonly = True

    def check():
        eq_(root.parent, None)
        eq_(root.children, tuple([s0, s1]))
        eq_(s0.parent, root)
        eq_(s0.children, tuple([s0b, s0a]))
        eq_(s0b.parent, s0)
        eq_(s0b.children, tuple())
        eq_(s0a.parent, s0)
        eq_(s0a.children, tuple())
        eq_(s1.parent, root)
        eq_(s1.children, tuple([s1a, s1b, s1c]))
        eq_(s1a.parent, s1)
        eq_(s1a.children, tuple())
        eq_(s1b.parent, s1)
        eq_(s1b.children, tuple())
        eq_(s1c.parent, s1)
        eq_(s1c.children, tuple([s1ca]))
        eq_(s1ca.parent, s1c)
        eq_(s1ca.children, tuple())
    check()
    with assert_raises(ReadonlyError, ''):
        s1ca.parent = s0
    check()
    with assert_raises(ReadonlyError, ''):
        s1ca.parent = None
    check()
    with assert_raises(ReadonlyError, ''):
        s0.children = []
    check()",self._is_readonly,135,self._is_readonly,True,100.00000000000004,N/A
"def with_setup(setup=None, teardown=None):

    def decorate(func, setup=setup, teardown=teardown):
<mask>:
            if hasattr(func, 'setup'):
                _old_s = func.setup

                def _s():
                    setup()
                    _old_s()
                func.setup = _s
            else:
                func.setup = setup
        if teardown:
            if hasattr(func, 'teardown'):
                _old_t = func.teardown

                def _t():
                    _old_t()
                    teardown()
                func.teardown = _t
            else:
                func.teardown = teardown
        return func
    return decorate",setup,48,setup,True,100.00000000000004,N/A
"def test_eq_overwrite():
    """"""Node with overwritten __eq__.""""""

    class EqOverwrittingNode(NodeMixin):

        def __init__(self, a, b, parent=None):
            super(EqOverwrittingNode, self).__init__()
            self.a = a
            self.b = b
            self.parent = parent

        def __eq__(self, other):
<mask>:
                return self.a == other.a and self.b == other.b
            else:
                return NotImplemented
    r = EqOverwrittingNode(0, 0)
    a = EqOverwrittingNode(1, 0, parent=r)
    b = EqOverwrittingNode(1, 0, parent=r)
    assert a.parent is r
    assert b.parent is r
    assert a.a == 1
    assert a.b == 0
    assert b.a == 1
    assert b.b == 0","isinstance(other, EqOverwrittingNode)",77,"isinstance(other, Node)",False,53.7284965911771,N/A
"def __init__(self, vertical, cont, end):
    super(AbstractStyle, self).__init__()
    self.vertical = vertical
    self.cont = cont
    self.end = end
<mask>:
        assert len(cont) == len(vertical) == len(end), ""'%s', '%s' and '%s' need to have equal length"" % (vertical, cont, end)",ASSERTIONS,36,self.vertical != vertical and self.end != end,False,0.0,N/A
"def __init__(self, node, style=ContStyle(), childiter=list, maxlevel=None):
<mask>:
        style = style()
    self.node = node
    self.style = style
    self.childiter = childiter
    self.maxlevel = maxlevel","not isinstance(style, AbstractStyle)",22,callable(style),False,16.70067963244422,N/A
"def __next(self, node, continues, level=0):
    yield RenderTree.__item(node, continues, self.style)
    level += 1
<mask>:
        children = node.children
        if children:
            children = self.childiter(children)
            for child, is_last in _is_last(children):
                for grandchild in self.__next(child, continues + (not is_last,), level=level):
                    yield grandchild",self.maxlevel is None or level < self.maxlevel,37,node.children,False,1.9119108411650758,N/A
"@staticmethod
def __item(node, continues, style):
<mask>:
        return Row('', '', node)
    items = [style.vertical if cont else style.empty for cont in continues]
    indent = ''.join(items[:-1])
    branch = style.cont if continues[-1] else style.end
    pre = indent + branch
    fill = ''.join(items)
    return Row(pre, fill, node)",not continues,43,len(parens) == 0,False,0.0,N/A
"def by_attr(self, attrname='name'):
    """"""
        Return rendered tree with node attribute `attrname`.

        >>> from anytree import AnyNode, RenderTree
        >>> root = AnyNode(id=""root"")
        >>> s0 = AnyNode(id=""sub0"", parent=root)
        >>> s0b = AnyNode(id=""sub0B"", parent=s0, foo=4, bar=109)
        >>> s0a = AnyNode(id=""sub0A"", parent=s0)
        >>> s1 = AnyNode(id=""sub1"", parent=root)
        >>> s1a = AnyNode(id=""sub1A"", parent=s1)
        >>> s1b = AnyNode(id=""sub1B"", parent=s1, bar=8)
        >>> s1c = AnyNode(id=""sub1C"", parent=s1)
        >>> s1ca = AnyNode(id=""sub1Ca"", parent=s1c)
        >>> print(RenderTree(root).by_attr('id'))
        root
        ├── sub0
        │   ├── sub0B
        │   └── sub0A
        └── sub1
            ├── sub1A
            ├── sub1B
            └── sub1C
                └── sub1Ca

        """"""

    def get():
<mask>:
            for row in self:
                attr = attrname(row.node)
                yield from _format_row_any(row, attr)
        else:
            for row in self:
                attr = getattr(row.node, attrname, '')
                yield from _format_row_any(row, attr)
    return '\n'.join(get())",callable(attrname),116,callable(attrname),True,100.00000000000004,N/A
"@staticmethod
def walk(start, end):
    """"""
        Walk from `start` node to `end` node.

        Returns:
            (upwards, common, downwards): `upwards` is a list of nodes to go upward to.
            `common` top node. `downwards` is a list of nodes to go downward to.

        Raises:
            WalkError: on no common root node.

        Example:

        >>> from anytree import Node, RenderTree, AsciiStyle
        >>> f = Node(""f"")
        >>> b = Node(""b"", parent=f)
        >>> a = Node(""a"", parent=b)
        >>> d = Node(""d"", parent=b)
        >>> c = Node(""c"", parent=d)
        >>> e = Node(""e"", parent=d)
        >>> g = Node(""g"", parent=f)
        >>> i = Node(""i"", parent=g)
        >>> h = Node(""h"", parent=i)
        >>> print(RenderTree(f, style=AsciiStyle()))
        Node('/f')
        |-- Node('/f/b')
        |   |-- Node('/f/b/a')
        |   +-- Node('/f/b/d')
        |       |-- Node('/f/b/d/c')
        |       +-- Node('/f/b/d/e')
        +-- Node('/f/g')
            +-- Node('/f/g/i')
                +-- Node('/f/g/i/h')

        Create a walker:

        >>> w = Walker()

        This class is made for walking:

        >>> w.walk(f, f)
        ((), Node('/f'), ())
        >>> w.walk(f, b)
        ((), Node('/f'), (Node('/f/b'),))
        >>> w.walk(b, f)
        ((Node('/f/b'),), Node('/f'), ())
        >>> w.walk(h, e)
        ((Node('/f/g/i/h'), Node('/f/g/i'), Node('/f/g')), Node('/f'), (Node('/f/b'), Node('/f/b/d'), Node('/f/b/d/e')))
        >>> w.walk(d, e)
        ((), Node('/f/b/d'), (Node('/f/b/d/e'),))

        For a proper walking the nodes need to be part of the same tree:

        >>> w.walk(Node(""a""), Node(""b""))
        Traceback (most recent call last):
          ...
        anytree.walker.WalkError: Node('/a') and Node('/b') are not part of the same tree.
        """"""
    startpath = start.path
    endpath = end.path
<mask>:
        msg = '%r and %r are not part of the same tree.' % (start, end)
        raise WalkError(msg)
    common = Walker.__calc_common(startpath, endpath)
    if ASSERTIONS:
        assert common[0] is start.root
    len_common = len(common)
    if start is common[-1]:
        upwards = tuple()
    else:
        upwards = tuple(reversed(startpath[len_common:]))
    if end is common[-1]:
        down = tuple()
    else:
        down = endpath[len_common:]
    return (upwards, common[-1], down)",start.root is not end.root,267,common == end,False,5.876350803261633,N/A
"def get(self, node, path):
    """"""
        Return instance at `path`.

        An example module tree:

        >>> from anytree import Node
        >>> top = Node(""top"", parent=None)
        >>> sub0 = Node(""sub0"", parent=top)
        >>> sub0sub0 = Node(""sub0sub0"", parent=sub0)
        >>> sub0sub1 = Node(""sub0sub1"", parent=sub0)
        >>> sub1 = Node(""sub1"", parent=top)

        A resolver using the `name` attribute:

        >>> resolver = Resolver('name')
        >>> relaxedresolver = Resolver('name', relax=True)  # never generate exceptions

        Relative paths:

        >>> resolver.get(top, ""sub0/sub0sub0"")
        Node('/top/sub0/sub0sub0')
        >>> resolver.get(sub1, "".."")
        Node('/top')
        >>> resolver.get(sub1, ""../sub0/sub0sub1"")
        Node('/top/sub0/sub0sub1')
        >>> resolver.get(sub1, ""."")
        Node('/top/sub1')
        >>> resolver.get(sub1, """")
        Node('/top/sub1')
        >>> resolver.get(top, ""sub2"")
        Traceback (most recent call last):
          ...
        anytree.resolver.ChildResolverError: Node('/top') has no child sub2. Children are: 'sub0', 'sub1'.
        >>> print(relaxedresolver.get(top, ""sub2""))
        None

        Absolute paths:

        >>> resolver.get(sub0sub0, ""/top"")
        Node('/top')
        >>> resolver.get(sub0sub0, ""/top/sub0"")
        Node('/top/sub0')
        >>> resolver.get(sub0sub0, ""/"")
        Traceback (most recent call last):
          ...
        anytree.resolver.ResolverError: root node missing. root is '/top'.
        >>> print(relaxedresolver.get(sub0sub0, ""/""))
        None
        >>> resolver.get(sub0sub0, ""/bar"")
        Traceback (most recent call last):
          ...
        anytree.resolver.ResolverError: unknown root node '/bar'. root is '/top'.
        >>> print(relaxedresolver.get(sub0sub0, ""/bar""))
        None

        Going above the root node raises a :any:`RootResolverError`:

        >>> resolver.get(top, "".."")
        Traceback (most recent call last):
            ...
        anytree.resolver.RootResolverError: Cannot go above root node Node('/top')

        .. note:: Please not that :any:`get()` returned `None` in exactly that case above,
                  which was a bug until version 1.8.1.

        Case insensitive matching:

        >>> resolver.get(top, '/TOP')
        Traceback (most recent call last):
            ...
        anytree.resolver.ResolverError: unknown root node '/TOP'. root is '/top'.

        >>> ignorecaseresolver = Resolver('name', ignorecase=True)
        >>> ignorecaseresolver.get(top, '/TOp')
        Node('/top')
        """"""
    node, parts = self.__start(node, path, self.__cmp)
<mask>:
        return None
    for part in parts:
        if part == '..':
            parent = node.parent
            if parent is None:
                if self.relax:
                    return None
                raise RootResolverError(node)
            node = parent
        elif part in ('', '.'):
            pass
        else:
            node = self.__get(node, part)
    return node",node is None and self.relax,278,"not isinstance(path, (tuple, list))",False,0.0,N/A
"def __get(self, node, name):
    namestr = str(name)
    for child in node.children:
<mask>:
            return child
    if self.relax:
        return None
    raise ChildResolverError(node, name, self.pathattr)","self.__cmp(_getattr(child, self.pathattr), namestr)",22,child.name == namestr,False,1.4456752008489673,N/A
"def glob(self, node, path):
    """"""
        Return instances at `path` supporting wildcards.

        Behaves identical to :any:`get`, but accepts wildcards and returns
        a list of found nodes.

        * `*` matches any characters, except '/'.
        * `?` matches a single character, except '/'.

        An example module tree:

        >>> from anytree import Node
        >>> top = Node(""top"", parent=None)
        >>> sub0 = Node(""sub0"", parent=top)
        >>> sub0sub0 = Node(""sub0"", parent=sub0)
        >>> sub0sub1 = Node(""sub1"", parent=sub0)
        >>> sub1 = Node(""sub1"", parent=top)
        >>> sub1sub0 = Node(""sub0"", parent=sub1)

        A resolver using the `name` attribute:

        >>> resolver = Resolver('name')
        >>> relaxedresolver = Resolver('name', relax=True)  # never generate exceptions

        Relative paths:

        >>> resolver.glob(top, ""sub0/sub?"")
        [Node('/top/sub0/sub0'), Node('/top/sub0/sub1')]
        >>> resolver.glob(sub1, "".././*"")
        [Node('/top/sub0'), Node('/top/sub1')]
        >>> resolver.glob(top, ""*/*"")
        [Node('/top/sub0/sub0'), Node('/top/sub0/sub1'), Node('/top/sub1/sub0')]
        >>> resolver.glob(top, ""*/sub0"")
        [Node('/top/sub0/sub0'), Node('/top/sub1/sub0')]
        >>> resolver.glob(top, ""sub1/sub1"")
        Traceback (most recent call last):
            ...
        anytree.resolver.ChildResolverError: Node('/top/sub1') has no child sub1. Children are: 'sub0'.
        >>> relaxedresolver.glob(top, ""sub1/sub1"")
        []

        Non-matching wildcards are no error:

        >>> resolver.glob(top, ""bar*"")
        []
        >>> resolver.glob(top, ""sub2"")
        Traceback (most recent call last):
          ...
        anytree.resolver.ChildResolverError: Node('/top') has no child sub2. Children are: 'sub0', 'sub1'.
        >>> relaxedresolver.glob(top, ""sub2"")
        []

        Absolute paths:

        >>> resolver.glob(sub0sub0, ""/top/*"")
        [Node('/top/sub0'), Node('/top/sub1')]
        >>> resolver.glob(sub0sub0, ""/"")
        Traceback (most recent call last):
          ...
        anytree.resolver.ResolverError: root node missing. root is '/top'.
        >>> relaxedresolver.glob(sub0sub0, ""/"")
        []
        >>> resolver.glob(sub0sub0, ""/bar"")
        Traceback (most recent call last):
          ...
        anytree.resolver.ResolverError: unknown root node '/bar'. root is '/top'.

        Going above the root node raises a :any:`RootResolverError`:

        >>> resolver.glob(top, "".."")
        Traceback (most recent call last):
            ...
        anytree.resolver.RootResolverError: Cannot go above root node Node('/top')
        >>> relaxedresolver.glob(top, "".."")
        []
        """"""
    node, parts = self.__start(node, path, self.__match)
<mask>:
        return []
    return self.__glob(node, parts)",node is None and self.relax,260,"not isinstance(path, (tuple, list))",False,0.0,N/A
"def __start(self, node, path, cmp_):
    sep = node.separator
    parts = path.split(sep)
<mask>:
        node = node.root
        rootpart = _getattr(node, self.pathattr)
        parts.pop(0)
        if not parts[0]:
            if self.relax:
                return (None, None)
            msg = ""root node missing. root is '%s%s'.""
            raise ResolverError(node, '', msg % (sep, str(rootpart)))
        if not cmp_(rootpart, parts[0]):
            if self.relax:
                return (None, None)
            msg = ""unknown root node '%s%s'. root is '%s%s'.""
            raise ResolverError(node, '', msg % (sep, parts[0], sep, str(rootpart)))
        parts.pop(0)
    return (node, parts)",path.startswith(sep),74,node.root,False,10.122592925934278,N/A
"def __glob(self, node, parts):
<mask>:
        assert node is not None
    if not parts:
        return [node]
    name = parts[0]
    remainder = parts[1:]
    if name == '..':
        parent = node.parent
        if parent is None:
            if self.relax:
                return []
            raise RootResolverError(node)
        return self.__glob(parent, remainder)
    if name in ('', '.'):
        return self.__glob(node, remainder)
    if name == '**':
        matches = []
        for subnode in PreOrderIter(node):
            try:
                for match in self.__glob(subnode, remainder):
                    if match not in matches:
                        matches.append(match)
            except ChildResolverError:
                pass
        return matches
    matches = self.__find(node, name, remainder)
    if not matches and (not Resolver.is_wildcard(name)) and (not self.relax):
        raise ChildResolverError(node, name, self.pathattr)
    return matches",ASSERTIONS,97,not parts,False,0.0,N/A
"def _findall(node, filter_, stop=None, maxlevel=None, mincount=None, maxcount=None):
    result = tuple(PreOrderIter(node, filter_, stop, maxlevel))
    resultlen = len(result)
<mask>:
        msg = 'Expecting at least %d elements, but found %d.'
        raise CountError(msg % (mincount, resultlen), result)
    if maxcount is not None and resultlen > maxcount:
        msg = 'Expecting %d elements at maximum, but found %d.'
        raise CountError(msg % (maxcount, resultlen), result)
    return result",mincount is not None and resultlen < mincount,60,mincount is not None and resultlen < mincount,True,100.00000000000004,N/A
"def __init__(self, msg, result):
    """"""Error raised on `mincount` or `maxcount` mismatch.""""""
<mask>:
        msg += ' ' + repr(result)
    super(CountError, self).__init__(msg)",result,20,result is not None,False,15.97357760615681,N/A
"def __import(self, data, parent=None):
<mask>:
        assert isinstance(data, dict)
        assert 'parent' not in data
    attrs = dict(data)
    children = attrs.pop('children', [])
    node = self.nodecls(parent=parent, **attrs)
    for child in children:
        self.__import(child, parent=node)
    return node",ASSERTIONS,32,parent is None,False,0.0,N/A
"def commonancestors(*nodes):
    """"""
    Determine common ancestors of `nodes`.

    >>> from anytree import Node, util
    >>> udo = Node(""Udo"")
    >>> marc = Node(""Marc"", parent=udo)
    >>> lian = Node(""Lian"", parent=marc)
    >>> dan = Node(""Dan"", parent=udo)
    >>> jet = Node(""Jet"", parent=dan)
    >>> jan = Node(""Jan"", parent=dan)
    >>> joe = Node(""Joe"", parent=dan)

    >>> util.commonancestors(jet, joe)
    (Node('/Udo'), Node('/Udo/Dan'))
    >>> util.commonancestors(jet, marc)
    (Node('/Udo'),)
    >>> util.commonancestors(jet)
    (Node('/Udo'), Node('/Udo/Dan'))
    >>> util.commonancestors()
    ()
    """"""
    ancestors = [node.ancestors for node in nodes]
    common = []
    for parentnodes in zip(*ancestors):
        parentnode = parentnodes[0]
<mask>:
            common.append(parentnode)
        else:
            break
    return tuple(common)",all((parentnode is p for p in parentnodes[1:])),88,parentnode is not None,False,1.5905552007142194,N/A
"def leftsibling(node):
    """"""
    Return Left Sibling of `node`.

    >>> from anytree import Node, util
    >>> dan = Node(""Dan"")
    >>> jet = Node(""Jet"", parent=dan)
    >>> jan = Node(""Jan"", parent=dan)
    >>> joe = Node(""Joe"", parent=dan)
    >>> print(util.leftsibling(dan))
    None
    >>> print(util.leftsibling(jet))
    None
    >>> print(util.leftsibling(jan))
    Node('/Dan/Jet')
    >>> print(util.leftsibling(joe))
    Node('/Dan/Jan')
    """"""
<mask>:
        pchildren = node.parent.children
        idx = pchildren.index(node)
        if idx:
            return pchildren[idx - 1]
    return None",node.parent,61,node.parent,True,100.00000000000004,N/A
"def rightsibling(node):
    """"""
    Return Right Sibling of `node`.

    >>> from anytree import Node, util
    >>> dan = Node(""Dan"")
    >>> jet = Node(""Jet"", parent=dan)
    >>> jan = Node(""Jan"", parent=dan)
    >>> joe = Node(""Joe"", parent=dan)
    >>> print(util.rightsibling(dan))
    None
    >>> print(util.rightsibling(jet))
    Node('/Dan/Jan')
    >>> print(util.rightsibling(jan))
    Node('/Dan/Joe')
    >>> print(util.rightsibling(joe))
    None
    """"""
<mask>:
        pchildren = node.parent.children
        idx = pchildren.index(node)
        try:
            return pchildren[idx + 1]
        except IndexError:
            return None
    else:
        return None",node.parent,65,node.parent,True,100.00000000000004,N/A
"def __iter_options(self, indent):
    options = self.options
<mask>:
        for option in options:
            yield ('%s%s' % (indent, option))",options,16,options,True,100.00000000000004,N/A
"def __iter_edges(self, indent, nodenamefunc, edgeattrfunc, edgetypefunc, filter_):
    maxlevel = self.maxlevel - 1 if self.maxlevel else None
    for node in PreOrderIter(self.node, filter_=filter_, stop=self.stop, maxlevel=maxlevel):
        nodename = nodenamefunc(node)
        for child in node.children:
<mask>:
                continue
            childname = nodenamefunc(child)
            edgeattr = edgeattrfunc(node, child)
            edgetype = edgetypefunc(node, child)
            edgeattr = ' [%s]' % edgeattr if edgeattr is not None else ''
            yield ('%s""%s"" %s ""%s""%s;' % (indent, DotExporter.esc(nodename), edgetype, DotExporter.esc(childname), edgeattr))",not filter_(child),66,"not isinstance(child, DotTree)",False,15.619699684601283,N/A
"def __export(self, node, dictcls, attriter, childiter, level=1):
    attr_values = attriter(self._iter_attr_values(node))
    data = dictcls(attr_values)
    maxlevel = self.maxlevel
<mask>:
        children = [self.__export(child, dictcls, attriter, childiter, level=level + 1) for child in childiter(node.children)]
        if children:
            data['children'] = children
    return data",maxlevel is None or level < maxlevel,37,level < maxlevel,False,26.359713811572682,N/A
"@staticmethod
def _iter_attr_values(node):
    for k, v in node.__dict__.items():
<mask>:
            continue
        yield (k, v)","k in ('_NodeMixin__children', '_NodeMixin__parent')",13,"k in ['value', 'keys']",False,3.4835119683384828,N/A
"def __iter_edges(self, indent, nodenamefunc, edgefunc, filter_, stop):
    maxlevel = self.maxlevel - 1 if self.maxlevel else None
    for node in PreOrderIter(self.node, filter_=filter_, stop=stop, maxlevel=maxlevel):
        nodename = nodenamefunc(node)
        for child in node.children:
<mask>:
                childname = nodenamefunc(child)
                edge = edgefunc(node, child)
                yield ('%s%s%s%s' % (indent, nodename, edge, childname))",filter_(child) and (not stop(child)),45,"isinstance(child, Node)",False,5.594422941553801,N/A
"def _export(self, node):
    dictexporter = self.dictexporter or DictExporter()
<mask>:
        dictexporter.maxlevel = self.maxlevel
    return dictexporter.export(node)",self.maxlevel is not None,14,self.maxlevel is not None,True,100.00000000000004,N/A
"def __getattr__(self, name):
<mask>:
        return super(SymlinkNodeMixin, self).__getattr__(name)
    if name == '__setstate__':
        raise AttributeError(name)
    return getattr(self.target, name)","name in ('_NodeMixin__parent', '_NodeMixin__children')",16,name == 'target',False,0.6193628179172647,N/A
"def __setattr__(self, name, value):
<mask>:
        super(SymlinkNodeMixin, self).__setattr__(name, value)
    else:
        setattr(self.target, name, value)","name in ('_NodeMixin__parent', '_NodeMixin__children', 'parent', 'children', 'target')",12,name == 'target',False,0.164346668917794,N/A
"def __init__(self, target, parent=None, children=None, **kwargs):
    self.target = target
    self.target.__dict__.update(kwargs)
    self.parent = parent
<mask>:
        self.children = children",children,17,children is not None,False,15.97357760615681,N/A
"@property
def parent(self):
    """"""
        Parent Node.

        On set, the node is detached from any previous parent node and attached
        to the new node.

        >>> from anytree import Node, RenderTree
        >>> udo = Node(""Udo"")
        >>> marc = Node(""Marc"")
        >>> lian = Node(""Lian"", parent=marc)
        >>> print(RenderTree(udo))
        Node('/Udo')
        >>> print(RenderTree(marc))
        Node('/Marc')
        └── Node('/Marc/Lian')

        **Attach**

        >>> marc.parent = udo
        >>> print(RenderTree(udo))
        Node('/Udo')
        └── Node('/Udo/Marc')
            └── Node('/Udo/Marc/Lian')

        **Detach**

        To make a node to a root node, just set this attribute to `None`.

        >>> marc.is_root
        False
        >>> marc.parent = None
        >>> marc.is_root
        True
        """"""
<mask>:
        return self.__parent
    return None","hasattr(self, '_LightNodeMixin__parent')",93,self.__parent is not None,False,8.400788786839632,N/A
"@parent.setter
def parent(self, value):
<mask>:
        parent = self.__parent
    else:
        parent = None
    if parent is not value:
        self.__check_loop(value)
        self.__detach(parent)
        self.__attach(value)","hasattr(self, '_LightNodeMixin__parent')",20,self.__parent,False,7.121297464907233,N/A
"def __check_loop(self, node):
<mask>:
        if node is self:
            msg = 'Cannot set parent. %r cannot be parent of itself.'
            raise LoopError(msg % (self,))
        if any((child is self for child in node.iter_path_reverse())):
            msg = 'Cannot set parent. %r is parent of %r.'
            raise LoopError(msg % (self, node))",node is not None,46,not self.parent,False,15.97357760615681,N/A
"def __detach(self, parent):
<mask>:
        self._pre_detach(parent)
        parentchildren = parent.__children_or_empty
        if ASSERTIONS:
            assert any((child is self for child in parentchildren)), 'Tree is corrupt.'
        parent.__children = [child for child in parentchildren if child is not self]
        self.__parent = None
        self._post_detach(parent)",parent is not None,37,parent,False,4.9787068367863965,N/A
"def __attach(self, parent):
<mask>:
        self._pre_attach(parent)
        parentchildren = parent.__children_or_empty
        if ASSERTIONS:
            assert not any((child is self for child in parentchildren)), 'Tree is corrupt.'
        parentchildren.append(self)
        self.__parent = parent
        self._post_attach(parent)",parent is not None,27,self.__parent is not None,False,34.57207846419409,N/A
"@parent.setter
def parent(self, value):
<mask>:
        msg = ""Parent node %r is not of type 'NodeMixin'."" % (value,)
        raise TreeError(msg)
    if hasattr(self, '_NodeMixin__parent'):
        parent = self.__parent
    else:
        parent = None
    if parent is not value:
        self.__check_loop(value)
        self.__detach(parent)
        self.__attach(value)","value is not None and (not isinstance(value, (NodeMixin, LightNodeMixin)))",37,"not isinstance(value, NodeMixin)",False,13.892958602871047,N/A
"def __init__(self, name, parent=None, children=None, **kwargs):
    self.__dict__.update(kwargs)
    self.name = name
    self.parent = parent
<mask>:
        self.children = children",children,17,children is not None,False,15.97357760615681,N/A
"def __init__(self, parent=None, children=None, **kwargs):
    self.__dict__.update(kwargs)
    self.parent = parent
<mask>:
        self.children = children",children,13,children is not None,False,15.97357760615681,N/A
"@staticmethod
def _iter(children, filter_, stop, maxlevel):
    level = 1
    while children:
        yield tuple((child for child in children if filter_(child)))
        level += 1
<mask>:
            break
        children = LevelOrderGroupIter._get_grandchildren(children, stop)","AbstractIter._abort_at_level(level, maxlevel)",28,level > maxlevel,False,1.2367482744213496,N/A
"@staticmethod
def _iter(children, filter_, stop, maxlevel):
    for child_ in children:
<mask>:
            continue
        if filter_(child_):
            yield child_
        if not AbstractIter._abort_at_level(2, maxlevel):
            descendantmaxlevel = maxlevel - 1 if maxlevel else None
            for descendant_ in PreOrderIter._iter(child_.children, filter_, stop, descendantmaxlevel):
                yield descendant_",stop(child_),38,stop(child_),True,100.00000000000004,N/A
"@staticmethod
def __next(children, level, filter_, stop, maxlevel):
<mask>:
        for child in children:
            grandchildren = AbstractIter._get_children(child.children, stop)
            for grandchild in PostOrderIter.__next(grandchildren, level + 1, filter_, stop, maxlevel):
                yield grandchild
            if filter_(child):
                yield child","not AbstractIter._abort_at_level(level, maxlevel)",32,level < len(children),False,2.8157908010020885,N/A
"@staticmethod
def _iter(children, filter_, stop, maxlevel):
    level = 1
    while children:
        next_children = []
        level += 1
<mask>:
            for child in children:
                if filter_(child):
                    yield child
        else:
            for child in children:
                if filter_(child):
                    yield child
                next_children += AbstractIter._get_children(child.children, stop)
        children = next_children","AbstractIter._abort_at_level(level, maxlevel)",42,level > maxlevel,False,1.2367482744213496,N/A
"@staticmethod
def _iter(children, filter_, stop, maxlevel):
<mask>:
        if ASSERTIONS:
            assert len(children) == 1
        _iter = LevelOrderGroupIter(children[0], filter_, stop, maxlevel)
        while True:
            try:
                yield next(_iter)
                yield tuple(reversed(next(_iter)))
            except StopIteration:
                break",children,29,children,True,100.00000000000004,N/A
"def os_walk_pre_35(top, topdown=True, onerror=None, followlinks=False):
    """"""Pre Python 3.5 implementation of os.walk() that doesn't use scandir.""""""
    islink, join, isdir = (os.path.islink, os.path.join, os.path.isdir)
    try:
        names = os.listdir(top)
    except OSError as err:
<mask>:
            onerror(err)
        return
    dirs, nondirs = ([], [])
    for name in names:
        if isdir(join(top, name)):
            dirs.append(name)
        else:
            nondirs.append(name)
    if topdown:
        yield (top, dirs, nondirs)
    for name in dirs:
        new_path = join(top, name)
        if followlinks or not islink(new_path):
            for x in os_walk_pre_35(new_path, topdown, onerror, followlinks):
                yield x
    if not topdown:
        yield (top, dirs, nondirs)",onerror is not None,83,onerror,False,4.9787068367863965,N/A
"def create_tree(path, depth=DEPTH):
    """"""Create a directory tree at path with given depth, and NUM_DIRS and
    NUM_FILES at each level.
    """"""
    os.mkdir(path)
    for i in range(NUM_FILES):
        filename = os.path.join(path, 'file{0:03}.txt'.format(i))
        with open(filename, 'wb') as f:
            f.write(b'foo')
<mask>:
        return
    for i in range(NUM_DIRS):
        dirname = os.path.join(path, 'dir{0:03}'.format(i))
        create_tree(dirname, depth - 1)",depth <= 1,49,depth == 0,False,18.99589214128981,N/A
"def get_tree_size(path):
    """"""Return total size of all files in directory tree at path.""""""
    size = 0
    try:
        for entry in scandir.scandir(path):
<mask>:
                pass
            elif entry.is_dir():
                size += get_tree_size(os.path.join(path, entry.name))
            else:
                size += entry.stat().st_size
    except OSError:
        pass
    return size",entry.is_symlink(),38,entry.is_file(),False,48.892302243490086,N/A
"def benchmark(path, get_size=False):
    sizes = {}
<mask>:

        def do_os_walk():
            size = 0
            for root, dirs, files in os.walk(path):
                for filename in files:
                    fullname = os.path.join(root, filename)
                    st = os.lstat(fullname)
                    if not stat.S_ISLNK(st.st_mode):
                        size += st.st_size
            sizes['os_walk'] = size

        def do_scandir_walk():
            sizes['scandir_walk'] = get_tree_size(path)
    else:

        def do_os_walk():
            for root, dirs, files in os.walk(path):
                pass

        def do_scandir_walk():
            for root, dirs, files in scandir.walk(path):
                pass
    print(""Priming the system's cache..."")
    do_scandir_walk()
    os_walk_time = 1000000
    scandir_walk_time = 1000000
    N = 3
    for i in range(N):
        print('Benchmarking walks on {0}, repeat {1}/{2}...'.format(path, i + 1, N))
        os_walk_time = min(os_walk_time, timeit.timeit(do_os_walk, number=1))
        scandir_walk_time = min(scandir_walk_time, timeit.timeit(do_scandir_walk, number=1))
    if get_size:
        if sizes['os_walk'] == sizes['scandir_walk']:
            equality = 'equal'
        else:
            equality = 'NOT EQUAL!'
        print('os.walk size {0}, scandir.walk size {1} -- {2}'.format(sizes['os_walk'], sizes['scandir_walk'], equality))
    print('os.walk took {0:.3f}s, scandir.walk took {1:.3f}s -- {2:.1f}x as fast'.format(os_walk_time, scandir_walk_time, os_walk_time / scandir_walk_time))",get_size,138,os.path.isdir(path),False,0.0,N/A
"def build_extension(self, ext):
    try:
        base_build_ext.build_extension(self, ext)
    except Exception:
<mask>:
            logging.error('SCANDIR_REQUIRE_C_EXTENSION is set, not falling back to Python implementation')
            raise
        info = sys.exc_info()
        logging.warn('building %s failed with %s: %s', ext.name, info[0], info[1])",require_c_extension,31,self.config.getboolean('SCANDIR_REQUIRE_C_EXTENSION'),False,3.377156414337854,N/A
"@property
def path(self):
<mask>:
        self._path = join(self._scandir_path, self.name)
    return self._path",self._path is None,10,not self._path,False,54.75182535069452,N/A
"def stat(self, follow_symlinks=True):
<mask>:
        if self._stat is None:
            self._stat = stat(self.path)
        return self._stat
    else:
        if self._lstat is None:
            self._lstat = lstat(self.path)
        return self._lstat",follow_symlinks,23,follow_symlinks,True,100.00000000000004,N/A
"def is_dir(self, follow_symlinks=True):
    try:
        st = self.stat(follow_symlinks=follow_symlinks)
    except OSError as e:
<mask>:
            raise
        return False
    return st.st_mode & 61440 == S_IFDIR",e.errno != ENOENT,21,e.errno != errno.EEXIST,False,51.697315395717055,N/A
"def is_file(self, follow_symlinks=True):
    try:
        st = self.stat(follow_symlinks=follow_symlinks)
    except OSError as e:
<mask>:
            raise
        return False
    return st.st_mode & 61440 == S_IFREG",e.errno != ENOENT,21,e.errno != errno.ENOENT,False,54.10822690539397,N/A
"def is_symlink(self):
    try:
        st = self.stat(follow_symlinks=False)
    except OSError as e:
<mask>:
            raise
        return False
    return st.st_mode & 61440 == S_IFLNK",e.errno != ENOENT,20,e.errno != errno.EEXIST,False,51.697315395717055,N/A
"def setup_symlinks():
    join = os.path.join
    os.mkdir(join(TEST_PATH, 'linkdir', 'linksubdir'))
    create_file(join(TEST_PATH, 'linkdir', 'file1.txt'))
    os.symlink(os.path.abspath(join(TEST_PATH, 'linkdir', 'file1.txt')), join(TEST_PATH, 'linkdir', 'link_to_file'))
    dir_name = os.path.abspath(join(TEST_PATH, 'linkdir', 'linksubdir'))
    dir_link = join(TEST_PATH, 'linkdir', 'link_to_dir')
<mask>:
        os.symlink(dir_name, dir_link, target_is_directory=True)
    else:
        os.symlink(dir_name, dir_link)",IS_PY3,34,os.path.isdir(dir_name),False,4.196114906296549,N/A
"def setUp(self):
<mask>:
        setup_main()
    if symlinks_supported and (not os.path.exists(os.path.join(TEST_PATH, 'linkdir', 'linksubdir'))):
        setup_symlinks()",not os.path.exists(TEST_PATH),12,main_supported,False,1.9119108411650758,N/A
"def test_stat(self):
    entries = list(self.scandir_func(TEST_PATH))
    for entry in entries:
        os_stat = os.stat(os.path.join(TEST_PATH, entry.name))
        scandir_stat = entry.stat()
        self.assertEqual(os_stat.st_mode, scandir_stat.st_mode)
        self.assertAlmostEqual(os_stat.st_mtime, scandir_stat.st_mtime, delta=1)
        self.assertAlmostEqual(os_stat.st_ctime, scandir_stat.st_ctime, delta=1)
<mask>:
            self.assertEqual(os_stat.st_size, scandir_stat.st_size)",entry.is_file(),27,entry.name == 'stat',False,13.741272855400096,N/A
"def test_file_attributes(self):
<mask>:
        return self.skipTest('st_file_attributes not supported')
    entries = dict(((e.name, e) for e in self.scandir_func(TEST_PATH)))
    result = entries['file1.txt'].stat()
    self.check_file_attributes(result)
    self.assertEqual(result.st_file_attributes & FILE_ATTRIBUTE_DIRECTORY, 0)
    result = entries['subdir'].stat()
    self.check_file_attributes(result)
    self.assertEqual(result.st_file_attributes & FILE_ATTRIBUTE_DIRECTORY, FILE_ATTRIBUTE_DIRECTORY)",sys.platform != 'win32' or not self.has_file_attributes,31,not ST_FILE_ATTRIBUTES,False,2.383515454163372,N/A
"def test_symlink(self):
<mask>:
        return self.skipTest('symbolic links not supported')
    entries = sorted(self.scandir_func(os.path.join(TEST_PATH, 'linkdir')), key=lambda e: e.name)
    self.assertEqual([(e.name, e.is_symlink()) for e in entries], [('file1.txt', False), ('link_to_dir', True), ('link_to_file', True), ('linksubdir', False)])
    self.assertEqual([(e.name, e.is_file(), e.is_file(follow_symlinks=False)) for e in entries], [('file1.txt', True, True), ('link_to_dir', False, False), ('link_to_file', True, False), ('linksubdir', False, False)])
    self.assertEqual([(e.name, e.is_dir(), e.is_dir(follow_symlinks=False)) for e in entries], [('file1.txt', False, False), ('link_to_dir', True, False), ('link_to_file', False, False), ('linksubdir', True, True)])",not symlinks_supported,67,not HAS_SYNCHRONOUS,False,18.99589214128981,N/A
"def test_traversal(self):
    walk_path = os.path.join(self.testfn, 'TEST1')
    sub1_path = os.path.join(walk_path, 'SUB1')
    sub11_path = os.path.join(sub1_path, 'SUB11')
    sub2_path = os.path.join(walk_path, 'SUB2')
    tmp1_path = os.path.join(walk_path, 'tmp1')
    tmp2_path = os.path.join(sub1_path, 'tmp2')
    tmp3_path = os.path.join(sub2_path, 'tmp3')
    link_path = os.path.join(sub2_path, 'link')
    t2_path = os.path.join(self.testfn, 'TEST2')
    tmp4_path = os.path.join(self.testfn, 'TEST2', 'tmp4')
    os.makedirs(sub11_path)
    os.makedirs(sub2_path)
    os.makedirs(t2_path)
    for path in (tmp1_path, tmp2_path, tmp3_path, tmp4_path):
        f = open(path, 'w')
        f.write(""I'm "" + path + ' and proud of it.  Blame test_os.\n')
        f.close()
    has_symlink = hasattr(os, 'symlink')
<mask>:
        try:
            if IS_PY3:
                os.symlink(os.path.abspath(t2_path), link_path, target_is_directory=True)
            else:
                os.symlink(os.path.abspath(t2_path), link_path)
            sub2_tree = (sub2_path, ['link'], ['tmp3'])
        except NotImplementedError:
            sub2_tree = (sub2_path, [], ['tmp3'])
    else:
        sub2_tree = (sub2_path, [], ['tmp3'])
    all = list(walk_func(walk_path))
    self.assertEqual(len(all), 4)
    flipped = all[0][1][0] != 'SUB1'
    all[0][1].sort()
    self.assertEqual(all[0], (walk_path, ['SUB1', 'SUB2'], ['tmp1']))
    self.assertEqual(all[1 + flipped], (sub1_path, ['SUB11'], ['tmp2']))
    self.assertEqual(all[2 + flipped], (sub11_path, [], []))
    self.assertEqual(all[3 - 2 * flipped], sub2_tree)
    all = []
    for root, dirs, files in walk_func(walk_path):
        all.append((root, dirs, files))
        if 'SUB1' in dirs:
            dirs.remove('SUB1')
    self.assertEqual(len(all), 2)
    self.assertEqual(all[0], (walk_path, ['SUB2'], ['tmp1']))
    self.assertEqual(all[1], sub2_tree)
    all = list(walk_func(walk_path, topdown=False))
    self.assertEqual(len(all), 4)
    flipped = all[3][1][0] != 'SUB1'
    all[3][1].sort()
    self.assertEqual(all[3], (walk_path, ['SUB1', 'SUB2'], ['tmp1']))
    self.assertEqual(all[flipped], (sub11_path, [], []))
    self.assertEqual(all[flipped + 1], (sub1_path, ['SUB11'], ['tmp2']))
    self.assertEqual(all[2 - 2 * flipped], sub2_tree)
    if has_symlink:
        for root, dirs, files in walk_func(walk_path, followlinks=True):
            if root == link_path:
                self.assertEqual(dirs, [])
                self.assertEqual(files, ['tmp4'])
                break
        else:
            self.fail(""Didn't follow symlink with followlinks=True"")
    sub3_path = os.path.join(walk_path, 'SUB3')
    all = []
    for root, dirs, files in walk_func(walk_path):
        all.append((root, dirs, files))
        if 'SUB1' in dirs:
            os.makedirs(sub3_path)
            dirs.append('SUB3')
    all.sort()
    self.assertEqual(os.path.split(all[-1][0])[1], 'SUB3')",has_symlink,243,has_symlink,True,100.00000000000004,N/A
"def tearDown(self):
    for root, dirs, files in os.walk(self.testfn, topdown=False):
        for name in files:
            os.remove(os.path.join(root, name))
        for name in dirs:
            dirname = os.path.join(root, name)
<mask>:
                os.rmdir(dirname)
            else:
                os.remove(dirname)
    os.rmdir(self.testfn)",not os.path.islink(dirname),28,os.path.isdir(dirname),False,44.12484512922978,N/A
"def test_symlink_to_file(self):
<mask>:
        return
    try:
        os.symlink(self.file_name, os.path.join(self.temp_dir, 'link_to_file'))
    except NotImplementedError:
        return
    output = sorted(walk_func(self.temp_dir))
    dirs = sorted(output[0][1])
    files = sorted(output[0][2])
    self.assertEqual(dirs, ['dir'])
    self.assertEqual(files, ['file', 'link_to_file'])
    self.assertEqual(len(output), 2)
    self.assertEqual(output[1][1], [])
    self.assertEqual(output[1][2], ['subfile'])","not hasattr(os, 'symlink')",31,not os.path.exists(self.temp_dir),False,4.368583925857938,N/A
"def test_symlink_to_directory(self):
<mask>:
        return
    link_name = os.path.join(self.temp_dir, 'link_to_dir')
    try:
        if IS_PY3:
            os.symlink(self.dir_name, link_name, target_is_directory=True)
        else:
            os.symlink(self.dir_name, link_name)
    except NotImplementedError:
        return
    output = sorted(walk_func(self.temp_dir))
    dirs = sorted(output[0][1])
    files = sorted(output[0][2])
    self.assertEqual(dirs, ['dir', 'link_to_dir'])
    self.assertEqual(files, ['file'])
    self.assertEqual(len(output), 2)
    self.assertEqual(output[1][1], [])
    self.assertEqual(output[1][2], ['subfile'])
    output = sorted(walk_func(self.temp_dir, followlinks=True))
    dirs = sorted(output[0][1])
    files = sorted(output[0][2])
    self.assertEqual(dirs, ['dir', 'link_to_dir'])
    self.assertEqual(files, ['file'])
    self.assertEqual(len(output), 3)
    self.assertEqual(output[1][1], [])
    self.assertEqual(output[1][2], ['subfile'])
    self.assertEqual(os.path.basename(output[2][0]), 'link_to_dir')
    self.assertEqual(output[2][1], [])
    self.assertEqual(output[2][2], ['subfile'])","not hasattr(os, 'symlink')",67,not IS_PY3,False,7.545383788761362,N/A
"def test_get_tile_tif(self):
    """"""Test reading of tile from geotiff""""""
    tile = '1087767-1046604-21'
    dest_folder = 'test'
    tiles_dir = op.join(dest_folder, 'tiles')
<mask>:
        makedirs(tiles_dir)
    get_tile_tif(tile, 'test/fixtures/drone.tif', tiles_dir, {})
    test_tile = Image.open('test/tiles/{}.tif'.format(tile))
    fixture_tile = Image.open('test/fixtures/{}.tif'.format(tile))
    self.assertEqual(test_tile, fixture_tile)",not op.isdir(tiles_dir),32,not op.exists(tiles_dir),False,59.694917920196445,N/A
"def test_get_tile_tif_offset(self):
    """"""Test reading of tile from geotiff with imagery_offset, test fixture""""""
    tile = '1087767-1046604-21'
    dest_folder = 'test'
    tiles_dir = op.join(dest_folder, 'tiles')
    print(tiles_dir)
<mask>:
        makedirs(tiles_dir)
    get_tile_tif(tile, 'test/fixtures/drone.tif', tiles_dir, {'imagery_offset': [128, 64]})
    test_tile = Image.open('test/tiles/{}.tif'.format(tile))
    fixture_tile = Image.open('test/fixtures/{}_offset.tif'.format(tile))
    self.assertEqual(test_tile, fixture_tile)",not op.isdir(tiles_dir),39,not op.exists(tiles_dir),False,59.694917920196445,N/A
"def test_get_tile_vrt(self):
    """"""Test reading of tile from a virtual raster""""""
    tile = '1087767-1046604-21'
    dest_folder = 'test'
    tiles_dir = op.join(dest_folder, 'tiles')
<mask>:
        makedirs(tiles_dir)
    get_tile_tif(tile, 'test/fixtures/drone.vrt', tiles_dir, {})
    test_tile = Image.open('test/tiles/{}.tif'.format(tile))
    fixture_tile = Image.open('test/fixtures/{}.tif'.format(tile))
    self.assertEqual(test_tile, fixture_tile)",not op.isdir(tiles_dir),34,not op.exists(tiles_dir),False,59.694917920196445,N/A
"def test_get_tile_wms(self):
    """"""Test reading of tile from a WMS endpoint""""""
    tile = '146-195-9'
    dest_folder = 'test'
    tiles_dir = op.join(dest_folder, 'tiles')
<mask>:
        makedirs(tiles_dir)
    nasa_url = 'https://gibs.earthdata.nasa.gov/wms/epsg4326/best/wms.cgi?SERVICE=WMS&REQUEST=GetMap&layers=MODIS_Aqua_CorrectedReflectance_TrueColor&version=1.3.0&crs=EPSG:4326&transparent=false&width=256&height=256&bbox={bbox}&format=image/jpeg&time=2019-03-05'
    get_tile_wms(tile, nasa_url, tiles_dir, {})
    test_tile = Image.open('test/tiles/{}.jpeg'.format(tile))
    fixture_tile = Image.open('test/fixtures/{}.jpeg'.format(tile))
    self.assertEqual(test_tile, fixture_tile)",not op.isdir(tiles_dir),37,not op.exists(tiles_dir),False,59.694917920196445,N/A
"def pred_bbox():
    pred_bboxes = list()
    with detection_graph.as_default():
        with tf.Session(graph=detection_graph) as sess:
            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
            detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
            detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
            detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')
            num_detections = detection_graph.get_tensor_by_name('num_detections:0')
            for image_path in test_imgs:
                image = Image.open(image_path)
                image_np = load_image_into_numpy_array(image)
                image_np_expanded = np.expand_dims(image_np, axis=0)
                boxes, scores, classes, num = sess.run([detection_boxes, detection_scores, detection_classes, num_detections], feed_dict={image_tensor: image_np_expanded})
                bboxe = (boxes * 256).astype(np.int)
                bboxe = np.squeeze(bboxe)
                score = np.squeeze((scores * 100).transpose().astype(np.int))
                bboxes = bboxe[score > 50]
<mask>:
                    bboxes_ls = bboxes.tolist()
                    for bbox in bboxes_ls:
                        pred_bboxes.append(bbox)
    return pred_bboxes",bboxes.any(),80,len(bboxes) > 0,False,10.682175159905853,N/A
"def gr_bbox():
    gr_data = np.load('labels.npz')
    tile_names = [tile for tile in gr_data.files]
    tiles = np.array(tile_names)
    gr_bboxes = list()
    for tile in tiles:
        bboxes = gr_data[tile].tolist()
        bbox_info = list()
<mask>:
            for bbox in bboxes:
                bbox = [max(0, min(255, x)) for x in bbox[:4]]
                bbox = [bbox[1], bbox[0], bbox[3], bbox[2]]
            gr_bboxes.append(bbox)
    return gr_bboxes",bboxes,51,len(bboxes) > 0,False,8.116697886877475,N/A
"def get_iou():
    pred_bboxes = pred_bbox()
    gr_bboxes = gr_bbox()
    iou_out = list()
    for pred_box in pred_bboxes:
        for gr_box in gr_bboxes:
            try:
                iou = bb_IOU(pred_box, gr_box)
<mask>:
                    iou_out.append(iou)
            except:
                pass
    return iou_out",iou >= 0.5,30,iou.is_valid(),False,6.567274736060395,N/A
"def main(_):
    labels = np.load(op.join(os.getcwd(), FLAGS.label_input))
    tile_names = [tile for tile in labels.files]
    tile_names.sort()
    tiles = np.array(tile_names)
    tf_tiles_info = []
    for tile in tiles:
        bboxes = labels[tile].tolist()
        width = 256
        height = 256
<mask>:
            for bbox in bboxes:
                if bbox[4] == 1:
                    cl_str = 'building'
                    bbox = [max(0, min(255, x)) for x in bbox[0:4]]
                    y = ['{}.jpg'.format(tile), width, height, cl_str, bbox[0], bbox[1], bbox[2], bbox[3]]
                    tf_tiles_info.append(y)
    split_index = int(len(tf_tiles_info) * 0.8)
    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']
    df = pd.DataFrame(tf_tiles_info, columns=column_name)
    df = df.sample(frac=1)
    train_df = df[:split_index]
    test_df = df[split_index:]
    print('You have {} training tiles and {} test tiles ready'.format(len(set(train_df['filename'])), len(set(test_df['filename']))))
    tiles_dir = op.join(os.getcwd(), 'tiles')
    train_dir = op.join(os.getcwd(), 'images', 'train')
    test_dir = op.join(os.getcwd(), 'images', 'test')
    if not op.isdir(train_dir):
        makedirs(train_dir)
    if not op.isdir(test_dir):
        makedirs(test_dir)
    for tile in train_df['filename']:
        tile_dir = op.join(tiles_dir, tile)
        shutil.copy(tile_dir, train_dir)
    for tile in test_df['filename']:
        tile_dir = op.join(tiles_dir, tile)
        shutil.copy(tile_dir, test_dir)
    writer = tf.python_io.TFRecordWriter(FLAGS.train_rd_path)
    grouped = split(train_df, 'filename')
    for group in grouped:
        tf_example = create_tf_example(group, train_dir)
        writer.write(tf_example.SerializeToString())
    writer.close()
    output_train = op.join(os.getcwd(), FLAGS.train_rd_path)
    print('Successfully created the TFRecords: {}'.format(output_train))
    writer = tf.python_io.TFRecordWriter(FLAGS.test_rd_path)
    grouped = split(test_df, 'filename')
    for group in grouped:
        tf_example = create_tf_example(group, test_dir)
        writer.write(tf_example.SerializeToString())
    writer.close()
    output_test = op.join(os.getcwd(), FLAGS.test_rd_path)
    print('Successfully created the TFRecords: {}'.format(output_test))",bboxes,198,len(bboxes) > 4,False,8.116697886877475,N/A
"def preview(dest_folder, number, classes, imagery, ml_type, imagery_offset=False, **kwargs):
    """"""Produce imagery examples for specified classes

    Parameters
    ------------
    dest_folder: str
        Folder to save labels and example tiles into
    number: int
        Number of preview images to download per class
    classes: list
        A list of classes for machine learning training. Each class is defined as a dict
        with two required properties:
          - name: class name
          - filter: A Mapbox GL Filter.
        See the README for more details
    imagery: str
        Imagery template to download satellite images from.
        Ex: http://a.tiles.mapbox.com/v4/mapbox.satellite/{z}/{x}/{y}.jpg?access_token=ACCESS_TOKEN
    ml_type: str
        Defines the type of machine learning. One of ""classification"", ""object-detection"", or ""segmentation""
    imagery_offset: list
        An optional list of integers representing the number of pixels to offset imagery. Ex. [15, -5] will
        move the images 15 pixels right and 5 pixels up relative to the requested tile bounds
    **kwargs: dict
        Other properties from CLI config passed as keywords to other utility functions
    """"""
    labels_file = op.join(dest_folder, 'labels.npz')
    tiles = np.load(labels_file)
    examples_dir = op.join(dest_folder, 'examples')
<mask>:
        makedirs(examples_dir)
    print('Writing example images to {}'.format(examples_dir))
    image_function = get_image_function(imagery)
    for i, cl in enumerate(classes):
        class_dir = op.join(dest_folder, 'examples', cl.get('name'))
        if not op.isdir(class_dir):
            makedirs(class_dir)
        class_tiles = (t for t in tiles.files if class_match(ml_type, tiles[t], i + 1))
        print('Downloading at most {} tiles for class {}'.format(number, cl.get('name')))
        for n, tile in enumerate(class_tiles):
            if n >= number:
                break
            kwargs['imagery_offset'] = imagery_offset
            tile_img = image_function(tile, imagery, class_dir, kwargs)
            if ml_type == 'object-detection':
                img = Image.open(tile_img)
                draw = ImageDraw.Draw(img)
                for box in tiles[tile]:
                    draw.rectangle(((box[0], box[1]), (box[2], box[3])), outline='red')
                img.save(tile_img)
            elif ml_type == 'segmentation':
                final = Image.new('RGB', (256, 256))
                img = Image.open(tile_img)
                mask = Image.fromarray(tiles[tile] * 255)
                final.paste(img, mask)
                final.save(tile_img)",not op.isdir(examples_dir),264,not op.isdir(examples_dir),True,100.00000000000004,N/A
"def download(url, path):
    """"""Download url to target path""""""
    file_size = int(requests.head(url).headers['Content-Length'])
    header = {'Range': 'bytes=%s-%s' % (0, file_size)}
    pbar = tqdm(total=file_size, unit='B', unit_scale=True, desc=url.split('/')[-1])
    req = requests.get(url, headers=header, stream=True)
    with open(path, 'ab') as f:
        for chunk in req.iter_content(chunk_size=1024):
<mask>:
                f.write(chunk)
                pbar.update(1024)
    pbar.close()
    return file_size",chunk,44,chunk,True,100.00000000000004,N/A
"def class_color(c):
    """"""Return 3-element tuple containing rgb values for a given class""""""
<mask>:
        return (0, 0, 0)
    return ImageColor.getrgb(colors[c % len(colors)])",c == 0,21,c == 0,True,100.00000000000004,N/A
"def class_match(ml_type, label, i):
    """"""Determine if a label matches a given class index""""""
<mask>:
        return label[i] > 0
    elif ml_type == 'object-detection':
        return len(list(filter(lambda bb: bb[4] == i, label)))
    elif ml_type == 'segmentation':
        return np.count_nonzero(label == i)
    return None",ml_type == 'classification',39,ml_type == 'boolean',False,75.98356856515926,N/A
"def get_image_format(imagery, kwargs):
<mask>:
        image_format = kwargs.get('tms_image_format')
    else:
        o = urlparse(imagery)
        _, image_format = op.splitext(o.path)
    return image_format",kwargs.get('tms_image_format'),17,'tms_image_format' in kwargs,False,41.91742490576712,N/A
"def download_tile_tms(tile, imagery, folder, kwargs):
    """"""Download a satellite image tile from a tms endpoint""""""
    image_format = get_image_format(imagery, kwargs)
<mask>:
        token = os.environ.get('ACCESS_TOKEN')
        imagery = imagery.format_map(SafeDict(ACCESS_TOKEN=token))
    r = requests.get(url(tile.split('-'), imagery), auth=kwargs.get('http_auth'))
    tile_img = op.join(folder, '{}{}'.format(tile, image_format))
    tile = tile.split('-')
    over_zoom = kwargs.get('over_zoom')
    if over_zoom:
        new_zoom = over_zoom + kwargs.get('zoom')
        child_tiles = children(int(tile[0]), int(tile[1]), int(tile[2]), zoom=new_zoom)
        child_tiles.sort()
        new_dim = 256 * (2 * over_zoom)
        w_lst = []
        for i in range(2 * over_zoom):
            for j in range(2 * over_zoom):
                window = Window(i * 256, j * 256, 256, 256)
                w_lst.append(window)
        with rasterio.open(tile_img, 'w', driver='jpeg', height=new_dim, width=new_dim, count=3, dtype=rasterio.uint8) as w:
            for num, t in enumerate(child_tiles):
                t = [str(t[0]), str(t[1]), str(t[2])]
                r = requests.get(url(t, imagery), auth=kwargs.get('http_auth'))
                img = np.array(Image.open(io.BytesIO(r.content)), dtype=np.uint8)
                try:
                    img = img.reshape((256, 256, 3))
                except ValueError:
                    img = img.reshape((256, 256, 4))
                img = img[:, :, :3]
                img = np.rollaxis(img, 2, 0)
                w.write(img, window=w_lst[num])
    else:
        r = requests.get(url(tile, imagery), auth=kwargs.get('http_auth'))
        with open(tile_img, 'wb') as w:
            w.write(r.content)
    return tile_img",os.environ.get('ACCESS_TOKEN'),156,'ACCESS_TOKEN' in os.environ,False,26.782849591300856,N/A
"def get_tile_wms(tile, imagery, folder, kwargs):
    """"""
    Read a WMS endpoint with query parameters corresponding to a TMS tile

    Converts the tile boundaries to the spatial/coordinate reference system
    (SRS or CRS) specified by the WMS query parameter.
    """"""
    query_dict = parse_qs(imagery.lower())
    image_format = query_dict.get('format')[0].split('/')[1]
    wms_version = query_dict.get('version')[0]
<mask>:
        wms_srs = query_dict.get('crs')[0]
    else:
        wms_srs = query_dict.get('srs')[0]
    bound = bounds(*[int(t) for t in tile.split('-')])
    xmin, ymin, xmax, ymax = transform_bounds(WGS84_CRS, CRS.from_string(wms_srs), *bound, densify_pts=21)
    bbox = [ymin, xmin, ymax, xmax] if wms_version == '1.3.0' else [xmin, ymin, xmax, ymax]
    wms_url = imagery.replace('{bbox}', ','.join([str(b) for b in bbox]))
    r = requests.get(wms_url, auth=kwargs.get('http_auth'))
    tile_img = op.join(folder, '{}.{}'.format(tile, image_format))
    with open(tile_img, 'wb') as w:
        w.write(r.content)
    return tile_img",wms_version == '1.3.0',110,wms_version == '1.3.0',True,100.00000000000004,N/A
"def is_tif(imagery):
    """"""Determine if an imagery path leads to a valid tif""""""
    valid_drivers = ['GTiff', 'VRT']
    try:
        with rasterio.open(imagery) as test_ds:
<mask>:
                valid_tif = False
            else:
                valid_tif = True
    except rasterio.errors.RasterioIOError:
        valid_tif = False
    return valid_tif",test_ds.meta['driver'] not in valid_drivers,36,test_ds[0] not in valid_drivers,False,50.30989330530425,N/A
"def _callback(tile_label):
    """"""Attach tile labels to a global tile_results dict""""""
<mask>:
        return
    global tile_results
    tile, label = tile_label
    tile_results[tile] = label",not tile_label,21,tile_label is None,False,39.76353643835252,N/A
"def _tile_results_summary(ml_type, classes):
    print('---')
    labels = list(tile_results.values())
    all_tiles = list(tile_results.keys())
<mask>:
        for i, cl in enumerate(classes):
            cl_features = len([bb for l in labels for bb in l if bb[4] == i + 1])
            cl_tiles = len([l for l in labels if len(list(filter(_bbox_class(i + 1), l)))])
            print('{}: {} features in {} tiles'.format(cl.get('name'), cl_features, cl_tiles))
    elif ml_type == 'classification':
        class_tile_counts = list(np.sum(labels, axis=0))
        for i, cl in enumerate(classes):
            print('{}: {} tiles'.format(cl.get('name'), int(class_tile_counts[i + 1])))
    elif ml_type == 'segmentation':
        for i, cl in enumerate(classes):
            count = len([l for l in labels if class_match(ml_type, l, i + 1)])
            print('{}: {} tiles'.format(cl.get('name'), count))
    print('Total tiles: {}'.format(len(all_tiles)))",ml_type == 'object-detection',101,ml_type == 'bbox',False,75.98356856515926,N/A
"def _compile(filt):
    """"""Return a string represented the compiled filter function""""""
<mask>:
        return 'True'
    op = filt[0]
    if len(filt) == 1:
        return 'False' if op == 'any' else 'True'
    if op in ['==', '!=', '<', '>', '<=', '>=']:
        return _compile_comparison_op(filt[1], filt[2], op)
    elif op == 'any':
        return _compile_logical_op(filt[1:], ' or ')
    elif op == 'all':
        return _compile_logical_op(filt[1:], ' and ')
    elif op == 'none':
        return _compile_negation(_compile_logical_op(filt[1:], ' or '))
    elif op == 'in':
        return _compile_in_op(filt[1], filt[2:])
    elif op == '!in':
        return _compile_negation(_compile_in_op(filt[1], filt[2:]))
    elif op == 'has':
        return _compile_has_op(filt[1])
    elif op == '!has':
        return _compile_negation(_compile_has_op(filt[1]))
    return 'True'",not filt,96,len(filt) == 0,False,6.567274736060395,N/A
"def _compile_property_reference(prop):
    """"""Find the correct reference on the input feature""""""
<mask>:
        return 'f.get(""geometry"").get(""type"")'
    elif prop == '$id':
        return 'f.get(""id"")'
    return 'p.get(""{}"")'.format(prop)",prop == '$type',21,prop == '$geometry',False,75.98356856515926,N/A
"def cli():
    """"""Validate input data and call the appropriate subcommand with necessary arguments""""""
    args = parse_args(sys.argv[1:])
    logger.setLevel(args.pop('log') * 10)
    cmd = args.pop('command')
    config = json.load(open(args.get('config')))
    dest_folder = args.get('dest')
<mask>:
        makedirs(dest_folder)
    v = Validator(schema)
    valid = v.validate(config)
    if not valid:
        raise Exception(v.errors)
    if 'geojson' not in config.keys() and (not ('country' in config.keys() and 'bounding_box' in config.keys())):
        raise Exception('either ""geojson"" or ""country"" and ""bounding_box"" must be present in the configuration JSON')
    if 'geojson' in config.keys():
        config['country'] = op.splitext(op.basename(config.get('geojson')))[0]
        config['bounding_box'] = get_bounds(json.load(open(config.get('geojson'), 'r')))
    if 'http_auth' in config.keys():
        config['http_auth'] = tuple(config['http_auth'])
    if cmd == 'download':
        download_mbtiles(dest_folder=dest_folder, **config)
    elif cmd == 'labels':
        sparse = args.get('sparse', False)
        make_labels(dest_folder=dest_folder, sparse=sparse, **config)
    elif cmd == 'preview':
        number = args.get('number')
        preview(dest_folder=dest_folder, number=number, **config)
    elif cmd == 'images':
        threadcount = args.get('threadcount')
        download_images(dest_folder=dest_folder, threadcount=threadcount, **config)
    elif cmd == 'package':
        package_directory(dest_folder=dest_folder, **config)",not op.isdir(dest_folder),130,not op.exists(dest_folder),False,59.694917920196445,N/A
"def __new__(cls, timestamp, nanoseconds=None):
    seconds = int(timestamp)
<mask>:
        nanoseconds = int(timestamp % 1 * 10 ** 9)
    return super().__new__(cls, code=0, data=struct.pack('>II', seconds, nanoseconds))",nanoseconds is None,23,nanoseconds is None,True,100.00000000000004,N/A
"def emit(self, label, data):
<mask>:
        cur_time = EventTime.from_unix_nano(time.time_ns())
    else:
        cur_time = int(time.time())
    return self.emit_with_time(label, cur_time, data)",self.nanosecond_precision,16,self.event_time_ns,False,14.535768424205482,N/A
"def emit_with_time(self, label, timestamp, data):
    try:
        bytes_ = self._make_packet(label, timestamp, data)
    except Exception as e:
<mask>:
            raise
        self.last_error = e
        bytes_ = self._make_packet(label, timestamp, {'level': 'CRITICAL', 'message': ""Can't output to log"", 'traceback': traceback.format_exc()})
    return self._send(bytes_)",not self.forward_packet_error,35,self.last_error != e,False,15.619699684601276,N/A
"def close(self):
    with self.lock:
<mask>:
            return
        self._closed = True
        if self.pendings:
            try:
                self._send_data(self.pendings)
            except Exception:
                self._call_buffer_overflow_handler(self.pendings)
        self._close()
        self.pendings = None",self._closed,20,self._closed,True,100.00000000000004,N/A
"def __init__(self, label, data, **kwargs):
    assert isinstance(data, dict), 'data must be a dict'
    sender_ = kwargs.get('sender', sender.get_global_sender())
    timestamp = kwargs.get('time', None)
<mask>:
        sender_.emit_with_time(label, timestamp, data)
    else:
        sender_.emit(label, data)",timestamp is not None,28,timestamp,False,4.9787068367863965,N/A
"def __init__(self, fmt=None, datefmt=None, style='%', fill_missing_fmt_key=False, format_json=True, exclude_attrs=None):
    super().__init__(None, datefmt)
<mask>:
        self.__style, basic_fmt_dict = {'{': (logging.StrFormatStyle, {'sys_host': '{hostname}', 'sys_name': '{name}', 'sys_module': '{module}'}), '$': (logging.StringTemplateStyle, {'sys_host': '${hostname}', 'sys_name': '${name}', 'sys_module': '${module}'})}[style]
    else:
        self.__style = None
        basic_fmt_dict = {'sys_host': '%(hostname)s', 'sys_name': '%(name)s', 'sys_module': '%(module)s'}
    if exclude_attrs is not None:
        self._exc_attrs = set(exclude_attrs)
        self._fmt_dict = None
        self._formatter = self._format_by_exclusion
        self.usesTime = super().usesTime
    else:
        self._exc_attrs = None
        if not fmt:
            self._fmt_dict = basic_fmt_dict
            self._formatter = self._format_by_dict
            self.usesTime = self._format_by_dict_uses_time
        elif callable(fmt):
            self._formatter = fmt
            self.usesTime = fmt.usesTime
        else:
            self._fmt_dict = fmt
            self._formatter = self._format_by_dict
            self.usesTime = self._format_by_dict_uses_time
    if format_json:
        self._format_msg = self._format_msg_json
    else:
        self._format_msg = self._format_msg_default
    self.hostname = socket.gethostname()
    self.fill_missing_fmt_key = fill_missing_fmt_key",style != '%',108,style,False,0.673794699908547,N/A
"def _structuring(self, data, record):
    """"""Melds `msg` into `data`.

        :param data: dictionary to be sent to fluent server
        :param msg: :class:`LogRecord`'s message to add to `data`.
          `msg` can be a simple string for backward compatibility with
          :mod:`logging` framework, a JSON encoded string or a dictionary
          that will be merged into dictionary generated in :meth:`format.
        """"""
    msg = record.msg
<mask>:
        self._add_dic(data, msg)
    elif isinstance(msg, str):
        self._add_dic(data, self._format_msg(record, msg))
    else:
        self._add_dic(data, {'message': msg})","isinstance(msg, dict)",70,"isinstance(msg, dict)",True,100.00000000000004,N/A
"def _format_msg_json(self, record, msg):
    try:
        json_msg = json.loads(str(msg))
<mask>:
            return json_msg
        else:
            return self._format_msg_default(record, msg)
    except ValueError:
        return self._format_msg_default(record, msg)","isinstance(json_msg, dict)",20,json_msg,False,18.887560283756194,N/A
"def _format_by_exclusion(self, record):
    data = {}
    for key, value in record.__dict__.items():
<mask>:
            data[key] = value
    return data",key not in self._exc_attrs,17,key not in self.exclusion_keys,False,47.750342648354646,N/A
"def _format_by_dict(self, record):
    data = {}
    for key, value in self._fmt_dict.items():
        try:
<mask>:
                value = self.__style(value).format(record)
            else:
                value = value % record.__dict__
        except KeyError as exc:
            value = None
            if not self.fill_missing_fmt_key:
                raise exc
        data[key] = value
    return data",self.__style,39,"isinstance(value, self.__style_func)",False,31.702331385234313,N/A
"def __init__(self, tag, host='localhost', port=24224, bufmax=1 * 1024 * 1024, timeout=3.0, verbose=False, buffer_overflow_handler=None, nanosecond_precision=False, msgpack_kwargs=None, queue_maxsize=DEFAULT_QUEUE_MAXSIZE, queue_circular=DEFAULT_QUEUE_CIRCULAR, queue_overflow_handler=None, **kwargs):
    """"""
        :param kwargs: This kwargs argument is not used in __init__. This will be removed in the next major version.
        """"""
    super().__init__(tag=tag, host=host, port=port, bufmax=bufmax, timeout=timeout, verbose=verbose, buffer_overflow_handler=buffer_overflow_handler, nanosecond_precision=nanosecond_precision, msgpack_kwargs=msgpack_kwargs, **kwargs)
    self._queue_maxsize = queue_maxsize
    self._queue_circular = queue_circular
<mask>:
        self._queue_overflow_handler = queue_overflow_handler
    else:
        self._queue_overflow_handler = self._queue_overflow_handler_default
    self._thread_guard = threading.Event()
    self._closed = False
    self._queue = Queue(maxsize=queue_maxsize)
    self._send_thread = threading.Thread(target=self._send_loop, name='AsyncFluentSender %d' % id(self))
    self._send_thread.daemon = True
    self._send_thread.start()",queue_circular and queue_overflow_handler,84,queue_overflow_handler,False,44.932896411722176,N/A
"def close(self, flush=True):
    with self.lock:
<mask>:
            return
        self._closed = True
        if not flush:
            while True:
                try:
                    self._queue.get(block=False)
                except Empty:
                    break
        self._queue.put(_TOMBSTONE)
        self._send_thread.join()",self._closed,22,self._closed,True,100.00000000000004,N/A
"def _send(self, bytes_):
    with self.lock:
<mask>:
            return False
        if self._queue_circular and self._queue.full():
            try:
                discarded_bytes = self._queue.get(block=False)
            except Empty:
                pass
            else:
                self._queue_overflow_handler(discarded_bytes)
        try:
            self._queue.put(bytes_, block=not self._queue_circular)
        except Full:
            return False
        return True",self._closed,31,not self._queue,False,39.76353643835252,N/A
"def _send_loop(self):
    send_internal = super()._send_internal
    try:
        while True:
            bytes_ = self._queue.get(block=True)
<mask>:
                break
            send_internal(bytes_)
    finally:
        self._close()",bytes_ is _TOMBSTONE,16,not bytes_,False,28.254432923044853,N/A
"def __init__(self, host='localhost', port=0):
    super().__init__()
<mask>:
        self.socket_proto = socket.AF_UNIX
        self.socket_type = socket.SOCK_STREAM
        self.socket_addr = host[len('unix://'):]
    else:
        self.socket_proto = socket.AF_INET
        self.socket_type = socket.SOCK_STREAM
        self.socket_addr = (host, port)
    self._sock = socket.socket(self.socket_proto, self.socket_type)
    self._sock.bind(self.socket_addr)
    if self.socket_proto == socket.AF_INET:
        self.port = self._sock.getsockname()[1]
    self._sock.listen(1)
    self._buf = BytesIO()
    self._con = None
    self.start()",host.startswith('unix://'),46,host.startswith('unix://'),True,100.00000000000004,N/A
"def run(self):
    sock = self._sock
    try:
        try:
            con, _ = sock.accept()
        except Exception:
            return
        self._con = con
        try:
            while True:
                try:
                    data = con.recv(16384)
<mask>:
                        break
                    self._buf.write(data)
                except OSError as e:
                    print('MockServer error: %s' % e)
                    break
        finally:
            con.close()
    finally:
        sock.close()",not data,41,not data,True,100.00000000000004,N/A
"def close(self):
    try:
        self._sock.close()
    except Exception:
        pass
    try:
        conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        try:
            conn.connect((self.socket_addr[0], self.port))
        finally:
            conn.close()
    except Exception:
        pass
<mask>:
        try:
            self._con.close()
        except Exception:
            pass
    self.join()",self._con,27,self._con is not None,False,41.11336169005198,N/A
"def test_broken_conn(self):
    with self._sender as sender:
        sender._send_internal(b'123')
        self.assertIsNone(sender.pendings, b'123')
        self.assertTrue(sender.socket)

        class FakeSocket:

            def __init__(self):
                self.to = 123
                self.send_side_effects = [3, 0, 9]
                self.send_idx = 0
                self.recv_side_effects = [OSError(errno.EWOULDBLOCK, 'Blah'), b'this data is going to be ignored', b'', OSError(errno.EWOULDBLOCK, 'Blah'), OSError(errno.EWOULDBLOCK, 'Blah'), OSError(errno.EACCES, 'This error will never happen')]
                self.recv_idx = 0

            def send(self, bytes_):
                try:
                    v = self.send_side_effects[self.send_idx]
<mask>:
                        raise v
                    if isinstance(v, type) and issubclass(v, Exception):
                        raise v()
                    return v
                finally:
                    self.send_idx += 1

            def shutdown(self, mode):
                pass

            def close(self):
                pass

            def settimeout(self, to):
                self.to = to

            def gettimeout(self):
                return self.to

            def recv(self, bufsize, flags=0):
                try:
                    v = self.recv_side_effects[self.recv_idx]
                    if isinstance(v, Exception):
                        raise v
                    if isinstance(v, type) and issubclass(v, Exception):
                        raise v()
                    return v
                finally:
                    self.recv_idx += 1
        old_sock = self._sender.socket
        sock = FakeSocket()
        try:
            self._sender.socket = sock
            sender.last_error = None
            self.assertTrue(sender._send_internal(b'456'))
            self.assertFalse(sender.last_error)
            self._sender.socket = sock
            sender.last_error = None
            self.assertFalse(sender._send_internal(b'456'))
            self.assertEqual(sender.last_error.errno, errno.EPIPE)
            self._sender.socket = sock
            sender.last_error = None
            self.assertFalse(sender._send_internal(b'456'))
            self.assertEqual(sender.last_error.errno, errno.EPIPE)
            self._sender.socket = sock
            sender.last_error = None
            self.assertFalse(sender._send_internal(b'456'))
            self.assertEqual(sender.last_error.errno, errno.EACCES)
        finally:
            self._sender.socket = old_sock","isinstance(v, Exception)",164,"isinstance(v, Exception)",True,100.00000000000004,N/A
"def derivatives_close(x, y):
    """"""
    Returns True iff the AffineScalarFunc objects x and y have
    derivatives that are close to each other (they must depend
    on the same variables).
    """"""
<mask>:
        return False
    return all((numbers_close(x.derivatives[var], y.derivatives[var]) for var in x.derivatives))",set(x.derivatives) != set(y.derivatives),39,"not isinstance(x, AffineScalarFunc) or not isinstance(y, AffineScalarFunc)",False,8.225964699966553,N/A
"@pytest.mark.parametrize('function_name', umath_core.deprecated_functions)
def test_deprecated_function(function_name):
    num_args = len(inspect.signature(getattr(math, function_name)).parameters)
    args = [ufloat(1, 0.1)]
<mask>:
        if function_name == 'factorial':
            args[0] = 6
    elif function_name == 'ldexp':
        args.append(3)
    else:
        args.append(ufloat(-12, 2.4))
    with pytest.warns(FutureWarning, match='will be removed'):
        getattr(umath_core, function_name)(*args)",num_args == 1,35,num_args == 1,True,100.00000000000004,N/A
"@pytest.mark.parametrize('val, std_dev, fmt_spec, expected_str', formatting_cases)
def test_format(val, std_dev, fmt_spec, expected_str):
    """"""Test the formatting of numbers with uncertainty.""""""
    x = ufloat(val, std_dev)
    actual_str = format(x, fmt_spec)
    assert actual_str == expected_str
<mask>:
        assert actual_str == str(x)
    if not set(fmt_spec).intersection('L,*%') and '0nan' not in actual_str.lower() and ('0inf' not in actual_str.lower()) and ('=====' not in actual_str):
        x_back = ufloat_fromstr(actual_str)
        '\n        The original number and the new one should be consistent with each other. The\n        nominal value can be rounded to 0 when the uncertainty is larger (because p\n        digits on the uncertainty can still show 0.00... for the nominal value). The\n        relative error is infinite, so this should not cause an error:\n        '
        if x_back.nominal_value:
            assert numbers_close(x.nominal_value, x_back.nominal_value, 0.24)
        assert numbers_close(x.std_dev, x_back.std_dev, 0.3)",not fmt_spec,119,"not isinstance(expected_str, str)",False,5.669791110976001,N/A
"@pytest.mark.parametrize('method_name', deprecated_methods)
def test_deprecated_method(method_name):
    x = ufloat(1, 0.1)
    y = ufloat(-12, 2.4)
    num_args = len(inspect.signature(getattr(float, method_name)).parameters)
    with pytest.warns(FutureWarning, match='will be removed'):
<mask>:
            getattr(x, method_name)()
        else:
            getattr(x, method_name)(y)",num_args == 1,27,num_args == 1,True,100.00000000000004,N/A
"def nan_close(first, second):
<mask>:
        return isnan(second)
    else:
        return isclose(first, second)",isnan(first),10,isnan(first),True,100.00000000000004,N/A
"def numbers_close(x, y, tolerance=1e-06):
    """"""
    Returns True if the given floats are close enough.

    The given tolerance is the relative difference allowed, or the absolute
    difference, if one of the numbers is 0.

    NaN is allowed: it is considered close to itself.
    """"""
<mask>:
        if isinf(x):
            return isinf(y)
        elif isnan(x):
            return isnan(y)
        else:
            return 2 * abs(x - y) / (abs(x) + abs(y)) < tolerance
    else:
        return abs(x or y) < tolerance",x != 0 and y != 0,72,isnan(x) or isnan(y),False,5.669791110976001,N/A
"def compare_derivatives(func, numerical_derivatives, num_args_list=None):
    """"""
    Checks the derivatives of a function 'func' (as returned by the
    wrap() wrapper), by comparing them to the
    'numerical_derivatives' functions.

    Raises a DerivativesDiffer exception in case of problem.

    These functions all take the number of arguments listed in
    num_args_list.  If num_args is None, it is automatically obtained.

    Tests are done on random arguments.
    """"""
    try:
        funcname = func.name
    except AttributeError:
        funcname = func.__name__
<mask>:
        num_args_table = {'atanh': [1], 'log': [1, 2]}
        if funcname in num_args_table:
            num_args_list = num_args_table[funcname]
        else:
            num_args_list = []
            for num_args in range(10):
                try:
                    func(*(1,) * num_args)
                except TypeError:
                    pass
                else:
                    num_args_list.append(num_args)
            if not num_args_list:
                raise Exception(""Can't find a reasonable number of arguments for function '%s'."" % funcname)
    for num_args in num_args_list:
        integer_arg_nums = set()
        if funcname == 'ldexp':
            integer_arg_nums.add(1)
        while True:
            try:
                args = []
                for arg_num in range(num_args):
                    if arg_num in integer_arg_nums:
                        args.append(random.choice(range(-10, 10)))
                    else:
                        args.append(uncert_core.Variable(random.random() * 4 - 2, 0))
                args_scalar = [uncert_core.nominal_value(v) for v in args]
                func_approx = func(*args)
                if isinstance(func_approx, AffineScalarFunc):
                    for arg_num, (arg, numerical_deriv) in enumerate(zip(args, numerical_derivatives)):
                        if isinstance(arg, int):
                            continue
                        fixed_deriv_value = func_approx.derivatives[arg]
                        num_deriv_value = numerical_deriv(*args_scalar)
                        print('Testing derivative #%d of %s at %s' % (arg_num, funcname, args_scalar))
                        if not numbers_close(fixed_deriv_value, num_deriv_value, 0.0001):
                            if not isnan(func_approx):
                                raise DerivativesDiffer(""Derivative #%d of function '%s' may be wrong: at args = %s, value obtained = %.16f, while numerical approximation = %.16f."" % (arg_num, funcname, args, fixed_deriv_value, num_deriv_value))
            except ValueError as err:
                if str(err).startswith('factorial'):
                    integer_arg_nums = set([0])
                continue
            except TypeError as err:
                if len(integer_arg_nums) == num_args:
                    raise Exception('Incorrect testing procedure: unable to find correct argument values for %s: %s' % (funcname, err))
                integer_arg_nums.add(random.choice(range(num_args)))
            else:
                break",not num_args_list,265,num_args_list is None,False,61.47881529512643,N/A
"def uarrays_close(m1, m2, precision=0.0001):
    """"""
        Returns True iff m1 and m2 are almost equal, where elements
        can be either floats or AffineScalarFunc objects.

        Two independent AffineScalarFunc objects are deemed equal if
        both their nominal value and uncertainty are equal (up to the
        given precision).

        m1, m2 -- NumPy arrays.

        precision -- precision passed through to
        uncertainties.test_uncertainties.numbers_close().
        """"""
    for elmt1, elmt2 in zip(m1.flat, m2.flat):
        elmt1 = uncert_core.to_affine_scalar(elmt1)
        elmt2 = uncert_core.to_affine_scalar(elmt2)
<mask>:
            return False
        if not numbers_close(elmt1.std_dev, elmt2.std_dev, precision):
            return False
    return True","not numbers_close(elmt1.nominal_value, elmt2.nominal_value, precision)",81,"not numbers_close(elmt1, elmt2)",False,22.397465712826726,N/A
"def parse_error_in_parentheses(representation):
    """"""
    Return (value, error) from a string representing a number with
    uncertainty like 12.34(5), 12.34(142), 12.5(3.4), 12.3(4.2)e3, or
    13.4(nan)e10.  If no parenthesis is given, an uncertainty of one
    on the last digit is assumed.

    The digits between parentheses correspond to the same number of digits
    at the end of the nominal value (the decimal point in the uncertainty
    is optional). Example: 12.34(142) = 12.34±1.42.

    Raises ValueError if the string cannot be parsed.
    """"""
    match = NUMBER_WITH_UNCERT_RE_MATCH(representation)
<mask>:
        sign, main, _, main_dec, uncert, uncert_int, uncert_dec, exponent = match.groups()
    else:
        raise NotParenUncert(""Unparsable number representation: '%s'. See the documentation of ufloat_fromstr()."" % representation)
    if exponent:
        factor = 10.0 ** nrmlze_superscript(exponent)
    else:
        factor = 1
    value = float((sign or '') + main) * factor
    if uncert is None:
        uncert_int = '1'
    if uncert_dec is not None or uncert in {'nan', 'NAN', 'inf', 'INF'}:
        uncert_value = float(uncert)
    else:
        if main_dec is None:
            num_digits_after_period = 0
        else:
            num_digits_after_period = len(main_dec) - 1
        uncert_value = int(uncert_int) / 10.0 ** num_digits_after_period
    uncert_value *= factor
    return (value, uncert_value)",match,171,match,True,100.00000000000004,N/A
"def to_float(value_str):
    """"""
    Converts a string representing a float to a float.

    The usual valid Python float() representations are correctly
    parsed.

    In addition, the pretty-print notation -1.2×10⁻¹² is also
    converted.

    ValueError is raised if no float can be obtained.
    """"""
    try:
        return float(value_str)
    except ValueError:
        pass
    match = PRETTY_PRINT_MATCH(value_str)
<mask>:
        try:
            return float(match.group(1)) * 10.0 ** nrmlze_superscript(match.group(2))
        except ValueError:
            raise ValueError('Mantissa or exponent incorrect in pretty-print form %s' % value_str)
    else:
        raise ValueError('No valid Python float or pretty-print form recognized in %s' % value_str)",match,84,match,True,100.00000000000004,N/A
"def str_to_number_with_uncert(representation):
    """"""
    Given a string that represents a number with uncertainty, returns the
    nominal value and the uncertainty.

    See the documentation for ufloat_fromstr() for a list of accepted
    formats.

    When no numerical error is given, an uncertainty of 1 on the last
    digit is implied.

    Raises ValueError if the string cannot be parsed.

    representation -- string with no leading or trailing spaces.
    """"""
<mask>:
        representation = representation[1:-1]
    match = NUMBER_WITH_UNCERT_GLOBAL_EXP_RE_MATCH(representation)
    if match:
        exp_value_str = match.group('exp_value')
        try:
            exponent = nrmlze_superscript(exp_value_str)
        except ValueError:
            raise ValueError(cannot_parse_ufloat_msg_pat % representation)
        factor = 10.0 ** exponent
        representation = match.group('simple_num_with_uncert')
    else:
        factor = 1
    match = re.match('(.*)(?:\\+/-|±)(.*)', representation)
    if match:
        nom_value, uncert = match.groups()
        try:
            parsed_value = (to_float(nom_value) * factor, to_float(uncert) * factor)
        except ValueError:
            raise ValueError(cannot_parse_ufloat_msg_pat % representation)
    else:
        try:
            parsed_value = parse_error_in_parentheses(representation)
        except NotParenUncert:
            raise ValueError(cannot_parse_ufloat_msg_pat % representation)
    return parsed_value",representation.startswith('(') and representation.endswith(')'),136,representation.startswith(' ') and representation.endswith(' '),False,67.12918266586138,N/A
"def log_der0(*args):
    """"""
    Derivative of math.log() with respect to its first argument.

    Works whether 1 or 2 arguments are given.
    """"""
<mask>:
        return 1 / args[0]
    else:
        return 1 / args[0] / math.log(args[1])",len(args) == 1,33,len(args) == 1,True,100.00000000000004,N/A
"def _deriv_copysign(x, y):
<mask>:
        return math.copysign(1, y)
    else:
        return -math.copysign(1, y)",x >= 0,11,x == 0,False,35.35533905932737,N/A
"def _deriv_pow_0(x, y):
<mask>:
        return 0.0
    elif x != 0 or y % 1 == 0:
        return y * math.pow(x, y - 1)
    else:
        return float('nan')",y == 0,26,x == 0 or y == 0,False,29.84745896009822,N/A
"def _deriv_pow_1(x, y):
<mask>:
        return 0.0
    else:
        return math.log(x) * math.pow(x, y)",x == 0 and y > 0,12,x == 0,False,36.78794411714425,N/A
"def correlated_values(nom_values, covariance_mat, tags=None):
    """"""
    Return numbers with uncertainties (AffineScalarFunc objects)
    that correctly reproduce the given covariance matrix, and have
    the given (float) values as their nominal value.

    The correlated_values_norm() function returns the same result,
    but takes a correlation matrix instead of a covariance matrix.

    The list of values and the covariance matrix must have the
    same length, and the matrix must be a square (symmetric) one.

    The numbers with uncertainties returned depend on newly
    created, independent variables (Variable objects).

    nom_values -- sequence with the nominal (real) values of the
    numbers with uncertainties to be returned.

    covariance_mat -- full covariance matrix of the returned numbers with
    uncertainties. For example, the first element of this matrix is the
    variance of the first number with uncertainty. This matrix must be a
    NumPy array-like (list of lists, NumPy array, etc.).

    tags -- if 'tags' is not None, it must list the tag of each new
    independent variable.

    This function raises NotImplementedError if numpy cannot be
    imported.
    """"""
<mask>:
        msg = 'uncertainties was not able to import numpy so correlated_values is unavailable.'
        raise NotImplementedError(msg)
    std_devs = numpy.sqrt(numpy.diag(covariance_mat))
    norm_vector = std_devs.copy()
    norm_vector[norm_vector == 0] = 1
    return correlated_values_norm(list(zip(nom_values, std_devs)), covariance_mat / norm_vector / norm_vector[:, numpy.newaxis], tags)",numpy is None,201,not numpy,False,30.326532985631665,N/A
"def correlated_values_norm(values_with_std_dev, correlation_mat, tags=None):
    """"""
    Return correlated values like correlated_values(), but takes
    instead as input:

    - nominal (float) values along with their standard deviation, and
    - a correlation matrix (i.e. a normalized covariance matrix).

    values_with_std_dev -- sequence of (nominal value, standard
    deviation) pairs. The returned, correlated values have these
    nominal values and standard deviations.

    correlation_mat -- correlation matrix between the given values, except
    that any value with a 0 standard deviation must have its correlations
    set to 0, with a diagonal element set to an arbitrary value (something
    close to 0-1 is recommended, for a better numerical precision).  When
    no value has a 0 variance, this is the covariance matrix normalized by
    standard deviations, and thus a symmetric matrix with ones on its
    diagonal.  This matrix must be an NumPy array-like (list of lists,
    NumPy array, etc.).

    tags -- like for correlated_values().

    This function raises NotImplementedError if numpy cannot be
    imported.
    """"""
<mask>:
        msg = 'uncertainties was not able to import numpy so correlated_values_norm is unavailable.'
        raise NotImplementedError(msg)
    if tags is None:
        tags = (None,) * len(values_with_std_dev)
    nominal_values, std_devs = numpy.transpose(values_with_std_dev)
    variances, transform = numpy.linalg.eigh(correlation_mat)
    variances[variances < 0] = 0.0
    variables = tuple((Variable(0, sqrt(variance), tag) for variance, tag in zip(variances, tags)))
    transform *= std_devs[:, numpy.newaxis]
    values_funcs = tuple((AffineScalarFunc(value, LinearCombination(dict(zip(variables, coords)))) for coords, value in zip(transform, nominal_values)))
    return values_funcs",numpy is None,218,"not hasattr(numpy, 'correlated_values_norm')",False,3.7477767366779213,N/A
"def correlation_matrix(nums_with_uncert):
    """"""
    Return the correlation matrix of the given sequence of
    numbers with uncertainties, as a NumPy array of floats.

    This function raises NotImplementedError if numpy cannot be
    imported.
    """"""
<mask>:
        msg = 'uncertainties was not able to import numpy so correlation_matrix is unavailable.'
        raise NotImplementedError(msg)
    cov_mat = numpy.array(covariance_matrix(nums_with_uncert))
    std_devs = numpy.sqrt(cov_mat.diagonal())
    return cov_mat / std_devs / std_devs[numpy.newaxis].T",numpy is None,59,not HAS_numpy,False,15.97357760615681,N/A
"def expand(self):
    """"""
        Expand the linear combination.

        The expansion is a collections.defaultdict(float).

        This should only be called if the linear combination is not
        yet expanded.
        """"""
    derivatives = collections.defaultdict(float)
    while self.linear_combo:
        main_factor, main_expr = self.linear_combo.pop()
<mask>:
            for var, factor in main_expr.linear_combo.items():
                derivatives[var] += main_factor * factor
        else:
            for factor, expr in main_expr.linear_combo:
                self.linear_combo.append((main_factor * factor, expr))
    self.linear_combo = derivatives",main_expr.expanded(),59,"isinstance(main_expr, QLinearComb)",False,23.356898886410015,N/A
"def __init__(self, nominal_value, linear_part):
    """"""
        nominal_value -- value of the function when the linear part is
        zero.

        linear_part -- LinearCombination that describes the linear
        part of the AffineScalarFunc.
        """"""
    self._nominal_value = float(nominal_value)
<mask>:
        linear_part = LinearCombination(linear_part)
    self._linear_part = linear_part","not isinstance(linear_part, LinearCombination)",39,linear_part is not None,False,19.692104496063735,N/A
"def PDG_precision(std_dev):
    """"""
    Return the number of significant digits to be used for the given
    standard deviation, according to the rounding rules of the
    Particle Data Group (2010)
    (http://pdg.lbl.gov/2010/reviews/rpp2010-rev-rpp-intro.pdf).

    Also returns the effective standard deviation to be used for
    display.
    """"""
    exponent = first_digit(std_dev)
<mask>:
        exponent, factor = (exponent - 2, 1)
    else:
        exponent, factor = (exponent + 1, 1000)
    digits = int(std_dev / 10.0 ** exponent * factor)
    if digits <= 354:
        return (2, std_dev)
    elif digits <= 949:
        return (1, std_dev)
    else:
        return (2, 10.0 ** exponent * (1000 / factor))",exponent >= 0,93,exponent > 2,False,39.43223765116288,N/A
"def signif_dgt_to_limit(value, num_signif_d):
    """"""
    Return the precision limit necessary to display value with
    num_signif_d significant digits.

    The precision limit is given as -1 for 1 digit after the decimal
    point, 0 for integer rounding, etc. It can be positive.
    """"""
    fst_digit = first_digit(value)
    limit_no_rounding = fst_digit - num_signif_d + 1
    rounded = round(value, -limit_no_rounding)
    fst_digit_rounded = first_digit(rounded)
<mask>:
        limit_no_rounding += 1
    return limit_no_rounding",fst_digit_rounded > fst_digit,63,fst_digit_rounded == '0',False,45.62272070865922,N/A
"def get_ops_with_reflection():
    """"""
    Return operators with a reflection, along with their partial derivatives.

    Operators are things like +, /, etc. Those considered here have two
    arguments and can be called through Python's reflected methods __r…__ (e.g.
    __radd__).

    See the code for details.
    """"""
    derivatives_list = {'add': ('1.', '1.'), 'div': ('1/y', '-x/y**2'), 'floordiv': ('0.', '0.'), 'mod': ('1.', 'partial_derivative(float.__mod__, 1)(x, y)'), 'mul': ('y', 'x'), 'sub': ('1.', '-1.'), 'truediv': ('1/y', '-x/y**2')}
    ops_with_reflection = {}
    for op, derivatives in derivatives_list.items():
        ops_with_reflection[op] = [eval('lambda x, y: %s' % expr) for expr in derivatives]
        ops_with_reflection['r' + op] = [eval('lambda y, x: %s' % expr) for expr in reversed(derivatives)]

    def pow_deriv_0(x, y):
<mask>:
            return 0.0
        elif x != 0 or y % 1 == 0:
            return y * x ** (y - 1)
        else:
            return float('nan')

    def pow_deriv_1(x, y):
        if x == 0 and y > 0:
            return 0.0
        else:
            return log(x) * x ** y
    ops_with_reflection['pow'] = [pow_deriv_0, pow_deriv_1]
    ops_with_reflection['rpow'] = [lambda y, x: pow_deriv_1(x, y), lambda y, x: pow_deriv_0(x, y)]
    for op in ['pow']:
        ops_with_reflection[op] = [nan_if_exception(func) for func in ops_with_reflection[op]]
        ops_with_reflection['r' + op] = [nan_if_exception(func) for func in ops_with_reflection['r' + op]]
    return ops_with_reflection",y == 0,189,x == 0 and y == 0,False,29.84745896009822,N/A
"def no_complex_result(func):
    """"""
    Return a function that does like func, but that raises a
    ValueError if the result is complex.
    """"""

    def no_complex_func(*args, **kwargs):
        '\n        Like %s, but raises a ValueError exception if the result\n        is complex.\n        ' % func.__name__
        value = func(*args, **kwargs)
<mask>:
            raise ValueError('The uncertainties module does not handle complex results')
        else:
            return value
    return no_complex_func","isinstance(value, complex)",59,value is None,False,10.122592925934278,N/A
"def add_arithmetic_ops(cls):
    """"""
    Adds many operators (__add__, etc.) to the AffineScalarFunc class.
    """"""

    def _simple_add_deriv(x):
<mask>:
            return 1.0
        else:
            return -1.0
    simple_numerical_operators_derivatives = {'abs': _simple_add_deriv, 'neg': lambda x: -1.0, 'pos': lambda x: 1.0, 'trunc': lambda x: 0.0}
    for op, derivative in iter(simple_numerical_operators_derivatives.items()):
        attribute_name = '__%s__' % op
        try:
            setattr(cls, attribute_name, _wrap(cls, getattr(float, attribute_name), [derivative]))
        except AttributeError:
            pass
        else:
            modified_operators.append(op)
    for op, derivatives in ops_with_reflection.items():
        attribute_name = '__%s__' % op
        try:
            if op not in custom_ops:
                func_to_wrap = getattr(float, attribute_name)
            else:
                func_to_wrap = custom_ops[op]
        except AttributeError:
            pass
        else:
            setattr(cls, attribute_name, _wrap(cls, func_to_wrap, derivatives))
            modified_ops_with_reflection.append(op)
    for coercion_type in ('complex', 'int', 'long', 'float'):

        def raise_error(self):
            raise TypeError(""can't convert an affine function (%s) to %s; use x.nominal_value"" % (self.__class__, coercion_type))
        setattr(cls, '__%s__' % coercion_type, raise_error)",x >= 0,121,x < 0.0,False,19.716118825581447,N/A
"def __getitem__(self, index):
    returned_elements = self.returned_elements
    try:
        return returned_elements[index]
    except IndexError:
        for pos in range(len(returned_elements), index + 1):
            value = next(self.iterable)
<mask>:
                value = self.none_converter(pos)
            returned_elements.append(value)
        return returned_elements[index]",value is None,28,value is None,True,100.00000000000004,N/A
"def unumpy_to_numpy_matrix(arr):
    """"""
    If arr in a unumpy.matrix, it is converted to a numpy.matrix.
    Otherwise, it is returned unchanged.
    """"""
<mask>:
        return arr.view(numpy.matrix)
    else:
        return arr","isinstance(arr, matrix)",26,"isinstance(arr, unumpy.matrix)",False,41.11336169005198,N/A
"def derivative(u, var):
    """"""
    Return the derivative of u along var, if u is an
    uncert_core.AffineScalarFunc instance, and if var is one of the
    variables on which it depends.  Otherwise, return 0.
    """"""
<mask>:
        try:
            return u.derivatives[var]
        except KeyError:
            return 0.0
    else:
        return 0.0","isinstance(u, uncert_core.AffineScalarFunc)",44,"isinstance(u, uncert_core.AffineScalarFunc)",True,100.00000000000004,N/A
"def uarray(nominal_values, std_devs=None):
    """"""
    Return a NumPy array of numbers with uncertainties
    initialized with the given nominal values and standard
    deviations.

    nominal_values, std_devs -- valid arguments for numpy.array, with
    identical shapes (list of numbers, list of lists, numpy.ndarray,
    etc.).

    std_devs=None is only used for supporting legacy code, where
    nominal_values can be the tuple of nominal values and standard
    deviations.
    """"""
<mask>:
        raise TypeError('uarray() should be called with two arguments.')
    return numpy.vectorize(lambda v, s: uncert_core.Variable(v, s), otypes=[object])(nominal_values, std_devs)",std_devs is None,77,len(nominal_values) != 2,False,4.767707020457095,N/A
"def check_ptr_stats_json(stats_file: Path) -> int:
    stats_errors = 0
<mask>:
        print(f'{stats_file} stats file does not exist')
        return 68
    try:
        with stats_file.open('r') as sfp:
            stats_json = json.load(sfp)
    except json.JSONDecodeError as jde:
        print(f'Stats JSON Error: {jde}')
        return 69
    print(json.dumps(stats_json, indent=2, sort_keys=True))
    any_fail = int(stats_json['total.fails']) + int(stats_json['total.timeouts'])
    if any_fail:
        print(f'Stats report {any_fail} fails/timeouts', file=sys.stderr)
        return any_fail
    if int(stats_json['total.setup_pys']) > 1:
        print('Somehow we had more than 1 setup.py - What?', file=sys.stderr)
        stats_errors += 1
    if int(stats_json['pct.setup_py_ptr_enabled']) != 100:
        print(""We didn't test all setup.py files ..."", file=sys.stderr)
        stats_errors += 1
    coverage_key_count = 0
    for key in stats_json.keys():
        if '_coverage.' in key:
            coverage_key_count += 1
    if coverage_key_count != 4:
        print(""We didn't get coverage stats for all ptr files + total"", file=sys.stderr)
        stats_errors += 1
    print(f'Stats check found {stats_errors} error(s)')
    return stats_errors",not stats_file.exists(),124,not stats_file.exists(),True,100.00000000000004,N/A
"def integration_test() -> int:
    print('Running `ptr` integration tests (aka run itself)', file=sys.stderr)
    stats_file = Path(gettempdir()) / 'ptr_ci_stats'
    ci_cmd = ['python', 'ptr.py', '-d', '--print-cov', '--run-disabled', '--error-on-warnings', '--stats-file', str(stats_file)]
<mask>:
        ci_cmd.extend(['--venv', environ['VIRTUAL_ENV']])
    cp = run(ci_cmd, check=True)
    return cp.returncode + check_ptr_stats_json(stats_file)",'VIRTUAL_ENV' in environ,38,'VIRTUAL_ENV' in environ,True,100.00000000000004,N/A
"def ci(show_env: bool=False) -> int:
    cp = run(('python', '-V'), check=True, stdout=PIPE, universal_newlines=True)
    print(f'Using {cp.stdout}', file=sys.stderr)
<mask>:
        print('- Environment:', file=sys.stderr)
        for key in sorted(environ.keys()):
            print(f'{key}: {environ[key]}', file=sys.stderr)
    if 'PTR_INTEGRATION' in environ or ('CI_ENV' in environ and environ['CI_ENV'] == 'PTR_INTEGRATION'):
        return integration_test()
    print('Running `ptr` unit tests', file=sys.stderr)
    return run(('python', 'ptr_tests.py', '-v'), check=True).returncode",show_env,50,show_env,True,100.00000000000004,N/A
"@patch('ptr.time')
@patch('ptr.LOG.error')
@patch('ptr.getpid', return_specific_pid)
def test_analyze_coverage(self, mock_log: Mock, mock_time: Mock) -> None:
    mock_time.return_value = 0
    fake_setup_py = Path('unittest/setup.py')
<mask>:
        fake_venv_path = Path(environ['VIRTUAL_ENV'])
    else:
        fake_venv_path = self.loop.run_until_complete(ptr.create_venv('https://pypi.com/s', install_pkgs=False))
    self.assertIsNone(ptr._analyze_coverage(fake_venv_path, fake_setup_py, {}, '', {}, 0))
    self.assertIsNone(ptr._analyze_coverage(fake_venv_path, fake_setup_py, {'bla': 69}, '', {}, 0))
    self.assertEqual(ptr._analyze_coverage(fake_venv_path, fake_setup_py, ptr_tests_fixtures.FAKE_REQ_COVERAGE, ptr_tests_fixtures.SAMPLE_REPORT_OUTPUT, {}, 0), ptr_tests_fixtures.EXPECTED_COVERAGE_FAIL_RESULT)
    self.assertEqual(ptr._analyze_coverage(fake_venv_path, fake_setup_py, ptr_tests_fixtures.FAKE_REQ_COVERAGE, ptr_tests_fixtures.SAMPLE_FLOAT_REPORT_OUTPUT, {}, 0), ptr_tests_fixtures.EXPECTED_COVERAGE_FAIL_RESULT)
    cov_report = ptr_tests_fixtures.SAMPLE_WIN_TG_REPORT_OUTPUT if ptr.WINDOWS else ptr_tests_fixtures.SAMPLE_NIX_TG_REPORT_OUTPUT
    self.assertEqual(ptr._analyze_coverage(fake_venv_path, fake_setup_py, ptr_tests_fixtures.FAKE_TG_REQ_COVERAGE, cov_report, {}, 0), ptr_tests_fixtures.EXPECTED_PTR_COVERAGE_FAIL_RESULT)
    self.assertEqual(ptr._analyze_coverage(fake_venv_path, fake_setup_py, {'fake_file.py': 48}, cov_report, {}, 0), ptr_tests_fixtures.EXPECTED_PTR_COVERAGE_MISSING_FILE_RESULT)
    self.assertTrue(mock_log.called)
    if 'VIRTUAL_ENV' not in environ:
        rmtree(fake_venv_path)",'VIRTUAL_ENV' in environ,83,'VIRTUAL_ENV' in environ,True,100.00000000000004,N/A
"def test_generate_pyre_cmd(self) -> None:
    with TemporaryDirectory() as td:
        td_path = Path(td)
        pyre_exe = Path('pyre')
        conf = {'run_pyre': True}
        expected = (str(pyre_exe), '--source-directory', str(td_path), 'check')
<mask>:
            expected = ()
        self.assertEqual(ptr._generate_pyre_cmd(td_path, pyre_exe, conf), expected)",ptr.WINDOWS,32,sys.version_info[0] < 3,False,4.196114906296549,N/A
"def test_gen_output(self) -> None:
    test_cmd = ('echo.exe', ""''"") if ptr.WINDOWS else ('/bin/echo',)
    stdout, stderr = self.loop.run_until_complete(ptr._gen_check_output(test_cmd))
    self.assertTrue(b'\n' in stdout)
    self.assertEqual(stderr, None)
<mask>:
        return
    with self.assertRaises(CalledProcessError):
        if ptr.MACOSX:
            false = '/usr/bin/false'
        else:
            false = '/bin/false'
        self.loop.run_until_complete(ptr._gen_check_output((false,)))",ptr.WINDOWS,35,ptr.MACOSX,False,55.03212081491043,N/A
"def _config_default() -> ConfigParser:
<mask>:
        venv_pkgs = 'black coverage flake8 mypy pip pylint setuptools usort'
    else:
        venv_pkgs = 'black coverage flake8 mypy pip pylint pyre-check setuptools usort'
    LOG.info('Using default config settings')
    cp = ConfigParser()
    cp['ptr'] = {}
    cp['ptr']['atonce'] = str(int((cpu_count() or 20) / 2) or 1)
    cp['ptr']['exclude_patterns'] = 'build* yocto'
    cp['ptr']['pypi_url'] = 'https://pypi.org/simple/'
    cp['ptr']['venv_pkgs'] = venv_pkgs
    return cp",WINDOWS,58,sys.version_info[0] == 2,False,0.0,N/A
"def _config_read(cwd: str, conf_name: str='.ptrconfig', cp: ConfigParser | None=None) -> ConfigParser:
    """"""Look from cwd to / for a ""conf_name"" file - If so read it in""""""
<mask>:
        cp = _config_default()
    cwd_path = Path(cwd)
    root_path = Path(f'{cwd_path.drive}\\') if WINDOWS else Path('/')
    while cwd_path:
        ptrconfig_path = cwd_path / conf_name
        if ptrconfig_path.exists():
            cp.read(str(ptrconfig_path))
            LOG.info(f'Loading found config @ {ptrconfig_path}')
            break
        if cwd_path == root_path:
            break
        cwd_path = cwd_path.parent
    return cp",cp is None,66,cp is None,True,100.00000000000004,N/A
"def _get_site_packages_path(venv_path: Path) -> None | Path:
    lib_path = venv_path / ('Lib' if WINDOWS else 'lib')
    for apath in lib_path.iterdir():
<mask>:
            return apath / 'site-packages'
        if apath.is_dir() and apath.name == 'site-packages':
            return apath
    LOG.error(f'Unable to find a python lib dir in {lib_path}')
    return None",apath.is_dir() and apath.match('python*'),44,apath.is_dir() and apath.name == 'python-packages',False,58.66839880277633,N/A
"def _analyze_coverage(venv_path: Path, setup_py_path: Path, required_cov: dict[str, float], coverage_report: str, stats: dict[str, int], test_run_start_time: float) -> None | test_result:
    module_path = setup_py_path.parent
    site_packages_path = _get_site_packages_path(venv_path)
<mask>:
        LOG.error('Analyze coverage is unable to find site-packages path')
        return None
    relative_site_packages = str(site_packages_path.relative_to(venv_path)) + sep
    if not coverage_report:
        LOG.error(f'No coverage report for {setup_py_path} - Unable to enforce coverage requirements')
        return None
    if not required_cov:
        LOG.error(f'No required coverage to enforce for {setup_py_path}')
        return None
    coverage_lines = {}
    for line in coverage_report.splitlines():
        if not line or line.startswith('-') or line.startswith('Name'):
            continue
        module_path_str = None
        sl_path = None
        sl = line.split(maxsplit=4)
        if sl[0] != 'TOTAL':
            sl_path = _max_osx_private_handle(sl[0], site_packages_path)
        if sl_path and sl_path.is_absolute() and site_packages_path:
            for possible_abs_path in (module_path, site_packages_path):
                try:
                    module_path_str = str(sl_path.relative_to(possible_abs_path))
                except ValueError as ve:
                    LOG.debug(ve)
        elif sl_path:
            module_path_str = str(sl_path).replace(relative_site_packages, '')
        else:
            module_path_str = sl[0]
        if not module_path_str:
            LOG.error(f'[{setup_py_path}] Unable to find path relative path for {sl[0]}')
            continue
        if len(sl) == 4:
            coverage_lines[module_path_str] = coverage_line(float(sl[1]), float(sl[2]), float(_remove_pct_symbol(sl[3])), '')
        else:
            coverage_lines[module_path_str] = coverage_line(float(sl[1]), float(sl[2]), float(_remove_pct_symbol(sl[3])), sl[4])
        if sl[0] != 'TOTAL':
            stats[f'suite.{module_path.name}_coverage.file.{module_path_str}'] = int(coverage_lines[module_path_str].cover)
        else:
            stats[f'suite.{module_path.name}_coverage.total'] = int(coverage_lines[module_path_str].cover)
    failed_output = 'The following files did not meet coverage requirements:\n'
    failed_coverage = False
    for afile, cov_req in required_cov.items():
        try:
            cover = coverage_lines[afile].cover
        except KeyError:
            err = f'{afile} has not reported any coverage. Does the file exist? ' + 'Does it get ran during tests? Remove from setup config.'
            keyerror_runtime = int(time() - test_run_start_time)
            return test_result(setup_py_path, StepName.analyze_coverage.value, err, keyerror_runtime, False)
        if cover < cov_req:
            failed_coverage = True
            cov_lines = coverage_lines[afile]
            failed_output += f'  {afile}: {cov_lines.cover} < {cov_req} - Missing: {cov_lines.missing}\n'
    if failed_coverage:
        failed_cov_runtime = int(time() - test_run_start_time)
        return test_result(setup_py_path, StepName.analyze_coverage.value, failed_output, failed_cov_runtime, False)
    return None",not site_packages_path,266,not site_packages_path,True,100.00000000000004,N/A
"def _max_osx_private_handle(potenital_path: str, site_packages_path: Path) -> None | Path:
    """"""On Mac OS X `coverage` seems to always resolve /private for anything stored in /var.
    ptr's usage of gettempdir() seems to result in using dirs within there
    This function strips /private if it exists on the path supplied from coverage
    ONLY IF site_packages_path is not based in /private""""""
<mask>:
        return Path(potenital_path)
    private_path = Path('/private')
    try:
        site_packages_path.relative_to(private_path)
        return Path(potenital_path)
    except ValueError:
        pass
    return Path(potenital_path.replace('/private', ''))",not MACOSX,73,os.name == 'nt',False,0.0,N/A
"def forward(self, *args):
    """"""Computes the loss between sampled measures.

        Documentation and examples: Soon!
        Until then, please check the tutorials :-)""""""
    l_x, α, x, l_y, β, y = self.process_args(*args)
    B, N, M, D, l_x, α, l_y, β = self.check_shapes(l_x, α, x, l_y, β, y)
    backend = self.backend
<mask>:
        if backend in ['auto', 'multiscale']:
            backend = 'multiscale'
        else:
            raise ValueError('Explicit cluster labels are only supported with the ""auto"" and ""multiscale"" backends.')
    elif backend == 'auto':
        if M * N <= 5000 ** 2:
            backend = 'tensorized'
        elif D <= 3 and self.loss == 'sinkhorn' and (M * N > 10000 ** 2) and (self.p == 2):
            backend = 'multiscale'
        else:
            backend = 'online'
    if backend in ['multiscale']:
        if B == 1:
            α, x, β, y = (α.squeeze(0), x.squeeze(0), β.squeeze(0), y.squeeze(0))
        elif B > 1:
            warnings.warn(""The 'multiscale' backend do not support batchsize > 1. "" + ""Using 'tensorized' instead: beware of memory overflows!"")
            backend = 'tensorized'
    if B == 0 and backend in ['tensorized', 'online']:
        α, x, β, y = (α.unsqueeze(0), x.unsqueeze(0), β.unsqueeze(0), y.unsqueeze(0))
    values = routines[self.loss][backend](α, x, β, y, p=self.p, blur=self.blur, reach=self.reach, diameter=self.diameter, scaling=self.scaling, truncate=self.truncate, cost=self.cost, kernel=self.kernel, cluster_scale=self.cluster_scale, debias=self.debias, potentials=self.potentials, labels_x=l_x, labels_y=l_y, verbose=self.verbose)
    if self.potentials:
        F, G = values
        return (F.view_as(α), G.view_as(β))
    elif backend in ['multiscale']:
        if B == 0:
            return values
        else:
            return values.view(-1)
    elif B == 0:
        return values[0]
    else:
        return values",l_x is not None or l_y is not None,222,self.loss is None,False,2.564755813286796,N/A
"def process_args(self, *args):
<mask>:
        return args
    if len(args) == 4:
        α, x, β, y = args
        return (None, α, x, None, β, y)
    elif len(args) == 2:
        x, y = args
        α = self.generate_weights(x)
        β = self.generate_weights(y)
        return (None, α, x, None, β, y)
    else:
        raise ValueError('A SamplesLoss accepts two (x, y), four (α, x, β, y) or six (l_x, α, x, l_y, β, y)  arguments.')",len(args) == 6,66,len(args) == 0,False,80.91067115702207,N/A
"def generate_weights(self, x):
<mask>:
        N = x.shape[0]
        return torch.ones(N).type_as(x) / N
    elif x.dim() == 3:
        B, N, _ = x.shape
        return torch.ones(B, N).type_as(x) / N
    else:
        raise ValueError(""Input samples 'x' and 'y' should be encoded as (N,D) or (B,N,D) (batch) tensors."")",x.dim() == 2,41,x.dim() == 2,True,100.00000000000004,N/A
"def gaussian_kernel(x, y, blur=0.05, use_keops=False, ranges=None):
    C2 = squared_distances(x / blur, y / blur, use_keops=use_keops)
    K = (-C2 / 2).exp()
<mask>:
        K.ranges = ranges
    return K",use_keops and ranges is not None,26,ranges is not None,False,36.78794411714425,N/A
"def laplacian_kernel(x, y, blur=0.05, use_keops=False, ranges=None):
    C = distances(x / blur, y / blur, use_keops=use_keops)
    K = (-C).exp()
<mask>:
        K.ranges = ranges
    return K",use_keops and ranges is not None,24,ranges is not None,False,36.78794411714425,N/A
"def kernel_loss(α, x, β, y, blur=0.05, kernel=None, name=None, potentials=False, use_keops=False, ranges_xx=None, ranges_yy=None, ranges_xy=None, **kwargs):
<mask>:
        kernel = kernel_routines[name]
    K_xx = kernel(double_grad(x), x.detach(), blur=blur, use_keops=use_keops, ranges=ranges_xx)
    K_yy = kernel(double_grad(y), y.detach(), blur=blur, use_keops=use_keops, ranges=ranges_yy)
    K_xy = kernel(x, y, blur=blur, use_keops=use_keops, ranges=ranges_xy)
    a_x = (K_xx @ α.detach().unsqueeze(-1)).squeeze(-1)
    b_y = (K_yy @ β.detach().unsqueeze(-1)).squeeze(-1)
    b_x = (K_xy @ β.unsqueeze(-1)).squeeze(-1)
    if potentials:
        Kt = K_xy.t() if use_keops else K_xy.transpose(1, 2)
        a_y = (Kt @ α.unsqueeze(-1)).squeeze(-1)
        return (a_x - b_x, b_y - a_y)
    else:
        batch = x.dim() > 2
        return 0.5 * scal(double_grad(α), a_x, batch=batch) + 0.5 * scal(double_grad(β), b_y, batch=batch) - scal(α, b_x, batch=batch)",kernel is None,98,kernel is None,True,100.00000000000004,N/A
"def kernel_multiscale(α, x, β, y, blur=0.05, kernel=None, name=None, truncate=5, diameter=None, cluster_scale=None, potentials=False, verbose=False, **kwargs):
<mask>:
        return kernel_online(α.unsqueeze(0), x.unsqueeze(0), β.unsqueeze(0), y.unsqueeze(0), blur=blur, kernel=kernel, truncate=truncate, name=name, potentials=potentials, **kwargs)
    center = (x.mean(-2, keepdim=True) + y.mean(-2, keepdim=True)) / 2
    x, y = (x - center, y - center)
    x_ = x / blur
    y_ = y / blur
    if cluster_scale is None:
        D = x.shape[-1]
        if diameter is None:
            diameter = max_diameter(x_.view(-1, D), y_.view(-1, D))
        else:
            diameter = diameter / blur
        cluster_scale = diameter / (np.sqrt(D) * 2000 ** (1 / D))
    cell_diameter = cluster_scale * np.sqrt(x_.shape[-1])
    x_lab = grid_cluster(x_, cluster_scale)
    y_lab = grid_cluster(y_, cluster_scale)
    ranges_x, x_c, α_c = cluster_ranges_centroids(x_, x_lab, weights=α)
    ranges_y, y_c, β_c = cluster_ranges_centroids(y_, y_lab, weights=β)
    if verbose:
        print('{}x{} clusters, computed at scale = {:2.3f}'.format(len(x_c), len(y_c), cluster_scale))
    (α, x), x_lab = sort_clusters((α, x), x_lab)
    (β, y), y_lab = sort_clusters((β, y), y_lab)
    with torch.no_grad():
        C_xx = squared_distances(x_c, x_c)
        C_yy = squared_distances(y_c, y_c)
        C_xy = squared_distances(x_c, y_c)
        keep_xx = C_xx <= (truncate + cell_diameter) ** 2
        keep_yy = C_yy <= (truncate + cell_diameter) ** 2
        keep_xy = C_xy <= (truncate + cell_diameter) ** 2
        ranges_xx = from_matrix(ranges_x, ranges_x, keep_xx)
        ranges_yy = from_matrix(ranges_y, ranges_y, keep_yy)
        ranges_xy = from_matrix(ranges_x, ranges_y, keep_xy)
    return kernel_loss(α, x, β, y, blur=blur, kernel=kernel, name=name, potentials=potentials, use_keops=True, ranges_xx=ranges_xx, ranges_yy=ranges_yy, ranges_xy=ranges_xy)",truncate is None or name == 'energy',209,kernel is None,False,10.394224994345743,N/A
"def scaling_parameters(x, y, p, blur, reach, diameter, scaling):
    """"""Turns high-level arguments into numerical values for the Sinkhorn loop.""""""
<mask>:
        D = x.shape[-1]
        diameter = max_diameter(x.view(-1, D), y.view(-1, D))
    eps = blur ** p
    rho = None if reach is None else reach ** p
    eps_list = epsilon_schedule(p, diameter, blur, scaling)
    return (diameter, eps, eps_list, rho)",diameter is None,55,diameter is None,True,100.00000000000004,N/A
"def scal(a, f, batch=False):
<mask>:
        B = a.shape[0]
        return (a.reshape(B, -1) * f.reshape(B, -1)).sum(1)
    else:
        return torch.dot(a.reshape(-1), f.reshape(-1))",batch,18,batch,True,100.00000000000004,N/A
"def squared_distances(x, y, use_keops=False):
<mask>:
        if x.dim() == 2:
            x_i = LazyTensor(x[:, None, :])
            y_j = LazyTensor(y[None, :, :])
        elif x.dim() == 3:
            x_i = LazyTensor(x[:, :, None, :])
            y_j = LazyTensor(y[:, None, :, :])
        else:
            print('x.shape : ', x.shape)
            raise ValueError('Incorrect number of dimensions')
        return ((x_i - y_j) ** 2).sum(-1)
    else:
        if x.dim() == 2:
            D_xx = (x * x).sum(-1).unsqueeze(1)
            D_xy = torch.matmul(x, y.permute(1, 0))
            D_yy = (y * y).sum(-1).unsqueeze(0)
        elif x.dim() == 3:
            D_xx = (x * x).sum(-1).unsqueeze(2)
            D_xy = torch.matmul(x, y.permute(0, 2, 1))
            D_yy = (y * y).sum(-1).unsqueeze(1)
        else:
            print('x.shape : ', x.shape)
            raise ValueError('Incorrect number of dimensions')
        return D_xx - 2 * D_xy + D_yy",use_keops and keops_available,109,y.dim() == 1,False,0.0,N/A
"def distances(x, y, use_keops=False):
<mask>:
        return squared_distances(x, y, use_keops=use_keops).sqrt()
    else:
        return torch.sqrt(torch.clamp_min(squared_distances(x, y), 1e-08))",use_keops,14,torch.cuda.is_available(),False,4.767707020457095,N/A
"def C_transform(G, tau=1, p=2):
    """"""
    Computes the forward C-transform of an array G of shape:
     - (Batch, Nx)         in 1D
     - (Batch, Nx, Ny)     in 2D
     - (Batch, Nx, Ny, Nz) in 3D

    i.e.
    F(x_i) <- max_j [G(x_j) - C(x_i, x_j)]

    with:
    C(x,y) = |x-y|^p / (p * tau)

    In this first demo, we assume that:
      - We are working with square images: Nx = Ny = Nz = N.
      - p = 1 or 2  (Manhattan or Euclidean distance).
      - Pixels have unit length in all dimensions.
    """"""
    D = G.ndim - 1
    B, N = (G.shape[0], G.shape[1])
    x = torch.arange(N).type_as(G)
<mask>:
        x = x / tau
    if p == 2:
        x = x / np.sqrt(2 * tau)
    else:
        raise NotImplementedError()
    if not keops_available:
        raise ImportError('This routine depends on the pykeops library.')

    def lines(g):
        g = g.contiguous()
        g_j = LazyTensor(g.view(-1, 1, N, 1))
        x_i = LazyTensor(x.view(1, N, 1, 1))
        x_j = LazyTensor(x.view(1, 1, N, 1))
        if p == 1:
            Cg_ij = g_j - (x_i - x_j).abs()
        elif p == 2:
            Cg_ij = g_j - (x_i - x_j) ** 2
        f_i = Cg_ij.max(dim=2)
        if D == 1:
            return f_i.view(B, N)
        elif D == 2:
            return f_i.view(B, N, N)
        elif D == 3:
            return f_i.view(B, N, N, N)
    if D == 1:
        G = lines(G)
    if D == 2:
        G = lines(G)
        G = lines(G.permute([0, 2, 1])).permute([0, 2, 1])
    elif D == 3:
        G = lines(G)
        G = lines(G.permute([0, 1, 3, 2])).permute([0, 1, 3, 2])
        G = lines(G.permute([0, 3, 2, 1])).permute([0, 3, 2, 1])
    return G",p == 1,255,p == 1,True,100.00000000000004,N/A
"def lse_lazytensor(p, D, batchdims=(1,)):
    """"""This implementation is currently disabled.""""""
    x_i = Vi(0, D)
    y_j = Vj(1, D)
    f_j = Vj(2, 1)
    epsinv = Pm(3, 1)
    x_i.batchdims = batchdims
    y_j.batchdims = batchdims
    f_j.batchdims = batchdims
    epsinv.batchdims = batchdims
<mask>:
        D_ij = ((x_i - y_j) ** 2).sum(-1) / 2
    elif p == 1:
        D_ij = ((x_i - y_j) ** 2).sum(-1).sqrt()
    smin = (f_j - epsinv * D_ij).logsumexp(2)
    return smin",p == 2,67,p == 0,False,59.460355750136046,N/A
"def sinkhorn_online(a, x, b, y, p=2, blur=0.05, reach=None, diameter=None, scaling=0.5, cost=None, debias=True, potentials=False, **kwargs):
    B, N, D = x.shape
    B, M, _ = y.shape
<mask>:
        if True:
            softmin = partial(softmin_online_lazytensor, p=p)
        else:
            my_lse = lse_lazytensor(p, D, batchdims=(B,))
            softmin = partial(softmin_online, log_conv=my_lse)
    else:
        if B > 1:
            raise ValueError('Custom cost functions are not yet supported with batches.')
        x = x.squeeze(0)
        y = y.squeeze(0)
        if cost is None:
            cost = cost_formulas[p]
        my_lse = lse_genred(cost, D, dtype=str(x.dtype)[6:])
        softmin = partial(softmin_online, log_conv=my_lse)
    C_xx, C_yy = ((x, x.detach()), (y, y.detach())) if debias else (None, None)
    C_xy, C_yx = ((x, y.detach()), (y, x.detach()))
    diameter, eps, eps_list, rho = scaling_parameters(x, y, p, blur, reach, diameter, scaling)
    f_aa, g_bb, g_ab, f_ba = sinkhorn_loop(softmin, log_weights(a), log_weights(b), C_xx, C_yy, C_xy, C_yx, eps_list, rho, debias=debias)
    return sinkhorn_cost(eps, rho, a, b, f_aa, g_bb, g_ab, f_ba, batch=True, debias=debias, potentials=potentials)",cost is None and B > 1,136,cost is None,False,26.359713811572682,N/A
"def clusterize(a, x, scale=None, labels=None):
    """"""
    Performs a simple 'voxelgrid' clustering on the input measure,
    putting points into cubic bins of size 'scale' = σ_c.
    The weights are summed, and the centroid position is that of the bin's center of mass.
    Most importantly, the ""fine"" lists of weights and points are *sorted*
    so that clusters are *contiguous in memory*: this allows us to perform
    kernel truncation efficiently on the GPU.

    If
        [a_c, a], [x_c, x], [x_ranges] = clusterize(a, x, σ_c),
    then
        a_c[k], x_c[k] correspond to
        a[x_ranges[k,0]:x_ranges[k,1]], x[x_ranges[k,0]:x_ranges[k,1],:]
    """"""
    perm = None
<mask>:
        return ([a], [x], [])
    else:
        x_lab = grid_cluster(x, scale) if labels is None else labels
        ranges_x, x_c, a_c = cluster_ranges_centroids(x, x_lab, weights=a)
        x_labels, perm = torch.sort(x_lab.view(-1))
        a, x = (a[perm], x[perm])
        return ([a_c, a], [x_c, x], [ranges_x], perm)",labels is None and scale is None,130,x.ndim == 1,False,0.0,N/A
"def ImagesBarycenter(measures, weights, blur=0, p=2, scaling_N=10, backward_iterations=5):
    a_k = measures
    w_k = weights
<mask>:
        blur = 1 / measures.shape[-1]
    with torch.set_grad_enabled(backward_iterations == 0):
        bar = (a_k * w_k[:, :, None, None]).sum(1)
        ak_s = pyramid(a_k)[1:]
        ak_log_s = list(map(log_dens, ak_s))
        sigma = 1
        eps = sigma ** p
        f_k, g_k = (softmin(eps, p, ak_log_s[0]), softmin(eps, p, ak_log_s[0]))
        d_log = torch.ones_like(ak_log_s[0]).sum(dim=1, keepdim=True)
        d_log = d_log - d_log.logsumexp([2, 3], keepdim=True)
        for n, ak_log in enumerate(ak_log_s):
            for _ in range(scaling_N):
                eps = sigma ** p
                f_k, g_k, d_log, bar_log = barycenter_iteration(f_k, g_k, d_log, eps, p, ak_log, w_k)
                sigma = max(sigma * 2 ** (-1 / scaling_N), blur)
            if n + 1 < len(ak_s):
                f_k = upsample(f_k)
                g_k = upsample(g_k)
                d_log = upsample(d_log)
    if (measures.requires_grad or weights.requires_grad) and backward_iterations > 0:
        for _ in range(backward_iterations):
            f_k, g_k, d_log, bar_log = barycenter_iteration(f_k, g_k, d_log, eps, p, ak_log, w_k)
    return bar_log.exp()",blur == 0,143,blur == 0,True,100.00000000000004,N/A
"def benchmark(Loss, dev, N, loops=10):
    """"""Times a loss computation+gradient on an N-by-N problem.""""""
    device = torch.device(dev)
    a, x, b, y = generate_samples(N, device)
    code = 'L = Loss( a, x, b, y ) ; L.backward()'
    Loss.verbose = True
    exec(code, locals())
    Loss.verbose = False
    t_0 = time.perf_counter()
<mask>:
        torch.cuda.synchronize()
    for i in range(loops):
        exec(code, locals())
    if use_cuda:
        torch.cuda.synchronize()
    elapsed = time.perf_counter() - t_0
    print('{:3} NxN loss, with N ={:7}: {:3}x{:3.6f}s'.format(loops, N, loops, elapsed / loops))
    return elapsed / loops",use_cuda,78,use_cuda,True,100.00000000000004,N/A
"def bench_config(Loss, dev):
    """"""Times a loss computation+gradient for an increasing number of samples.""""""
    print('Backend : {}, Device : {} -------------'.format(Loss.backend, dev))
    times = []

    def run_bench():
        try:
            Nloops = [100, 10, 1]
            nloops = Nloops.pop(0)
            for n in NS:
                elapsed = benchmark(Loss, dev, n, loops=nloops)
                times.append(elapsed)
<mask>:
                    nloops = Nloops.pop(0)
        except IndexError:
            print('**\nToo slow !')
    try:
        run_bench()
    except RuntimeError as err:
        if str(err)[:4] == 'CUDA':
            print('**\nMemory overflow !')
        else:
            run_bench()
    return times + (len(NS) - len(times)) * [np.nan]",nloops * elapsed > MAXTIME or (nloops * elapsed > REDTIME and len(Nloops) > 0),78,len(Nloops) > 100,False,7.368276169123442,N/A
"def sinkhorn_loop(a_i, x_i, b_j, y_j, blur=0.01, nits=100, backend='keops'):
    """"""Straightforward implementation of the Sinkhorn-IPFP-SoftAssign loop in the log domain.""""""
    loga_i, logb_j = (a_i.log(), b_j.log())
    loga_i, logb_j = (loga_i[:, None, None], logb_j[None, :, None])
<mask>:
        x_i, y_j = (LazyTensor(x_i[:, None, :]), LazyTensor(y_j[None, :, :]))
        C_ij = ((x_i - y_j) ** 2).sum(-1) / 2
    elif backend == 'pytorch':
        D_xx = (x_i ** 2).sum(-1)[:, None]
        D_xy = x_i @ y_j.t()
        D_yy = (y_j ** 2).sum(-1)[None, :]
        C_ij = (D_xx + D_yy) / 2 - D_xy
        C_ij = C_ij[:, :, None]
    eps = blur ** 2
    F_i, G_j = (torch.zeros_like(loga_i), torch.zeros_like(logb_j))
    for _ in range(nits):
        F_i = -(-C_ij / eps + (G_j + logb_j)).logsumexp(dim=1)[:, None, :]
        G_j = -(-C_ij / eps + (F_i + loga_i)).logsumexp(dim=0)[None, :, :]
    return (eps * F_i, eps * G_j)",backend == 'keops',129,backend == 'keops',True,100.00000000000004,N/A
"def full_benchmark(source, target, blur, maxtime=None):
    OT_solver = SamplesLoss('sinkhorn', p=2, blur=blur, backend='online', scaling=0.999, debias=False, potentials=True)
    _, _, ground_truth = benchmark_solver(OT_solver, blur, sources[0], targets[0])
    results = {}
    for name in ['multiscale-1', 'multiscale-5', 'online', 'tensorized']:
<mask>:
            backend, truncate = ('multiscale', 1)
        elif name == 'multiscale-5':
            backend, truncate = ('multiscale', 5)
        else:
            backend, truncate = (name, None)
        OT_solvers = [SamplesLoss('sinkhorn', p=2, blur=blur, scaling=scaling, truncate=truncate, backend=backend, debias=False, potentials=True) for scaling in [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]]
        results[name] = benchmark_solvers('GeomLoss - ' + name, OT_solvers, source, target, ground_truth, blur=blur, display=False, maxtime=maxtime)
    for backend in ['pytorch', 'keops']:
        OT_solvers = [sinkhorn_solver(blur, nits=nits, backend=backend) for nits in [5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000]]
        results[backend] = benchmark_solvers('Sinkhorn loop - ' + backend, OT_solvers, source, target, ground_truth, blur=blur, display=False, maxtime=maxtime)
    return (results, ground_truth)",name == 'multiscale-1',129,name == 'multiscale-1',True,100.00000000000004,N/A
"def marginal_error(blur, a_i, x_i, b_j, y_j, F_i, G_j, mode='blurred'):
    """"""Measures how well the transport plan encoded in the dual vectors F_i and G_j satisfies the marginal constraints.""""""
    A_i, B_j = plan_marginals(blur, a_i, x_i, b_j, y_j, F_i, G_j)
<mask>:
        return ((A_i - a_i).abs().sum() + (B_j - b_j).abs().sum()) / 2
    elif mode == 'blurred':
        norm_x = blurred_relative_error(blur, x_i, a_i, A_i)
        norm_y = blurred_relative_error(blur, y_j, b_j, B_j)
        return (norm_x + norm_y) / 2
    else:
        raise NotImplementedError()",mode == 'TV',73,mode == 'blurred',False,59.460355750136046,N/A
"def benchmark_solver(OT_solver, blur, source, target):
    """"""Returns a (timing, relative error on the marginals, wasserstein distance) triplet for OT_solver(source, target).""""""
    a_i, x_i = source
    b_j, y_j = target
    a_i, x_i = (a_i.contiguous(), x_i.contiguous())
    b_j, y_j = (b_j.contiguous(), y_j.contiguous())
<mask>:
        torch.cuda.synchronize()
    start = time.time()
    F_i, G_j = OT_solver(a_i, x_i, b_j, y_j)
    if x_i.is_cuda:
        torch.cuda.synchronize()
    end = time.time()
    F_i, G_j = (F_i.view(-1), G_j.view(-1))
    return (end - start, marginal_error(blur, a_i, x_i, b_j, y_j, F_i, G_j).item(), wasserstein_distance(a_i, b_j, F_i, G_j).item())",x_i.is_cuda,75,x_i.is_cuda,True,100.00000000000004,N/A
"def benchmark_solvers(name, OT_solvers, source, target, ground_truth, blur=0.01, display=False, maxtime=None):
    timings, errors, costs = ([], [], [])
    break_loop = False
    print('Benchmarking the ""{}"" family of OT solvers - ground truth = {:.6f}:'.format(name, ground_truth))
    for i, OT_solver in enumerate(OT_solvers):
        try:
            timing, error, cost = benchmark_solver(OT_solver, blur, source, target)
            timings.append(timing)
            errors.append(error)
            costs.append(cost)
            print('{}-th solver : t = {:.4f}, error on the constraints = {:.3f}, cost = {:.6f}'.format(i + 1, timing, error, cost))
        except RuntimeError:
            print('** Memory overflow ! **')
            break_loop = True
            timings.append(np.nan)
            errors.append(np.nan)
            costs.append(np.nan)
<mask>:
            not_performed = len(OT_solvers) - (i + 1)
            timings += [np.nan] * not_performed
            errors += [np.nan] * not_performed
            costs += [np.nan] * not_performed
            break
    print('')
    timings, errors, costs = (np.array(timings), np.array(errors), np.array(costs))
    if display:
        fig = plt.figure(figsize=(12, 8))
        ax_1 = fig.subplots()
        ax_1.set_title('Benchmarking ""{}""\non a {:,}-by-{:,} entropic OT problem, with a blur radius of {:.3f}'.format(name, len(source[0]), len(target[0]), blur))
        ax_1.set_xlabel('time (s)')
        ax_1.plot(timings, errors, color='b')
        ax_1.set_ylabel('Relative error on the marginal constraints', color='b')
        ax_1.tick_params('y', colors='b')
        ax_1.set_yscale('log')
        ax_1.set_ylim(bottom=1e-05)
        ax_2 = ax_1.twinx()
        ax_2.plot(timings, abs(costs - ground_truth) / ground_truth, color='r')
        ax_2.set_ylabel('Relative error on the cost value', color='r')
        ax_2.tick_params('y', colors='r')
        ax_2.set_yscale('log')
        ax_2.set_ylim(bottom=1e-05)
    return (timings, errors, costs)",break_loop or (maxtime is not None and timing > maxtime),180,i + 1 < len(OT_solvers),False,3.701773936489291,N/A
"def gradient_flow(loss, lr=0.01):
    """"""Flows along the gradient of the cost function, using a simple Euler scheme.

    Parameters:
        loss ((x_i,y_j) -> torch float number):
            Real-valued loss function.
        lr (float, default = .025):
            Learning rate, i.e. time step.
    """"""
    Nsteps = int(5 / lr) + 1
    display_its = [int(t / lr) for t in [0, 0.25, 0.5, 1.0, 2.0, 5.0]]
    x_i, y_j = (X_i.clone(), Y_j.clone())
    x_i.requires_grad = True
    t_0 = time.time()
    plt.figure(figsize=(12, 8))
    k = 1
    for i in range(Nsteps):
        L_αβ = loss(x_i, y_j)
        [g] = torch.autograd.grad(L_αβ, [x_i])
<mask>:
            ax = plt.subplot(2, 3, k)
            k = k + 1
            display_samples(ax, y_j, (0.55, 0.55, 0.95))
            display_samples(ax, x_i, (0.95, 0.55, 0.55))
            ax.set_title('t = {:1.2f}'.format(lr * i))
            plt.axis([-0.1, 1.1, -0.1, 5.5])
            plt.xticks([], [])
            plt.yticks([], [])
            plt.tight_layout()
        x_i.data -= lr * len(x_i) * g
    plt.title('t = {:1.2f}, elapsed time: {:.2f}s/it'.format(lr * i, (time.time() - t_0) / Nsteps))",i in display_its,141,k % 2 == 0,False,0.0,N/A
"def gradient_flow(loss, lr=0.05):
    """"""Flows along the gradient of the cost function, using a simple Euler scheme.

    Parameters:
        loss ((x_i,y_j) -> torch float number):
            Real-valued loss function.
        lr (float, default = .05):
            Learning rate, i.e. time step.
    """"""
    Nsteps = int(5 / lr) + 1
    display_its = [int(t / lr) for t in [0, 0.25, 0.5, 1.0, 2.0, 5.0]]
    colors = (10 * X_i[:, 0]).cos() * (10 * X_i[:, 1]).cos()
    colors = colors.detach().cpu().numpy()
    x_i, y_j = (X_i.clone(), Y_j.clone())
    x_i.requires_grad = True
    t_0 = time.time()
    plt.figure(figsize=(12, 8))
    k = 1
    for i in range(Nsteps):
        L_αβ = loss(x_i, y_j)
        [g] = torch.autograd.grad(L_αβ, [x_i])
<mask>:
            ax = plt.subplot(2, 3, k)
            k = k + 1
            plt.set_cmap('hsv')
            plt.scatter([10], [10])
            display_samples(ax, y_j, [(0.55, 0.55, 0.95)])
            display_samples(ax, x_i, colors)
            ax.set_title('t = {:1.2f}'.format(lr * i))
            plt.axis([0, 1, 0, 1])
            plt.gca().set_aspect('equal', adjustable='box')
            plt.xticks([], [])
            plt.yticks([], [])
            plt.tight_layout()
        x_i.data -= lr * len(x_i) * g
    plt.title('t = {:1.2f}, elapsed time: {:.2f}s/it'.format(lr * i, (time.time() - t_0) / Nsteps))",i in display_its,158,k % 2 == 0,False,0.0,N/A
"def display_samples(ax, x, weights, color, v=None):
    x_ = x.detach().cpu().numpy()
    weights_ = weights.detach().cpu().numpy()
    weights_[weights_ < 1e-05] = 0
    ax.scatter(x_[:, 0], x_[:, 1], 10 * 500 * weights_, color, edgecolors='none')
<mask>:
        v_ = v.detach().cpu().numpy()
        ax.quiver(x_[:, 0], x_[:, 1], v_[:, 0], v_[:, 1], scale=1, scale_units='xy', color='#5CBF3A', zorder=3, width=2.0 / len(x_))",v is not None,47,v is not None,True,100.00000000000004,N/A
"def display_4d_samples(ax1, ax2, x, color):
    x_ = x.detach().cpu().numpy()
<mask>:
        color = color.detach().cpu().numpy()
    ax1.scatter(x_[:, 0], x_[:, 1], 25 * 500 / len(x_), color, edgecolors='none', cmap='tab10')
    ax2.scatter(x_[:, 2], x_[:, 3], 25 * 500 / len(x_), color, edgecolors='none', cmap='tab10')","not type(color) in [str, list]",36,color is not None,False,3.300991086751251,N/A
"def KMeans(x, K=10, Niter=10, verbose=True):
    N, D = x.shape
    nn_search = generic_argmin('SqDist(x,y)', 'ind = Vi(1)', 'x = Vi({})'.format(D), 'y = Vj({})'.format(D))
    start = time.time()
    perm = torch.randperm(N)
    idx = perm[:K]
    c = x[idx, :].clone()
    for i in range(Niter):
        cl = nn_search(x, c).view(-1)
        Ncl = torch.bincount(cl).type(dtype)
        for d in range(D):
            c[:, d] = torch.bincount(cl, weights=x[:, d]) / Ncl
<mask>:
        torch.cuda.synchronize()
    end = time.time()
    if verbose:
        print('KMeans performed in {:.3f}s.'.format(end - start))
    return (cl, c)",use_cuda,73,torch.cuda.is_available(),False,5.669791110976001,N/A
"def display_samples(ax, x, color='black'):
    x_ = x.detach().cpu().numpy()
<mask>:
        color = color.detach().cpu().numpy()
    ax.scatter(x_[:, 0], x_[:, 1], 25 * 500 / len(x_), color, edgecolors='none')",type(color) is not str,22,color is not None,False,16.70067963244422,N/A
"def draw_samples(fname, n, dtype=torch.FloatTensor, labels=False):
    A = load_image(fname)
    xg, yg = np.meshgrid(np.arange(A.shape[0]), np.arange(A.shape[1]), indexing='xy')
    A_gray = (1 - A).sum(2)
    grid = list(zip(xg.ravel(), yg.ravel()))
    dens = A_gray.ravel() / A_gray.sum()
    dots = np.array(choices(grid, dens, k=n))
<mask>:
        labs = A[dots[:, 1], dots[:, 0]].reshape((n, 3))
    dots = (dots.astype(float) + 0.5) / np.array([A.shape[0], A.shape[1]])
    dots += 0.5 / A.shape[0] * np.random.standard_normal(dots.shape)
    if labels:
        return (torch.from_numpy(dots).type(dtype), torch.from_numpy(labs).type(dtype))
    else:
        return torch.from_numpy(dots).type(dtype)",labels,64,n > 3,False,0.0,N/A
"def mypause(interval):
    """"""Pause matplotlib without stealing focus.""""""
    backend = plt.rcParams['backend']
<mask>:
        figManager = matplotlib._pylab_helpers.Gcf.get_active()
        if figManager is not None:
            canvas = figManager.canvas
            if canvas.figure.stale:
                canvas.draw()
            canvas.start_event_loop(interval)
            return",backend in matplotlib.rcsetup.interactive_bk,27,backend == 'pylab',False,4.576506607182439,N/A
"def model_to_numpy(model, grad=False):
    """"""
    The fortran routines used by scipy.optimize expect float64 vectors
    instead of the gpu-friendly float32 matrices: we need conversion routines.
    """"""
<mask>:
        raise ValueError('Scipy optimization routines are only compatible with parameters given as *contiguous* tensors.')
    if grad:
        tensors = [param.grad.data.view(-1).cpu().numpy() for param in model.parameters()]
    else:
        tensors = [param.data.view(-1).cpu().numpy() for param in model.parameters()]
    return np.ascontiguousarray(np.hstack(tensors), dtype='float64')",not all((param.is_contiguous() for param in model.parameters())),58,len(model.parameters()) != len(model.parameters()),False,28.97340601116356,N/A
"def numpy_to_model(model, vec):
    i = 0
    for param in model.parameters():
        offset = param.numel()
        param.data = torch.from_numpy(vec[i:i + offset]).view(param.data.size()).type(param.data.type())
        i += offset
<mask>:
        raise ValueError(""The total number of variables in model is not the same as in 'vec'."")",i != len(vec),37,model.total_number() != vec.size(),False,7.141816289329644,N/A
"def fit_model(Model, method='L-BFGS', tol=1e-10, nits=500, nlogs=10, lr=0.1, eps=0.01, maxcor=10, gtol=1e-10, display=False, **params):
    """"""""""""
    use_scipy = False
<mask>:
        optimizer = torch.optim.Adam(Model.parameters(), lr=lr, eps=eps)
    elif method == 'L-BFGS':
        optimizer = torch.optim.SGD(Model.parameters(), lr=1.0)
        use_scipy = True
        method = 'L-BFGS-B'
        options = dict(maxiter=nits, ftol=tol, gtol=gtol, maxcor=maxcor)
    else:
        raise NotImplementedError('Optimization method not supported : ""' + method + '"". Available values are ""Adam"" and ""L-BFGS"".')
    costs = []
    fit_model.nit = -1
    fit_model.breakloop = False

    def closure(final_it=False):
        """"""
        Encapsulates a problem + display iteration into a single callable statement.
        This wrapper is needed if you choose to use LBFGS-like algorithms, which
        (should) implement a careful line search along the gradient's direction.
        """"""
        fit_model.nit += 1
        it = fit_model.nit
        optimizer.zero_grad()
        cost = Model.forward()
        costs.append(cost.item())
        cost.backward()
        if len(costs) > 1 and abs(costs[-1] - costs[-2]) < tol or fit_model.nit == nits - 1:
            fit_model.breakloop = True
        if display:
            Model.plot(nit=fit_model.nit, cost=cost.item())
        return cost

    def numpy_closure(vec, final_it=False):
        """"""
        Wraps the PyTorch closure into a 'float64'-vector routine,
        as expected by scipy.optimize.
        """"""
        vec = lr * vec.astype('float64')
        numpy_to_model(Model, vec)
        c = closure(final_it).item()
        dvec_c = lr * model_to_numpy(Model, grad=True)
        return (c, dvec_c)
    if use_scipy:
        res = minimize(numpy_closure, model_to_numpy(Model), method=method, jac=True, options=options)
        numpy_closure(res.x, final_it=True)
    else:
        for i in range(nits + 1):
            optimizer.step(closure)
            if fit_model.breakloop:
                closure(final_it=True)
                break",method == 'Adam',202,method == 'Adam',True,100.00000000000004,N/A
"def color_transfer(loss, lr=1):
    """"""Flows along the gradient of the loss function.

    Parameters:
        loss ((x_i,y_j) -> torch float number):
            Real-valued loss function.
        lr (float, default = 1):
            Learning rate, i.e. time step.
    """"""
    Nsteps = 11
    display_its = [1, 10]
    x_i, y_j = (X_i.clone(), Y_j.clone())
    x_i.requires_grad = True
    t_0 = time.time()
    plt.figure(figsize=(12, 12))
    k = 3
    ax = plt.subplot(2, 2, 1)
    display_image(ax, X_i)
    ax.set_title('Source image')
    plt.xticks([], [])
    plt.yticks([], [])
    ax = plt.subplot(2, 2, 2)
    display_image(ax, Y_j)
    ax.set_title('Target image')
    plt.xticks([], [])
    plt.yticks([], [])
    for i in range(Nsteps):
        L_αβ = loss(x_i, y_j)
        [g] = torch.autograd.grad(L_αβ, [x_i])
<mask>:
            ax = plt.subplot(2, 2, k)
            display_image(ax, x_i)
            ax.set_title('it = {}'.format(i))
            k = k + 1
            plt.xticks([], [])
            plt.yticks([], [])
        x_i.data -= lr * len(x_i) * g
    plt.title('it = {}, elapsed time: {:.2f}s/it'.format(i, (time.time() - t_0) / Nsteps))
    plt.tight_layout()",i in display_its,132,k % 2 == 0,False,0.0,N/A
"def normalize(measure, n=None):
    """"""Reduce a point cloud to at most n points and normalize the weights and point cloud.""""""
    weights, locations = measure
    N = len(weights)
<mask>:
        n = int(n)
        indices = torch.randperm(N)
        indices = indices[:n]
        weights, locations = (weights[indices], locations[indices])
    weights = weights / weights.sum()
    weights, locations = (weights.contiguous(), locations.contiguous())
    mean = (weights.view(-1, 1) * locations).sum(dim=0)
    locations -= mean
    std = (weights.view(-1) * (locations ** 2).sum(dim=1).view(-1)).sum().sqrt()
    locations /= std
    return (weights, locations)",n is not None and n < N,73,n is not None,False,36.78794411714425,N/A
"def OT_registration(source, target, name):
    a, x = source
    b, y = target
    x.requires_grad = True
    z = x.clone()
<mask>:
        torch.cuda.synchronize()
    start = time.time()
    nits = 4 if fast_demo else 10
    for it in range(nits):
        wasserstein_zy = Loss(a, z, b, y)
        [grad_z] = torch.autograd.grad(wasserstein_zy, [z])
        z -= grad_z / a[:, None]
    end = time.time()
    print('Registered {} in {:.3f}s.'.format(name, end - start))
    return z",use_cuda,62,torch.cuda.is_available(),False,5.669791110976001,N/A
"def plot(self, nit=0, cost=0, ax=None, title=None):
    """"""Displays the descent using a custom 'waffle' layout.

        N.B.: As the L-BFGS descent typically induces high-frequencies in
              the optimization process, we blur the 'interpolating' measure
              a little bit more than the two endpoints.
        """"""
<mask>:
        if nit == 0 or nit % 16 == 4:
            plt.pause(0.01)
            plt.figure(figsize=(16, 4))
        if nit <= 4 or nit % 4 == 0:
            if nit < 4:
                index = nit + 1
            else:
                index = (nit // 4 - 1) % 4 + 1
            ax = plt.subplot(1, 4, index)
    if ax is not None:
        display_samples(ax, self.x_i, (0.95, 0.55, 0.55))
        display_samples(ax, self.y_j, (0.55, 0.55, 0.95))
        display_samples(ax, self.z_k, (0.55, 0.95, 0.55), weights=self.weights(), blur=0.005)
        if title is None:
            ax.set_title('nit = {}, cost = {:3.4f}'.format(nit, cost))
        else:
            ax.set_title(title)
        ax.axis([-0.1, 1.1, -0.1, 20.5])
        ax.set_xticks([], [])
        ax.set_yticks([], [])
        plt.tight_layout()",ax is None,134,ax is None,True,100.00000000000004,N/A
"def gradient_descent(loss, lr=1):
    """"""Flows along the gradient of the loss function.

    Parameters:
        loss ((x_i,y_j) -> torch float number):
            Real-valued loss function.
        lr (float, default = 1):
            Learning rate, i.e. time step.
    """"""
    Nsteps = 11
    display_its = [0, 1, 2, 10]
    colors = (10 * X_i[:, 0]).cos() * (10 * X_i[:, 1]).cos()
    colors = colors.detach().cpu().numpy()
    x_i, y_j = (X_i.clone(), Y_j.clone())
    x_i.requires_grad = True
    t_0 = time.time()
    plt.figure(figsize=(12, 12))
    k = 1
    for i in range(Nsteps):
        L_αβ = loss(x_i, y_j)
        [g] = torch.autograd.grad(L_αβ, [x_i])
<mask>:
            ax = plt.subplot(2, 2, k)
            k = k + 1
            plt.set_cmap('hsv')
            plt.scatter([10], [10])
            display_samples(ax, y_j, [(0.55, 0.55, 0.95)])
            display_samples(ax, x_i, colors)
            ax.set_title('it = {}'.format(i))
            plt.axis([0, 1, 0, 1])
            plt.gca().set_aspect('equal', adjustable='box')
            plt.xticks([], [])
            plt.yticks([], [])
            plt.tight_layout()
        x_i.data -= lr * len(x_i) * g
    plt.title('it = {}, elapsed time: {:.2f}s/it'.format(i, (time.time() - t_0) / Nsteps))",i in display_its,137,k % 2 == 0,False,0.0,N/A
"def read_vtk(filename):
<mask>:
        polydata_reader = vtk.vtkXMLPolyDataReader()
    else:
        polydata_reader = vtk.vtkPolyDataReader()
    polydata_reader.SetFileName(filename)
    polydata_reader.Update()
    polydata = polydata_reader.GetOutput()
    return vtkPolyData_to_tracts(polydata)",filename.endswith('xml') or filename.endswith('vtp'),17,vtk.vtkVersion.GetVTKMajorVersion() >= 6,False,4.396165418527572,N/A
"def vtkPolyData_to_tracts(polydata):
    result = {}
    result['lines'] = ns.vtk_to_numpy(polydata.GetLines().GetData())
    result['points'] = ns.vtk_to_numpy(polydata.GetPoints().GetData())
    result['numberOfLines'] = polydata.GetNumberOfLines()
    data = {}
<mask>:
        data['ActiveScalars'] = polydata.GetPointData().GetScalars().GetName()
        result['Scalars'] = polydata.GetPointData().GetScalars()
    if polydata.GetPointData().GetVectors():
        data['ActiveVectors'] = polydata.GetPointData().GetVectors().GetName()
    if polydata.GetPointData().GetTensors():
        data['ActiveTensors'] = polydata.GetPointData().GetTensors().GetName()
    for i in xrange(polydata.GetPointData().GetNumberOfArrays()):
        array = polydata.GetPointData().GetArray(i)
        np_array = ns.vtk_to_numpy(array)
        if np_array.ndim == 1:
            np_array = np_array.reshape(len(np_array), 1)
        data[polydata.GetPointData().GetArrayName(i)] = np_array
    result['pointData'] = data
    tracts, data = vtkPolyData_dictionary_to_tracts_and_data(result)
    return (tracts, data)",polydata.GetPointData().GetScalars(),65,polydata.GetPointData().GetScalars(),True,100.00000000000004,N/A
"def vtkPolyData_dictionary_to_tracts_and_data(dictionary):
    dictionary_keys = set(('lines', 'points', 'numberOfLines'))
<mask>:
        raise ValueError('Dictionary must have the keys lines and points' + repr(dictionary.keys()))
    tract_data = {}
    tracts = []
    lines = np.asarray(dictionary['lines']).squeeze()
    points = dictionary['points']
    actual_line_index = 0
    number_of_tracts = dictionary['numberOfLines']
    original_lines = []
    for l in xrange(number_of_tracts):
        tracts.append(points[lines[actual_line_index + 1:actual_line_index + lines[actual_line_index] + 1]])
        original_lines.append(np.array(lines[actual_line_index + 1:actual_line_index + lines[actual_line_index] + 1], copy=True))
        actual_line_index += lines[actual_line_index] + 1
    if 'pointData' in dictionary:
        point_data_keys = [it[0] for it in dictionary['pointData'].items() if isinstance(it[1], np.ndarray)]
        for k in point_data_keys:
            array_data = dictionary['pointData'][k]
            if not k in tract_data:
                tract_data[k] = [array_data[f] for f in original_lines]
            else:
                np.vstack(tract_data[k])
                tract_data[k].extend([array_data[f] for f in original_lines[-number_of_tracts:]])
    return (tracts, tract_data)",not dictionary_keys.issubset(dictionary.keys()),107,len(dictionary_keys) != len(dictionary_keys),False,15.396503757846457,N/A
"def save_vtk(filename, tracts, lines_indices=None, scalars=None):
    lengths = [len(p) for p in tracts]
    line_starts = ns.numpy.r_[0, ns.numpy.cumsum(lengths)]
<mask>:
        lines_indices = [ns.numpy.arange(length) + line_start for length, line_start in izip(lengths, line_starts)]
    ids = ns.numpy.hstack([ns.numpy.r_[c[0], c[1]] for c in izip(lengths, lines_indices)])
    vtk_ids = ns.numpy_to_vtkIdTypeArray(ids, deep=True)
    cell_array = vtk.vtkCellArray()
    cell_array.SetCells(len(tracts), vtk_ids)
    points = ns.numpy.vstack(tracts).astype(ns.get_vtk_to_numpy_typemap()[vtk.VTK_DOUBLE])
    points_array = ns.numpy_to_vtk(points, deep=True)
    poly_data = vtk.vtkPolyData()
    vtk_points = vtk.vtkPoints()
    vtk_points.SetData(points_array)
    poly_data.SetPoints(vtk_points)
    poly_data.SetLines(cell_array)
    poly_data.BuildCells()
    if filename.endswith('.xml') or filename.endswith('.vtp'):
        writer = vtk.vtkXMLPolyDataWriter()
        writer.SetDataModeToBinary()
    else:
        writer = vtk.vtkPolyDataWriter()
        writer.SetFileTypeToBinary()
    writer.SetFileName(filename)
    if hasattr(vtk, 'VTK_MAJOR_VERSION') and vtk.VTK_MAJOR_VERSION > 5:
        writer.SetInputData(poly_data)
    else:
        writer.SetInput(poly_data)
    writer.Write()",lines_indices is None,88,lines_indices is None,True,100.00000000000004,N/A
"def save_vtk_labels(filename, tracts, scalars, lines_indices=None):
    lengths = [len(p) for p in tracts]
    line_starts = ns.numpy.r_[0, ns.numpy.cumsum(lengths)]
<mask>:
        lines_indices = [ns.numpy.arange(length) + line_start for length, line_start in izip(lengths, line_starts)]
    ids = ns.numpy.hstack([ns.numpy.r_[c[0], c[1]] for c in izip(lengths, lines_indices)])
    vtk_ids = ns.numpy_to_vtkIdTypeArray(ids, deep=True)
    cell_array = vtk.vtkCellArray()
    cell_array.SetCells(len(tracts), vtk_ids)
    points = ns.numpy.vstack(tracts).astype(ns.get_vtk_to_numpy_typemap()[vtk.VTK_DOUBLE])
    points_array = ns.numpy_to_vtk(points, deep=True)
    poly_data = vtk.vtkPolyData()
    vtk_points = vtk.vtkPoints()
    vtk_points.SetData(points_array)
    poly_data.SetPoints(vtk_points)
    poly_data.SetLines(cell_array)
    poly_data.GetPointData().SetScalars(ns.numpy_to_vtk(scalars))
    poly_data.BuildCells()
    if filename.endswith('.xml') or filename.endswith('.vtp'):
        writer = vtk.vtkXMLPolyDataWriter()
        writer.SetDataModeToBinary()
    else:
        writer = vtk.vtkPolyDataWriter()
        writer.SetFileTypeToBinary()
    writer.SetFileName(filename)
    if hasattr(vtk, 'VTK_MAJOR_VERSION') and vtk.VTK_MAJOR_VERSION > 5:
        writer.SetInputData(poly_data)
    else:
        writer.SetInput(poly_data)
    writer.Write()",lines_indices is None,89,lines_indices is None,True,100.00000000000004,N/A
"def fetch_file(name):
<mask>:
        import urllib.request
        print('Fetching the atlas... ', end='', flush=True)
        urllib.request.urlretrieve(f'https://www.kernel-operations.io/data/{name}.nii.gz', f'data/{name}.nii.gz')
        with gzip.open(f'data/{name}.nii.gz', 'rb') as f_in:
            with open(f'data/{name}.nii', 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        print('Done.')",not os.path.exists(f'data/{name}.nii.gz'),26,not os.path.exists(f'data/{name}.nii.gz'),True,100.00000000000004,N/A
"def fetch_file(name):
<mask>:
        import urllib.request
        print('Fetching the atlas... ', end='', flush=True)
        urllib.request.urlretrieve(f'https://www.kernel-operations.io/data/{name}.npy', f'data/{name}.npy')
        print('Done.')",not os.path.exists(f'data/{name}.npy'),14,not os.path.exists(f'https://www.kernel-operations.io/data/{name}.npy'),False,45.869538998376264,N/A
"def get_config_from_root(root):
    """"""Read the project setup.cfg file to determine Versioneer config.""""""
    setup_cfg = os.path.join(root, 'setup.cfg')
    parser = configparser.ConfigParser()
    with open(setup_cfg) as cfg_file:
        parser.read_file(cfg_file)
    VCS = parser.get('versioneer', 'VCS')
    section = parser['versioneer']
    cfg = VersioneerConfig()
    cfg.VCS = VCS
    cfg.style = section.get('style', '')
    cfg.versionfile_source = section.get('versionfile_source')
    cfg.versionfile_build = section.get('versionfile_build')
    cfg.tag_prefix = section.get('tag_prefix')
<mask>:
        cfg.tag_prefix = ''
    cfg.parentdir_prefix = section.get('parentdir_prefix')
    cfg.verbose = section.get('verbose')
    return cfg","cfg.tag_prefix in (""''"", '""""')",61,cfg.tag_prefix is None,False,16.996005791513966,N/A
"def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):
    """"""Call the given command(s).""""""
    assert isinstance(commands, list)
    process = None
    for command in commands:
        try:
            dispcmd = str([command] + args)
            process = subprocess.Popen([command] + args, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE if hide_stderr else None)
            break
        except OSError:
            e = sys.exc_info()[1]
<mask>:
                continue
            if verbose:
                print('unable to run %s' % dispcmd)
                print(e)
            return (None, None)
    else:
        if verbose:
            print('unable to find command, tried {}'.format(commands))
        return (None, None)
    stdout = process.communicate()[0].strip().decode()
    if process.returncode != 0:
        if verbose:
            print('unable to run %s (error)' % dispcmd)
            print('stdout was %s' % stdout)
        return (None, process.returncode)
    return (stdout, process.returncode)",e.errno == errno.ENOENT,99,e.errno == errno.ENOENT,True,100.00000000000004,N/A
"@register_vcs_handler('git', 'get_keywords')
def git_get_keywords(versionfile_abs):
    """"""Extract version information from the given file.""""""
    keywords = {}
    try:
        with open(versionfile_abs) as fobj:
            for line in fobj:
<mask>:
                    mo = re.search('=\\s*""(.*)""', line)
                    if mo:
                        keywords['refnames'] = mo.group(1)
                if line.strip().startswith('git_full ='):
                    mo = re.search('=\\s*""(.*)""', line)
                    if mo:
                        keywords['full'] = mo.group(1)
                if line.strip().startswith('git_date ='):
                    mo = re.search('=\\s*""(.*)""', line)
                    if mo:
                        keywords['date'] = mo.group(1)
    except OSError:
        pass
    return keywords",line.strip().startswith('git_refnames ='),62,line.strip().startswith('git_refnames ='),True,100.00000000000004,N/A
"def resolve_commandline_arguments():
    """"""Parse CLI arguments and return practice text details.

    Returns:
        (str, Union[str, int]): Tuple of text content and text ID.
    """"""
    opt = parse_arguments()
<mask>:
        display_version()
        sys.exit(0)
    elif opt.history is not None:
        show_history(opt.history)
        sys.exit(0)
    elif opt.file:
        text, text_id = load_text_from_file(opt.file)
    elif opt.id:
        text, text_id = load_from_database(opt.id)
    elif opt.difficulty is not None:
        text, text_id = load_based_on_difficulty(opt.difficulty)
    else:
        text, text_id = load_based_on_difficulty()
    return (text, text_id)",opt.version,64,opt.version is not None,False,30.213753973567677,N/A
"def load_text_from_file(file_path):
    """"""Load file contents.

    Args:
        file_path (str): Full path to text to load.

    Returns:
        (str, str): Tuple of text content followed by file path.
    """"""
<mask>:
        with open(file_path, encoding='utf-8') as file:
            text = file.read()
            filename = os.path.basename(file_path)
        return (text, filename)
    print('Cannot open file -', file_path)
    sys.exit(0)",os.path.isfile(file_path),47,os.path.isfile(file_path),True,100.00000000000004,N/A
"def load_from_database(text_id):
    """"""Load given text from database with given id.

    Args:
        text_id (int): Row identifier of database text to load.

    Returns:
        (str, int): Tuple of text content followed by DB row identifier.
    """"""
    row_count = 6000
<mask>:
        text = mitype.database.fetch_text_from_id(text_id)
        return (text, text_id)
    print('ID must be in range [1,6000]')
    sys.exit(1)",1 <= text_id <= row_count,50,1 <= row_count <= text_id,False,76.91605673134588,N/A
"def load_based_on_difficulty(difficulty_level=random.randrange(1, 6)):
    """"""
    Load text of given difficulty from database if parameter is passed.

    Defaults to random difficulty level when none is provided.

    Args:
        difficulty_level (int): difficulty level in a range of 1 - 5

    Returns:
        (str, int): Tuple of text content followed by DB row identifier.
    """"""
    max_level = 5
<mask>:
        upper_limit = difficulty_level * 1200
        lower_limit = upper_limit - 1200 + 1
        text_id = random.randrange(lower_limit, upper_limit + 1)
        text = mitype.database.fetch_text_from_id(text_id)
        return (text, text_id)
    print('Select a difficulty level in range [1,5]')
    sys.exit(1)",1 <= difficulty_level <= max_level,85,difficulty_level < max_level,False,33.5783404331301,N/A
"def first_index_at_which_strings_differ(string1, string2):
    """"""Return index at which there is a change in strings.

    This is used to determine the index up to which text must be dimmed and
    after which must be coloured red (indicating mismatch).

    Args:
        string1 (str): The string which is a combination of
            last typed keys in a session.
        string2 (str): The string corresponding to sample text.

    Returns:
        int: Index at which mismatch occurs for the first time.
    """"""
    length = min(len(string1), len(string2))
    for index in range(length):
<mask>:
            return index
    return length",string1[index] != string2[index],85,string1[index] == string2[index],False,65.80370064762461,N/A
"def word_wrap(text, width):
    """"""Wrap text on the screen according to the window width.

    Returns text with extra spaces which makes the string word wrap.

    Args:
        text (str): Text to wrap.
        width (int): Width to wrap around.

    Returns:
        str: Return altered text.
    """"""
    for line in range(1, number_of_lines_to_fit_text_in_window(text, width) + 1):
<mask>:
            continue
        index = line * width - 1
        if text[index] == ' ':
            continue
        index = text[:index].rfind(' ')
        space_count = line * width - index
        space_string = ' ' * space_count
        text = text[:index] + space_string + text[index + 1:]
    return text",line * width >= len(text),93,line == 0,False,5.4424142191183185,N/A
"def main(self, win):
    """"""Respond to user inputs.

        This is where the infinite loop is executed to continuously serve events.

        Args:
            win (any): Curses window object.
        """"""
    self.initialize(win)
    while True:
        key = self.keyinput(win)
<mask>:
            if is_escape(key) or is_ctrl_c(key):
                sys.exit(0)
            if is_left_arrow_key(key):
                self.switch_text(win, -1)
            if is_right_arrow_key(key):
                self.switch_text(win, 1)
        if self.mode == 0:
            self.typing_mode(win, key)
        else:
            if is_tab(key):
                win.clear()
                self.reset_test()
                self.setup_print(win)
                self.update_state(win)
            if is_enter(key):
                self.replay(win)
            if is_ctrl_t(key):
                self.share_result()
        win.refresh()",not self.first_key_pressed,66,key,False,0.09118819655545167,N/A
"def update_state(self, win):
    """"""Report on typing session results.

        Args:
            win (any): Curses window.
        """"""
    self.clear_line(win, self.number_of_lines_to_print_text)
    self.clear_line(win, self.number_of_lines_to_print_text + 2)
    self.clear_line(win, self.number_of_lines_to_print_text + 4)
<mask>:
        win.addstr(self.number_of_lines_to_print_text, 0, self.current_word, self.Color.RED)
    else:
        win.addstr(self.number_of_lines_to_print_text, 0, self.current_word)
    win.addstr(2, 0, self.text, curses.A_BOLD)
    win.addstr(2, 0, self.text[0:len(self.current_string)], curses.A_DIM)
    index = first_index_at_which_strings_differ(self.current_string, self.text)
    if index < len(self.current_string) <= len(self.text):
        self.mistyped_keys.append(len(self.current_string) - 1)
    win.addstr(2 + index // self.window_width, index % self.window_width, self.text[index:len(self.current_string)], self.Color.RED)
    if index == len(self.text):
        self.test_end(win)
    win.refresh()",len(self.current_word) >= self.current_word_limit,70,self.use_word_print,False,7.366730902344775,N/A
"def test_end(self, win):
    """"""Trigger at the end of the test.

        Display options for the user to choose at the end of the test.
        Display stats.

        Args:
            win (any): Curses window.
        """"""
    for i in self.mistyped_keys:
        win.addstr(2 + i // self.window_width, i % self.window_width, self.text[i], self.Color.RED)
    curses.curs_set(0)
<mask>:
        self.current_speed_wpm = speed_in_wpm(self.tokens, self.start_time)
        total_chars_in_text = len(self.text_backup)
        wrongly_typed_chars = self.total_chars_typed - total_chars_in_text
        self.accuracy = accuracy(self.total_chars_typed, wrongly_typed_chars)
        self.time_taken = get_elapsed_minutes_since_first_keypress(self.start_time)
        self.mode = 1
        for index in range(len(self.key_strokes) - 1, 0, -1):
            self.key_strokes[index][0] -= self.key_strokes[index - 1][0]
        self.key_strokes[0][0] = 0
    win.addstr(self.number_of_lines_to_print_text, 0, ' Your typing speed is ')
    win.addstr(' ' + self.current_speed_wpm + ' ', self.Color.MAGENTA)
    win.addstr(' WPM ')
    win.addstr(self.number_of_lines_to_print_text + 2, 1, ' Enter ', self.Color.BLACK)
    win.addstr(' to see replay, ')
    win.addstr(' Tab ', self.Color.BLACK)
    win.addstr(' to retry.')
    win.addstr(self.number_of_lines_to_print_text + 3, 1, ' Arrow keys ', self.Color.BLACK)
    win.addstr(' to change text.')
    win.addstr(self.number_of_lines_to_print_text + 4, 1, ' CTRL+T ', self.Color.BLACK)
    win.addstr(' to tweet result.')
    self.print_stats(win)
    self.first_key_pressed = False
    self.end_time = time.time()
    self.current_string = ''
    self.current_word = ''
    self.token_index = 0
    self.start_time = 0
    if not self.test_complete:
        win.refresh()
        save_history(self.text_id, self.current_speed_wpm, f'{self.accuracy:.2f}')
        self.test_complete = True",self.mode == 0,178,self.current_speed_wpm != 0,False,11.868405219520975,N/A
"def typing_mode(self, win, key):
    """"""Start recording typing session progress.

        Args:
            win (any): Curses window.
            key (str): First typed character of the session.
        """"""
<mask>:
        self.start_time = time.time()
        self.first_key_pressed = True
    if is_resize(key):
        self.resize(win)
    if not self.first_key_pressed:
        return
    self.key_strokes.append([time.time(), key])
    self.print_realtime_wpm(win)
    self.key_printer(win, key)",not self.first_key_pressed and is_valid_initial_key(key),42,not self.first_key_pressed,False,25.283959580474658,N/A
"@staticmethod
def keyinput(win):
    """"""Retrieve next character of text input.

        Args:
            win (any): Curses window.

        Returns:
            str: Value of typed key.
        """"""
    key = ''
    try:
        key = win.get_wch()
<mask>:
            if key in (curses.KEY_BACKSPACE, curses.KEY_DC):
                return 'KEY_BACKSPACE'
            if key == curses.KEY_RESIZE:
                return 'KEY_RESIZE'
        return key
    except curses.error:
        return ''","isinstance(key, int)",48,key,False,0.673794699908547,N/A
"def is_escape(key):
    """"""Detect ESC key.

    This is used to exit the application.

    Args:
        key (str): Individual characters are returned as 1-character
            strings, and special keys such as function keys
            return longer strings containing a key name such as
            KEY_UP or ^G.

    Returns:
        bool: Returns `True` if pressed key is ESC key.
        Returns `False` otherwise.
    """"""
<mask>:
        return ord(key) == curses.ascii.ESC
    return False","isinstance(key, str) and len(key) == 1",62,key,False,0.00022603294069810552,N/A
"def is_backspace(key):
    """"""Detect BACKSPACE key.

    Args:
        key (str): Character to check.

    Returns:
        bool: Returns `True` if pressed key is BACKSPACE key.
        Returns `False` otherwise.
    """"""
<mask>:
        return True
    return key in (curses.KEY_BACKSPACE, curses.KEY_DC)","key in ('KEY_BACKSPACE', '\x08', '\x7f')",33,key == curses.KEY_NONE,False,2.7376474102577792,N/A
"def is_null(key):
    """"""Detect null keys like super key.

    Args:
        key (str): Character to check.

    Returns:
        bool: `True` if ""null"" key or `False` otherwise.
    """"""
<mask>:
        return ord(key) == 0
    return key == ''","isinstance(key, str) and len(key) == 1",33,"isinstance(key, str)",False,26.359713811572682,N/A
"def get_key_mapping(key):
    """"""Map key to its corresponding character.

    Addresses a bug in windows-curses where the apostrophe key is
    incorrectly returned as 530 -
    https://github.com/zephyrproject-rtos/windows-curses/issues/41

    Args:
        key (Union[int, str]): Character to check.

    Returns:
        Union[int, str]: Returns character if mapping is present or original key otherwise.
    """"""
<mask>:
        return ""'""
    return key",key == 530,50,"key == ""'""",False,30.213753973567677,N/A
"def is_right_arrow_key(key):
    """"""Detect right arrow key.

    Args:
        key (str): Character to check.

    Returns:
        bool: `True` if key is a valid text character or `False` otherwise.
    """"""
<mask>:
        return True
    return key == curses.KEY_RIGHT",key == 'KEY_RIGHT',33,key == curses.KEY_LEFT,False,22.089591134157878,N/A
"def get_history_records(number_of_records):
    """"""Get records from history.

    Defaults to -1 if argument value not provided on command line.

    Args:
        number_of_records (int): Number of last records to print.
    Returns:
        list: A list of records. The len of this list is `number_of_records` or all records
    """"""
    history_file_path = history_file_absolute_path()
<mask>:
        return []
    with open(history_file_path, encoding='utf-8') as file:
        history_reader = csv.reader(file)
        try:
            next(history_reader)
        except StopIteration:
            return []
        data = list(history_reader)
        total_records = len(data)
        if number_of_records <= -1 or number_of_records >= total_records:
            number_of_records = total_records
        start_count = 0
        if number_of_records < total_records and number_of_records != -1:
            start_count = total_records - number_of_records
        return data[start_count:total_records]",not os.path.exists(history_file_path),98,not os.path.exists(history_file_path),True,100.00000000000004,N/A
"def show_history(number_of_records):
    """"""Show records from history.

    Defaults to -1 if argument value not provided on command line.

    Args:
        number_of_records (int): Number of last records to print.
    """"""
    records = get_history_records(number_of_records)
<mask>:
        print('0 records found')
    print('Last {} records:'.format(len(records)))
    print('ID\tWPM\tDATE\t\tTIME\t\tACCURACY')
    for record in records:
        formatted_row_data = '\t'.join(record)
        print(formatted_row_data + '%')",len(records) == 0,48,len(records) == 0,True,100.00000000000004,N/A
"def save_history(text_id, current_speed_wpm, accuracy):
    """"""Save test stats to history file.

    Args:
        text_id (int): Row identifier of database text loaded.
        current_speed_wpm (float): Speed result from test.
        accuracy (str): Accuracy result from test.
    """"""
    history_path = history_file_absolute_path()
    file_exists = os.path.isfile(history_path)
    with open(history_path, mode='a', newline='', encoding='utf-8') as history:
        csv_history = csv.writer(history)
<mask>:
            row = ['ID', 'WPM', 'DATE', 'TIME', 'ACCURACY']
            csv_history.writerow(row)
        current_time = time.strftime('%H:%M:%S', time.localtime())
        test_data = [text_id, current_speed_wpm, date.today(), current_time, accuracy]
        csv_history.writerow(test_data)",not file_exists,69,file_exists,False,71.65313105737896,N/A
"def load_repo() -> None:
    global REPOSITORY
    repofile: str = f'{os.path.dirname(os.path.realpath(__file__))}/repo.json'
    try:
<mask>:
            raise FileNotFoundError('repository file not found')
        REPOSITORY = json.load(open(repofile, 'r'))
    except Exception as ex:
        error(f'Error while loading repository: {str(ex)}')
        exit(-1)",not os.path.isfile(repofile),31,not os.path.exists(repofile),False,59.694917920196445,N/A
"def decompress_file(infilename: str) -> None:
    filename: str = os.path.basename(infilename).lower()
    try:
        info(f'decompressing {infilename}')
<mask>:
            tar: tarfile.TarFile = tarfile.open(infilename)
            tar.extractall(os.path.dirname(infilename))
        elif filename.endswith('.gz'):
            gf: gzip.GzipFile = gzip.GzipFile(infilename)
            outfile = open(infilename.split('.gz')[0], 'wb')
            copyfileobj(gf, outfile)
            outfile.close()
        else:
            warning(f""decompressing {infilename.split('.')[-1]} file type not supported"")
            return
        success(f'decompressing {filename} completed')
        os.remove(infilename)
    except Exception as ex:
        error(f'Unable to decompress {infilename}: {ex}')",filename.endswith('.tar.gz'),52,filename.endswith('.tar.gz'),True,100.00000000000004,N/A
"def fetch_file(url: str, path: str, useragent: str, decompress: bool) -> None:
    filename: str = os.path.basename(path)
    partpath: str = f'{path}.part'
    headers = {'User-Agent': useragent}
    try:
<mask>:
            warning(f'{filename} already exists -- skipping')
        else:
            if os.path.isfile(partpath):
                info(f'resume downloading {filename} to {partpath}')
                size: int = os.stat(partpath).st_size
                headers['Range'] = f'bytes={size}-'
            else:
                info(f'downloading {filename} to {partpath}')
            for _ in range(RETRY_COUNT):
                rq: requests.Response = requests.get(url, stream=True, headers=headers)
                if rq.status_code == 404:
                    raise FileNotFoundError('host returned 404')
                elif rq.status_code not in [200, 206]:
                    time.sleep(5)
                    continue
                mode: str = 'ab' if rq.status_code == 206 else 'wb'
                with open(partpath, mode) as fp:
                    for data in rq.iter_content(chunk_size=1024):
                        fp.write(data)
                os.rename(partpath, path)
                success(f'downloading {filename} completed')
                break
        if decompress:
            decompress_file(path)
    except KeyboardInterrupt:
        return
    except Exception as ex:
        error(f'Error while downloading {filename}: {ex}')",os.path.isfile(path),117,os.path.exists(partpath),False,38.260294162784454,N/A
"def check_dir(dir_name: str) -> None:
    try:
<mask>:
            info(f'creating directory {dir_name}')
            os.mkdir(dir_name)
    except Exception as ex:
        error(f'unable to create directory: {str(ex)}')
        exit(-1)",os.path.isdir(dir_name) is False,21,not os.path.exists(dir_name),False,53.66551979187755,N/A
"def fetch_func(parser: argparse.ArgumentParser) -> None:
    global REPOSITORY
<mask>:
        error('no wordlist specified')
        return
    if parser.workers > 25:
        warning('Number of workers is too big, you might get banned.')
    executer: ThreadPoolExecutor = ThreadPoolExecutor(parser.workers)
    check_dir(parser.basedir)
    for group in ['usernames', 'passwords', 'discovery', 'fuzzing', 'misc']:
        check_dir(f'{parser.basedir}/{group}')
    wordlists: list = []
    if parser.wordlist is not None:
        wordlists = [wordlist for wordlist in parser.wordlist]
    if parser.fetch_term is not None:
        for wordlist in REPOSITORY:
            if parser.fetch_term in wordlist:
                wordlists.append(wordlist)
    if parser.group is not None:
        for wordlist in REPOSITORY:
            if REPOSITORY[wordlist]['group'] in parser.group:
                wordlists.append(wordlist)
    for wordlist in wordlists:
        if wordlist not in REPOSITORY:
            error(f'wordlist not found: {wordlist}')
            continue
        group: str = REPOSITORY[wordlist]['group']
        filename: str = REPOSITORY[wordlist]['url'].split('/')[-1]
        path: str = f'{parser.basedir}/{group}/{filename}'
        executer.submit(fetch_file, REPOSITORY[wordlist]['url'], path, parser.useragent, parser.decompress)
    executer.shutdown(wait=True)",parser.wordlist is None and parser.group is None and (parser.fetch_term is None),116,not parser.wordlist,False,0.8481564227322494,N/A
"def rgb_to_lab(r, g, b):
    """"""Convert RGB colours to LAB colours
       thank you Roman Nazarkin, http://stackoverflow.com/a/16020102/1418014""""""
    inputColor = [r, g, b]
    num = 0
    RGB = [0, 0, 0]
    for value in inputColor:
        value = float(value) / 255
<mask>:
            value = ((value + 0.055) / 1.055) ** 2.4
        else:
            value = value / 12.92
        RGB[num] = value * 100
        num = num + 1
    XYZ = [0, 0, 0]
    X = RGB[0] * 0.4124 + RGB[1] * 0.3576 + RGB[2] * 0.1805
    Y = RGB[0] * 0.2126 + RGB[1] * 0.7152 + RGB[2] * 0.0722
    Z = RGB[0] * 0.0193 + RGB[1] * 0.1192 + RGB[2] * 0.9505
    XYZ[0] = round(X, 4)
    XYZ[1] = round(Y, 4)
    XYZ[2] = round(Z, 4)
    XYZ[0] = float(XYZ[0]) / 95.047
    XYZ[1] = float(XYZ[1]) / 100.0
    XYZ[2] = float(XYZ[2]) / 108.883
    num = 0
    for value in XYZ:
        if value > 0.008856:
            value = value ** 0.3333333333333333
        else:
            value = 7.787 * value + 16 / 116
        XYZ[num] = value
        num = num + 1
    Lab = [0, 0, 0]
    L = 116 * XYZ[1] - 16
    a = 500 * (XYZ[0] - XYZ[1])
    b = 200 * (XYZ[1] - XYZ[2])
    Lab[0] = round(L, 4)
    Lab[1] = round(a, 4)
    Lab[2] = round(b, 4)
    return Lab",value > 0.04045,208,value > 0.055,False,55.03212081491043,N/A
"def deltaE(labA, labB):
    """"""deltaE is the standard way to compare two colours
    for how visibly alike they are""""""
    deltaL = labA[0] - labB[0]
    deltaA = labA[1] - labB[1]
    deltaB = labA[2] - labB[2]
    c1 = math.sqrt(labA[1] * labA[1] + labA[2] * labA[2])
    c2 = math.sqrt(labB[1] * labB[1] + labB[2] * labB[2])
    deltaC = c1 - c2
    deltaH = deltaA * deltaA + deltaB * deltaB - deltaC * deltaC
<mask>:
        deltaH = 0
    else:
        deltaH = math.sqrt(deltaH)
    sc = 1.0 + 0.045 * c1
    sh = 1.0 + 0.015 * c1
    deltaLKlsl = deltaL / 1.0
    deltaCkcsc = deltaC / sc
    deltaHkhsh = deltaH / sh
    i = deltaLKlsl * deltaLKlsl + deltaCkcsc * deltaCkcsc + deltaHkhsh * deltaHkhsh
    if i < 0:
        return 0
    else:
        return math.sqrt(i)",deltaH < 0,128,deltaH < 0,True,100.00000000000004,N/A
"def handle_commandline(self, app, cmdline):
<mask>:
        if '--about' in cmdline.get_arguments():
            self.show_about_dialog()
        if '--pick' in cmdline.get_arguments():
            GLib.idle_add(self.grab, self.btngrab)
        return 0
    if '--pick' in cmdline.get_arguments():
        self.start_everything_first_time(self.pick_after_window_mapped)
    else:
        self.start_everything_first_time()
    if '--about' in cmdline.get_arguments():
        self.show_about_dialog()
    return 0","hasattr(self, 'w')",32,not self.is_window_enabled(),False,5.522397783539471,N/A
"def window_configure(self, window, ev):
<mask>:
        return False
    if self.resize_timeout:
        GLib.source_remove(self.resize_timeout)
    self.resize_timeout = GLib.timeout_add_seconds(1, self.save_window_metrics_after_timeout, {'x': ev.x, 'y': ev.y, 'w': ev.width, 'h': ev.height})",not self.window_metrics_restored,22,not window,False,3.520477365831487,N/A
"def format_metrics(metric_name, label_keys, value_dict):
    metrics = {}
    for label_values, value in value_dict.items():
<mask>:
            labels = '{'
            labels += ','.join([format_label(label_keys[i], label_values[i]) for i in range(len(label_keys))])
            labels += '}'
        else:
            labels = ''
        metrics[metric_name + labels] = value
    return metrics",len(label_keys) > 0,38,len(label_keys) > 0,True,100.00000000000004,N/A
"def format_labels(label_dict):
    """"""
    Formats metric label dictionaries.

    Takes metric labels as a dictionary of label key -> label value.

    Label values can be list of strings. These will be joined together with
    underscores.

    Disallowed characters in label keys and values will be replaced with
    underscores.
    """"""
    formatted_label_dict = OrderedDict()
    for label_key, label_value in label_dict.items():
        formatted_label_key = format_label_key(label_key)
<mask>:
            formatted_label_value = format_label_value(label_value)
        else:
            formatted_label_value = format_label_value(*label_value)
        formatted_label_dict[formatted_label_key] = formatted_label_value
    return formatted_label_dict","isinstance(label_value, str)",70,"isinstance(label_value, list)",False,70.71067811865478,N/A
"def group_metrics(metrics):
    """"""
    Groups metrics with the same name but different label values.

    Takes metrics as a list of tuples containing:
    * metric name,
    * metric documentation,
    * dict of label key -> label value,
    * metric value.

    The metrics are grouped by metric name. All metrics with the same metric
    name must have the same set of label keys.

    A dict keyed by metric name is returned. Each metric name maps to a tuple
    containing:
    * metric documentation
    * label keys tuple,
    * dict of label values tuple -> metric value.
    """"""
    metric_dict = {}
    for metric_name, metric_doc, label_dict, value in metrics:
        curr_label_keys = tuple(label_dict.keys())
<mask>:
            label_keys = metric_dict[metric_name][1]
            assert set(curr_label_keys) == set(label_keys), 'Not all values for metric {} have the same keys. {} vs. {}.'.format(metric_name, curr_label_keys, label_keys)
        else:
            label_keys = curr_label_keys
            metric_dict[metric_name] = (metric_doc, label_keys, {})
        label_values = tuple([label_dict[k] for k in label_keys])
        metric_dict[metric_name][2][label_values] = value
    return metric_dict",metric_name in metric_dict,150,len(curr_label_keys) > 1,False,4.990049701936832,N/A
"def gauge_generator(metric_dict):
    """"""
    Generates GaugeMetricFamily instances for a list of metrics.

    Takes metrics as a dict keyed by metric name. Each metric name maps to a
    tuple containing:
    * metric documentation
    * label keys tuple,
    * dict of label values tuple -> metric value.

    Yields a GaugeMetricFamily instance for each unique metric name, containing
    children for the various label combinations. Suitable for use in a collect()
    method of a Prometheus collector.
    """"""
    for metric_name, (metric_doc, label_keys, value_dict) in metric_dict.items():
<mask>:
            gauge = GaugeMetricFamily(metric_name, metric_doc, labels=label_keys)
            for label_values in sorted(value_dict.keys()):
                value = value_dict[label_values]
                gauge.add_metric(label_values, value)
        else:
            gauge = GaugeMetricFamily(metric_name, metric_doc, value=list(value_dict.values())[0])
        yield gauge",label_keys,102,label_keys,True,100.00000000000004,N/A
"def count_object_fields(object_mappings, counts=None):
<mask>:
        counts = {}
    else:
        counts = counts.copy()
    for field, mapping in object_mappings['properties'].items():
        if 'properties' in mapping:
            field_type = 'object'
            if field_type in counts:
                counts[field_type] += 1
            else:
                counts[field_type] = 1
            counts = count_object_fields(mapping, counts=counts)
        else:
            field_type = mapping['type']
            if field_type in counts:
                counts[field_type] += 1
            else:
                counts[field_type] = 1
            if 'fields' in mapping:
                for mfield, mfield_mapping in mapping['fields'].items():
                    mfield_type = mfield_mapping['type']
                    if mfield_type in counts:
                        counts[mfield_type] += 1
                    else:
                        counts[mfield_type] = 1
    return counts",counts is None,78,counts is None,True,100.00000000000004,N/A
"def parse_index(index, mappings, metric=None):
<mask>:
        metric = []
    metric = metric + ['field', 'count']
    labels = OrderedDict([('index', index)])
    if 'properties' in mappings:
        counts = count_object_fields(mappings)
    elif any((isinstance(value, dict) and 'properties' in value for value in mappings.values())):
        counts = {}
        for mapping_type, type_mappings in mappings.items():
            if mapping_type == '_default_':
                continue
            if 'properties' not in type_mappings:
                continue
            counts = count_object_fields(type_mappings, counts=counts)
    else:
        counts = {}
    metrics = []
    for field_type, count in counts.items():
        metrics.append((metric, '', merge_dicts_ordered(labels, field_type=field_type), count))
    return metrics",metric is None,78,metric is None,True,100.00000000000004,N/A
"def parse_response(response, metric=None):
<mask>:
        metric = []
    metrics = []
    for index, data in response.items():
        metrics.extend(parse_index(index, data['mappings'], metric=metric))
    return [(format_metric_name(*metric_name), metric_doc, format_labels(label_dict), value) for metric_name, metric_doc, label_dict, value in metrics]",metric is None,30,metric is None,True,100.00000000000004,N/A
"def parse_block(block, metric=None, labels=None):
<mask>:
        metric = []
    if labels is None:
        labels = OrderedDict()
    metrics = []
    status = block['status']
    if status == 'green':
        status_int = 0
    elif status == 'yellow':
        status_int = 1
    elif status == 'red':
        status_int = 2
    metrics.append((metric + ['status'], '', labels, status_int))
    for colour in ['green', 'yellow', 'red']:
        metrics.append((metric + ['status', colour], '', labels, 1 if status == colour else 0))
    for key, value in block.items():
        if isinstance(value, bool):
            metrics.append((metric + [key], '', labels, int(value)))
        elif isinstance(value, (int, float)):
            metrics.append((metric + [key], '', labels, value))
        elif isinstance(value, dict):
            if key in singular_forms:
                singular_key = singular_forms[key]
            else:
                singular_key = key
            for n_key, n_value in value.items():
                metrics.extend(parse_block(n_value, metric=metric + [key], labels=merge_dicts_ordered(labels, {singular_key: [n_key]})))
    return metrics",metric is None,119,metric is None,True,100.00000000000004,N/A
"def parse_response(response, metric=None):
<mask>:
        metric = []
    metrics = []
    response = response.copy()
    if not response['timed_out']:
        del response['timed_out']
        metrics.extend(parse_block(response, metric=metric))
    return [(format_metric_name(*metric_name), metric_doc, format_labels(label_dict), value) for metric_name, metric_doc, label_dict, value in metrics]",metric is None,32,not metric,False,30.326532985631665,N/A
"def run_query(es_client, query_name, indices, query, timeout, on_error, on_missing):
    try:
        response = es_client.search(index=indices, body=query, request_timeout=timeout)
        metrics = parse_response(response, [query_name])
        metric_dict = group_metrics(metrics)
    except Exception:
        log.exception('Error while querying indices %(indices)s, query %(query)s.', {'indices': indices, 'query': query})
<mask>:
            old_metric_dict = METRICS_BY_QUERY[query_name]
            if on_error == 'preserve':
                metric_dict = old_metric_dict
            elif on_error == 'drop':
                metric_dict = {}
            elif on_error == 'zero':
                metric_dict = merge_metric_dicts(old_metric_dict, {}, zero_missing=True)
            METRICS_BY_QUERY[query_name] = metric_dict
    else:
        if query_name in METRICS_BY_QUERY:
            old_metric_dict = METRICS_BY_QUERY[query_name]
            if on_missing == 'preserve':
                metric_dict = merge_metric_dicts(old_metric_dict, metric_dict, zero_missing=False)
            elif on_missing == 'drop':
                pass
            elif on_missing == 'zero':
                metric_dict = merge_metric_dicts(old_metric_dict, metric_dict, zero_missing=True)
        METRICS_BY_QUERY[query_name] = metric_dict",query_name in METRICS_BY_QUERY,98,query_name in METRICS_BY_QUERY,True,100.00000000000004,N/A
"def convert_one(self, value, param, ctx):
<mask>:
        return value
    normed_value = value
    normed_choices = self.choices
    if ctx is not None and ctx.token_normalize_func is not None:
        normed_value = ctx.token_normalize_func(value)
        normed_choices = [ctx.token_normalize_func(choice) for choice in self.choices]
    if not self.case_sensitive:
        normed_value = normed_value.lower()
        normed_choices = [choice.lower() for choice in normed_choices]
    if normed_value in normed_choices:
        return normed_value
    return None",value in self.choices,55,value in self.choices,True,100.00000000000004,N/A
"def convert(self, value, param, ctx):
    values = value.split(',')
    valid_choices = []
    invalid_values = []
    for value in values:
        choice = self.convert_one(value, param, ctx)
<mask>:
            invalid_values.append(value)
        else:
            valid_choices.append(choice)
    if invalid_values:
        msg = 'invalid choice(s): %s (choose from %s)' % (', '.join(invalid_values), ', '.join(self.choices))
        self.fail(msg, param, ctx)
    return valid_choices",choice is None,47,choice is None,True,100.00000000000004,N/A
"def indices_stats_indices_parser(ctx, param, value):
<mask>:
        return None
    if value in ('*', '_all', ''):
        return value
    else:
        return value.split(',')",value is None,18,value is None,True,100.00000000000004,N/A
"def indices_stats_fields_parser(ctx, param, value):
<mask>:
        return None
    if value == '*':
        return value
    else:
        return value.split(',')",value is None,16,value is None,True,100.00000000000004,N/A
"def parse_block(block, metric=None, labels=None):
<mask>:
        metric = []
    if labels is None:
        labels = OrderedDict()
    metrics = []
    for key, value in block.items():
        if key not in excluded_keys:
            if isinstance(value, bool):
                metrics.append((metric + [key], '', labels, int(value)))
            elif isinstance(value, (int, float)):
                metrics.append((metric + [key], '', labels, value))
            elif isinstance(value, dict):
                if key in bucket_dict_keys:
                    if key in singular_forms:
                        singular_key = singular_forms[key]
                    else:
                        singular_key = key
                    for n_key, n_value in value.items():
                        metrics.extend(parse_block(n_value, metric=metric + [key], labels=merge_dicts_ordered(labels, {singular_key: [n_key]})))
                else:
                    metrics.extend(parse_block(value, metric=metric + [key], labels=labels))
            elif isinstance(value, list) and key in bucket_list_keys:
                bucket_name_key = bucket_list_keys[key]
                for n_value in value:
                    bucket_name = n_value[bucket_name_key]
                    metrics.extend(parse_block(n_value, metric=metric + [key], labels=merge_dicts_ordered(labels, {bucket_name_key: [bucket_name]})))
    return metrics",metric is None,109,metric is None,True,100.00000000000004,N/A
"def parse_response(response, parse_indices=False, metric=None):
<mask>:
        metric = []
    metrics = []
    if '_shards' not in response or not response['_shards']['failed']:
        if parse_indices:
            for key, value in response['indices'].items():
                metrics.extend(parse_block(value, metric=metric, labels=OrderedDict({'index': [key]})))
        else:
            metrics.extend(parse_block(response['_all'], metric=metric, labels=OrderedDict({'index': ['_all']})))
    return [(format_metric_name(*metric_name), metric_doc, format_labels(label_dict), value) for metric_name, metric_doc, label_dict, value in metrics]",metric is None,47,metric is None,True,100.00000000000004,N/A
"def add_label(label_key, label_value, labels):
    labels = labels.copy()
<mask>:
        labels[label_key] = labels[label_key] + [label_value]
    else:
        labels[label_key] = [label_value]
    return labels",label_key in labels.keys(),19,label_key in labels,False,44.932896411722176,N/A
"def parse_buckets(agg_key, buckets, metric=None, labels=None):
<mask>:
        metric = []
    if labels is None:
        labels = OrderedDict()
    result = []
    for index, bucket in enumerate(buckets):
        labels_nest = labels.copy()
        if 'key' in bucket.keys():
            if isinstance(bucket['key'], dict):
                for comp_key, comp_value in bucket['key'].items():
                    label_key = '_'.join([agg_key, comp_key])
                    labels_nest = add_label(label_key, str(comp_value), labels_nest)
            else:
                labels_nest = add_label(agg_key, str(bucket['key']), labels_nest)
            del bucket['key']
        else:
            bucket_key = 'filter_' + str(index)
            labels_nest = add_label(agg_key, bucket_key, labels_nest)
        result.extend(parse_agg(agg_key, bucket, metric=metric, labels=labels_nest))
    return result",metric is None,73,metric is None,True,100.00000000000004,N/A
"def parse_buckets_fixed(agg_key, buckets, metric=None, labels=None):
<mask>:
        metric = []
    if labels is None:
        labels = OrderedDict()
    result = []
    for bucket_key, bucket in buckets.items():
        labels_nest = labels.copy()
        labels_next = add_label(agg_key, bucket_key, labels_nest)
        result.extend(parse_agg(agg_key, bucket, metric=metric, labels=labels_next))
    return result",metric is None,38,metric is None,True,100.00000000000004,N/A
"def parse_agg(agg_key, agg, metric=None, labels=None):
<mask>:
        metric = []
    if labels is None:
        labels = OrderedDict()
    result = []
    for key, value in agg.items():
        if key == 'buckets' and isinstance(value, list):
            result.extend(parse_buckets(agg_key, value, metric=metric, labels=labels))
        elif key == 'buckets' and isinstance(value, dict):
            result.extend(parse_buckets_fixed(agg_key, value, metric=metric, labels=labels))
        elif key == 'after_key' and 'buckets' in agg:
            continue
        elif isinstance(value, dict):
            result.extend(parse_agg(key, value, metric=metric + [key], labels=labels))
        elif isinstance(value, (int, float)):
            result.append((metric + [key], '', labels, value))
    return result",metric is None,76,metric is None,True,100.00000000000004,N/A
"def parse_response(response, metric=None):
<mask>:
        metric = []
    metrics = []
    if not response['timed_out']:
        total = response['hits']['total']
        if isinstance(total, dict):
            total = total['value']
        metrics.append((metric + ['hits'], '', {}, total))
        metrics.append((metric + ['took', 'milliseconds'], '', {}, response['took']))
        if 'aggregations' in response.keys():
            for key, value in response['aggregations'].items():
                metrics.extend(parse_agg(key, value, metric=metric + [key]))
    return [(format_metric_name(*metric_name), metric_doc, format_labels(label_dict), value) for metric_name, metric_doc, label_dict, value in metrics]",metric is None,61,metric is None,True,100.00000000000004,N/A
"def log_exceptions(exit_on_exception=False):
    """"""
    Logs any exceptions raised.

    By default, exceptions are then re-raised. If set to exit on exception,
    sys.exit(1) is called instead.
    """"""

    def decorator(func):

        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except Exception:
<mask>:
                    log.exception('Unrecoverable exception encountered. Exiting.')
                    sys.exit(1)
                else:
                    log.exception('Exception encountered.')
                    raise
        return wrapper
    return decorator",exit_on_exception,50,exit_on_exception,True,100.00000000000004,N/A
"def schedule_job(scheduler, executor, interval, func, *args, **kwargs):
    """"""
    Schedule a function to be run on a fixed interval.

    Works with schedulers from the stdlib sched module.
    """"""

    def scheduled_run(scheduled_time, *args, **kwargs):

        def run_func(func, *args, **kwargs):
            try:
                func(*args, **kwargs)
            except Exception:
                log.exception('Error while running scheduled job.')
<mask>:
            executor.submit(run_func, func, *args, **kwargs)
        else:
            run_func(func, *args, **kwargs)
        current_time = time.monotonic()
        next_scheduled_time = scheduled_time + interval
        while next_scheduled_time < current_time:
            next_scheduled_time += interval
        scheduler.enterabs(time=next_scheduled_time, priority=1, action=scheduled_run, argument=(next_scheduled_time, *args), kwargs=kwargs)
    next_scheduled_time = time.monotonic()
    scheduler.enterabs(time=next_scheduled_time, priority=1, action=scheduled_run, argument=(next_scheduled_time, *args), kwargs=kwargs)",executor is not None,84,executor,False,4.9787068367863965,N/A
"def parse_index(index, aliases, metric=None):
<mask>:
        metric = []
    metric = metric + ['alias']
    labels = OrderedDict([('index', index)])
    metrics = []
    for alias in aliases.keys():
        metrics.append((metric, '', merge_dicts_ordered(labels, alias=alias), 1))
    return metrics",metric is None,31,metric is None,True,100.00000000000004,N/A
"def parse_response(response, metric=None):
<mask>:
        metric = []
    metrics = []
    for index, data in response.items():
        metrics.extend(parse_index(index, data['aliases'], metric=metric))
    return [(format_metric_name(*metric_name), metric_doc, format_labels(label_dict), value) for metric_name, metric_doc, label_dict, value in metrics]",metric is None,30,metric is None,True,100.00000000000004,N/A
"def parse_block(block, metric=None, labels=None):
<mask>:
        metric = []
    if labels is None:
        labels = OrderedDict()
    metrics = []
    for key, value in block.items():
        if key not in excluded_keys:
            if isinstance(value, bool):
                metrics.append((metric + [key], '', labels, int(value)))
            elif isinstance(value, (int, float)):
                metrics.append((metric + [key], '', labels, value))
            elif isinstance(value, dict):
                if key in bucket_dict_keys:
                    if key in singular_forms:
                        singular_key = singular_forms[key]
                    else:
                        singular_key = key
                    for n_key, n_value in value.items():
                        metrics.extend(parse_block(n_value, metric=metric + [key], labels=merge_dicts_ordered(labels, {singular_key: [n_key]})))
                else:
                    metrics.extend(parse_block(value, metric=metric + [key], labels=labels))
            elif isinstance(value, list) and key in bucket_list_keys:
                bucket_name_key = bucket_list_keys[key]
                for n, n_value in enumerate(value):
                    if bucket_name_key in n_value:
                        bucket_name = n_value[bucket_name_key]
                    else:
                        bucket_name = str(n)
                    metrics.extend(parse_block(n_value, metric=metric + [key], labels=merge_dicts_ordered(labels, {bucket_name_key: [bucket_name]})))
    return metrics",metric is None,118,metric is None,True,100.00000000000004,N/A
"def parse_node(node, metric=None, labels=None):
<mask>:
        metric = []
    if labels is None:
        labels = OrderedDict()
    labels = merge_dicts_ordered(labels, node_name=[node['name']])
    return parse_block(node, metric=metric, labels=labels)",metric is None,23,metric is None,True,100.00000000000004,N/A
"def parse_response(response, metric=None):
<mask>:
        metric = []
    metrics = []
    if '_nodes' not in response or not response['_nodes']['failed']:
        for key, value in response['nodes'].items():
            metrics.extend(parse_node(value, metric=metric, labels=OrderedDict({'node_id': [key]})))
    return [(format_metric_name(*metric_name), metric_doc, format_labels(label_dict), value) for metric_name, metric_doc, label_dict, value in metrics]",metric is None,39,metric is None,True,100.00000000000004,N/A
"def iou(gt, pred):
    gt[gt > 0] = 1.0
    pred[pred > 0] = 1.0
    intersection = gt * pred
    union = gt + pred
    union[union > 0] = 1.0
    intersection = np.sum(intersection)
    union = np.sum(union)
<mask>:
        union = 1e-09
    return intersection / union",union == 0,42,union > 1e-09,False,19.716118825581447,N/A
"def compute_ious(gt, predictions):
    gt_ = get_segmentations(gt)
    predictions_ = get_segmentations(predictions)
<mask>:
        return np.zeros((1, 1))
    iscrowd = [0 for _ in predictions_]
    ious = cocomask.iou(gt_, predictions_, iscrowd)
    return ious",len(predictions_) == 0,27,len(gt_) == 0 or len(predictions_) == 0,False,41.41238765665519,N/A
"def fit(self, datagen, validation_datagen=None, meta_valid=None):
    self._initialize_model_weights()
    self.model = nn.DataParallel(self.model)
<mask>:
        self.model = self.model.cuda()
    self.callbacks.set_params(self, validation_datagen=validation_datagen, meta_valid=meta_valid)
    self.callbacks.on_train_begin()
    batch_gen, steps = datagen
    for epoch_id in range(self.training_config['epochs']):
        self.callbacks.on_epoch_begin()
        for batch_id, data in enumerate(batch_gen):
            self.callbacks.on_batch_begin()
            metrics = self._fit_loop(data)
            self.callbacks.on_batch_end(metrics=metrics)
            if batch_id == steps:
                break
        self.callbacks.on_epoch_end()
        if self.callbacks.training_break():
            break
    self.callbacks.on_train_end()
    return self",torch.cuda.is_available(),48,self.training_config['cuda'],False,5.795599612995366,N/A
"def transform(self, datagen, validation_datagen=None, *args, **kwargs):
    outputs = self._transform(datagen, validation_datagen)
    for name, prediction in outputs.items():
<mask>:
            outputs[name] = [softmax(single_prediction, axis=0) for single_prediction in prediction]
        elif self.activation_func == 'sigmoid':
            outputs[name] = [sigmoid(np.squeeze(mask)) for mask in prediction]
        else:
            raise Exception('Only softmax and sigmoid activations are allowed')
    return outputs",self.activation_func == 'softmax',46,self.activation_func == 'softmax',True,100.00000000000004,N/A
"def _transform(self, datagen, validation_datagen=None):
    self.model.eval()
    batch_gen, steps = datagen
    outputs = {}
    for batch_id, data in enumerate(batch_gen):
<mask>:
            X = data[0]
        else:
            X = data
        if torch.cuda.is_available():
            X = Variable(X, volatile=True).cuda()
        else:
            X = Variable(X, volatile=True)
        outputs_batch = self.model(X)
        if len(self.output_names) == 1:
            outputs.setdefault(self.output_names[0], []).append(outputs_batch.data.cpu().numpy())
        else:
            for name, output in zip(self.output_names, outputs_batch):
                output_ = output.data.cpu().numpy()
                outputs.setdefault(name, []).append(output_)
        if batch_id == steps:
            break
    self.model.train()
    outputs = {'{}_prediction'.format(name): get_list_of_image_predictions(outputs_) for name, outputs_ in outputs.items()}
    return outputs","isinstance(data, list)",74,len(data) == 1,False,14.535768424205482,N/A
"def set_model(self):
    encoder = self.architecture_config['model_params']['encoder']
<mask>:
        self.model = UNet(**self.architecture_config['model_params'])
    else:
        config = PRETRAINED_NETWORKS[encoder]
        self.model = config['model'](num_classes=self.architecture_config['model_params']['out_channels'], **config['model_config'])
        self._initialize_model_weights = lambda: None",encoder == 'from_scratch',21,encoder == 'unet',False,36.06452879987789,N/A
"def set_loss(self):
<mask>:
        loss_function = partial(mixed_dice_cross_entropy_loss, dice_loss=multiclass_dice_loss, cross_entropy_loss=nn.CrossEntropyLoss(), dice_activation='softmax')
    elif self.activation_func == 'sigmoid':
        loss_function = partial(mixed_dice_bce_loss, dice_loss=multiclass_dice_loss, bce_loss=nn.BCEWithLogitsLoss(), dice_activation='sigmoid')
    else:
        raise Exception('Only softmax and sigmoid activations are allowed')
    self.loss_function = [('mask', loss_function, 1.0)]",self.activation_func == 'softmax',33,self.activation_func == 'softmax',True,100.00000000000004,N/A
"def _perspective_transform_augment_images(self, images, random_state, parents, hooks):
    result = images
<mask>:
        result = list(result)
    matrices, max_heights, max_widths = self._create_matrices([image.shape for image in images], random_state)
    for i, (M, max_height, max_width) in enumerate(zip(matrices, max_heights, max_widths)):
        warped = cv2.warpPerspective(images[i], M, (max_width, max_height))
        if warped.ndim == 2 and images[i].ndim == 3:
            warped = np.expand_dims(warped, 2)
        if self.keep_size:
            h, w = images[i].shape[0:2]
            warped = ia.imresize_single_image(warped, (h, w))
        result[i] = warped
    return result",not self.keep_size,66,"not isinstance(result, list)",False,6.567274736060395,N/A
"def _pad(self, img):
    img_ = img.copy()
<mask>:
        img_ = np.squeeze(img_, axis=-1)
    h_pad, w_pad = self.pad
    img_ = cv2.copyMakeBorder(img_.copy(), h_pad, h_pad, w_pad, w_pad, PadFixed.PAD_FUNCTION[self.pad_method])
    if self._is_expanded_grey_format(img):
        img_ = np.expand_dims(img_, axis=-1)
    return img_",self._is_expanded_grey_format(img),31,self._is_expanded_grey_format(img),True,100.00000000000004,N/A
"def __init__(self, px=None, name=None, deterministic=False, random_state=None):
    super(RandomCropFixedSize, self).__init__(name=name, deterministic=deterministic, random_state=random_state)
    self.px = px
<mask>:
        self.px_h, self.px_w = self.px
    elif isinstance(self.px, int):
        self.px_h = self.px
        self.px_w = self.px
    else:
        raise NotImplementedError","isinstance(self.px, tuple)",30,"isinstance(self.px, tuple)",True,100.00000000000004,N/A
"def _random_crop(self, seed, image):
    height, width = image.shape[:2]
    np.random.seed(seed)
<mask>:
        crop_top = np.random.randint(height - self.px_h)
    elif height == self.px_h:
        crop_top = 0
    else:
        raise ValueError('To big crop height')
    crop_bottom = crop_top + self.px_h
    np.random.seed(seed + 1)
    if width > self.px_w:
        crop_left = np.random.randint(width - self.px_w)
    elif width == self.px_w:
        crop_left = 0
    else:
        raise ValueError('To big crop width')
    crop_right = crop_left + self.px_w
    if len(image.shape) == 2:
        image_cropped = image[crop_top:crop_bottom, crop_left:crop_right]
    else:
        image_cropped = image[crop_top:crop_bottom, crop_left:crop_right, :]
    return image_cropped",height > self.px_h,79,height > self.px_h,True,100.00000000000004,N/A
"def transform(self, meta, train_mode):
    X_ = meta[self.x_columns].values
    X = self.load_images(X_, filetype='png', grayscale=False)
<mask>:
        y_ = meta[self.y_columns].values
        y = self.load_images(y_, filetype=self.target_format, grayscale=True)
    else:
        y = None
    return {'X': X, 'y': y}",train_mode,30,train_mode,True,100.00000000000004,N/A
"def load_images(self, filepaths, filetype, grayscale=False):
    X = []
    for i in range(filepaths.shape[1]):
        column = filepaths[:, i]
        X.append([])
        for filepath in tqdm(column):
<mask>:
                data = self.load_image(filepath, grayscale=grayscale)
            elif filetype == 'json':
                data = self.read_json(filepath)
            else:
                raise Exception('files must be png or json')
            X[i].append(data)
    return X",filetype == 'png',44,filetype == 'png',True,100.00000000000004,N/A
"def load_image(self, img_filepath, grayscale):
    image = Image.open(img_filepath, 'r')
<mask>:
        image = image.convert('RGB')
    else:
        image = image.convert('L')
    return image",not grayscale,18,grayscale,False,36.78794411714425,N/A
"def __init__(self, X, y, train_mode, image_transform, image_augment_with_target, mask_transform, image_augment, image_source='memory'):
    super().__init__()
    self.X = X
<mask>:
        self.y = y
    else:
        self.y = None
    self.train_mode = train_mode
    self.image_transform = image_transform
    self.mask_transform = mask_transform
    self.image_augment = image_augment if image_augment is not None else ImgAug(iaa.Noop())
    self.image_augment_with_target = image_augment_with_target if image_augment_with_target is not None else ImgAug(iaa.Noop())
    self.image_source = image_source",y is not None,54,y is not None,True,100.00000000000004,N/A
"def __len__(self):
<mask>:
        return len(self.X[0])
    elif self.image_source == 'disk':
        return self.X.shape[0]",self.image_source == 'memory',11,self.image_source == 'image',False,84.08964152537145,N/A
"def unet11(pretrained=False, **kwargs):
    """"""
    pretrained:
            False - no pre-trained network is used
            True  - encoder is pre-trained with VGG11
            carvana - all weights are pre-trained on
                Kaggle: Carvana dataset https://www.kaggle.com/c/carvana-image-masking-challenge
    """"""
    model = UNet11(pretrained=pretrained, **kwargs)
<mask>:
        state = torch.load('TernausNet.pt')
        model.load_state_dict(state['model'])
    return model",pretrained == 'carvana',42,pretrained,False,4.9787068367863965,N/A
"def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):
    super(DecoderBlockV2, self).__init__()
    self.in_channels = in_channels
<mask>:
        '\n                Paramaters for Deconvolution were chosen to avoid artifacts, following\n                link https://distill.pub/2016/deconv-checkerboard/\n            '
        self.block = nn.Sequential(ConvRelu(in_channels, middle_channels), nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2, padding=1), nn.ReLU(inplace=True))
    else:
        self.block = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear'), ConvRelu(in_channels, middle_channels), ConvRelu(middle_channels, out_channels))",is_deconv,44,is_deconv,True,100.00000000000004,N/A
"def __init__(self, encoder_depth, num_classes, num_filters=32, dropout_2d=0.2, pretrained=False, is_deconv=False):
    super().__init__()
    self.num_classes = num_classes
    self.dropout_2d = dropout_2d
<mask>:
        self.encoder = torchvision.models.resnet34(pretrained=pretrained)
        bottom_channel_nr = 512
    elif encoder_depth == 101:
        self.encoder = torchvision.models.resnet101(pretrained=pretrained)
        bottom_channel_nr = 2048
    elif encoder_depth == 152:
        self.encoder = torchvision.models.resnet152(pretrained=pretrained)
        bottom_channel_nr = 2048
    else:
        raise NotImplementedError('only 34, 101, 152 version of Resnet are implemented')
    self.pool = nn.MaxPool2d(2, 2)
    self.relu = nn.ReLU(inplace=True)
    self.conv1 = nn.Sequential(self.encoder.conv1, self.encoder.bn1, self.encoder.relu, self.pool)
    self.conv2 = self.encoder.layer1
    self.conv3 = self.encoder.layer2
    self.conv4 = self.encoder.layer3
    self.conv5 = self.encoder.layer4
    self.center = DecoderBlockV2(bottom_channel_nr, num_filters * 8 * 2, num_filters * 8, is_deconv)
    self.dec5 = DecoderBlockV2(bottom_channel_nr + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)
    self.dec4 = DecoderBlockV2(bottom_channel_nr // 2 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)
    self.dec3 = DecoderBlockV2(bottom_channel_nr // 4 + num_filters * 8, num_filters * 4 * 2, num_filters * 2, is_deconv)
    self.dec2 = DecoderBlockV2(bottom_channel_nr // 8 + num_filters * 2, num_filters * 2 * 2, num_filters * 2 * 2, is_deconv)
    self.dec1 = DecoderBlockV2(num_filters * 2 * 2, num_filters * 2 * 2, num_filters, is_deconv)
    self.dec0 = ConvRelu(num_filters, num_filters)
    self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)",encoder_depth == 34,185,encoder_depth == 34,True,100.00000000000004,N/A
"def decompose(labeled):
    nr_true = labeled.max()
    masks = []
    for i in range(1, nr_true + 1):
        msk = labeled.copy()
        msk[msk != i] = 0.0
        msk[msk == i] = 255.0
        masks.append(msk)
<mask>:
        return [labeled]
    else:
        return masks",not masks,35,len(masks) == 0,False,6.567274736060395,N/A
"def create_submission(experiments_dir, meta, predictions, logger):
    image_ids, encodings = ([], [])
    output = []
    for image_id, prediction in zip(meta['ImageId'].values, predictions):
        for mask in decompose(prediction):
            rle_encoded = ' '.join((str(rle) for rle in run_length_encoding(mask > 128.0)))
<mask>:
                image_ids.append(image_id)
                encodings.append(rle_encoded)
                output.append([image_id, rle_encoded])
            else:
                logger.info('*** image_id {}'.format(image_id))
                logger.info('*** rle_encoded {} is empty'.format(rle_encoded))
    submission = pd.DataFrame(output, columns=['ImageId', 'EncodedPixels']).astype(str)
    submission = submission[submission['EncodedPixels'] != 'nan']
    submission_filepath = os.path.join(experiments_dir, 'submission.csv')
    submission.to_csv(submission_filepath, index=None, encoding='utf-8')
    logger.info('submission saved to {}'.format(submission_filepath))
    logger.info('submission head \n\n{}'.format(submission.head()))",len(rle_encoded) != 0,71,image_id not in image_ids and encodings != [],False,6.285596338261262,N/A
"def read_masks(masks_filepaths):
    masks = []
    for mask_dir in tqdm(masks_filepaths):
        mask = []
<mask>:
            mask_dir = mask_dir[0]
        for i, mask_filepath in enumerate(glob.glob('{}/*'.format(mask_dir))):
            blob = np.asarray(Image.open(mask_filepath))
            blob_binarized = (blob > 128.0).astype(np.uint8) * (i + 1)
            mask.append(blob_binarized)
        mask = np.sum(np.stack(mask, axis=0), axis=0).astype(np.uint8)
        masks.append(mask)
    return masks",len(mask_dir) == 1,42,len(mask_dir) > 0,False,60.042877124855906,N/A
"def run_length_encoding(x):
    bs = np.where(x.T.flatten())[0]
    rle = []
    prev = -2
    for b in bs:
<mask>:
            rle.extend((b + 1, 0))
        rle[-1] += 1
        prev = b
    if len(rle) != 0 and rle[-1] + rle[-2] == x.size:
        rle[-2] = rle[-2] - 1
    return rle",b > prev + 1,43,b != prev,False,14.794015674776452,N/A
"def read_params(ctx):
<mask>:
        neptune_config = read_yaml('neptune.yaml')
        params = neptune_config.parameters
    else:
        params = ctx.params
    return params",ctx.params.__class__.__name__ == 'OfflineContextParams',15,ctx.params is None,False,2.953380133732491,N/A
"def _send_image_channels(self):
    self.model.eval()
    pred_masks = self.get_prediction_masks()
    self.model.train()
    for name, pred_mask in pred_masks.items():
        for i, image_duplet in enumerate(pred_mask):
            h, w = image_duplet.shape[1:]
            image_glued = np.zeros((h, 2 * w + 10))
            image_glued[:, :w] = image_duplet[0, :, :]
            image_glued[:, w + 10:] = image_duplet[1, :, :]
            pill_image = Image.fromarray((image_glued * 255.0).astype(np.uint8))
            h_, w_ = image_glued.shape
            pill_image = pill_image.resize((int(self.image_resize * w_), int(self.image_resize * h_)), Image.ANTIALIAS)
            self.ctx.channel_send('{} {}'.format(self.model_name, name), neptune.Image(name='epoch{}_batch{}_idx{}'.format(self.epoch_id, self.batch_id, i), description='true and prediction masks', data=pill_image))
<mask>:
                break",i == self.image_nr,74,i == 0,False,21.874242445215206,N/A
"def get_prediction_masks(self):
    prediction_masks = {}
    batch_gen, steps = self.validation_datagen
    for batch_id, data in enumerate(batch_gen):
<mask>:
            raise ValueError('incorrect targets provided')
        X = data[0]
        targets_tensors = data[1:]
        if torch.cuda.is_available():
            X = Variable(X).cuda()
        else:
            X = Variable(X)
        outputs_batch = self.model(X)
        if len(outputs_batch) == len(self.output_names):
            for name, output, target in zip(self.output_names, outputs_batch, targets_tensors):
                prediction = sigmoid(np.squeeze(output.data.cpu().numpy(), axis=1))
                ground_truth = np.squeeze(target.cpu().numpy(), axis=1)
                prediction_masks[name] = np.stack([prediction, ground_truth], axis=1)
        else:
            for name, target in zip(self.output_names, targets_tensors):
                prediction = sigmoid(np.squeeze(outputs_batch.data.cpu().numpy(), axis=1))
                ground_truth = np.squeeze(target.cpu().numpy(), axis=1)
                prediction_masks[name] = np.stack([prediction, ground_truth], axis=1)
        break
    return prediction_masks",len(data) != len(self.output_names) + 1,85,len(data) != len(self.output_names),False,86.6877899750182,N/A
"def _transform(self):
    self.model.eval()
    batch_gen, steps = self.validation_datagen
    partial_batch_losses = []
    outputs = {}
    for batch_id, data in enumerate(batch_gen):
        X = data[0]
        targets_tensors = data[1:]
<mask>:
            X = Variable(X, volatile=True).cuda()
            targets_var = []
            for target_tensor in targets_tensors:
                targets_var.append(Variable(target_tensor, volatile=True).cuda())
        else:
            X = Variable(X, volatile=True)
            targets_var = []
            for target_tensor in targets_tensors:
                targets_var.append(Variable(target_tensor, volatile=True))
        outputs_batch = self.model(X)
        if len(self.output_names) == 1:
            for (name, loss_function_one, weight), target in zip(self.loss_function, targets_var):
                loss_sum = loss_function_one(outputs_batch, target) * weight
            outputs.setdefault(self.output_names[0], []).append(outputs_batch.data.cpu().numpy())
        else:
            batch_losses = []
            for (name, loss_function_one, weight), output, target in zip(self.loss_function, outputs_batch, targets_var):
                loss = loss_function_one(output, target) * weight
                batch_losses.append(loss)
                partial_batch_losses.setdefault(name, []).append(loss)
                output_ = output.data.cpu().numpy()
                outputs.setdefault(name, []).append(output_)
            loss_sum = sum(batch_losses)
        partial_batch_losses.append(loss_sum)
        if batch_id == steps:
            break
    self.model.train()
    average_losses = sum(partial_batch_losses) / steps
    outputs = {'{}_prediction'.format(name): get_list_of_image_predictions(outputs_) for name, outputs_ in outputs.items()}
    for name, prediction in outputs.items():
        if self.activation_func == 'softmax':
            outputs[name] = [softmax(single_prediction, axis=0) for single_prediction in prediction]
        elif self.activation_func == 'sigmoid':
            outputs[name] = [sigmoid(np.squeeze(mask)) for mask in prediction]
        else:
            raise Exception('Only softmax and sigmoid activations are allowed')
    return (outputs, average_losses)",torch.cuda.is_available(),167,self.use_cuda,False,6.316906128202129,N/A
"def on_epoch_end(self, *args, **kwargs):
<mask>:
        self.model.eval()
        val_loss = self.get_validation_loss()
        loss_sum = val_loss[self.metric_name]
        loss_sum = loss_sum.data.cpu().numpy()[0]
        self.model.train()
        if self.best_score is None:
            self.best_score = loss_sum
        if self.minimize and loss_sum < self.best_score or (not self.minimize and loss_sum > self.best_score) or self.epoch_id == 0:
            self.best_score = loss_sum
            save_model(self.model, self.filepath)
            logger.info('epoch {0} model saved to {1}'.format(self.epoch_id, self.filepath))
    self.epoch_id += 1",self.epoch_every and self.epoch_id % self.epoch_every == 0,55,self.training,False,0.19037687152469163,N/A
"def on_epoch_end(self, *args, **kwargs):
    self.model.eval()
    val_loss = self.get_validation_loss()
    loss_sum = val_loss[self.metric_name]
    loss_sum = loss_sum.data.cpu().numpy()[0]
    self.model.train()
<mask>:
        self.best_score = loss_sum
    if self.minimize and loss_sum < self.best_score or (not self.minimize and loss_sum > self.best_score):
        self.best_score = loss_sum
        self.epoch_since_best = 0
    else:
        self.epoch_since_best += 1
    if self.epoch_since_best > self.patience:
        self._training_break = True
    self.epoch_id += 1",not self.best_score,52,self.minimize and (not self.minimize and loss_sum < self.best_score),False,21.31456897111116,N/A
"def categorize_image(image, activation='softmax', threshold=0.5):
    """"""Maps probability map to categories. Each pixel is assigned with a category with highest probability.

    Args:
        image (numpy.ndarray): Probability map of shape (C x H x W).
        activation (string): Activation function, either softmax or sigmoid. Defaults to 'softmax'.
        threshold (float or list): Single threshold for sigmoid activation or list of per class thresholds.

    Returns:
        numpy.ndarray: Categorized image of shape (C x H x W).

    """"""
    categorized_image = []
<mask>:
        class_map = np.argmax(image, axis=0)
        for class_nr in range(image.shape[0]):
            categorized_image.append((class_map == class_nr).astype(np.uint8))
    if activation == 'sigmoid':
        if isinstance(threshold, float):
            threshold = [threshold] * image.shape[0]
        for thrs, class_instance in zip(threshold, image):
            categorized_image.append((class_instance > thrs).astype(np.uint8))
    return np.stack(categorized_image)",activation == 'softmax',107,activation == 'softmax',True,100.00000000000004,N/A
"def drop_artifacts(mask_after, mask_pre, min_coverage=0.5):
    connected, nr_connected = ndi.label(mask_after)
    mask = np.zeros_like(mask_after)
    for i in range(1, nr_connected + 1):
        conn_blob = np.where(connected == i, 1, 0)
        initial_space = np.where(connected == i, mask_pre, 0)
        blob_size = np.sum(conn_blob)
        initial_blob_size = np.sum(initial_space)
        coverage = float(initial_blob_size) / float(blob_size)
<mask>:
            mask = mask + conn_blob
        else:
            mask = mask + initial_space
    return mask",coverage > min_coverage,57,coverage < min_coverage,False,42.72870063962342,N/A
"def mean_blob_size(mask):
    labels, labels_nr = ndi.label(mask)
<mask>:
        mean_area = 1
        mean_radius = 1
    else:
        mean_area = int(itemfreq(labels)[1:, 1].mean())
        mean_radius = int(np.round(np.sqrt(mean_area / np.pi)))
    return (mean_area, mean_radius)",labels_nr < 2,26,labels_nr == 0,False,30.213753973567677,N/A
"def pad_mask(mask, pad):
<mask>:
        pad = 2
    h, w = mask.shape
    h_pad = h + 2 * pad
    w_pad = w + 2 * pad
    mask_padded = np.zeros((h_pad, w_pad))
    mask_padded[pad:pad + h, pad:pad + w] = mask
    mask_padded[pad - 1, :] = 1
    mask_padded[pad + h + 1, :] = 1
    mask_padded[:, pad - 1] = 1
    mask_padded[:, pad + w + 1] = 1
    return mask_padded",pad <= 1,67,pad == 0,False,18.99589214128981,N/A
"def crop_mask(mask, crop):
<mask>:
        crop = 2
    h, w = mask.shape
    mask_cropped = mask[crop:h - crop, crop:w - crop]
    return mask_cropped",crop <= 1,21,crop == 1,False,35.35533905932737,N/A
"def unet(config, train_mode):
<mask>:
        save_output = False
        load_saved_output = False
        preprocessing = preprocessing_train(config, model_name='unet')
    else:
        save_output = False
        load_saved_output = False
        preprocessing = preprocessing_inference(config)
    unet = Step(name='unet', transformer=PyTorchUNet(**config.model['unet']), input_data=['callback_input'], input_steps=[preprocessing], is_trainable=True, cache_dirpath=config.env.cache_dirpath, save_output=save_output, load_saved_output=load_saved_output)
    if train_mode:
        return unet
    mask_postprocessed = mask_postprocessing(unet, config, save_output=save_output)
    output = Step(name='output', transformer=Dummy(), input_steps=[mask_postprocessed], adapter={'y_pred': [(mask_postprocessed.name, 'nuclei_images')]}, cache_dirpath=config.env.cache_dirpath)
    return output",train_mode,54,config.model['unet'] == 'train',False,0.0,N/A
"def preprocessing_train(config, model_name='unet'):
<mask>:
        Loader = loaders.ImageSegmentationLoaderCropPad
    elif config.execution.loader_mode == 'resize':
        Loader = loaders.ImageSegmentationLoaderResize
    else:
        raise NotImplementedError
    if config.loader.dataset_params.image_source == 'memory':
        reader_train = Step(name='reader_train', transformer=loaders.ImageReader(**config.reader[model_name]), input_data=['input', 'specs'], adapter={'meta': [('input', 'meta')], 'train_mode': [('specs', 'train_mode')]}, cache_dirpath=config.env.cache_dirpath)
        reader_inference = Step(name='reader_inference', transformer=loaders.ImageReader(**config.reader[model_name]), input_data=['callback_input', 'specs'], adapter={'meta': [('callback_input', 'meta_valid')], 'train_mode': [('specs', 'train_mode')]}, cache_dirpath=config.env.cache_dirpath)
    elif config.loader.dataset_params.image_source == 'disk':
        reader_train = Step(name='xy_train', transformer=XYSplit(**config.xy_splitter[model_name]), input_data=['input', 'specs'], adapter={'meta': [('input', 'meta')], 'train_mode': [('specs', 'train_mode')]}, cache_dirpath=config.env.cache_dirpath)
        reader_inference = Step(name='xy_inference', transformer=XYSplit(**config.xy_splitter[model_name]), input_data=['callback_input', 'specs'], adapter={'meta': [('callback_input', 'meta_valid')], 'train_mode': [('specs', 'train_mode')]}, cache_dirpath=config.env.cache_dirpath)
    else:
        raise NotImplementedError
    loader = Step(name='loader', transformer=Loader(**config.loader), input_data=['specs'], input_steps=[reader_train, reader_inference], adapter={'X': ([(reader_train.name, 'X')], squeeze_inputs_if_needed), 'y': ([(reader_train.name, 'y')], squeeze_inputs_if_needed), 'train_mode': [('specs', 'train_mode')], 'X_valid': ([(reader_inference.name, 'X')], squeeze_inputs_if_needed), 'y_valid': ([(reader_inference.name, 'y')], squeeze_inputs_if_needed)}, cache_dirpath=config.env.cache_dirpath)
    return loader",config.execution.loader_mode == 'crop_and_pad',109,config.execution.loader_mode == 'crop',False,58.99565399238539,N/A
"def preprocessing_inference(config, model_name='unet'):
<mask>:
        Loader = loaders.ImageSegmentationLoaderCropPad
    elif config.execution.loader_mode == 'resize':
        Loader = loaders.ImageSegmentationLoaderResize
    else:
        raise NotImplementedError
    if config.loader.dataset_params.image_source == 'memory':
        reader_inference = Step(name='reader_inference', transformer=loaders.ImageReader(**config.reader[model_name]), input_data=['input', 'specs'], adapter={'meta': [('input', 'meta')], 'train_mode': [('specs', 'train_mode')]}, cache_dirpath=config.env.cache_dirpath)
    elif config.loader.dataset_params.image_source == 'disk':
        reader_inference = Step(name='xy_inference', transformer=XYSplit(**config.xy_splitter[model_name]), input_data=['input', 'specs'], adapter={'meta': [('input', 'meta')], 'train_mode': [('specs', 'train_mode')]}, cache_dirpath=config.env.cache_dirpath)
    else:
        raise NotImplementedError
    loader = Step(name='loader', transformer=Loader(**config.loader), input_data=['specs'], input_steps=[reader_inference], adapter={'X': ([(reader_inference.name, 'X')], squeeze_inputs_if_needed), 'y': ([(reader_inference.name, 'y')], squeeze_inputs_if_needed), 'train_mode': [('specs', 'train_mode')]}, cache_dirpath=config.env.cache_dirpath, cache_output=True)
    return loader",config.execution.loader_mode == 'crop_and_pad',75,config.execution.loader_mode == 'crop',False,58.99565399238539,N/A
"def preprocessing_inference_tta(config, model_name='unet'):
<mask>:
        Loader = loaders.ImageSegmentationLoaderCropPadTTA
    elif config.execution.loader_mode == 'resize':
        Loader = loaders.ImageSegmentationLoaderResizeTTA
    else:
        raise NotImplementedError
    if config.loader.dataset_params.image_source == 'memory':
        reader_inference = Step(name='reader_inference', transformer=loaders.ImageReader(**config.reader[model_name]), input_data=['input', 'specs'], adapter={'meta': [('input', 'meta')], 'train_mode': [('specs', 'train_mode')]}, cache_dirpath=config.env.cache_dirpath)
        tta_generator = Step(name='tta_generator', transformer=loaders.TestTimeAugmentationGenerator(**config.tta_generator), input_steps=[reader_inference], adapter={'X': [('reader_inference', 'X')]}, cache_dirpath=config.env.cache_dirpath)
    elif config.loader.dataset_params.image_source == 'disk':
        reader_inference = Step(name='reader_inference', transformer=XYSplit(**config.xy_splitter[model_name]), input_data=['input', 'specs'], adapter={'meta': [('input', 'meta')], 'train_mode': [('specs', 'train_mode')]}, cache_dirpath=config.env.cache_dirpath)
        tta_generator = Step(name='tta_generator', transformer=loaders.MetaTestTimeAugmentationGenerator(**config.tta_generator), input_steps=[reader_inference], adapter={'X': [('reader_inference', 'X')]}, cache_dirpath=config.env.cache_dirpath)
    else:
        raise NotImplementedError
    loader = Step(name='loader', transformer=Loader(**config.loader), input_data=['specs'], input_steps=[tta_generator], adapter={'X': ([(tta_generator.name, 'X_tta')], squeeze_inputs_if_needed), 'tta_params': [(tta_generator.name, 'tta_params')]}, cache_dirpath=config.env.cache_dirpath, cache_output=True)
    return (loader, tta_generator)",config.execution.loader_mode == 'crop_and_pad',90,config.execution.loader_mode == 'crop',False,58.99565399238539,N/A
"def mask_postprocessing(model, config, model_name='unet', save_output=False):
<mask>:
        size_adjustment_function = crop_image
    elif config.execution.loader_mode == 'resize':
        size_adjustment_function = resize_image
    else:
        raise NotImplementedError
    activation_func = config.model[model_name]['architecture_config']['model_params']['activation']
    channels = config.postprocessor.channels[activation_func]
    mask_resize = Step(name='mask_resize', transformer=make_apply_transformer(size_adjustment_function, output_name='resized_images', apply_on=['images', 'target_sizes']), input_data=['input'], input_steps=[model], adapter={'images': [(model.name, 'mask_prediction')], 'target_sizes': [('input', 'target_sizes')]}, cache_dirpath=config.env.cache_dirpath, save_output=save_output)
    category_mapper = Step(name='category_mapper', transformer=make_apply_transformer(partial(categorize_image, activation='sigmoid', threshold=config.thresholder.threshold_masks), output_name='categorized_images'), input_steps=[mask_resize], adapter={'images': [('mask_resize', 'resized_images')]}, cache_dirpath=config.env.cache_dirpath, save_output=save_output)
    dropper = Step(name='dropper', transformer=make_apply_transformer(partial(drop_small_unlabeled, min_size=config.dropper.min_mask_size), output_name='cleaned_images'), input_steps=[category_mapper], adapter={'images': [('category_mapper', 'categorized_images')]}, cache_dirpath=config.env.cache_dirpath, save_output=save_output)
    labeler = Step(name='labeler', transformer=make_apply_transformer(label_multiclass_image, output_name='labeled_images'), input_steps=[dropper], adapter={'images': [('dropper', 'cleaned_images')]}, cache_dirpath=config.env.cache_dirpath, save_output=save_output)
    nuclei_filter = Step(name='nuclei_filter', transformer=make_apply_transformer(partial(get_channel, channel=channels.index('nuclei')), output_name='nuclei_images'), input_steps=[labeler], adapter={'images': [('labeler', 'labeled_images')]}, cache_dirpath=config.env.cache_dirpath, save_output=save_output)
    return nuclei_filter",config.execution.loader_mode == 'crop_and_pad',92,config.execution.loader_mode == 'crop',False,58.99565399238539,N/A
"def split_on_column(meta, column, test_size, random_state=1, valid_category_ids=None):
<mask>:
        categories = meta[column].unique()
        np.random.seed(random_state)
        valid_category_ids = np.random.choice(categories, int(test_size * len(categories)))
    valid = meta[meta[column].isin(valid_category_ids)].sample(frac=1, random_state=random_state)
    train = meta[~meta[column].isin(valid_category_ids)].sample(frac=1, random_state=random_state)
    return (train, valid)",valid_category_ids is None,28,valid_category_ids is None,True,100.00000000000004,N/A
"def overlay_cut_masks(images_dir, subdir_name, target_dir, cut_size=1):
    train_dir = os.path.join(images_dir, subdir_name)
    for mask_dirname in tqdm(glob.glob('{}/*/masks'.format(train_dir))):
        masks = []
        for ind, image_filepath in enumerate(glob.glob('{}/*'.format(mask_dirname))):
            image = np.asarray(Image.open(image_filepath))
            image = np.where(image > 0, ind + 1, 0)
            masks.append(image)
        labeled_masks = np.sum(masks, axis=0)
        overlayed_masks = np.where(labeled_masks, 1, 0)
        watershed_mask = watershed(overlayed_masks.astype(np.bool), labeled_masks, watershed_line=True)
<mask>:
            cut_masks = overlayed_masks
        else:
            borders = (watershed_mask == 0) & overlayed_masks
            selem = rectangle(cut_size, cut_size)
            dilated_borders = dilation(borders, selem=selem)
            cut_masks = np.where(dilated_borders, 0, overlayed_masks)
        target_filepath = '/'.join(mask_dirname.replace(images_dir, target_dir).split('/')[:-1]) + '.png'
        os.makedirs(os.path.dirname(target_filepath), exist_ok=True)
        imwrite(target_filepath, cut_masks)",watershed_mask.max() == watershed_mask.min(),83,watershed_mask == 0,False,7.1757211592468355,N/A
"def overlay_masks_with_borders(images_dir, subdir_name, target_dir, borders_size=3, dilation_size=5):
    train_dir = os.path.join(images_dir, subdir_name)
    for mask_dirname in tqdm(glob.glob('{}/*/masks'.format(train_dir))):
        masks = []
        for ind, image_filepath in enumerate(glob.glob('{}/*'.format(mask_dirname))):
            image = np.asarray(Image.open(image_filepath))
            image = np.where(image > 0, ind + 1, 0)
            masks.append(image)
        labeled_masks = np.sum(masks, axis=0)
        overlayed_masks = np.where(labeled_masks, 1, 0)
        selem = rectangle(dilation_size, dilation_size)
        dilated_mask = dilation(overlayed_masks, selem=selem)
        watershed_mask = watershed((dilated_mask >= 0).astype(np.bool), labeled_masks, watershed_line=True)
<mask>:
            masks_with_borders = overlayed_masks
        else:
            borders = (watershed_mask == 0) & (dilated_mask > 0)
            selem = rectangle(borders_size, borders_size)
            dilated_borders = dilation(borders, selem=selem)
            masks_with_borders = np.where(dilated_borders, 2, overlayed_masks)
        target_filepath = '/'.join(mask_dirname.replace(images_dir, target_dir).split('/')[:-1]) + '.png'
        os.makedirs(os.path.dirname(target_filepath), exist_ok=True)
        imwrite(target_filepath, masks_with_borders)",watershed_mask.max() == watershed_mask.min(),96,watershed_mask == 0,False,7.1757211592468355,N/A
"def overlay_masks_with_borders_json(images_dir, subdir_name, target_dir, borders_size=3, dilation_size=5):
    train_dir = os.path.join(images_dir, subdir_name)
    for mask_dirname in tqdm(glob.glob('{}/*/masks'.format(train_dir))):
        masks = []
        for ind, image_filepath in enumerate(glob.glob('{}/*'.format(mask_dirname))):
            image = np.asarray(Image.open(image_filepath))
            image = np.where(image > 0, ind + 1, 0)
            masks.append(image)
        labeled_masks = np.sum(masks, axis=0)
        overlayed_masks = np.where(labeled_masks, 1, 0)
        selem = rectangle(dilation_size, dilation_size)
        dilated_mask = dilation(overlayed_masks, selem=selem)
        watershed_mask = watershed((dilated_mask >= 0).astype(np.bool), labeled_masks, watershed_line=True)
<mask>:
            dilated_borders = np.zeros_like(overlayed_masks)
        else:
            borders = (watershed_mask == 0) & (dilated_mask > 0)
            selem = rectangle(borders_size, borders_size)
            dilated_borders = dilation(borders, selem=selem)
        nuclei = prepare_class_encoding(overlayed_masks)
        borders = prepare_class_encoding(dilated_borders)
        target_filepath = '/'.join(mask_dirname.replace(images_dir, target_dir).split('/')[:-1]) + '.json'
        os.makedirs(os.path.dirname(target_filepath), exist_ok=True)
        save_target_masks(target_filepath, nuclei, borders)",watershed_mask.max() == watershed_mask.min(),98,watershed_mask == 0,False,7.1757211592468355,N/A
"def overlay_contours(images_dir, subdir_name, target_dir, touching_only=False):
    train_dir = os.path.join(images_dir, subdir_name)
    for mask_dirname in tqdm(glob.glob('{}/*/masks'.format(train_dir))):
        masks = []
        for image_filepath in glob.glob('{}/*'.format(mask_dirname)):
            image = np.asarray(Image.open(image_filepath))
            image = image / 255.0
            masks.append(get_contour(image))
<mask>:
            overlayed_masks = np.where(np.sum(masks, axis=0) > 128.0 + 255.0, 255.0, 0.0).astype(np.uint8)
        else:
            overlayed_masks = np.where(np.sum(masks, axis=0) > 128.0, 255.0, 0.0).astype(np.uint8)
        target_filepath = '/'.join(mask_dirname.replace(images_dir, target_dir).split('/')[:-1]) + '.png'
        os.makedirs(os.path.dirname(target_filepath), exist_ok=True)
        imwrite(target_filepath, overlayed_masks)",touching_only,59,touching_only,True,100.00000000000004,N/A
"def train(pipeline_name, validation_size, dev_mode, logger, params):
    logger.info('training')
<mask>:
        shutil.rmtree(params.experiment_dir)
    meta = pd.read_csv(os.path.join(params.meta_dir, 'stage1_metadata.csv'))
    meta_train = meta[meta['is_train'] == 1]
    meta_train_split, meta_valid_split = train_valid_split(meta_train.query('is_external==0'), validation_size, random_state=SEED)
    meta_train_split = meta_train_split.append(meta_train.query('is_external==1'))
    if dev_mode:
        meta_train_split = meta_train_split.sample(params.dev_mode_size, random_state=SEED)
        meta_valid_split = meta_valid_split.sample(int(params.dev_mode_size / 2), random_state=SEED)
    data = {'input': {'meta': meta_train_split, 'target_sizes': meta_train_split[SIZE_COLUMNS].values}, 'specs': {'train_mode': True}, 'callback_input': {'meta_valid': meta_valid_split}}
    pipeline = PIPELINES[pipeline_name]['train'](SOLUTION_CONFIG)
    pipeline.clean_cache()
    pipeline.fit_transform(data)
    pipeline.clean_cache()",bool(params.overwrite) and os.path.isdir(params.experiment_dir),58,params.experiment_dir,False,6.0810062625218,N/A
"def evaluate(pipeline_name, validation_size, dev_mode, logger, params, ctx):
    logger.info('evaluating')
    meta = pd.read_csv(os.path.join(params.meta_dir, 'stage1_metadata.csv'))
    meta_train = meta[meta['is_train'] == 1]
    try:
        validation_size = float(validation_size)
    except ValueError:
        pass
<mask>:
        meta_train_split, meta_valid_split = train_valid_split(meta_train.query('is_external==0'), validation_size, random_state=SEED)
        y_true = read_masks(meta_valid_split[Y_COLUMNS_SCORING].values)
    elif validation_size == 'test':
        meta_valid_split = meta[meta['is_train'] == 0]
        solution_dir = os.path.join(params.data_dir, 'stage1_solution.csv')
        image_ids = meta_valid_split['ImageId'].values
        y_true = read_masks_from_csv(image_ids, solution_dir)
    else:
        raise NotImplementedError
    if dev_mode:
        meta_valid_split = meta_valid_split.sample(params.dev_mode_size, random_state=SEED)
    data = {'input': {'meta': meta_valid_split, 'target_sizes': meta_valid_split[SIZE_COLUMNS].values}, 'specs': {'train_mode': False}, 'callback_input': {'meta_valid': None}}
    pipeline = PIPELINES[pipeline_name]['inference'](SOLUTION_CONFIG)
    pipeline.clean_cache()
    output = pipeline.transform(data)
    pipeline.clean_cache()
    y_pred = output['y_pred']
    logger.info('Calculating IOU and IOUT Scores')
    iou_score = intersection_over_union(y_true, y_pred)
    logger.info('IOU score on validation is {}'.format(iou_score))
    ctx.channel_send('IOU Score', 0, iou_score)
    iout_score = intersection_over_union_thresholds(y_true, y_pred)
    logger.info('IOUT score on validation is {}'.format(iout_score))
    ctx.channel_send('IOUT Score', 0, iout_score)","isinstance(validation_size, float)",120,validation_size == 'external',False,21.64910073203448,N/A
"def predict(pipeline_name, dev_mode, logger, params):
    logger.info('predicting')
    meta = pd.read_csv(os.path.join(params.meta_dir, 'stage1_metadata.csv'))
    meta_test = meta[meta['is_train'] == 0]
<mask>:
        meta_test = meta_test.sample(params.dev_mode_size, random_state=SEED)
    data = {'input': {'meta': meta_test, 'meta_valid': None, 'train_mode': False, 'target_sizes': meta_test[SIZE_COLUMNS].values}}
    pipeline = PIPELINES[pipeline_name]['inference'](SOLUTION_CONFIG)
    pipeline.clean_cache()
    output = pipeline.transform(data)
    pipeline.clean_cache()
    y_pred = output['y_pred']
    create_submission(params.experiment_dir, meta_test, y_pred, logger)",dev_mode,46,dev_mode,True,100.00000000000004,N/A
"def transform(self, prediction_proba_list):
<mask>:
        reshaped_weights = self._reshape_weights(prediction_proba_list.shape)
        prediction_proba_list *= reshaped_weights
        avg_pred = np.sum(prediction_proba_list, axis=0)
    else:
        avg_pred = np.mean(prediction_proba_list, axis=0)
    return {'prediction_probability': avg_pred}",self.weights is not None,22,self.reserve_weights,False,19.3576934939088,N/A
"def __init__(self, name, transformer, input_steps=[], input_data=[], adapter=None, cache_dirpath=None, is_trainable=False, cache_output=False, save_output=False, load_saved_output=False, save_graph=False, force_fitting=False):
    self.name = name
    self.transformer = transformer
    self.input_steps = input_steps
    self.input_data = input_data
    self.adapter = adapter
    self.is_trainable = is_trainable
    self.force_fitting = force_fitting
    self.cache_output = cache_output
    self.save_output = save_output
    self.load_saved_output = load_saved_output
    self.cache_dirpath = cache_dirpath
    self._prep_cache(cache_dirpath)
<mask>:
        graph_filepath = os.path.join(self.cache_dirpath, '{}_graph.json'.format(self.name))
        logger.info('Saving graph to {}'.format(graph_filepath))
        joblib.dump(self.graph_info, graph_filepath)",save_graph,59,save_graph,True,100.00000000000004,N/A
"def fit_transform(self, data):
<mask>:
        logger.info('step {} loading output...'.format(self.name))
        step_output_data = self._cached_output
    elif self.output_is_saved and self.load_saved_output and (not self.force_fitting):
        logger.info('step {} loading output...'.format(self.name))
        step_output_data = self._load_output(self.save_filepath_step_output)
    else:
        step_inputs = {}
        if self.input_data is not None:
            for input_data_part in self.input_data:
                step_inputs[input_data_part] = data[input_data_part]
        for input_step in self.input_steps:
            step_inputs[input_step.name] = input_step.fit_transform(data)
        if self.adapter:
            step_inputs = self.adapt(step_inputs)
        else:
            step_inputs = self.unpack(step_inputs)
        step_output_data = self._cached_fit_transform(step_inputs)
    return step_output_data",self.output_is_cached and (not self.force_fitting),62,self.output_is_cached and self.load_saved_output and (not self.force_fitting),False,60.56776616241531,N/A
"def _cached_fit_transform(self, step_inputs):
<mask>:
        if self.transformer_is_cached and (not self.force_fitting):
            logger.info('step {} loading transformer...'.format(self.name))
            self.transformer.load(self.cache_filepath_step_transformer)
            logger.info('step {} transforming...'.format(self.name))
            step_output_data = self.transformer.transform(**step_inputs)
        else:
            logger.info('step {} fitting and transforming...'.format(self.name))
            step_output_data = self.transformer.fit_transform(**step_inputs)
            logger.info('step {} saving transformer...'.format(self.name))
            self.transformer.save(self.cache_filepath_step_transformer)
    else:
        logger.info('step {} transforming...'.format(self.name))
        step_output_data = self.transformer.transform(**step_inputs)
    if self.cache_output:
        logger.info('step {} caching outputs...'.format(self.name))
        self._cached_output = step_output_data
    if self.save_output:
        logger.info('step {} saving outputs...'.format(self.name))
        self._save_output(step_output_data, self.save_filepath_step_output)
    return step_output_data",self.is_trainable,60,self.transformer_is_cached,False,18.575057999133602,N/A
"def make_transformer(estimator, mode='classifier'):
<mask>:
        transformer = SklearnClassifier(estimator)
    elif mode == 'regressor':
        transformer = SklearnRegressor(estimator)
    elif mode == 'transformer':
        transformer = SklearnTransformer(estimator)
    elif mode == 'pipeline':
        transformer = SklearnPipeline(estimator)
    else:
        raise NotImplementedError('Only classifier, regressor and transformer modes are available')
    return transformer",mode == 'classifier',40,mode == 'classifier',True,100.00000000000004,N/A
"def transform(self, meta, train_mode):
    X = meta[self.x_columns].values
<mask>:
        y = meta[self.y_columns].values
    else:
        y = None
    return {'X': X, 'y': y}",train_mode,20,train_mode,True,100.00000000000004,N/A
"def transform(self, meta, train_mode):
    X_ = meta[self.x_columns].values
    X = self.load_images(X_, grayscale=False)
<mask>:
        y_ = meta[self.y_columns].values
        y = self.load_images(y_, grayscale=True)
    else:
        y = None
    return {'X': X, 'y': y}",train_mode,28,train_mode,True,100.00000000000004,N/A
"def transform(self, X):
    X = pd.DataFrame(X, columns=['text']).astype(str)
    X['text'] = X['text'].apply(self._transform)
<mask>:
        X['text'] = X['text'].fillna(self.fill_na_with).values
    return {'X': X['text'].values}",self.fill_na_with,17,self.fill_na_with,True,100.00000000000004,N/A
"def _transform(self, x):
<mask>:
        x = self._lower(x)
    if self.drop_punctuation:
        x = self._remove_punctuation(x)
    if self.drop_newline:
        x = self._remove_newline(x)
    if self.drop_multispaces:
        x = self._substitute_multiple_spaces(x)
    if self.deduplication_threshold is not None:
        x = self._deduplicate(x)
    if self.anonymize:
        x = self._anonymize(x)
    if self.apostrophes:
        x = self._apostrophes(x)
    if self.use_stopwords:
        x = self._use_stopwords(x)
    return x",self.all_lower_case,47,self.lower,False,16.605579150202516,N/A
"def _deduplicate(self, x):
    word_list = x.split()
    num_words = len(word_list)
<mask>:
        return x
    else:
        num_unique_words = len(set(word_list))
        unique_ratio = num_words / num_unique_words
        if unique_ratio > self.deduplication_threshold:
            x = ' '.join(x.split()[:num_unique_words])
        return x",num_words == 0,31,num_words == 0,True,100.00000000000004,N/A
"def _initialize_model_weights(self):
    logger.info('initializing model weights...')
    weights_init_config = self.architecture_config['weights_init']
<mask>:
        weights_init_func = partial(init_weights_normal, **weights_init_config['params'])
    elif weights_init_config['function'] == 'xavier':
        weights_init_func = init_weights_xavier
    elif weights_init_config['function'] == 'he':
        weights_init_func = init_weights_he
    else:
        raise NotImplementedError
    self.model.apply(weights_init_func)",weights_init_config['function'] == 'normal',31,weights_init_config['function'] == 'normal',True,100.00000000000004,N/A
"def fit(self, datagen, validation_datagen=None):
    self._initialize_model_weights()
    self.model = nn.DataParallel(self.model)
<mask>:
        self.model = self.model.cuda()
    self.callbacks.set_params(self, validation_datagen=validation_datagen)
    self.callbacks.on_train_begin()
    batch_gen, steps = datagen
    for epoch_id in range(self.training_config['epochs']):
        self.callbacks.on_epoch_begin()
        for batch_id, data in enumerate(batch_gen):
            self.callbacks.on_batch_begin()
            metrics = self._fit_loop(data)
            self.callbacks.on_batch_end(metrics=metrics)
            if batch_id == steps:
                break
        self.callbacks.on_epoch_end()
        if self.callbacks.training_break():
            break
    self.callbacks.on_train_end()
    return self",torch.cuda.is_available(),46,self.training_config['cuda'],False,5.795599612995366,N/A
"def _fit_loop(self, data):
    X = data[0]
    targets_tensors = data[1:]
<mask>:
        X = Variable(X).cuda()
        targets_var = []
        for target_tensor in targets_tensors:
            targets_var.append(Variable(target_tensor).cuda())
    else:
        X = Variable(X)
        targets_var = []
        for target_tensor in targets_tensors:
            targets_var.append(Variable(target_tensor))
    self.optimizer.zero_grad()
    outputs_batch = self.model(X)
    partial_batch_losses = {}
    if len(self.output_names) == 1:
        for (name, loss_function, weight), target in zip(self.loss_function, targets_var):
            batch_loss = loss_function(outputs_batch, target) * weight
    else:
        for (name, loss_function, weight), output, target in zip(self.loss_function, outputs_batch, targets_var):
            partial_batch_losses[name] = loss_function(output, target) * weight
        batch_loss = sum(partial_batch_losses.values())
    partial_batch_losses['sum'] = batch_loss
    batch_loss.backward()
    self.optimizer.step()
    return partial_batch_losses",torch.cuda.is_available(),85,self.use_cuda,False,6.316906128202129,N/A
"def _transform(self, datagen, validation_datagen=None):
    self.model.eval()
    batch_gen, steps = datagen
    outputs = {}
    for batch_id, data in enumerate(batch_gen):
<mask>:
            X = data[0]
        else:
            X = data
        if torch.cuda.is_available():
            X = Variable(X, volatile=True).cuda()
        else:
            X = Variable(X, volatile=True)
        outputs_batch = self.model(X)
        if len(self.output_names) == 1:
            outputs.setdefault(self.output_names[0], []).append(outputs_batch.data.cpu().numpy())
        else:
            for name, output in zip(self.output_names, outputs_batch):
                output_ = output.data.cpu().numpy()
                outputs.setdefault(name, []).append(output_)
        if batch_id == steps:
            break
    self.model.train()
    outputs = {'{}_prediction'.format(name): np.vstack(outputs_) for name, outputs_ in outputs.items()}
    return outputs","isinstance(data, list)",74,len(data) == 1,False,14.535768424205482,N/A
"def load(self, filepath):
    self.model.eval()
<mask>:
        self.model = nn.DataParallel(self.model)
    if torch.cuda.is_available():
        self.model.cpu()
        self.model.load_state_dict(torch.load(filepath))
        self.model = self.model.cuda()
    else:
        self.model.load_state_dict(torch.load(filepath, map_location=lambda storage, loc: storage))
    return self","not isinstance(self.model, nn.DataParallel)",23,torch.cuda.is_available(),False,5.3990167242108145,N/A
"def __init__(self, X, y, image_transform, target_transform, image_augment):
    super().__init__()
    self.X = X
<mask>:
        self.y = y
    else:
        self.y = None
    self.image_transform = image_transform
    self.image_augment = image_augment
    self.target_transform = target_transform",y is not None,28,y is not None,True,100.00000000000004,N/A
"def __getitem__(self, index):
    img_filepath = self.X[index]
    Xi = self.load_image(img_filepath)
<mask>:
        Xi = self.image_augment(Xi)
    if self.image_transform is not None:
        Xi = self.image_transform(Xi)
    if self.y is not None:
        yi = self.y[index]
        if self.target_transform is not None:
            yi = self.target_transform(yi)
        return (Xi, yi)
    else:
        return Xi",self.image_augment is not None,43,self.image_augment is not None,True,100.00000000000004,N/A
"def transform(self, X, y, validation_data, train_mode):
<mask>:
        flow, steps = self.get_datagen(X, y, train_mode, self.loader_params['training'])
    else:
        flow, steps = self.get_datagen(X, y, train_mode, self.loader_params['inference'])
    if validation_data is not None:
        X_valid, y_valid = validation_data
        valid_flow, valid_steps = self.get_datagen(X_valid, y_valid, False, self.loader_params['inference'])
    else:
        valid_flow = None
        valid_steps = None
    return {'datagen': (flow, steps), 'validation_datagen': (valid_flow, valid_steps)}",train_mode,52,self.loader_params['training'] is not None,False,3.7477767366779213,N/A
"def get_datagen(self, X, y, train_mode, loader_params):
<mask>:
        dataset = self.dataset(X, y, image_augment=self.image_augment, image_transform=self.image_transform, target_transform=self.target_transform)
    else:
        dataset = self.dataset(X, y, image_augment=None, image_transform=self.image_transform, target_transform=self.target_transform)
    datagen = DataLoader(dataset, **loader_params)
    steps = ceil(X.shape[0] / loader_params['batch_size'])
    return (datagen, steps)",train_mode,34,train_mode == 'train',False,30.213753973567677,N/A
"def save_model(model, path):
    model.eval()
<mask>:
        model.cpu()
        torch.save(model.state_dict(), path)
        model.cuda()
    else:
        torch.save(model.state_dict(), path)
    model.train()",torch.cuda.is_available(),13,torch.cuda.is_available(),True,100.00000000000004,N/A
"@property
def value(self):
<mask>:
        return 0
    else:
        return 1.0 * self.current_total / self.iterations",self.iterations == 0,13,self.iterations == 0,True,100.00000000000004,N/A
"def __init__(self, augmenters):
<mask>:
        augmenters = [augmenters]
    self.augmenters = augmenters
    self.seq_det = None","not isinstance(augmenters, list)",13,"isinstance(augmenters, str)",False,45.48019047027906,N/A
"def transform(self, *images):
    images = [self.seq_det.augment_image(image) for image in images]
<mask>:
        return images[0]
    else:
        return images",len(images) == 1,16,len(images) == 1,True,100.00000000000004,N/A
"def reseed(augmenter, deterministic=True):
    augmenter.random_state = ia.new_random_state(get_seed())
<mask>:
        augmenter.deterministic = True
    for lists in augmenter.get_children_lists():
        for aug in lists:
            aug = reseed(aug, deterministic=True)
    return augmenter",deterministic,24,deterministic,True,100.00000000000004,N/A
"def get_validation_loss(self):
<mask>:
        self.validation_loss[self.epoch_id] = score_model(self.model, self.loss_function, self.validation_datagen)
    return self.validation_loss[self.epoch_id]",self.epoch_id not in self.validation_loss.keys(),10,self.epoch_id not in self.validation_loss,False,71.65313105737896,N/A
"def __init__(self, callbacks=None):
<mask>:
        self.callbacks = []
    elif isinstance(callbacks, Callback):
        self.callbacks = [callbacks]
    else:
        self.callbacks = callbacks",callbacks is None,17,callbacks is None,True,100.00000000000004,N/A
"def __init__(self, epoch_every=None, batch_every=None):
    super().__init__()
    self.epoch_loss_averagers = {}
<mask>:
        self.epoch_every = False
    else:
        self.epoch_every = epoch_every
    if batch_every == 0:
        self.batch_every = False
    else:
        self.batch_every = batch_every",epoch_every == 0,27,epoch_every == 0,True,100.00000000000004,N/A
"def on_epoch_end(self, *args, **kwargs):
    for name, averager in self.epoch_loss_averagers.items():
        epoch_avg_loss = averager.value
        averager.reset()
<mask>:
            logger.info('epoch {0} {1}:     {2:.5f}'.format(self.epoch_id, name, epoch_avg_loss))
    self.epoch_id += 1",self.epoch_every and self.epoch_id % self.epoch_every == 0,23,self.verbose,False,0.19037687152469163,N/A
"def on_batch_end(self, metrics, *args, **kwargs):
    for name, loss in metrics.items():
        loss = loss.data.cpu().numpy()[0]
<mask>:
            self.epoch_loss_averagers[name].send(loss)
        else:
            self.epoch_loss_averagers[name] = Averager()
            self.epoch_loss_averagers[name].send(loss)
        if self.batch_every and self.batch_id % self.batch_every == 0:
            logger.info('epoch {0} batch {1} {2}:     {3:.5f}'.format(self.epoch_id, self.batch_id, name, loss))
    self.batch_id += 1",name in self.epoch_loss_averagers.keys(),40,self.epoch_loss_averagers[name] is None,False,49.05378138718246,N/A
"def cross_entropy(output, target, squeeze=False):
<mask>:
        target = target.squeeze(1)
    return F.nll_loss(output, target)",squeeze,11,squeeze,True,100.00000000000004,N/A
"def mse(output, target, squeeze=False):
<mask>:
        target = target.squeeze(1)
    return F.mse_loss(output, target)",squeeze,11,squeeze,True,100.00000000000004,N/A
"def score_model(model, loss_function, datagen):
    batch_gen, steps = datagen
    partial_batch_losses = {}
    for batch_id, data in enumerate(batch_gen):
        X = data[0]
        targets_tensors = data[1:]
<mask>:
            X = Variable(X, volatile=True).cuda()
            targets_var = []
            for target_tensor in targets_tensors:
                targets_var.append(Variable(target_tensor, volatile=True).cuda())
        else:
            X = Variable(X, volatile=True)
            targets_var = []
            for target_tensor in targets_tensors:
                targets_var.append(Variable(target_tensor, volatile=True))
        outputs = model(X)
        if len(loss_function) == 1:
            for (name, loss_function_one, weight), target in zip(loss_function, targets_var):
                loss_sum = loss_function_one(outputs, target) * weight
        else:
            batch_losses = []
            for (name, loss_function_one, weight), output, target in zip(loss_function, outputs, targets_var):
                loss = loss_function_one(output, target) * weight
                batch_losses.append(loss)
                partial_batch_losses.setdefault(name, []).append(loss)
            loss_sum = sum(batch_losses)
        partial_batch_losses.setdefault('sum', []).append(loss_sum)
        if batch_id == steps:
            break
    average_losses = {name: sum(losses) / steps for name, losses in partial_batch_losses.items()}
    return average_losses",torch.cuda.is_available(),117,is_cuda,False,8.525588607164655,N/A
"def torch_acc_score_multi_output(outputs, targets, take_first=None):
    accuracies = []
    for i, (output, target) in enumerate(zip(outputs, targets)):
<mask>:
            break
        accuracy = torch_acc_score(output, target)
        accuracies.append(accuracy)
    avg_accuracy = sum(accuracies) / len(accuracies)
    return avg_accuracy",i == take_first,28,take_first and i == 0,False,34.57207846419412,N/A
"def _input_block(self):
    stride = 1
    padding = get_downsample_pad(stride=stride, kernel=self.conv_kernel)
<mask>:
        input_block = nn.Sequential(nn.Conv2d(in_channels=self.in_channels, out_channels=self.n_filters, kernel_size=(self.conv_kernel, self.conv_kernel), stride=stride, padding=padding), nn.BatchNorm2d(num_features=self.n_filters), nn.ReLU(), nn.Conv2d(in_channels=self.n_filters, out_channels=self.n_filters, kernel_size=(self.conv_kernel, self.conv_kernel), stride=stride, padding=padding), nn.BatchNorm2d(num_features=self.n_filters), nn.ReLU(), nn.Dropout(self.dropout))
    else:
        input_block = nn.Sequential(nn.Conv2d(in_channels=self.in_channels, out_channels=self.n_filters, kernel_size=(self.conv_kernel, self.conv_kernel), stride=stride, padding=padding), nn.ReLU(), nn.Conv2d(in_channels=self.n_filters, out_channels=self.n_filters, kernel_size=(self.conv_kernel, self.conv_kernel), stride=stride, padding=padding), nn.ReLU(), nn.Dropout(self.dropout))
    return input_block",self.batch_norm,49,self.use_batch,False,25.40663740773074,N/A
"def _classification_block(self):
    in_block = int(2 * self.n_filters)
    stride = 1
    padding = get_downsample_pad(stride=stride, kernel=self.conv_kernel)
<mask>:
        classification_block = nn.Sequential(nn.Conv2d(in_channels=in_block, out_channels=self.n_filters, kernel_size=(self.conv_kernel, self.conv_kernel), stride=stride, padding=padding), nn.BatchNorm2d(num_features=self.n_filters), nn.ReLU(), nn.Dropout(self.dropout), nn.Conv2d(in_channels=self.n_filters, out_channels=self.n_filters, kernel_size=(self.conv_kernel, self.conv_kernel), stride=stride, padding=padding), nn.BatchNorm2d(num_features=self.n_filters), nn.ReLU())
    else:
        classification_block = nn.Sequential(nn.Conv2d(in_channels=in_block, out_channels=self.n_filters, kernel_size=(self.conv_kernel, self.conv_kernel), stride=stride, padding=padding), nn.ReLU(), nn.Dropout(self.dropout), nn.Conv2d(in_channels=self.n_filters, out_channels=self.n_filters, kernel_size=(self.conv_kernel, self.conv_kernel), stride=stride, padding=padding), nn.ReLU())
    return classification_block",self.batch_norm,54,self.training,False,28.254432923044853,N/A
"def _down_conv(self):
    stride = 1
    padding = get_downsample_pad(stride=stride, kernel=self.kernel_size)
<mask>:
        down_conv = nn.Sequential(nn.Conv2d(in_channels=self.in_channels, out_channels=self.block_channels, kernel_size=(self.kernel_size, self.kernel_size), stride=stride, padding=padding), nn.BatchNorm2d(num_features=self.block_channels), nn.ReLU(), nn.Conv2d(in_channels=self.block_channels, out_channels=self.block_channels, kernel_size=(self.kernel_size, self.kernel_size), stride=stride, padding=padding), nn.BatchNorm2d(num_features=self.block_channels), nn.ReLU(), nn.Dropout(self.dropout))
    else:
        down_conv = nn.Sequential(nn.Conv2d(in_channels=self.in_channels, out_channels=self.block_channels, kernel_size=(self.kernel_size, self.kernel_size), stride=stride, padding=padding), nn.ReLU(), nn.Conv2d(in_channels=self.block_channels, out_channels=self.block_channels, kernel_size=(self.kernel_size, self.kernel_size), stride=stride, padding=padding), nn.ReLU(), nn.Dropout(self.dropout))
    return down_conv",self.batch_norm,49,self.block_channels is not None,False,12.22307556087252,N/A
"def _up_conv(self):
    stride = 1
    padding = get_downsample_pad(stride=stride, kernel=self.kernel_size)
<mask>:
        up_conv = nn.Sequential(nn.Conv2d(in_channels=self.in_channels, out_channels=self.block_channels, kernel_size=(self.kernel_size, self.kernel_size), stride=stride, padding=padding), nn.BatchNorm2d(num_features=self.block_channels), nn.ReLU(), nn.Conv2d(in_channels=self.block_channels, out_channels=self.block_channels, kernel_size=(self.kernel_size, self.kernel_size), stride=stride, padding=padding), nn.BatchNorm2d(num_features=self.block_channels), nn.ReLU(), nn.Dropout(self.dropout))
    else:
        up_conv = nn.Sequential(nn.Conv2d(in_channels=self.in_channels, out_channels=self.block_channels, kernel_size=(self.kernel_size, self.kernel_size), stride=stride, padding=padding), nn.ReLU(), nn.Conv2d(in_channels=self.block_channels, out_channels=self.block_channels, kernel_size=(self.kernel_size, self.kernel_size), stride=stride, padding=padding), nn.ReLU(), nn.Dropout(self.dropout))
    return up_conv",self.batch_norm,49,self.use_batch,False,25.40663740773074,N/A
"def get_upsample_pad(stride, kernel, dilation=1):
<mask>:
        return (int((kernel - stride) / 2), 0)
    elif kernel - stride < 0:
        return (0, stride - kernel)
    else:
        return (int(math.ceil((kernel - stride) / 2)), 1)",kernel - stride >= 0 and (kernel - stride) % 2 == 0,31,dilation == 1,False,1.2387256358345295,N/A
"def _bits2int(self, b: bytes) -> int:
    """"""http://tools.ietf.org/html/rfc6979#section-2.3.2""""""
    i = int.from_bytes(b, 'big')
    blen = len(b) * 8
<mask>:
        i >>= blen - self.qlen
    return i",blen > self.qlen,24,blen > self.qlen,True,100.00000000000004,N/A
"def gen_nonce(self) -> int:
    """"""http://tools.ietf.org/html/rfc6979#section-3.2""""""
    hash_size = self.hashfunc().digest_size
<mask>:
        h1 = self.msg
    else:
        h1 = self.hashfunc(self.msg).digest()
    key_and_msg = self._int2octets(self.x) + self._bits2octets(h1)
    v = b''.join([b'\x01' for _ in range(hash_size)])
    k = b''.join([b'\x00' for _ in range(hash_size)])
    k = hmac.new(k, v + b'\x00' + key_and_msg, self.hashfunc).digest()
    v = hmac.new(k, v, self.hashfunc).digest()
    k = hmac.new(k, v + b'\x01' + key_and_msg, self.hashfunc).digest()
    v = hmac.new(k, v, self.hashfunc).digest()
    while True:
        t = b''
        while len(t) * 8 < self.qlen:
            v = hmac.new(k, v, self.hashfunc).digest()
            t = t + v
        nonce = self._bits2int(t)
        if nonce >= 1 and nonce < self.q:
            return nonce
        k = hmac.new(k, v + b'\x00', self.hashfunc).digest()
        v = hmac.new(k, v, self.hashfunc).digest()",self.prehashed,109,self.hashfunc.hexdigest() == 0,False,8.392229812593097,N/A
"def _tonelli_shanks(n: int, p: int) -> Tuple[int, int]:
    """"""A generic algorithm for computing modular square roots.""""""
    Q, S = (p - 1, 0)
    while Q % 2 == 0:
        Q, S = (Q // 2, S + 1)
    z = 2
    while pow(z, (p - 1) // 2, p) != -1 % p:
        z += 1
    M, c, t, R = (S, pow(z, Q, p), pow(n, Q, p), pow(n, (Q + 1) // 2, p))
    while t != 1:
        for i in range(1, M):
<mask>:
                break
        b = pow(c, 2 ** (M - i - 1), p)
        M, c, t, R = (i, pow(b, 2, p), t * b * b % p, R * b % p)
    return (R, -R % p)","pow(t, 2 ** i, p) == 1",123,M <= i,False,1.5592777802775042,N/A
"def mod_sqrt(a: int, p: int) -> Tuple[int, int]:
    """"""Compute the square root of :math:`a \\pmod{p}`

    In other words, find a value :math:`x` such that :math:`x^2 \\equiv a \\pmod{p}`.

    Args:
        |  a (int): The value whose root to take.
        |  p (int): The prime whose field to perform the square root in.

    Returns:
        (int, int): the two values of :math:`x` satisfying :math:`x^2 \\equiv a \\pmod{p}`.
    """"""
<mask>:
        k = (p - 3) // 4
        x = pow(a, k + 1, p)
        return (x, -x % p)
    else:
        return _tonelli_shanks(a, p)",p % 4 == 3,89,p > 3,False,12.753667906901528,N/A
"def msg_bytes(msg: SignableMessage) -> bytes:
    """"""Return bytes in a consistent way for a given message.

    The message is expected to be either a string, bytes, or an array of bytes.

    Args:
        |  msg (str|bytes|bytearray): The data to transform.

    Returns:
        bytes: The byte encoded data.

    Raises:
        ValueError: If the data cannot be encoded as bytes.
    """"""
<mask>:
        return msg
    elif isinstance(msg, str):
        return msg.encode()
    elif isinstance(msg, bytearray):
        return bytes(msg)
    else:
        raise ValueError(f'Msg ""{msg}"" of type {type(msg)} cannot be converted to bytes')","isinstance(msg, bytes)",80,"isinstance(msg, bytes)",True,100.00000000000004,N/A
"def get_public_keys_from_sig(sig: EcdsaSignature, msg: SignableMessage, curve: Curve, hashfunc: Callable) -> Tuple[Point, Point]:
    """"""Recover the public keys that can verify a signature / message pair.

    Args:
        |  sig (int, int): A ECDSA signature.
        |  msg (str|bytes|bytearray): The message corresponding to the signature.
        |  curve (fastecdsa.curve.Curve): The curve used to sign the message.
        |  hashfunc (_hashlib.HASH): The hash function used to compress the message.

    Returns:
        (fastecdsa.point.Point, fastecdsa.point.Point): The public keys that can verify the
                                                        signature for the message.
    """"""
    r, s = sig
    rinv = pow(r, curve.q - 2, curve.q)
    z = int.from_bytes(hashfunc(msg_bytes(msg)).digest(), 'big')
    hash_bit_length = hashfunc().digest_size * 8
<mask>:
        z >>= hash_bit_length - curve.q.bit_length()
    y_squared = (r * r * r + curve.a * r + curve.b) % curve.p
    y1, y2 = mod_sqrt(y_squared, curve.p)
    R1, R2 = (Point(r, y1, curve=curve), Point(r, y2, curve=curve))
    Qs = (rinv * (s * R1 - z * curve.G), rinv * (s * R2 - z * curve.G))
    for Q in Qs:
        if not verify(sig, msg, Q, curve=curve, hashfunc=hashfunc):
            raise ValueError(f'Could not recover public key, is the signature ({sig}) a valid signature for the message ({msg!r}) over the given curve ({curve}) using the given hash function ({hashfunc})?')
    return Qs",curve.q.bit_length() < hash_bit_length,193,hash_bit_length > curve.q.bit_length(),False,80.03203203845001,N/A
"def export_private_key(key: int, curve: Curve, encoder: KeyEncoder, filepath: Optional[str]=None) -> Optional[bytes]:
    """"""Export a private EC key using the given encoder.

    Args:
        |   key (int): A private EC key.
        |   curve (fastecdsa.curve.Curve): The curve corresponding to the key.
        |   encoder (fastecdsa.encoding.KeyEncoder): An instance of an encoder that can encode a private key.
        |   filepath (str): Where to save the exported key. If :code:`None` the key is simply printed.

    Returns:
        bytes | None: If no filepath is provided the bytes of the encoded key are returned.
    """"""
<mask>:
        raise TypeError('curve must be an instance of the Curve type.')
    if not isinstance(encoder, KeyEncoder):
        raise TypeError('encoder must be an instance of a subclass of the KeyEncoder type.')
    encoded: bytes = encoder.encode_private_key(key, curve)
    if filepath is None:
        return encoded
    with open(filepath, 'wb') as f:
        f.write(encoded)
    return None","not isinstance(curve, Curve)",132,"not isinstance(curve, Curve)",True,100.00000000000004,N/A
"def export_public_key(key: Point, encoder: KeyEncoder, filepath: Optional[str]=None) -> Optional[bytes]:
    """"""Export a private EC key using the given encoder.

    Args:
        |   key (fastecdsa.point.Point): A public EC key.
        |   encoder (fastecdsa.encoding.KeyEncoder): An instance of an encoder that can encode a private key.
        |   filepath (str): Where to save the exported key. If :code:`None` the key is simply printed.

    Returns:
        bytes | None: If no filepath is provided the bytes of the encoded key are returned.
    """"""
<mask>:
        raise TypeError('encoder must be a subclass of KeyEncoder.')
    encoded: bytes = encoder.encode_public_key(key)
    if filepath is None:
        return encoded
    with open(filepath, 'wb') as f:
        f.write(encoded)
    return None","not isinstance(encoder, KeyEncoder)",101,"not issubclass(encoder, KeyEncoder)",False,64.34588841607616,N/A
"def import_private_key(filepath: str, decoder: KeyEncoder) -> int:
    """"""Import a private EC key.

    Args:
        |  filepath (str): The location of the key file.
        |  decoder (fastecdsa.encoding.KeyEncoder): The decoder used to parse the key.

    Returns:
        (int): A decoded private key.
    """"""
<mask>:
        raise TypeError('decoder must be a subclass of KeyEncoder.')
    with open(filepath, 'rb') as f:
        data = f.read()
    return decoder.decode_private_key(data)","not isinstance(decoder, KeyEncoder)",58,"not issubclass(decoder, KeyEncoder)",False,64.34588841607616,N/A
"def import_public_key(filepath: str, curve: Curve, decoder: KeyEncoder) -> Point:
    """"""Import a public EC key.

    Args:
        |  filepath (str): The location of the key file.
        |  decoder (fastecdsa.encoding.KeyEncoder): The decoder used to parse the key.

    Returns:
        (int): A decoded private key.
    """"""
<mask>:
        raise TypeError('curve must be an instance of the Curve type.')
    if not isinstance(decoder, KeyEncoder):
        raise TypeError('decoder must be a subclass of KeyEncoder.')
    with open(filepath, 'rb') as f:
        data = f.read()
    return decoder.decode_public_key(data, curve)","not isinstance(curve, Curve)",75,"not isinstance(curve, Curve)",True,100.00000000000004,N/A
"def __init__(self, name: str, p: int, a: int, b: int, q: int, gx: int, gy: int, oid: Optional[bytes]=None) -> None:
    """"""Initialize the parameters of an elliptic curve.

        WARNING: Do not generate your own parameters unless you know what you are doing or you could
        generate a curve severely less secure than you think. Even then, consider using a
        standardized curve for the sake of interoperability.

        Currently only curves defined via the equation :math:`y^2 \\equiv x^3 + ax + b \\pmod{p}` are
        supported.

        Args:
            |  name (string): The name of the curve
            |  p (int): The value of :math:`p` in the curve equation.
            |  a (int): The value of :math:`a` in the curve equation.
            |  b (int): The value of :math:`b` in the curve equation.
            |  q (int): The order of the base point of the curve.
            |  gx (int): The x coordinate of the base point of the curve.
            |  gy (int): The y coordinate of the base point of the curve.
            |  oid (bytes): The object identifier of the curve.
        """"""
    self.name = name
    self.p = p
    self.a = a
    self.b = b
    self.q = q
    self.gx = gx
    self.gy = gy
    self.oid = oid
<mask>:
        self._oid_lookup[oid] = self",oid is not None,200,oid is not None,True,100.00000000000004,N/A
"def __init__(self, x: int, y: int, curve: Curve) -> None:
    """"""Initialize a point on an elliptic curve.

        The x and y parameters must satisfy the equation :math:`y^2 \\equiv x^3 + ax + b \\pmod{p}`,
        where a, b, and p are attributes of the curve parameter.

        Args:
            |  x (int): The x coordinate of the point.
            |  y (int): The y coordinate of the point.
            |  curve (:class:`Curve`): The curve that the point lies on.
        """"""
<mask>:
        x = x % curve.p
        y = y % curve.p
    if not (x == 0 and y == 0 and (curve is None)) and (not curve.is_point_on_curve((x, y))):
        raise ValueError(f'coordinates are not on curve <{curve}>\n\tx={x:x}\n\ty={y:x}')
    else:
        self.x = x
        self.y = y
        self.curve = curve",curve is not None,120,curve is not None,True,100.00000000000004,N/A
"def __str__(self) -> str:
<mask>:
        return '<POINT AT INFINITY>'
    else:
        return f'X: 0x{self.x:x}\nY: 0x{self.y:x}\n(On curve <{self.curve}>)'",self._is_identity(),16,self.curve is None,False,12.975849993980741,N/A
"def __eq__(self, other: object) -> bool:
<mask>:
        raise TypeError(f'Cannot compare Point to {type(other)}')
    return self.x == other.x and self.y == other.y and (self.curve is other.curve)","not isinstance(other, Point)",25,"not isinstance(other, Point)",True,100.00000000000004,N/A
"def __add__(self, other: Point) -> Point:
    """"""Add two points on the same elliptic curve.

        Args:
            | self (:class:`Point`): a point :math:`P` on the curve
            | other (:class:`Point`): a point :math:`Q` on the curve

        Returns:
            :class:`Point`: A point :math:`R` such that :math:`R = P + Q`
        """"""
<mask>:
        raise TypeError(f'Cannot add {type(other)} to Point')
    if self._is_identity():
        return other
    elif other._is_identity():
        return self
    elif self.curve is not other.curve:
        raise CurveMismatchError(self.curve, other.curve)
    elif self == -other:
        return self._identity_element()
    else:
        x, y = curvemath.add(str(self.x), str(self.y), str(other.x), str(other.y), str(self.curve.p), str(self.curve.a), str(self.curve.b), str(self.curve.q), str(self.curve.gx), str(self.curve.gy))
        return Point(int(x), int(y), self.curve)","not isinstance(other, Point)",93,"not isinstance(other, Point)",True,100.00000000000004,N/A
"def __sub__(self, other: Point) -> Point:
    """"""Subtract two points on the same elliptic curve.

        Args:
            | self (:class:`Point`): a point :math:`P` on the curve
            | other (:class:`Point`): a point :math:`Q` on the curve

        Returns:
            :class:`Point`: A point :math:`R` such that :math:`R = P - Q`
        """"""
<mask>:
        raise TypeError(f'Cannot subtract {type(other)} from Point')
    if self == other:
        return self._identity_element()
    elif other._is_identity():
        return self
    negative = Point(other.x, -other.y % other.curve.p, other.curve)
    return self.__add__(negative)","not isinstance(other, Point)",72,"not isinstance(other, Point)",True,100.00000000000004,N/A
"def sign(msg: SignableMessage, d: int, curve: Curve=P256, hashfunc: HashFunction=sha256, prehashed: bool=False) -> EcdsaSignature:
    """"""Sign a message using the elliptic curve digital signature algorithm.

    The elliptic curve signature algorithm is described in full in FIPS 186-4 Section 6. Please
    refer to http://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.186-4.pdf for more information.

    Args:
        |  msg (str|bytes|bytearray): A message to be signed.
        |  d (int): The ECDSA private key of the signer.
        |  curve (fastecdsa.curve.Curve): The curve to be used to sign the message.
        |  hashfunc (Callable): The hash function used to compress the message.
        |  prehashed (bool): The message being passed has already been hashed by :code:`hashfunc`.

    Returns:
        (int, int): The signature (r, s) as a tuple.
    """"""
    rfc6979 = RFC6979(msg, d, curve.q, hashfunc, prehashed=prehashed)
    k = rfc6979.gen_nonce()
    ks = k + curve.q
    kt = ks + curve.q
<mask>:
        k = kt
    else:
        k = ks
    hashed = _hex_digest(msg, hashfunc, prehashed)
    r, s = _ecdsa.sign(hashed, str(d), str(k), str(curve.p), str(curve.a), str(curve.b), str(curve.q), str(curve.gx), str(curve.gy))
    return (int(r), int(s))",ks.bit_length() == curve.q.bit_length(),158,kt > k,False,0.0,N/A
"def verify(sig: EcdsaSignature, msg: SignableMessage, Q: Point, curve: Curve=P256, hashfunc: HashFunction=sha256, prehashed: bool=False) -> bool:
    """"""Verify a message signature using the elliptic curve digital signature algorithm.

    The elliptic curve signature algorithm is described in full in FIPS 186-4 Section 6. Please
    refer to http://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.186-4.pdf for more information.

    Args:
        |  sig (int, int): The signature for the message.
        |  msg (str|bytes|bytearray): A message to be signed.
        |  Q (fastecdsa.point.Point): The ECDSA public key of the signer.
        |  curve (fastecdsa.curve.Curve): The curve to be used to sign the message.
        |  hashfunc (_hashlib.HASH): The hash function used to compress the message.
        |  prehashed (bool): The message being passed has already been hashed by :code:`hashfunc`.

    Returns:
        bool: True if the signature is valid, False otherwise.

    Raises:
        fastecdsa.ecdsa.EcdsaError: If the signature or public key are invalid. Invalid signature
            in this case means that it has values less than 1 or greater than the curve order.
    """"""
    r, s = sig
<mask>:
        raise EcdsaError(f'Invalid public key, point is not on curve {curve}')
    elif r > curve.q or r < 1:
        raise EcdsaError('Invalid Signature: r is not a positive integer smaller than the curve order')
    elif s > curve.q or s < 1:
        raise EcdsaError('Invalid Signature: s is not a positive integer smaller than the curve order')
    hashed = _hex_digest(msg, hashfunc, prehashed)
    return _ecdsa.verify(str(r), str(s), hashed, str(Q.x), str(Q.y), str(curve.p), str(curve.a), str(curve.b), str(curve.q), str(curve.gx), str(curve.gy))","not curve.is_point_on_curve((Q.x, Q.y))",227,r < curve.p or s < 1,False,2.513507851933498,N/A
"def _hex_digest(msg: SignableMessage, hashfunc: HashFunction, prehashed: bool) -> str:
<mask>:
        if not isinstance(msg, (bytes, bytearray)):
            raise TypeError(f'Prehashed message must be bytes, got {type(msg)}')
        return hexlify(msg).decode()
    else:
        return hashfunc(msg_bytes(msg)).hexdigest()",prehashed,28,prehashed,True,100.00000000000004,N/A
"def _parse_asn1_structure(self, data: bytes) -> None:
    """"""Recursively parse ASN.1 data""""""
    data_type = data[:1]
    _, data, remaining = parse_asn1_length(data[1:])
<mask>:
        self.asn1_parsed_data.append((data_type, data))
    elif data_type in [SEQUENCE, PUBLIC_KEY, PARAMETERS]:
        self._parse_asn1_structure(data)
    if remaining:
        self._parse_asn1_structure(remaining)","data_type in [OCTET_STRING, BIT_STRING, OBJECT_IDENTIFIER]",31,"data_type in [SEQUENCE, PUBLIC_KEY, PARAMETERS]",False,24.623798326813905,N/A
"def decode_public_key(self, pemdata: bytes, curve: Curve) -> Point:
    """"""Decode a PEM encoded public key as described in
        `RFC 5480 <https://tools.ietf.org/html/rfc5480>`_.

        Args:
            pemdata (bytes): A sequence of bytes representing an encoded EC key.

        Returns:
            (long, fastecdsa.point.Point): A private key, public key tuple. If the encoded key was a
            public key the first entry in the tuple is None.
        """"""
    parsed = self._parse_ascii_armored_base64(pemdata)
    self._parse_asn1_structure(parsed)
    x, y = (None, None)
    for value_type, value in self.asn1_parsed_data:
<mask>:
            encoded_curve = Curve.get_curve_by_oid(value)
            if encoded_curve is not None:
                encoded_curve = curve
        elif value_type == BIT_STRING:
            value = value[2:]
            x = int.from_bytes(value[:len(value) // 2], 'big')
            y = int.from_bytes(value[len(value) // 2:], 'big')
    self.asn1_parsed_data = []
    if curve is None or x is None or y is None:
        raise PEMEncoderError(f'Could not parse public key. x={x!r}, y={y!r}, curve={curve!r}')
    return Point(x, y, curve)",value_type == OBJECT_IDENTIFIER,131,value_type == BIT_CURVE,False,54.10822690539397,N/A
"def decode_private_key(self, pemdata: bytes) -> int:
    """"""Decode a PEM encoded EC private key as described in
        `RFC 5915 <https://tools.ietf.org/html/rfc5915.html>`_.

        Args:
            pemdata (bytes): A sequence of bytes representing an encoded EC key.

        Returns:
            int: The private key.
        """"""
    parsed = self._parse_ascii_armored_base64(pemdata)
    self._parse_asn1_structure(parsed)
    d = None
    for value_type, value in self.asn1_parsed_data:
<mask>:
            d = int(hexlify(value), 16)
    self.asn1_parsed_data = []
    if d is None:
        raise PEMEncoderError('Could not parse private key.')
    return d",value_type == OCTET_STRING,69,value_type == 'EC',False,54.44460596606694,N/A
"def int_to_bytes(x: int, length: Optional[int]=None) -> bytes:
<mask>:
        length = int_bytelen(x)
    return int.to_bytes(x, length, 'big')",length is None,15,length is None,True,100.00000000000004,N/A
"@staticmethod
def encode_public_key(point: Point, compressed: bool=True) -> bytes:
    """"""Encode a public key as described in http://www.secg.org/SEC1-Ver-1.0.pdf
            in sections 2.3.3/2.3.4
                uncompressed:   04 + x_bytes + y_bytes
                compressed:     02 or 03 + x_bytes
        Args:
            point (fastecdsa.point.Point): Public key to encode
            compressed (bool): Set to False if you want an uncompressed format

        Returns:
            bytes: The SEC1 encoded public key
        """"""
    bytelen = int_bytelen(point.curve.q)
<mask>:
        if point.y & 1:
            return b'\x03' + int_to_bytes(point.x, bytelen)
        else:
            return b'\x02' + int_to_bytes(point.x, bytelen)
    return b'\x04' + int_to_bytes(point.x, bytelen) + int_to_bytes(point.y, bytelen)",compressed,84,compressed,True,100.00000000000004,N/A
"@staticmethod
def decode_public_key(key: bytes, curve: Curve) -> Point:
    """"""Decode a public key as described in http://www.secg.org/SEC1-Ver-1.0.pdf
            in sections 2.3.3/2.3.4

                uncompressed:   04 + x_bytes + y_bytes
                compressed:     02 or 03 + x_bytes

        Args:
            key (bytes): public key encoded using the SEC1 format
            curve (fastecdsa.curve.Curve): Curve to use when decoding the public key

        Returns:
            Point: The decoded public key

        Raises:
            InvalidSEC1PublicKey
        """"""
    bytelen = int_bytelen(curve.q)
<mask>:
        if len(key) != bytelen * 2 + 1:
            raise InvalidSEC1PublicKey('An uncompressed public key must be %d bytes long' % (bytelen * 2 + 1))
        x, y = (bytes_to_int(key[1:bytelen + 1]), bytes_to_int(key[bytelen + 1:]))
    else:
        if len(key) != bytelen + 1:
            raise InvalidSEC1PublicKey('A compressed public key must be %d bytes long' % (bytelen + 1))
        x = bytes_to_int(key[1:])
        root = mod_sqrt(curve.evaluate(x), curve.p)[0]
        if key.startswith(b'\x03'):
            y = root if root % 2 == 1 else -root % curve.p
        elif key.startswith(b'\x02'):
            y = root if root % 2 == 0 else -root % curve.p
        else:
            raise InvalidSEC1PublicKey('Wrong key format')
    return Point(x, y, curve=curve)",key.startswith(b'\x04'),164,bytelen > 0,False,0.0,N/A
"@staticmethod
def encode_signature(r: int, s: int) -> bytes:
    """"""Encode an EC signature in serialized DER format as described in
           https://tools.ietf.org/html/rfc2459 (section 7.2.2) and as detailed by
           bip-0066

        Args:
            r, s

        Returns:
            bytes: The DER encoded signature

        """"""
    r_bytes = int_to_bytes(r)
<mask>:
        r_bytes = b'\x00' + r_bytes
    s_bytes = int_to_bytes(s)
    if s_bytes[0] & 128:
        s_bytes = b'\x00' + s_bytes
    r_asn1 = asn1_structure(INTEGER, r_bytes)
    s_asn1 = asn1_structure(INTEGER, s_bytes)
    return asn1_structure(SEQUENCE, r_asn1 + s_asn1)",r_bytes[0] & 128,71,r_bytes[0] & 128,True,100.00000000000004,N/A
"@staticmethod
def decode_signature(sig: bytes) -> Tuple[int, int]:
    """"""Decode an EC signature from serialized DER format as described in
        https://tools.ietf.org/html/rfc2459 (section 7.2.2) and as detailed by
        bip-0066

        Returns (r,s)
        """"""

    def _validate_int_bytes(data: bytes) -> None:
<mask>:
            raise InvalidDerSignature('Signature contains a negative value')
        if data[0] == 0 and (not data[1] & 128):
            raise InvalidDerSignature('Invalid leading 0x00 byte in ASN.1 integer')
    if not sig or sig[0] != ord(SEQUENCE):
        raise InvalidDerSignature('First byte should be ASN.1 SEQUENCE')
    try:
        seqlen, sequence, leftover = parse_asn1_length(sig[1:])
    except ASN1EncodingError as asn1_error:
        raise InvalidDerSignature(asn1_error)
    if leftover:
        raise InvalidDerSignature(f'Expected a sequence of {seqlen} bytes, got {len(sequence + leftover)}')
    try:
        rlen, r, sdata = parse_asn1_int(sequence)
        slen, s, _ = parse_asn1_int(sdata)
    except ASN1EncodingError as asn1_error:
        raise InvalidDerSignature(asn1_error)
    _validate_int_bytes(r)
    _validate_int_bytes(s)
    return (bytes_to_int(r), bytes_to_int(s))",data[0] & 128,119,data[0] < 0 or data[1] < 0,False,19.338531381761726,N/A
"def _asn1_len(data: bytes) -> bytes:
    dlen = len(data)
<mask>:
        return pack('=B', dlen)
    else:
        encoded = b''
        while dlen:
            len_byte = pack('=B', dlen & 255)
            encoded = len_byte + encoded
            dlen >>= 8
        return pack('=B', 128 | len(encoded)) + encoded",dlen < 128,39,dlen < 128,True,100.00000000000004,N/A
"def asn1_oid(curve: Curve) -> bytes:
    oid_bytes = curve.oid
<mask>:
        return b''
    return asn1_structure(OBJECT_IDENTIFIER, oid_bytes)",oid_bytes is None,14,oid_bytes is None,True,100.00000000000004,N/A
"def parse_asn1_length(data: bytes) -> Tuple[int, bytes, bytes]:
    """"""
    Parse an ASN.1 encoded structure.

    Args:
        data (bytes): A sequence of bytes representing an ASN.1 encoded structure

    Returns:
        (int, bytes, bytes): A tuple of the integer length in bytes, the byte representation of the integer,
                             and the remaining bytes after the integer bytes in the sequence
    """"""
    initial_byte, = unpack('=B', data[:1])
    data = data[1:]
<mask>:
        length = initial_byte
    else:
        count = initial_byte & 127
        fmt = {1: '=B', 2: '=H', 3: '=L', 4: '=L', 5: '=Q', 6: '=Q', 7: '=Q', 8: '=Q'}
        zero_padding = b'\x00' * ((1 << count.bit_length() - 1) - count)
        length, = unpack(fmt[count], zero_padding + data[:count])
        data = data[count:]
    if length > len(data):
        raise ASN1EncodingError(f'Parsed length of ASN.1 structure to be {length} bytes but only {len(data)} bytesremain in the provided data')
    return (length, data[:length], data[length:])",not initial_byte & 128,136,initial_byte & 128 == 0,False,51.697315395717055,N/A
"def parse_asn1_int(data: bytes) -> Tuple[int, bytes, bytes]:
    """"""
    Parse an ASN.1 encoded integer.

    Args:
        data (bytes): A sequence of bytes whose start is an ASN.1 integer encoding

    Returns:
        (int, bytes, bytes): A tuple of the integer length in bytes, the byte representation of the integer,
                             and the remaining bytes after the integer bytes in the sequence
    """"""
<mask>:
        raise ASN1EncodingError('ASN.1 encoded integer must be at least 3 bytes long')
    if data[0] != ord(INTEGER):
        raise ASN1EncodingError('Value should be a ASN.1 INTEGER')
    length, data, remaining = parse_asn1_length(data[1:])
    if length != len(data):
        raise ASN1EncodingError(f'Expected ASN.1 INTEGER to be {length} bytes, got {len(data)} bytes')
    return (length, data, remaining)",len(data) < 3,104,len(data) < 3,True,100.00000000000004,N/A
"def test_W25519_arith(self):
    subgroup = [(0, 0), (19624287790469256669057814461137428606839005560001276145469620721820115669041, 25869741026945134960544184956460972567356779614910045322022475500191642319642), (19298681539552699237261830834781317975544997444273427339909597334652188435538, 9094040566125962849133224048217411091405536248825867518642941381412595940312), (784994156384216107199399111990385161439916830893843497063691184659069321411, 47389623381099381275796781267935282128369626952426857267179501978497824351671), (19298681539552699237261830834781317975544997444273427339909597334652188435537, 0), (784994156384216107199399111990385161439916830893843497063691184659069321411, 10506421237558716435988711236408671798265365380393424752549290025458740468278), (19298681539552699237261830834781317975544997444273427339909597334652188435538, 48802004052532134862652268456126542835229456083994414501085850622543968879637), (19624287790469256669057814461137428606839005560001276145469620721820115669041, 32026303591712962751241307547882981359278212717910236697706316503764922500307)]
    S = Point(subgroup[1][0], subgroup[1][1], curve=W25519)
    m = randint(1, S.curve.q - 1)
    for i in range(2 * len(subgroup)):
        R = (m + i) * S
        idx = (m + i) % len(subgroup)
<mask>:
            expected = Point._identity_element()
        else:
            expected = Point(subgroup[idx][0], subgroup[idx][1], curve=S.curve)
        self.assertEqual(R, expected)
    S = Point(subgroup[4][0], subgroup[4][1], curve=S.curve)
    R = S + S
    self.assertEqual(R, Point._identity_element())",idx == 0,75,len(subgroup) == 1,False,13.134549472120788,N/A
"def test_W448_arith(self):
    subgroup = [(0, 0), (484559149530404593699549205258669689569094240458212040187660132787074885444487181790930922465784363953392589641229091574035665345629067, 197888467295464439538354009753858038256835152591059802148199779196087404232002515713604263127793030747855424464185691766453844835192428), (484559149530404593699549205258669689569094240458212040187660132787074885444487181790930922465784363953392589641229091574035665345629068, 0), (484559149530404593699549205258669689569094240458212040187660132787074885444487181790930922465784363953392589641229091574035665345629067, 528950257000142451010969798134146496096806208096258258133290419984524923934728256972792120570883515182233459997657945594599653183173011)]
    S = Point(subgroup[1][0], subgroup[1][1], curve=W448)
    m = randint(1, S.curve.q - 1)
    for i in range(2 * len(subgroup)):
        R = (m + i) * S
        idx = (m + i) % len(subgroup)
<mask>:
            expected = Point._identity_element()
        else:
            expected = Point(subgroup[idx][0], subgroup[idx][1], curve=S.curve)
        self.assertEqual(R, expected)
    S = Point(subgroup[2][0], subgroup[2][1], curve=S.curve)
    R = S + S
    self.assertEqual(R, Point._identity_element())",idx == 0,67,idx == 0,True,100.00000000000004,N/A
"def test_empty_ref_empty_hyp():
    for i in range(10):
<mask>:
            ref = ''
            hyp = ''
        else:
            ref = [''] * i
            hyp = [''] * i
        out = jiwer.process_words(reference=ref, hypothesis=hyp)
        assert out.hits == 0
        assert out.deletions == 0
        assert out.insertions == 0
        assert out.substitutions == 0
        assert out.wer == 0
        assert out.mer == 0
        assert out.wip == 1
        assert out.wil == 0",i == 0,60,i == 0,True,100.00000000000004,N/A
"def _apply_test_on(self, cases):
    for ref, hyp, correct_cer in cases:
        cer = jiwer.cer(reference=ref, hypothesis=hyp)
        self.assertTrue(isinstance(cer, float))
<mask>:
            self.assertAlmostEqual(cer, correct_cer, delta=1e-16)","isinstance(cer, float)",19,correct_cer is not None,False,8.116697886877475,N/A
"def __call__(self, sentences: Union[str, List[str]]):
    """"""
        Transforms one or more strings.

        Args:
            sentences: The strings to transform.

        Returns:
            (Union[str, List[str]]): The transformed strings.

        """"""
<mask>:
        return self.process_string(sentences)
    elif isinstance(sentences, list):
        return self.process_list(sentences)
    else:
        raise ValueError('input {} was expected to be a string or list of strings'.format(sentences))","isinstance(sentences, str)",46,"isinstance(sentences, str)",True,100.00000000000004,N/A
"def process_list(self, inp: List[str]):
    sentence_collection = []
    for sentence in inp:
        list_of_words = self.process_string(sentence)[0]
        sentence_collection.append(list_of_words)
<mask>:
        return [[]]
    return sentence_collection",len(sentence_collection) == 0,20,len(sentence_collection) == 0,True,100.00000000000004,N/A
"def process_list(self, inp: List[str]):
    filtered_inp = [i for i in inp if len(i) >= 1]
<mask>:
        return []
    else:
        return ['{}'.format(self.word_delimiter).join(filtered_inp)]",len(filtered_inp) == 0,21,not filtered_inp,False,17.035677145427364,N/A
"def __init__(self, replace_by_space: bool=False):
    """"""

        Args:
            replace_by_space: every white space character is replaced with a space (` `)
        """"""
    characters = [c for c in string.whitespace]
<mask>:
        replace_token = ' '
    else:
        replace_token = ''
    super().__init__(characters, replace_token=replace_token)",replace_by_space,37,replace_by_space,True,100.00000000000004,N/A
"def _construct_comparison_string(reference: List[str], hypothesis: List[str], ops: List[AlignmentChunk], include_space_seperator: bool=False, line_width: Optional[int]=None) -> str:
    ref_str = 'REF: '
    hyp_str = 'HYP: '
    op_str = '     '
    agg_str = ''
    for op in ops:
<mask>:
            ref = reference[op.ref_start_idx:op.ref_end_idx]
            hyp = hypothesis[op.hyp_start_idx:op.hyp_end_idx]
            op_char = ' ' if op.type == 'equal' else 's'
        elif op.type == 'delete':
            ref = reference[op.ref_start_idx:op.ref_end_idx]
            hyp = ['*' for _ in range(len(ref))]
            op_char = 'd'
        elif op.type == 'insert':
            hyp = hypothesis[op.hyp_start_idx:op.hyp_end_idx]
            ref = ['*' for _ in range(len(hyp))]
            op_char = 'i'
        else:
            raise ValueError(f'unparseable op name={op.type}')
        op_chars = [op_char for _ in range(len(ref))]
        for rf, hp, c in zip(ref, hyp, op_chars):
            str_len = max(len(rf), len(hp), len(c))
            if line_width is not None:
                if len(ref_str) + str_len > line_width:
                    if include_space_seperator:
                        agg_str += f'{ref_str[:-1]}\n{hyp_str[:-1]}\n{op_str[:-1]}\n\n'
                    else:
                        agg_str += f'{ref_str}\n{hyp_str}\n{op_str}\n\n'
                    ref_str = 'REF: '
                    hyp_str = 'HYP: '
                    op_str = '     '
            if rf == '*':
                rf = ''.join(['*'] * str_len)
            elif hp == '*':
                hp = ''.join(['*'] * str_len)
            ref_str += f'{rf:>{str_len}}'
            hyp_str += f'{hp:>{str_len}}'
            op_str += f'{c.upper():>{str_len}}'
            if include_space_seperator:
                ref_str += ' '
                hyp_str += ' '
                op_str += ' '
    if include_space_seperator:
        return agg_str + f'{ref_str[:-1]}\n{hyp_str[:-1]}\n{op_str[:-1]}\n'
    else:
        return agg_str + f'{ref_str}\n{hyp_str}\n{op_str}\n'",op.type == 'equal' or op.type == 'substitute',192,op.type == 'search',False,23.661528215726477,N/A
"def collect_error_counts(output: Union[WordOutput, CharacterOutput]):
    """"""
    Retrieve three dictionaries, which count the frequency of how often
    each word or character was substituted, inserted, or deleted.
    The substitution dictionary has, as keys, a 2-tuple (from, to).
    The other two dictionaries have the inserted/deleted words or characters as keys.

    Args:
        output: The processed output of reference and hypothesis pair(s).

    Returns:
        (Tuple[dict, dict, dict]): A three-tuple of dictionaries, in the order substitutions, insertions, deletions.
    """"""
    substitutions = defaultdict(lambda: 0)
    insertions = defaultdict(lambda: 0)
    deletions = defaultdict(lambda: 0)
    for idx, sentence_chunks in enumerate(output.alignments):
        ref = output.references[idx]
        hyp = output.hypotheses[idx]
        sep = ' ' if isinstance(output, WordOutput) else ''
        for chunk in sentence_chunks:
<mask>:
                inserted = sep.join(hyp[chunk.hyp_start_idx:chunk.hyp_end_idx])
                insertions[inserted] += 1
            if chunk.type == 'delete':
                deleted = sep.join(ref[chunk.ref_start_idx:chunk.ref_end_idx])
                deletions[deleted] += 1
            if chunk.type == 'substitute':
                replaced = sep.join(ref[chunk.ref_start_idx:chunk.ref_end_idx])
                by = sep.join(hyp[chunk.hyp_start_idx:chunk.hyp_end_idx])
                substitutions[replaced, by] += 1
    return (substitutions, insertions, deletions)",chunk.type == 'insert',142,chunk.type == 'insert',True,100.00000000000004,N/A
"@click.command()
@click.option('-r', '--reference', 'reference_file', type=pathlib.Path, required=True, help='Path to new-line delimited text file of reference sentences.')
@click.option('-h', '--hypothesis', 'hypothesis_file', type=pathlib.Path, required=True, help='Path to new-line delimited text file of hypothesis sentences.')
@click.option('--cer', '-c', 'compute_cer', is_flag=True, default=False, help='Compute CER instead of WER.')
@click.option('--align', '-a', 'show_alignment', is_flag=True, default=False, help='Print alignment of each sentence.')
@click.option('--global', '-g', 'global_alignment', is_flag=True, default=False, help='Apply a global minimal alignment between reference and hypothesis sentences before computing the WER.')
def cli(reference_file: pathlib.Path, hypothesis_file: pathlib.Path, compute_cer: bool, show_alignment: bool, global_alignment: bool):
    """"""
    JiWER is a python tool for computing the word-error-rate of ASR systems. To use
    this CLI, store the reference and hypothesis sentences in a text file, where
    each sentence is delimited by a new-line character.
    The text files are expected to have an equal number of lines, unless the `-g` flag
    is used. The `-g` flag joins computation of the WER by doing a global minimal
    alignment.

    """"""
    with reference_file.open('r') as f:
        reference_sentences = [ln.strip() for ln in f.readlines() if len(ln.strip()) > 1]
    with hypothesis_file.open('r') as f:
        hypothesis_sentences = [ln.strip() for ln in f.readlines() if len(ln.strip()) > 1]
<mask>:
        raise ValueError(f""Number of reference sentences ({len(reference_sentences)} in '{reference_file}') and hypothesis sentences ({len(hypothesis_sentences)} in '{hypothesis_file}') do not match! Use the `--global` flag to compute the measures over a global alignment of the reference and hypothesis sentences."")
    if compute_cer:
        if global_alignment:
            out = jiwer.process_characters(reference_sentences, hypothesis_sentences, reference_transform=jiwer.cer_contiguous, hypothesis_transform=jiwer.cer_contiguous)
        else:
            out = jiwer.process_characters(reference_sentences, hypothesis_sentences)
    elif global_alignment:
        out = jiwer.process_words(reference_sentences, hypothesis_sentences, reference_transform=jiwer.wer_contiguous, hypothesis_transform=jiwer.wer_contiguous)
    else:
        out = jiwer.process_words(reference_sentences, hypothesis_sentences)
    if show_alignment:
        print(jiwer.visualize_alignment(out, show_measures=True), end='')
    elif compute_cer:
        print(out.cer)
    else:
        print(out.wer)",not global_alignment and len(reference_sentences) != len(hypothesis_sentences),252,len(reference_sentences) != len(hypothesis_sentences),False,69.96725373751305,N/A
"def __post_init__(self):
<mask>:
        raise ValueError('')
    if self.type == 'replace':
        self.type = 'substitute'
    if self.ref_start_idx > self.ref_end_idx:
        raise ValueError(f'ref_start_idx={self.ref_start_idx} is larger than ref_end_idx={self.ref_end_idx}')
    if self.hyp_start_idx > self.hyp_end_idx:
        raise ValueError(f'hyp_start_idx={self.hyp_start_idx} is larger than hyp_end_idx={self.hyp_end_idx}')","self.type not in ['replace', 'insert', 'delete', 'equal', 'substitute']",32,"self.type not in ['replace', 'hyp_start_idx']",False,45.91050315190768,N/A
"def _apply_transform(sentence: Union[str, List[str]], transform: Union[tr.Compose, tr.AbstractTransform], is_reference: bool):
    transformed_sentence = transform(sentence)
<mask>:
        raise ValueError(f""After applying the transformation, each {('reference' if is_reference else 'hypothesis')} should be a list of strings, with each string being a single word or character.Please ensure the given transformation reduces the input to a list of list strings."")
    return transformed_sentence",not _is_list_of_list_of_strings(transformed_sentence),54,"not isinstance(transformed_sentence, str) or len(transformed_sentence) != 1",False,20.614773521563748,N/A
"def _is_list_of_list_of_strings(x: Any):
<mask>:
        return False
    for e in x:
        if not isinstance(e, list):
            return False
        if not all([isinstance(s, str) for s in e]):
            return False
    return True","not isinstance(x, list)",28,"not isinstance(x, (tuple, list))",False,45.384078730076126,N/A
"def __init__(self, model):
<mask>:
        return
    self.patched_model = model
    if issubclass(model, Page):
        self._patch_page_models(model)
    else:
        self._patch_other_models(model)
    WagtailTranslator._patched_models.append(model)",model in WagtailTranslator._patched_models,15,model is WagtailTranslator,False,6.5479514338598115,N/A
"def _patch_fields(self, model):
    translation_registered_fields = translator.get_options_for_model(model).all_fields
    model_fields = model._meta.get_fields()
    for field in model_fields:
<mask>:
            descriptor = getattr(model, field.name)
            _patch_stream_field_meaningful_value(descriptor)","isinstance(field, StreamField) and field.name in translation_registered_fields",19,field.name in translation_registered_fields,False,45.94258240359268,N/A
"def _patch_page_models(self, model):
<mask>:
        tabs = model.edit_handler.children
        for tab in tabs:
            tab.children = self._patch_panels(tab.children)
    else:
        if hasattr(model, 'content_panels'):
            model.content_panels = self._patch_panels(model.content_panels)
        if hasattr(model, 'promote_panels'):
            model.promote_panels = self._patch_panels(model.promote_panels)
        if hasattr(model, 'settings_panels'):
            model.settings_panels = self._patch_panels(model.settings_panels)
    model.get_edit_handler.cache_clear()
    translation_registered_fields = translator.get_options_for_model(model).all_fields
    for field in model.search_fields:
        if isinstance(field, SearchField) and field.field_name in translation_registered_fields:
            for language in mt_settings.AVAILABLE_LANGUAGES:
                translated_field = copy.deepcopy(field)
                translated_field.field_name = build_localized_fieldname(field.field_name, language)
                model.search_fields = list(model.search_fields) + [translated_field]
    self._patch_fields(model)
    model.base_form_class = patch_admin_page_form(model.base_form_class)
    if TRANSLATE_SLUGS:
        model.set_url_path = _new_set_url_path
        model.route = _new_route
        model._update_descendant_url_paths = _new_update_descendant_url_paths
        if not hasattr(model, '_get_site_root_paths'):
            model.get_url_parts = _new_get_url_parts
        model._get_site_root_paths = _new_get_site_root_paths
        _patch_clean(model)
        if not model.save.__name__.startswith('localized'):
            setattr(model, 'save', LocalizedSaveDescriptor(model.save))","hasattr(model, 'edit_handler')",96,"hasattr(model, 'edit_handler')",True,100.00000000000004,N/A
"def _patch_other_models(self, model):
    self._patch_fields(model)
<mask>:
        edit_handler = model.edit_handler
        for tab in edit_handler.children:
            tab.children = self._patch_panels(tab.children)
    elif hasattr(model, 'panels'):
        model.panels = self._patch_panels(model.panels)
    elif hasattr(model, 'snippet_viewset'):
        edit_handler = model.snippet_viewset.get_edit_handler()
        if isinstance(edit_handler, ObjectList):
            edit_handler.children = self._patch_ObjectList(edit_handler.children, model)
            model.edit_handler = edit_handler.children.bind_to_model(model=model)
        else:
            for tab in edit_handler.children:
                tab.children = self._patch_panels(tab.children)
            model.snippet_viewset.edit_handler = edit_handler.bind_to_model(model)
    else:
        panels = extract_panel_definitions_from_model_class(model)
        edit_handler = self._patch_ObjectList(panels, model)
        model.edit_handler = edit_handler.bind_to_model(model=model)","hasattr(model, 'edit_handler')",59,"hasattr(model, 'edit_handler')",True,100.00000000000004,N/A
"def _patch_panels(self, panels_list, related_model=None):
    """"""
        Patching of the admin panels. If we're patching an InlinePanel panels we must provide
         the related model for that class, otherwise its used the model passed on init.
        """"""
    patched_panels = []
    current_patching_model = related_model or self.patched_model
    for panel in panels_list:
<mask>:
            patched_panels += self._patch_simple_panel(current_patching_model, panel)
        elif panel.__class__ in COMPOSED_PANEL_CLASSES:
            patched_panels.append(self._patch_composed_panel(panel, related_model))
        elif panel.__class__ in INLINE_PANEL_CLASSES:
            patched_panels.append(self._patch_inline_panel(current_patching_model, panel))
        else:
            patched_panels.append(panel)
    return patched_panels",panel.__class__ in SIMPLE_PANEL_CLASSES,67,panel.__class__ in SIMPLE_PANEL_CLASSES,True,100.00000000000004,N/A
"@hooks.register('insert_editor_js')
def translation_settings():
    lang_codes = []
    for lang in settings.LANGUAGES:
        lang_codes.append(""'%s'"" % lang[0])
<mask>:
        locale_picker_default = ', '.join((f""'{v}'"" for v in wmt_settings.LOCALE_PICKER_DEFAULT))
    else:
        locale_picker_default = f""'{mt_settings.DEFAULT_LANGUAGE}'""
    js_languages = ""\n    <script>\n        wagtailModelTranslations = {{\n            languages: [{languages}],\n            defaultLanguage: '{language_code}',\n            viewEditString: '{view_edit_string}',\n            translate_slugs: {translate_slugs},\n            locale_picker_default: [{locale_picker_default}],\n            locale_picker_restore: {locale_picker_restore}\n        }};\n    </script>\n    "".format(languages=', '.join(lang_codes), language_code=mt_settings.DEFAULT_LANGUAGE, view_edit_string=_('View / edit fields for'), translate_slugs='true' if wmt_settings.TRANSLATE_SLUGS else 'false', locale_picker_default=locale_picker_default, locale_picker_restore='true' if wmt_settings.LOCALE_PICKER_RESTORE else 'false')
    return js_languages",wmt_settings.LOCALE_PICKER_DEFAULT is not None,68,wmt_settings.LOCALE_PICKER_DEFAULT,False,71.65313105737896,N/A
"@csrf_exempt
def return_translation_target_field_rendered_html(request, page_id):
    """"""
    Ajax view that allows to duplicate content
    between translated streamfields
    """"""
    page = Page.objects.get(pk=page_id)
<mask>:
        origin_field_name = request.POST.get('origin_field_name')
        target_field_name = request.POST.get('target_field_name')
        origin_field_serialized = json.loads(request.POST.get('serializedOriginField'))
        target_field_patched = []
        for item in origin_field_serialized:
            patched_item = {'name': None, 'value': None}
            for att in iteritems(item):
                target_value = att[1]
                if att[0] == 'name':
                    target_value = att[1].replace(origin_field_name, target_field_name)
                    patched_item['name'] = target_value
                else:
                    patched_item['value'] = att[1]
            target_field_patched.append(patched_item)
        q_data = QueryDict('', mutable=True)
        for item in target_field_patched:
            q_data.update({item['name']: item['value']})
        target_field = page.specific._meta.get_field(target_field_name)
        value_data = target_field.stream_block.value_from_datadict(q_data, {}, target_field_name)
        target_field_content_html = target_field.formfield().widget.render(target_field_name, value_data)
    return HttpResponse(json.dumps(target_field_content_html), content_type='application/json')",request.headers.get('X-Requested-With') == 'XMLHttpRequest',90,request.method == 'POST',False,9.977283359412501,N/A
"@hooks.register('register_rich_text_link_handler')
def register_localized_page_link_handler():

    class LocalizedPageLinkHandler(PageLinkHandler):

        @staticmethod
        def expand_db_attributes(attrs, for_editor):
            try:
                page = Page.objects.get(id=attrs['id'])
<mask>:
                    editor_attrs = 'data-linktype=""page"" data-id=""%d"" ' % page.id
                    parent_page = page.get_parent()
                    if parent_page:
                        editor_attrs += 'data-parent-id=""%d"" ' % parent_page.id
                else:
                    editor_attrs = ''
                return '<a %shref=""%s"">' % (editor_attrs, escape(page.specific.url))
            except Page.DoesNotExist:
                return '<a>'
    return ('page', LocalizedPageLinkHandler)",for_editor,49,for_editor,True,100.00000000000004,N/A
"@hooks.register('before_copy_page')
def before_copy_page(request, page):
    parent_page = page.get_parent()
    can_publish = parent_page.permissions_for_user(request.user).can_publish_subpage()
    form = PatchedCopyForm(request.POST or None, user=request.user, page=page, can_publish=can_publish)
    next_url = get_valid_next_url_from_request(request)
<mask>:
        parent_page = Page.objects.get(id=request.POST['new_parent_page'])
        if form.is_valid():
            if form.cleaned_data['new_parent_page']:
                parent_page = form.cleaned_data['new_parent_page']
            if not page.permissions_for_user(request.user).can_copy_to(parent_page, form.cleaned_data.get('copy_subpages')):
                raise PermissionDenied
            can_publish = parent_page.permissions_for_user(request.user).can_publish_subpage()
            update_attrs = {}
            for code, name in settings.LANGUAGES:
                if wmt_settings.TRANSLATE_SLUGS:
                    slug = build_localized_fieldname('slug', code)
                else:
                    slug = 'slug'
                title = build_localized_fieldname('title', code)
                update_attrs[slug] = form.cleaned_data['new_{}'.format(slug)]
                update_attrs[title] = form.cleaned_data['new_{}'.format(title)]
            new_page = page.copy(recursive=form.cleaned_data.get('copy_subpages'), to=parent_page, update_attrs=update_attrs, keep_live=can_publish and form.cleaned_data.get('publish_copies'), user=request.user)
            if form.cleaned_data.get('copy_subpages'):
                messages.success(request, _(""Page '{0}' and {1} subpages copied."").format(page.get_admin_display_title(), new_page.get_descendants().count()))
            else:
                messages.success(request, _(""Page '{0}' copied."").format(page.get_admin_display_title()))
            for fn in hooks.get_hooks('after_copy_page'):
                result = fn(request, page, new_page)
                if hasattr(result, 'status_code'):
                    return result
            if next_url:
                return redirect(next_url)
            return redirect('wagtailadmin_explore', parent_page.id)
    return render(request, 'modeltranslation_copy.html', {'page': page, 'form': form, 'next': next_url})",request.method == 'POST',123,request.method == 'POST',True,100.00000000000004,N/A
"def __init__(self, *args, **kwargs):
    self.page = kwargs.pop('page')
    self.user = kwargs.pop('user', None)
    can_publish = kwargs.pop('can_publish')
    super(CopyForm, self).__init__(*args, **kwargs)
    for code, name in settings.LANGUAGES:
        localized_fieldname = build_localized_fieldname('title', code)
        localized_field = Page._meta.get_field(localized_fieldname)
        locale_title = 'new_{}'.format(localized_fieldname)
        locale_label = '{} [{}]'.format(_('New title'), code)
        self.fields[locale_title] = forms.CharField(initial=getattr(self.page, localized_fieldname), label=locale_label, required=not localized_field.blank)
<mask>:
        for code, name in settings.LANGUAGES:
            localized_fieldname = build_localized_fieldname('slug', code)
            localized_field = Page._meta.get_field(localized_fieldname)
            locale_title = 'new_{}'.format(localized_fieldname)
            locale_label = '{} [{}]'.format(_('New slug'), code)
            self.fields[locale_title] = forms.SlugField(initial=getattr(self.page, localized_fieldname), label=locale_label, required=not localized_field.blank)
    else:
        self.fields['new_slug'] = forms.SlugField(initial=self.page.slug, label=_('New slug'))
    self.fields['new_parent_page'] = forms.ModelChoiceField(initial=self.page.get_parent(), queryset=Page.objects.all(), widget=widgets.AdminPageChooser(can_choose_root=True, user_perms='copy_to'), label=_('New parent page'), help_text=_('This copy will be a child of this given parent page.'))
    pages_to_copy = self.page.get_descendants(inclusive=True)
    subpage_count = pages_to_copy.count() - 1
    if subpage_count > 0:
        self.fields['copy_subpages'] = forms.BooleanField(required=False, initial=True, label=_('Copy subpages'), help_text=ngettext('This will copy %(count)s subpage.', 'This will copy %(count)s subpages.', subpage_count) % {'count': subpage_count})
    if can_publish:
        pages_to_publish_count = pages_to_copy.live().count()
        if pages_to_publish_count > 0:
            if subpage_count == 0:
                label = _('Publish copied page')
                help_text = _('This page is live. Would you like to publish its copy as well?')
            else:
                label = _('Publish copies')
                help_text = ngettext('%(count)s of the pages being copied is live. Would you like to publish its copy?', '%(count)s of the pages being copied are live. Would you like to publish their copies?', pages_to_publish_count) % {'count': pages_to_publish_count}
            self.fields['publish_copies'] = forms.BooleanField(required=False, initial=True, label=label, help_text=help_text)",wmt_settings.TRANSLATE_SLUGS,212,can_publish,False,7.253154775624655,N/A
"def clean(self):
    cleaned_data = super(CopyForm, self).clean()
    parent_page = cleaned_data.get('new_parent_page') or self.page.get_parent()
<mask>:
        raise ValidationError({'new_parent_page': _('You do not have permission to copy to page ""%(page_title)s""') % {'page_title': parent_page.get_admin_display_title()}})
    current_lang = get_language()
    slug_errors = {}
    for code, name in settings.LANGUAGES:
        locale_slug = 'new_{}'.format(build_localized_fieldname('slug', code))
        slug = cleaned_data.get(locale_slug)
        activate(code)
        children_slugs = [child.slug for child in parent_page.get_children()]
        if slug and slug in children_slugs:
            slug_errors[locale_slug] = _('This slug is already in use within the context of its parent page ""%s""' % parent_page)
    activate(current_lang)
    if slug_errors:
        raise ValidationError(slug_errors)
    if cleaned_data.get('copy_subpages') and (self.page == parent_page or parent_page.is_descendant_of(self.page)):
        raise ValidationError({'new_parent_page': _('You cannot copy a page into itself when copying subpages')})
    return cleaned_data",not parent_page.permissions_for_user(self.user).can_add_subpage(),104,not settings.COPY_TO_PAGE,False,1.197679263629884,N/A
"def patch_admin_page_form(current_page_form):

    class WagtailFixedAdminPageForm(current_page_form):
        """"""
        Validate unicity of the slugs in every language. Take fallbacks into account.
        """"""

        def clean(self):
            cleaned_data = super().clean()
            siblings = self.parent_page.get_children()
<mask>:
                siblings = siblings.not_page(self.instance)
            for code, name in settings.LANGUAGES:
                field_name = build_localized_fieldname('slug', code)
                slug_value = self.cleaned_data.get(field_name, None)
                if slug_value is None:
                    continue
                with translation.override(code):
                    siblings_slugs = [sibling.slug for sibling in siblings]
                    if slug_value in siblings_slugs:
                        self.add_error(field_name, forms.ValidationError(_('This slug is already in use')))
            return cleaned_data
    return WagtailFixedAdminPageForm",self.instance.pk,72,"hasattr(siblings, 'not_page')",False,0.0,N/A
"@register.simple_tag(takes_context=True)
def change_lang(context, lang=None, page=None, *args, **kwargs):
    current_language = get_language()
<mask>:
        request = context['request']
        try:
            match = resolve(unquote(request.path, errors='strict'))
        except Resolver404:
            return ''
        non_prefixed_path = re.sub(current_language + '/', '', request.path, count=1)
        if match.url_name == 'wagtail_serve':
            activate(lang)
            translated_url = page.get_url()
            activate(current_language)
            return translated_url
        elif match.url_name == 'wagtailsearch_search':
            path_components = [component for component in non_prefixed_path.split('/') if component]
            translated_url = '/' + lang + '/' + path_components[0] + '/'
            if request.GET:
                translated_url += '?'
                for count, (key, value) in enumerate(iteritems(request.GET)):
                    if count != 0:
                        translated_url += '&'
                    translated_url += key + '=' + value
            return translated_url
    return ''",'request' in context and lang and current_language and page,95,lang,False,0.004539992976248487,N/A
"@register.simple_tag(takes_context=True)
def slugurl_trans(context, slug, language=None):
    """"""
    Examples:
        {% slugurl_trans 'default_lang_slug' %}
        {% slugurl_trans 'de_lang_slug' 'de' %}

    Returns the URL for the page that has the given slug.
    """"""
    language = language or DEFAULT_LANGUAGE
    with use_language(language):
        page = Page.objects.filter(slug=slug).first()
<mask>:
        return pageurl(context, page)",page,42,page,True,100.00000000000004,N/A
"@register.tag('get_available_languages_wmt')
def do_get_available_languages(unused_parser, token):
    """"""
    Store a list of available languages in the context.

    Usage::

        {% get_available_languages_wmt as languages %}
        {% for language in languages %}
        ...
        {% endfor %}

    This will just pull the MODELTRANSLATION_LANGUAGES (or LANGUAGES) setting
    from your setting file (or the default settings) and
    put it into the named variable.
    """"""
    args = token.contents.split()
<mask>:
        raise template.TemplateSyntaxError(""'get_available_languages_wmt' requires 'as variable' (got %r)"" % args)
    return GetAvailableLanguagesNode(args[2])",len(args) != 3 or args[1] != 'as',70,len(args) != 3,False,31.890655732397057,N/A
"def create_page_tree(self, nodes=None):
    """"""
        Creates a page tree with a dict of page nodes following the below structure:
         {
            'model': Page,
            'args': [],
            'kwargs': {'title': 'root',},
            'children': {
                'child': {
                    'model': Page,
                    'args': [],
                    'kwargs': {'title': 'child',},
                    'children': {},
                },
            },
        },

        :param nodes: representing a page tree
        :return: site
        """"""
<mask>:
        return None
    from .models import TestRootPage
    from wagtail.models import Site
    all_nodes = {'model': TestRootPage, 'kwargs': {'title_de': 'Root', 'slug_de': 'root'}, 'children': {'site_root': nodes}}
    self.create_instance(all_nodes)
    site_root_node = nodes['instance']
    site = Site.objects.create(root_page=site_root_node, hostname='localhost', port=80, is_default_site=True)
    return site",not nodes,86,not nodes,True,100.00000000000004,N/A
"def create_instance(self, node, parent=None, order=None):
<mask>:
        path = '{}{}'.format(parent.path, '%04d' % (order,))
        depth = parent.depth + 1
    else:
        path = '%04d' % (self.path,)
        depth = 1
    args = node.get('args', [])
    kwargs = node.get('kwargs', {})
    kwargs['path'] = kwargs.get('path', path)
    kwargs['depth'] = kwargs.get('depth', depth)
    if parent:
        node_page = parent.add_child(instance=node['model'](*args, **kwargs))
        node_page.save()
    else:
        node_page = node['model'].objects.create(*args, **kwargs)
    node_page.save_revision().publish()
    node['instance'] = node_page
    for n, child in enumerate(node.get('children', {}).values()):
        self.create_instance(child, node_page, n + 1)
    return node_page",parent,71,order,False,0.0,N/A
"def test_page_form(self):
    """"""
        In this test we use the InlinePanelPage model because it has all the possible ""patchable"" fields
        so if the created form has all fields the the form was correctly patched
        """"""
    page_edit_handler = models.InlinePanelPage.get_edit_handler()
    form = page_edit_handler.get_form_class()
    page_base_fields = ['slug_de', 'slug_en', 'seo_title_de', 'seo_title_en', 'search_description_de', 'search_description_en', 'show_in_menus', 'go_live_at', 'expire_at']
    form_fields = list(form.base_fields.keys())
<mask>:
        form_fields.remove('comment_notifications')
    self.assertEqual(page_base_fields, form_fields)
    inline_model_fields = ['field_name_de', 'field_name_en', 'image_chooser_de', 'image_chooser_en', 'fieldrow_name_de', 'fieldrow_name_en', 'name_de', 'name_en', 'image_de', 'image_en', 'other_name_de', 'other_name_en']
    related_formset_form = form.formsets['related_page_model'].form
    self.assertEqual(inline_model_fields, list(related_formset_form.base_fields.keys()))",'comment_notifications' in form_fields,77,'comment_notifications' in form_fields,True,100.00000000000004,N/A
"def changes_decorator(func):

    def wrapper(self, graph, trim_to_apps=None, convert_apps=None, migration_name=None):
        changes = func(self, graph, trim_to_apps, convert_apps, migration_name)
<mask>:
            del changes['wagtailcore']
        return changes
    return wrapper",'wagtailcore' in changes,22,'wagtailcore' in changes,True,100.00000000000004,N/A
"def correct_scanpy(adatas, **kwargs):
    """"""Batch correct a list of `scanpy.api.AnnData`.

    Parameters
    ----------
    adatas : `list` of `scanpy.api.AnnData`
        Data sets to integrate and/or correct.
        `adata.var_names` must be set to the list of genes.
    return_dimred : `bool`, optional (default=`False`)
        When `True`, the returned `adatas` are each modified to
        also have the integrated low-dimensional embeddings in
        `adata.obsm['X_scanorama']`.
    kwargs : `dict`
        See documentation for the `correct()` method for a full list of
        parameters to use for batch correction.

    Returns
    -------
    corrected
        By default (`return_dimred=False`), returns a list of new
        `scanpy.api.AnnData`.
        When `return_dimred=True`, `corrected` also includes the
        integrated low-dimensional embeddings in
        `adata.obsm['X_scanorama']`.
    """"""
<mask>:
        datasets_dimred, datasets, genes = correct([adata.X for adata in adatas], [adata.var_names.values for adata in adatas], **kwargs)
    else:
        datasets, genes = correct([adata.X for adata in adatas], [adata.var_names.values for adata in adatas], **kwargs)
    from anndata import AnnData
    new_adatas = []
    for i in range(len(adatas)):
        adata = AnnData(datasets[i])
        adata.obs = adatas[i].obs
        adata.obsm = adatas[i].obsm
        adata.var_names = genes
        gene2idx = {gene: idx for idx, gene in zip(adatas[i].var.index, adatas[i].var_names.values)}
        var_idx = [gene2idx[gene] for gene in genes]
        adata.var = adatas[i].var.loc[var_idx]
        adata.uns = adatas[i].uns
        new_adatas.append(adata)
    if 'return_dimred' in kwargs and kwargs['return_dimred']:
        for adata, X_dimred in zip(new_adatas, datasets_dimred):
            adata.obsm['X_scanorama'] = X_dimred
    return new_adatas",'return_dimred' in kwargs and kwargs['return_dimred'],192,kwargs.get('return_dimred'),False,13.398014283383116,N/A
"def plot_clusters(coords, clusters, s=1, colors=None):
<mask>:
        sys.stderr.write('Error: mismatch, {} cells, {} labels\n'.format(coords.shape[0], clusters.shape[0]))
        sys.exit(1)
    if colors is None:
        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a', '#f781bf', '#a65628', '#984ea3', '#999999', '#e41a1c', '#dede00', '#ffe119', '#e6194b', '#ffbea3', '#911eb4', '#46f0f0', '#f032e6', '#d2f53c', '#008080', '#e6beff', '#aa6e28', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000080', '#808080', '#fabebe', '#a3f4ff']), int(max(clusters) + 1))))
    plt.figure()
    plt.scatter(coords[:, 0], coords[:, 1], c=colors[clusters], s=s)",coords.shape[0] != clusters.shape[0],57,len(coords.shape) != clusters.shape,False,35.72379274367096,N/A
"def merge_datasets(datasets, genes, ds_names=None, verbose=True, union=False):
<mask>:
        sys.stderr.write('WARNING: Integrating based on the union of genes is highly discouraged, consider taking the intersection or requantifying gene expression.\n')
    keep_genes = set()
    for idx, gene_list in enumerate(genes):
        if len(keep_genes) == 0:
            keep_genes = set(gene_list)
        elif union:
            keep_genes |= set(gene_list)
        else:
            keep_genes &= set(gene_list)
        if not union and (not ds_names is None) and verbose:
            print('After {}: {} genes'.format(ds_names[idx], len(keep_genes)))
        if len(keep_genes) == 0:
            print('Error: No genes found in all datasets, exiting...')
            sys.exit(1)
    if verbose:
        print('Found {} genes among all datasets'.format(len(keep_genes)))
    if union:
        union_genes = sorted(keep_genes)
        for i in range(len(datasets)):
            if verbose:
                print('Processing data set {}'.format(i))
            X_new = np.zeros((datasets[i].shape[0], len(union_genes)))
            X_old = csc_matrix(datasets[i])
            gene_to_idx = {gene: idx for idx, gene in enumerate(genes[i])}
            for j, gene in enumerate(union_genes):
                if gene in gene_to_idx:
                    X_new[:, j] = X_old[:, gene_to_idx[gene]].toarray().flatten()
            datasets[i] = csr_matrix(X_new)
        ret_genes = np.array(union_genes)
    else:
        ret_genes = np.array(sorted(keep_genes))
        for i in range(len(datasets)):
            uniq_genes, uniq_idx = np.unique(genes[i], return_index=True)
            datasets[i] = datasets[i][:, uniq_idx]
            gene_sort_idx = np.argsort(uniq_genes)
            gene_idx = [idx for idx in gene_sort_idx if uniq_genes[idx] in keep_genes]
            datasets[i] = datasets[i][:, gene_idx]
            assert np.array_equal(uniq_genes[gene_idx], ret_genes)
    return (datasets, ret_genes)",union,178,union and (not ds_names),False,5.522397783539471,N/A
"def visualize_cluster(coords, cluster, cluster_labels, cluster_name=None, size=1, viz_prefix='vc', image_suffix='.svg'):
<mask>:
        cluster_name = cluster
    labels = [1 if c_i == cluster else 0 for c_i in cluster_labels]
    c_idx = [i for i in range(len(labels)) if labels[i] == 1]
    nc_idx = [i for i in range(len(labels)) if labels[i] == 0]
    colors = np.array(['#cccccc', '#377eb8'])
    image_fname = '{}_cluster{}{}'.format(viz_prefix, cluster, image_suffix)
    plt.figure()
    plt.scatter(coords[nc_idx, 0], coords[nc_idx, 1], c=colors[0], s=size)
    plt.scatter(coords[c_idx, 0], coords[c_idx, 1], c=colors[1], s=size)
    plt.title(str(cluster_name))
    plt.savefig(image_fname, dpi=500)",not cluster_name,72,cluster_name is None,False,39.76353643835252,N/A
"def visualize_expr(X, coords, genes, viz_gene, image_suffix='.svg', new_fig=True, size=1, viz_prefix='ve'):
    genes = [gene.upper() for gene in genes]
    viz_gene = viz_gene.upper()
<mask>:
        sys.stderr.write('Warning: Could not find gene {}\n'.format(viz_gene))
        return
    image_fname = '{}_{}{}'.format(viz_prefix, viz_gene, image_suffix)
    x_gene = X[:, list(genes).index(viz_gene)].toarray()
    colors = np.zeros(x_gene.shape)
    n_tiles = 100
    prev_percentile = min(x_gene)
    for i in range(n_tiles):
        q = (i + 1) / float(n_tiles) * 100.0
        percentile = np.percentile(x_gene, q)
        idx = np.logical_and(prev_percentile <= x_gene, x_gene <= percentile)
        colors[idx] = i
        prev_percentile = percentile
    colors = colors.flatten()
    if new_fig:
        plt.figure()
        plt.title(viz_gene)
    plt.scatter(coords[:, 0], coords[:, 1], c=colors, cmap=cm.get_cmap('Reds'), s=size)
    plt.savefig(image_fname, dpi=500)",not viz_gene.upper() in genes,92,viz_gene not in genes,False,20.415280320726456,N/A
"def visualize_dropout(X, coords, image_suffix='.svg', new_fig=True, size=1, viz_prefix='dropout'):
    image_fname = '{}{}'.format(viz_prefix, image_suffix)
    x_gene = np.array(np.sum(X != 0, axis=1))
    colors = np.zeros(x_gene.shape)
    n_tiles = 100
    prev_percentile = min(x_gene)
    for i in range(n_tiles):
        q = (i + 1) / float(n_tiles) * 100.0
        percentile = np.percentile(x_gene, q)
        idx = np.logical_and(prev_percentile <= x_gene, x_gene <= percentile)
        colors[idx] = i
        prev_percentile = percentile
    colors = colors.flatten()
<mask>:
        plt.figure()
        plt.title(viz_prefix)
    plt.scatter(coords[:, 0], coords[:, 1], c=colors, cmap=cm.get_cmap('Reds'), s=size)
    plt.savefig(image_fname, dpi=500)",new_fig,72,new_fig,True,100.00000000000004,N/A
"def handle_zeros_in_scale(scale, copy=True):
    """""" Makes sure that whenever scale is zero, we handle it correctly.
    This happens in most scalers when we have constant features.
    Adapted from sklearn.preprocessing.data""""""
<mask>:
        if scale == 0.0:
            scale = 1.0
        return scale
    elif isinstance(scale, np.ndarray):
        if copy:
            scale = scale.copy()
        scale[scale == 0.0] = 1.0
    return scale",np.isscalar(scale),53,"isinstance(scale, float)",False,17.965205598154213,N/A
"def entropy_test(datasets_dimred, ds_labels):
    ds_labels = np.array(ds_labels)
    X_dimred = np.concatenate(datasets_dimred)
    embedding = None
    for k in range(10, 21):
        km = KMeans(n_clusters=k, n_jobs=-1, verbose=0)
        km.fit(X_dimred)
<mask>:
            embedding = visualize(datasets_dimred, km.labels_, NAMESPACE + '_km{}'.format(k), [str(x) for x in range(k)], embedding=embedding)
        print('k = {}, average normalized entropy = {}'.format(k, avg_norm_entropy(ds_labels, km.labels_)))",False and k % 5 == 0,47,embedding is None,False,0.0,N/A
"def avg_norm_entropy(ds_labels, cluster_labels):
    assert len(ds_labels) == len(cluster_labels)
    clusters = sorted(set(cluster_labels))
    datasets = sorted(set(ds_labels))
    Hs = []
    for cluster in clusters:
        cluster_idx = cluster_labels == cluster
        ds_rep = ds_labels[cluster_idx]
        n_cluster = float(sum(cluster_idx))
        H = 0
        for ds in datasets:
            n_ds = float(sum(ds_rep == ds))
<mask>:
                continue
            H += n_ds / n_cluster * np.log(n_ds / n_cluster)
        H *= -1
        H /= np.log(len(datasets))
        Hs.append(H)
    return np.mean(Hs)",n_ds == 0,63,n_cluster == 0,False,37.99178428257963,N/A
"def write_table(dataset, genes, name, cell_name=None):
<mask>:
        prefix = cell_name
    else:
        prefix = name.split('/')[-1]
    with open(name + '_table.txt', 'w') as f:
        header = '\t'.join([prefix + str(i) for i in range(dataset.shape[0])])
        f.write(header + '\n')
        for i in range(dataset.shape[1]):
            line = '\t'.join([str(j) for j in dataset[:, i]])
            f.write(genes[i] + '\t' + line + '\n')",cell_name is not None,51,cell_name,False,36.78794411714425,N/A
"def diff_expr(A, B, genes, permute_cutoff, verbose=True):
    p_vals = []
    for idx, gene in enumerate(genes):
<mask>:
            p_vals.append(1.0)
            continue
        u, p = mannwhitneyu(A[:, idx], B[:, idx])
        p_vals.append(p)
    de_genes = []
    for idx, gene in enumerate(genes):
        if p_vals[idx] < permute_cutoff:
            if verbose:
                print('{}\t{}'.format(gene, p_vals[idx]))
            de_genes.append(gene)
    return de_genes","sum(A[:, idx]) == 0 and sum(B[:, idx]) == 0",44,gene.shape[1] < permute_cutoff,False,1.11343058914179,N/A
"def check_number_of_labels(n_labels, n_samples):
<mask>:
        raise ValueError('Number of labels is %d. Valid values are 2 to n_samples - 1 (inclusive)' % n_labels)",not 1 < n_labels < n_samples,21,n_labels < 2 or n_samples > 1,False,35.08439695638686,N/A
"def panorama(datasets_full, genes_list):
<mask>:
        print('Processing and reducing dimensionality...')
    datasets, genes = merge_datasets(datasets_full, genes_list)
    datasets_dimred, genes = process_data(datasets, genes)
    if VERBOSE:
        print('Finding panoramas...')
    panoramas = connect(datasets_dimred)
    if VERBOSE:
        print(panoramas)
    return panoramas",VERBOSE,30,VERBOSE,True,100.00000000000004,N/A
"def load_tab(fname, max_genes=40000):
<mask>:
        opener = gzip.open
    else:
        opener = open
    with opener(fname, 'r') as f:
        if fname.endswith('.gz'):
            header = f.readline().decode('utf-8').rstrip().split('\t')
        else:
            header = f.readline().rstrip().split('\t')
        cells = header[1:]
        X = np.zeros((len(cells), max_genes))
        genes = []
        for i, line in enumerate(f):
            if i > max_genes:
                break
            if fname.endswith('.gz'):
                line = line.decode('utf-8')
            fields = line.rstrip().split('\t')
            genes.append(fields[0])
            X[:, i] = [float(f) for f in fields[1:]]
    return (X[:, range(len(genes))], np.array(cells), np.array(genes))",fname.endswith('.gz'),67,fname.endswith('.gz'),True,100.00000000000004,N/A
"def load_mtx(dname):
    with open(dname + '/matrix.mtx', 'r') as f:
        while True:
            header = f.readline()
<mask>:
                break
        header = header.rstrip().split()
        n_genes, n_cells = (int(header[0]), int(header[1]))
        data, i, j = ([], [], [])
        for line in f:
            fields = line.rstrip().split()
            data.append(float(fields[2]))
            i.append(int(fields[1]) - 1)
            j.append(int(fields[0]) - 1)
        X = csr_matrix((data, (i, j)), shape=(n_cells, n_genes))
    genes = []
    with open(dname + '/genes.tsv', 'r') as f:
        for line in f:
            fields = line.rstrip().split()
            genes.append(fields[1])
    assert len(genes) == n_genes
    return (X, np.array(genes))",not header.startswith('%'),77,not header,False,3.0197383422318516,N/A
"def load_h5(fname, genome='mm10'):
    try:
        import tables
    except ImportError:
        sys.stderr.write('Please install PyTables to read .h5 files: https://www.pytables.org/usersguide/installation.html\n')
        exit(1)
    with tables.open_file(str(fname), 'r') as f:
        try:
            dsets = {}
            for node in f.walk_nodes('/' + genome, 'Array'):
                dsets[node.name] = node.read()
            n_genes, n_cells = dsets['shape']
            data = dsets['data']
<mask>:
                data = dsets['data'].view('float32')
                data[:] = dsets['data']
            X = csr_matrix((data, dsets['indices'], dsets['indptr']), shape=(n_cells, n_genes))
            genes = [gene for gene in dsets['genes'].astype(str)]
            assert len(genes) == n_genes
            assert len(genes) == X.shape[1]
        except tables.NoSuchNodeError:
            raise Exception('Genome %s does not exist in this file.' % genome)
        except KeyError:
            raise Exception('File is missing one or more required datasets.')
    return (X, np.array(genes))",dsets['data'].dtype == np.dtype('int32'),99,data.dtype == np.float32,False,32.138636182240575,N/A
"def process_tab(fname, min_trans=MIN_TRANSCRIPTS):
    X, cells, genes = load_tab(fname)
    gt_idx = [i for i, s in enumerate(np.sum(X != 0, axis=1)) if s >= min_trans]
    X = X[gt_idx, :]
    cells = cells[gt_idx]
<mask>:
        print('Warning: 0 cells passed QC in {}'.format(fname))
    if fname.endswith('.txt'):
        cache_prefix = '.'.join(fname.split('.')[:-1])
    elif fname.endswith('.txt.gz'):
        cache_prefix = '.'.join(fname.split('.')[:-2])
    elif fname.endswith('.tsv'):
        cache_prefix = '.'.join(fname.split('.')[:-1])
    elif fname.endswith('.tsv.gz'):
        cache_prefix = '.'.join(fname.split('.')[:-2])
    else:
        sys.stderr.write('Tab files should end with "".txt"" or "".tsv""\n')
        exit(1)
    cache_fname = cache_prefix + '.npz'
    np.savez(cache_fname, X=X, genes=genes)
    return (X, cells, genes)",len(gt_idx) == 0,80,len(cells) > 0,False,11.708995388048026,N/A
"def process_mtx(dname, min_trans=MIN_TRANSCRIPTS):
    X, genes = load_mtx(dname)
    gt_idx = [i for i, s in enumerate(np.sum(X != 0, axis=1)) if s >= min_trans]
    X = X[gt_idx, :]
<mask>:
        print('Warning: 0 cells passed QC in {}'.format(dname))
    cache_fname = dname + '/tab.npz'
    scipy.sparse.save_npz(cache_fname, X, compressed=False)
    with open(dname + '/tab.genes.txt', 'w') as of:
        of.write('\n'.join(genes) + '\n')
    return (X, genes)",len(gt_idx) == 0,55,len(X.shape) > 0,False,11.59119922599073,N/A
"def time_align_correlate(alignments, time):
    time_dist = euclidean_distances(time, time)
    assert time_dist.shape == alignments.shape
    time_dists, scores = ([], [])
    for i in range(time_dist.shape[0]):
        for j in range(time_dist.shape[1]):
<mask>:
                continue
            time_dists.append(time_dist[i, j])
            scores.append(alignments[i, j])
    print('Spearman rho = {}'.format(spearmanr(time_dists, scores)))
    print('Pearson rho = {}'.format(pearsonr(time_dists, scores)))",i >= j,40,"time_dist[i, j] < time_dist[i, j]",False,2.719665272174911,N/A
"def load_alignments_from_log(fname):
    names = []
    alignments = []
    with open(fname) as f:
        row = []
        in_row = False
        while True:
            line = f.readline().strip()
<mask>:
                break
            if line.startswith('Loaded'):
                names.append(line.split()[1])
                continue
            if line.startswith('[') and (not 't-SNE' in line):
                fields = line.replace('[', '').replace(']', '').strip()
                row += [float(field) for field in fields.split()]
                in_row = True
                if line.endswith(']'):
                    alignments.append(row)
                    row = []
                    in_row = False
                    continue
            if line.endswith(']'):
                fields = line.replace('[', '').replace(']', '').strip()
                row += [float(field) for field in fields.split()]
                if len(alignments) > 0:
                    assert len(alignments[-1]) == len(row)
                alignments.append(row)
                row = []
                in_row = False
            if in_row and (not line.startswith('[')):
                fields = line.replace('[', '').replace(']', '').strip()
                row += [float(field) for field in fields.split()]
    A = np.array(alignments)
    return (A, names)",not line,113,not line,True,100.00000000000004,N/A
"def time_dist(datasets_dimred, time):
    time_dist = euclidean_distances(time, time)
    time_dists, scores = ([], [])
    for i in range(time_dist.shape[0]):
        for j in range(time_dist.shape[1]):
<mask>:
                continue
            score = np.mean(euclidean_distances(datasets_dimred[i], datasets_dimred[j]))
            time_dists.append(time_dist[i, j])
            scores.append(score)
    print('Spearman rho = {}'.format(spearmanr(time_dists, scores)))
    print('Pearson rho = {}'.format(pearsonr(time_dists, scores)))",i >= j,39,"datasets_dimred[i, j] is None",False,4.990049701936832,N/A
"def generate_tests():
    """"""
    Generate the list of tests.
    """"""
    expected_os_errors = {}
    for os_name, os_cfg in NL_BASE.config_dict.items():
        expected_os_errors[os_name] = []
        for message in os_cfg['messages']:
            expected_os_errors[os_name].append(message['error'])
    test_cases = []
    cwd = os.path.dirname(__file__)
    test_path = os.path.join(cwd, 'config')
    os_dir_list = [name for name in os.listdir(test_path) if os.path.isdir(os.path.join(test_path, name))]
    expected_oss = set(expected_os_errors.keys())
    tested_oss = set(os_dir_list)
    missing_oss = expected_oss - tested_oss
    for missing_os in missing_oss:
        test_cases.append(('__missing__{}'.format(missing_os), '', ''))
    for os_name in os_dir_list:
        os_path = os.path.join(test_path, os_name)
        errors = [name for name in os.listdir(os_path) if os.path.isdir(os.path.join(os_path, name))]
        expected_errors = set(expected_os_errors[os_name])
        defined_errors = set(errors)
        missing_errors = expected_errors - defined_errors
        for mising_err in missing_errors:
            test_cases.append((os_name, '__missing__{}'.format(mising_err), ''))
        for error_name in errors:
            error_path = os.path.join(os_path, error_name)
            cases = [name for name in os.listdir(error_path) if os.path.isdir(os.path.join(error_path, name))]
<mask>:
                test_cases.append((os_name, error_name, '__missing__'))
            for test_case in cases:
                test_cases.append((os_name, error_name, test_case))
    return test_cases",not cases,130,error_name in cases,False,10.682175159905848,N/A
"@pytest.mark.parametrize('os_name,error_name,test_case', tests)
def test_config(os_name, error_name, test_case):
    assert not os_name.startswith('__missing__'), 'No tests defined for {}'.format(os_name.replace('__missing__', ''))
    assert not error_name.startswith('__missing__'), 'No tests defined for {}, under {}'.format(error_name.replace('__missing__', ''), os_name)
    assert test_case != '__missing__', 'No test cases defined for {}, under {}'.format(error_name, os_name)
    print('Testing {} for {}, under the test case ""{}""'.format(error_name, os_name, test_case))
    cwd = os.path.dirname(__file__)
    test_path = os.path.join(cwd, 'config', os_name, error_name, test_case)
    raw_message_filepath = os.path.join(test_path, 'syslog.msg')
    log.debug('Looking for %s', raw_message_filepath)
    assert os.path.isfile(raw_message_filepath)
    with open(raw_message_filepath, 'r') as raw_message_fh:
        raw_message = raw_message_fh.read()
    log.debug('Read raw message:')
    log.debug(raw_message)
    yang_message_filepath = os.path.join(test_path, 'yang.json')
    log.debug('Looking for %s', yang_message_filepath)
    try:
        with open(yang_message_filepath, 'r') as yang_message_fh:
            yang_message = yang_message_fh.read()
    except IOError:
        yang_message = ''
    log.debug('Read YANG text:')
    log.debug(yang_message)
<mask>:
        struct_yang_message = json.loads(yang_message)
    else:
        struct_yang_message = {}
    log.debug('Struct YANG message:')
    log.debug(struct_yang_message)
    log.debug('Sending the raw message to the napalm-logs daemon')
    TEST_SKT.sendto(raw_message.strip().encode('utf-8'), (NAPALM_LOGS_TEST_ADDR, NAPALM_LOGS_TEST_PORT))
    zmq_msg = TEST_CLIENT.recv()
    deserialised_zmq_msg = napalm_logs.utils.unserialize(zmq_msg)
    log.debug('Received from the napalm-logs daemon:')
    log.debug(deserialised_zmq_msg)
    returned_yang = json.loads(json.dumps(deserialised_zmq_msg))
    if not struct_yang_message:
        assert False, json.dumps(deserialised_zmq_msg, indent=2)
    assert struct_yang_message.pop('timestamp', False), 'Yang test file does not contain a timestamp key for {} under {}'.format(error_name, os_name)
    assert returned_yang.pop('timestamp', False), 'The returned yang does not contain a timestamp key for {} under {}'.format(error_name, os_name)
    assert struct_yang_message == returned_yang",yang_message,192,yang_message,True,100.00000000000004,N/A
"def gen_messages_rst():
    nl_ = NapalmLogs(publisher=[])
    defined_errors = {}
    for os_name, os_cfg in nl_.config_dict.items():
        for message in os_cfg['messages']:
            error_name = message['error']
<mask>:
                defined_errors[error_name] = {'doc': '', 'os': [], 'model': ''}
            if not defined_errors[error_name]['doc'] or len(defined_errors[error_name]['doc']) < len(message['__doc__']):
                defined_errors[error_name]['doc'] = message['__doc__']
            if not defined_errors[error_name]['model']:
                defined_errors[error_name]['model'] = message['model']
            defined_errors[error_name]['os'].append(os_name)
    cwd = os.path.dirname(__file__)
    test_root_path = os.path.join(cwd, '..', 'tests', 'config')
    env = jinja2.Environment(loader=jinja2.FileSystemLoader('.'))
    for error_name, error_details in defined_errors.items():
        os_name = error_details['os'][0]
        error_path = os.path.join(test_root_path, os_name, error_name)
        test_cases = [name for name in os.listdir(error_path) if os.path.isdir(os.path.join(error_path, name))]
        test_case_name = 'default' if 'default' in test_cases else test_cases[0]
        test_case_path = os.path.join(error_path, test_case_name)
        raw_message_filepath = os.path.join(test_case_path, 'syslog.msg')
        log.debug('Looking for %s', raw_message_filepath)
        assert os.path.isfile(raw_message_filepath)
        with open(raw_message_filepath, 'r') as raw_message_fh:
            raw_message = raw_message_fh.read()
        log.debug('Read raw message:')
        log.debug(raw_message)
        yang_message_filepath = os.path.join(test_case_path, 'yang.json')
        log.debug('Looking for %s', yang_message_filepath)
        assert os.path.isfile(yang_message_filepath)
        with open(yang_message_filepath, 'r') as yang_message_fh:
            yang_message = yang_message_fh.read()
        log.debug('Read YANG text:')
        log.debug(yang_message)
        struct_yang_message = json.loads(yang_message)
        indented_yang_message = json.dumps(struct_yang_message, indent=4, sort_keys=True)
        log.debug('Struct YANG message:')
        log.debug(struct_yang_message)
        msg_template = env.get_template('message_template.jinja')
        rendered_template = msg_template.render(error_name=error_name, error_doc=error_details['doc'], error_yang=error_details['model'], error_os_list=list(set(error_details['os'])), error_txt_example=raw_message.strip(), error_json_example=indented_yang_message.replace('\n}', '\n  }'))
        message_rst_path = 'messages/{error_name}.rst'.format(error_name=error_name)
        with open(message_rst_path, 'w') as rst_fh:
            rst_fh.write(rendered_template)
    index_tpl_file = env.get_template('messages_index_template.jinja')
    messages_list = list(defined_errors.keys())
    messages_list.extend(['RAW', 'UNKNOWN'])
    messages_list.sort()
    rendered_template = index_tpl_file.render(error_list=messages_list)
    with open('messages/index.rst', 'w') as index_fh:
        index_fh.write(rendered_template)",error_name not in defined_errors,190,error_name not in defined_errors,True,100.00000000000004,N/A
"def _handshake(self, conn, addr):
    """"""
        Ensures that the client receives the AES key.
        """"""
    msg = conn.recv(len(MAGIC_REQ))
    log.debug('Received message %s from %s', msg, addr)
<mask>:
        log.warning('%s is not a valid REQ message from %s', msg, addr)
        return
    log.debug('Sending the private key')
    conn.send(self.__key)
    log.debug('Waiting for the client to confirm')
    msg = conn.recv(len(MAGIC_ACK))
    if msg != MAGIC_ACK:
        return
    log.debug('Sending the signature key')
    conn.send(self.__sgn)
    log.debug('Waiting for the client to confirm')
    msg = conn.recv(len(MAGIC_ACK))
    if msg != MAGIC_ACK:
        return
    log.info('%s is now authenticated', addr)
    self.keep_alive(conn)",msg != MAGIC_REQ,81,msg != MAGIC_REQ,True,100.00000000000004,N/A
"def keep_alive(self, conn):
    """"""
        Maintains auth sessions
        """"""
    while self.__up:
        msg = conn.recv(len(AUTH_KEEP_ALIVE))
<mask>:
            log.error('Received something other than %s', AUTH_KEEP_ALIVE)
            conn.close()
            return
        try:
            conn.send(AUTH_KEEP_ALIVE_ACK)
        except (IOError, socket.error) as err:
            log.error('Unable to send auth keep alive: %s', err)
            conn.close()
            return",msg != AUTH_KEEP_ALIVE,39,msg != AUTH_KEEP_ALIVE,True,100.00000000000004,N/A
"def _create_skt(self):
    """"""
        Create the authentication socket.
        """"""
    log.debug('Creating the auth socket')
<mask>:
        self.socket = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)
    else:
        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        self.socket.bind((self.auth_address, self.auth_port))
    except socket.error as msg:
        error_string = 'Unable to bind (auth) to port {} on {}: {}'.format(self.auth_port, self.auth_address, msg)
        log.error(error_string, exc_info=True)
        raise BindException(error_string)",':' in self.auth_address,47,self.auth_port == 6,False,30.509752160562883,N/A
"def start(self):
    """"""
        Listen to auth requests and send the AES key.
        Each client connection starts a new thread.
        """"""
    log.debug('Starting the auth process')
    self.verify_cert()
    self._create_skt()
    log.debug('The auth process can receive at most %d parallel connections', AUTH_MAX_CONN)
    self.socket.listen(AUTH_MAX_CONN)
    thread = threading.Thread(target=self._suicide_when_without_parent, args=(os.getppid(),))
    thread.start()
    signal.signal(signal.SIGTERM, self._exit_gracefully)
    self.__up = True
    while self.__up:
        try:
            clientsocket, address = self.socket.accept()
            wrapped_auth_skt = ssl.wrap_socket(clientsocket, server_side=True, certfile=self.certificate, keyfile=self.keyfile)
        except ssl.SSLError:
            log.exception('SSL error', exc_info=True)
            continue
        except socket.error as error:
<mask>:
                return
            else:
                msg = 'Received auth socket error: {}'.format(error)
                log.error(msg, exc_info=True)
                raise NapalmLogsExit(msg)
        log.info('%s connected', address)
        log.debug('Starting the handshake')
        client_thread = threading.Thread(target=self._handshake, args=(wrapped_auth_skt, address))
        client_thread.start()",self.__up is False,97,error.errno == EAGAIN,False,6.870636427700047,N/A
"def _cleanup_buffer(self):
    """"""
        Periodically cleanup the buffer.
        """"""
<mask>:
        return
    while True:
        time.sleep(60)
        log.debug('Cleaning up buffer')
        items = self._buffer.items()
        log.debug('Collected items')
        log.debug(list(items))",not self._buffer,22,not self._buffer,True,100.00000000000004,N/A
"def _compile_prefixes(self):
    """"""
        Create a dict of all OS prefixes and their compiled regexs
        """"""
    self.compiled_prefixes = {}
    for dev_os, os_config in self.config.items():
<mask>:
            continue
        self.compiled_prefixes[dev_os] = []
        for prefix in os_config.get('prefixes', []):
            values = prefix.get('values', {})
            line = prefix.get('line', '')
            if prefix.get('__python_fun__'):
                self.compiled_prefixes[dev_os].append({'__python_fun__': prefix['__python_fun__'], '__python_mod__': prefix['__python_mod__']})
                continue
            line = '{{pri}}{}{{message}}'.format(line)
            values['pri'] = '\\<(\\d+)\\>'
            values['message'] = '(.*)'
            position = {}
            for key in values.keys():
                position[line.find('{' + key + '}')] = key
            sorted_position = {}
            for i, elem in enumerate(sorted(position.items())):
                sorted_position[elem[1]] = i + 1
            escaped = re.escape(line).replace('\\{', '{').replace('\\}', '}')
            escaped = escaped.replace('\\ ', '\\s+')
            self.compiled_prefixes[dev_os].append({'prefix': re.compile(escaped.format(**values)), 'prefix_positions': sorted_position, 'raw_prefix': escaped.format(**values), 'values': values, 'state': prefix.get('state'), 'state_tag': prefix.get('state_tag')})",not os_config,106,dev_os not in self.compiled_prefixes,False,5.522397783539471,N/A
"def _identify_prefix(self, msg, data):
    """"""
        Check the message again each OS prefix and if matched return the
        message dict
        """"""
    prefix_id = -1
    for prefix in data:
        msg_dict = {}
        prefix_id += 1
        match = None
<mask>:
            log.debug('Trying to match using the %s custom python profiler', prefix['__python_mod__'])
            try:
                match = prefix['__python_fun__'](msg)
            except Exception:
                log.error('Exception while parsing %s with the %s python profiler', msg, prefix['__python_mod__'], exc_info=True)
        else:
            log.debug('Matching using YAML-defined profiler:')
            log.debug(prefix['raw_prefix'])
            match = prefix['prefix'].search(msg)
        if not match:
            log.debug('Match not found')
            continue
        if '__python_fun__' in prefix:
            log.debug('%s matched using the custom python profiler %s', msg, prefix['__python_mod__'])
            msg_dict = match
        else:
            positions = prefix.get('prefix_positions', {})
            values = prefix.get('values')
            msg_dict = {}
            for key in values.keys():
                msg_dict[key] = match.group(positions.get(key))
        msg_dict['__prefix_id__'] = prefix_id
        msg_dict['message'] = msg_dict['message'].strip()
        if 'pri' in msg_dict:
            msg_dict['facility'] = int(int(msg_dict['pri']) / 8)
            msg_dict['severity'] = int(int(msg_dict['pri']) - msg_dict['facility'] * 8)
        return msg_dict",'__python_fun__' in prefix,140,'__python_mod__' in prefix,False,70.16879391277372,N/A
"def _identify_os(self, msg):
    """"""
        Using the prefix of the syslog message,
        we are able to identify the operating system and then continue parsing.
        """"""
    ret = []
    for dev_os, data in self.compiled_prefixes.items():
        log.debug('Matching under %s', dev_os)
        msg_dict = self._identify_prefix(msg, data)
<mask>:
            log.debug('Adding %s to list of matched OS', dev_os)
            ret.append((dev_os, msg_dict))
        else:
            log.debug('No match found for %s', dev_os)
    if not ret:
        log.debug('Not matched any OS, returning original log')
        msg_dict = {'message': msg}
        ret.append((None, msg_dict))
    return ret",msg_dict,76,msg_dict,True,100.00000000000004,N/A
"def start(self):
    """"""
        Listen to messages and publish them.
        """"""
    c_logs_ingested = Counter('napalm_logs_listener_logs_ingested', 'Count of ingested log messages', ['listener_type', 'address', 'port'])
    c_messages_published = Counter('napalm_logs_listener_messages_published', 'Count of published messages', ['listener_type', 'address', 'port'])
    self._setup_ipc()
    log.debug('Using the %s listener', self._listener_type)
    self._setup_listener()
    self.listener.start()
    thread = threading.Thread(target=self._suicide_when_without_parent, args=(os.getppid(),))
    thread.start()
    signal.signal(signal.SIGTERM, self._exit_gracefully)
    self.__up = True
    while self.__up:
        try:
            log_message, log_source = self.listener.receive()
        except ListenerException as lerr:
<mask>:
                log.info('Exiting on process shutdown')
                return
            else:
                log.error(lerr, exc_info=True)
                raise NapalmLogsExit(lerr)
        log.debug('Received %s from %s. Queueing to the server.', log_message, log_source)
        if not log_message:
            log.info('Empty message received from %s. Not queueing to the server.', log_source)
            continue
        c_logs_ingested.labels(listener_type=self._listener_type, address=self.address, port=self.port).inc()
        self.pub.send(umsgpack.packb((log_message, log_source)))
        c_messages_published.labels(listener_type=self._listener_type, address=self.address, port=self.port).inc()",self.__up is False,104,lerr.errno == errno.EPIPE,False,5.522397783539471,N/A
"def _compile_messages(self):
    """"""
        Create a list of all OS messages and their compiled regexs
        """"""
    self.compiled_messages = []
<mask>:
        return
    for message_dict in self._config.get('messages', {}):
        error = message_dict['error']
        tag = message_dict['tag']
        model = message_dict['model']
        match_on = message_dict.get('match_on', 'tag')
        if '__python_fun__' in message_dict:
            self.compiled_messages.append({'error': error, 'tag': tag, 'match_on': match_on, 'model': model, '__python_fun__': message_dict['__python_fun__']})
            continue
        values = message_dict['values']
        line = message_dict['line']
        mapping = message_dict['mapping']
        position = {}
        replace = {}
        for key in list(values.keys()):
            if '|' in key:
                new_key, replace[new_key] = key.replace(' ', '').split('|')
                values[new_key] = values.pop(key)
                key = new_key
            position[line.find('{' + key + '}')] = key
        sorted_position = {}
        for i, elem in enumerate(sorted(position.items())):
            sorted_position[elem[1]] = i + 1
        escaped = re.escape(line).replace('\\{', '{').replace('\\}', '}')
        escaped = escaped.replace('\\ ', '\\s+')
        self.compiled_messages.append({'error': error, 'tag': tag, 'match_on': match_on, 'line': re.compile(escaped.format(**values)), 'positions': sorted_position, 'values': values, 'replace': replace, 'model': model, 'mapping': mapping, 'state': message_dict.get('state'), 'state_tag': message_dict.get('state_tag')})
    log.debug('Compiled messages:')
    log.debug(self.compiled_messages)",not self._config,143,not self._config.get('messages'),False,39.281465090051306,N/A
"def _parse(self, msg_dict):
    """"""
        Parse a syslog message and check what OpenConfig object should
        be generated.
        """"""
    error_present = False
    for message in self.compiled_messages:
        match_on = message['match_on']
<mask>:
            continue
        if message['tag'] != msg_dict[match_on]:
            continue
        if '__python_fun__' in message:
            return {'model': message['model'], 'error': message['error'], '__python_fun__': message['__python_fun__']}
        error_present = True
        match = message['line'].search(msg_dict['message'])
        if not match:
            continue
        positions = message.get('positions', {})
        values = message.get('values')
        ret = {'model': message['model'], 'mapping': message['mapping'], 'replace': message['replace'], 'error': message['error'], '_state': message['state'], '_state_tag': message['state_tag']}
        for key in values.keys():
            if key in message['replace']:
                result = napalm_logs.utils.cast(match.group(positions.get(key)), message['replace'][key])
            else:
                result = match.group(positions.get(key))
            ret[key] = result
        return ret
    if error_present is True:
        log.info('Configured regex did not match for os: %s tag %s', self._name, msg_dict.get('tag', ''))
    else:
        log.info('Syslog message not configured for os: %s tag %s', self._name, msg_dict.get('tag', ''))",match_on not in msg_dict,127,not match_on,False,23.50540321304655,N/A
"def _format_time(self, time, date, timezone, prefix_id):
    date_time = None
<mask>:
        date_time = dateparser.parse('{} {}'.format(date, time))
    if not date_time:
        tz = dateutil.tz.gettz(timezone)
        date_time = datetime.datetime.now(tz)
    return int(calendar.timegm(date_time.utctimetuple()))",time and date,26,date,False,13.533528323661276,N/A
"def start(self):
    """"""
        Start the worker process.
        """"""
    napalm_logs_device_messages_received = Counter('napalm_logs_device_messages_received', 'Count of messages received by the device process', ['device_os'])
    napalm_logs_device_raw_published_messages = Counter('napalm_logs_device_raw_published_messages', 'Count of raw type published messages', ['device_os'])
    napalm_logs_device_published_messages = Counter('napalm_logs_device_published_messages', 'Count of published messages', ['device_os'])
    napalm_logs_device_oc_object_failed = Counter('napalm_logs_device_oc_object_failed', 'Counter of failed OpenConfig object generations', ['device_os'])
<mask>:
        napalm_logs_device_published_messages_attrs = Counter('napalm_logs_device_published_messages_attrs', 'Counter of published messages, with more granular selection', ['device_os', 'host', 'error'])
    self._setup_ipc()
    thread = threading.Thread(target=self._suicide_when_without_parent, args=(os.getppid(),))
    thread.start()
    signal.signal(signal.SIGTERM, self._exit_gracefully)
    self.__up = True
    while self.__up:
        try:
            bin_obj = self.sub.recv()
            msg_dict, address = umsgpack.unpackb(bin_obj, use_list=False)
        except zmq.ZMQError as error:
            if self.__up is False:
                log.info('Exiting on process shutdown [%s]', self._name)
                return
            else:
                raise NapalmLogsExit(error)
        log.debug('%s: dequeued %s, received from %s', self._name, msg_dict, address)
        napalm_logs_device_messages_received.labels(device_os=self._name).inc()
        host = msg_dict.get('host')
        prefix_id = msg_dict.pop('__prefix_id__')
        if 'timestamp' in msg_dict:
            timestamp = msg_dict.pop('timestamp')
        else:
            timestamp = self._format_time(msg_dict.get('time', ''), msg_dict.get('date', ''), msg_dict.get('timeZone', 'UTC'), prefix_id)
        facility = msg_dict.get('facility')
        severity = msg_dict.get('severity')
        kwargs = self._parse(msg_dict)
        if not kwargs:
            to_publish = {'ip': address, 'host': host, 'timestamp': timestamp, 'message_details': msg_dict, 'os': self._name, 'error': 'RAW', 'model_name': 'raw', 'facility': facility, 'severity': severity}
            log.debug('Queueing to be published:')
            log.debug(to_publish)
            self.pub.send(umsgpack.packb(to_publish))
            napalm_logs_device_raw_published_messages.labels(device_os=self._name).inc()
            continue
        try:
            if '__python_fun__' in kwargs:
                log.debug('Using the Python parser to determine the YANG-equivalent object')
                yang_obj = kwargs['__python_fun__'](msg_dict)
            else:
                yang_obj = self._emit(**kwargs)
        except Exception:
            log.exception('Unexpected error when generating the OC object.', exc_info=True)
            napalm_logs_device_oc_object_failed.labels(device_os=self._name).inc()
            continue
        log.debug('Generated OC object:')
        log.debug(yang_obj)
        error = kwargs.get('error')
        model_name = kwargs.get('model')
        to_publish = {'error': error, 'host': host, 'ip': address, 'timestamp': timestamp, 'yang_message': yang_obj, 'message_details': msg_dict, 'yang_model': model_name, 'os': self._name, 'facility': facility, 'severity': severity}
        if kwargs.get('_state') is not None:
            to_publish['state'] = kwargs['_state']
            if kwargs.get('_state_tag'):
                to_publish['state_tag'] = kwargs['_state_tag']
        log.debug('Queueing to be published:')
        log.debug(to_publish)
        self.pub.send(umsgpack.packb(to_publish))
        napalm_logs_device_published_messages.labels(device_os=self._name).inc()
        if self.opts.get('metrics_include_attributes', True):
            napalm_logs_device_published_messages_attrs.labels(device_os=self._name, error=to_publish['error'], host=to_publish['host']).inc()","self.opts.get('metrics_include_attributes', True)",267,napalm_logs_device_messages_received.size > 0,False,3.7644257151903666,N/A
"def start(self):
    """"""
        Listen to messages and publish them.
        """"""
    self._setup_ipc()
    thread = threading.Thread(target=self._suicide_when_without_parent, args=(os.getppid(),))
    thread.start()
    signal.signal(signal.SIGTERM, self._exit_gracefully)
    try:
        zmq.proxy(self.sub, self.pub)
    except zmq.ZMQError as error:
<mask>:
            log.info('Exiting on process shutdown')
            return
        else:
            log.error(error, exc_info=True)
            raise NapalmLogsExit(error)",self.__up is False,36,error.errno == errno.EINTR,False,5.522397783539471,N/A
"def __init__(self, opts, address, port, transport_type, serializer, private_key, signing_key, publisher_opts, disable_security=False, pub_id=None):
    self.__up = False
    self.opts = opts
    self.pub_id = pub_id
    self.address = publisher_opts.pop('address', None) or address
    self.port = publisher_opts.pop('port', None) or port
    log.debug('Publishing to %s:%d', self.address, self.port)
    self.serializer = publisher_opts.get('serializer') or serializer
    self.default_serializer = self.serializer == SERIALIZER
    self.disable_security = publisher_opts.get('disable_security', disable_security)
    self._transport_type = transport_type
    self.publisher_opts = publisher_opts
    self.error_whitelist = publisher_opts.get('error_whitelist', [])
    self.error_blacklist = publisher_opts.get('error_blacklist', [])
<mask>:
        self.__safe = nacl.secret.SecretBox(private_key)
        self.__signing_key = signing_key
    self._strip_message_details = publisher_opts.pop('strip_message_details', False)
    self._setup_transport()",not disable_security,78,private_key,False,19.716118825581447,N/A
"def _setup_transport(self):
    """"""
        Setup the transport.
        """"""
<mask>:
        log.info('%s %d will publish partially parsed messages', self._transport_type, self.pub_id)
    if 'UNKNOWN' in self.error_whitelist:
        log.info('%s %d will publish unknown messages', self._transport_type, self.pub_id)
    transport_class = get_transport(self._transport_type)
    log.debug('Serializing the object for %s using %s', self._transport_type, self.serializer)
    self.serializer_fun = get_serializer(self.serializer)
    self.transport = transport_class(self.address, self.port, **self.publisher_opts)
    self.__transport_encrypt = True
    if hasattr(self.transport, 'NO_ENCRYPT') and getattr(self.transport, 'NO_ENCRYPT') is True:
        self.__transport_encrypt = False",'RAW' in self.error_whitelist,63,self.parsed,False,14.506309551249304,N/A
"def start(self):
    """"""
        Listen to messages and publish them.
        """"""
    napalm_logs_publisher_received_messages = Counter('napalm_logs_publisher_received_messages', 'Count of messages received by the publisher', ['publisher_type', 'address', 'port'])
    napalm_logs_publisher_whitelist_blacklist_check_fail = Counter('napalm_logs_publisher_whitelist_blacklist_check_fail', 'Count of messages which fail the whitelist/blacklist check', ['publisher_type', 'address', 'port'])
    napalm_logs_publisher_messages_published = Counter('napalm_logs_publisher_messages_published', 'Count of published messages', ['publisher_type', 'address', 'port'])
    self._setup_ipc()
    thread = threading.Thread(target=self._suicide_when_without_parent, args=(os.getppid(),))
    thread.start()
    signal.signal(signal.SIGTERM, self._exit_gracefully)
    self.transport.start()
    self.__up = True
    while self.__up:
        try:
            bin_obj = self.sub.recv()
        except zmq.ZMQError as error:
<mask>:
                log.info('Exiting on process shutdown')
                return
            else:
                log.error(error, exc_info=True)
                raise NapalmLogsExit(error)
        obj = umsgpack.unpackb(bin_obj)
        if self._strip_message_details:
            obj.pop('message_details', None)
            bin_obj = self.serializer_fun(obj)
        napalm_logs_publisher_received_messages.labels(publisher_type=self._transport_type, address=self.address, port=self.port).inc()
        if not napalm_logs.utils.check_whitelist_blacklist(obj['error'], whitelist=self.error_whitelist, blacklist=self.error_blacklist):
            log.debug('This error type is %s. Skipping for %s #%d', obj['error'], self._transport_type, self.pub_id)
            napalm_logs_publisher_whitelist_blacklist_check_fail.labels(publisher_type=self._transport_type, address=self.address, port=self.port).inc()
            continue
        serialized_obj = self._serialize(obj, bin_obj)
        log.debug('Publishing the OC object')
        if not self.disable_security and self.__transport_encrypt:
            serialized_obj = self._prepare(serialized_obj)
        self.transport.publish(serialized_obj)
        napalm_logs_publisher_messages_published.labels(publisher_type=self._transport_type, address=self.address, port=self.port).inc()",self.__up is False,134,error.errno == EAGAIN,False,6.870636427700047,N/A
"def __init__(self, address='0.0.0.0', port=514, listener='udp', publisher='zmq', publish_address='0.0.0.0', publish_port=49017, auth_address='0.0.0.0', auth_port=49018, metrics_enabled=False, metrics_address='0.0.0.0', metrics_port='9215', metrics_dir='/tmp/napalm_logs_metrics', certificate=None, keyfile=None, disable_security=False, config_path=None, config_dict=None, extension_config_path=None, extension_config_dict=None, log_level='warning', log_format='%(asctime)s,%(msecs)03.0f [%(name)-17s][%(levelname)-8s] %(message)s', device_blacklist=[], device_whitelist=[], hwm=None, device_worker_processes=1, serializer='msgpack', buffer=None, opts=None):
    """"""
        Init the napalm-logs engine.

        :param address: The address to bind the syslog client. Default: 0.0.0.0.
        :param port: Listen port. Default: 514.
        :param listener: Listen type. Default: udp.
        :param publish_address: The address to bing when publishing the OC
                                 objects. Default: 0.0.0.0.
        :param publish_port: Publish port. Default: 49017.
        """"""
    self.opts = opts if opts else {}
    sentry_dsn = self.opts.get('sentry_dsn') or os.getenv('SENTRY_DSN')
<mask>:
        if HAS_SENTRY:
            sentry_sdk.init(sentry_dsn, **self.opts.get('sentry_opts', {'traces_sample_rate': 1.0}))
        else:
            log.warning('Sentry DSN provided, but the sentry_sdk library is not installed')
    self.address = address
    self.port = port
    self.listener = listener
    self.publisher = publisher
    self.publish_address = publish_address
    self.publish_port = publish_port
    self.auth_address = auth_address
    self.auth_port = auth_port
    self.metrics_enabled = metrics_enabled
    self.metrics_address = metrics_address
    self.metrics_port = metrics_port
    self.metrics_dir = metrics_dir
    self.certificate = certificate
    self.keyfile = keyfile
    self.disable_security = disable_security
    self.config_path = config_path
    self.config_dict = config_dict
    self.extension_config_path = extension_config_path
    self.extension_config_dict = extension_config_dict
    self.log_level = log_level
    self.log_format = log_format
    self.device_whitelist = device_whitelist
    self.device_blacklist = device_blacklist
    self.serializer = serializer
    self.device_worker_processes = device_worker_processes
    self.hwm = hwm
    self._buffer_cfg = buffer
    self._buffer = None
    self._setup_log()
    self._build_config()
    self._verify_config()
    self._post_preparation()
    self._setup_metrics()
    self._setup_buffer()
    self.__priv_key = None
    self.__signing_key = None
    self._processes = []
    self.up = True",sentry_dsn,212,sentry_dsn,True,100.00000000000004,N/A
"def __exit__(self, exc_type, exc_value, exc_traceback):
    self.stop_engine()
<mask>:
        log.error('Exiting due to unhandled exception', exc_info=True)
        self.__raise_clean_exception(exc_type, exc_value, exc_traceback)",exc_type is not None,16,self.exception_handler,False,8.745825313180626,N/A
"def _setup_buffer(self):
    """"""
        Setup the buffer subsystem.
        """"""
<mask>:
        return
    buffer_name = list(self._buffer_cfg.keys())[0]
    buffer_class = napalm_logs.buffer.get_interface(buffer_name)
    log.debug('Setting up buffer interface ""%s""', buffer_name)
    if 'expire_time' not in self._buffer_cfg[buffer_name]:
        self._buffer_cfg[buffer_name]['expire_time'] = CONFIG.BUFFER_EXPIRE_TIME
    self._buffer = buffer_class(**self._buffer_cfg[buffer_name])","not self._buffer_cfg or not isinstance(self._buffer_cfg, dict)",33,self._buffer,False,1.8315638888734187,N/A
"def _setup_metrics(self):
    """"""
        Start metric exposition
        """"""
    path = os.environ.get('prometheus_multiproc_dir')
<mask>:
        try:
            log.info('Creating metrics directory')
            os.makedirs(self.metrics_dir)
        except OSError:
            log.error('Failed to create metrics directory!')
            raise ConfigurationException('Failed to create metrics directory!')
        path = self.metrics_dir
    elif path != self.metrics_dir:
        path = self.metrics_dir
    os.environ['prometheus_multiproc_dir'] = path
    log.info('Cleaning metrics collection directory')
    log.debug('Metrics directory set to: {}'.format(path))
    files = os.listdir(path)
    for f in files:
        if f.endswith('.db'):
            os.remove(os.path.join(path, f))
        log.debug('Starting metrics exposition')
    if self.metrics_enabled:
        registry = CollectorRegistry()
        multiprocess.MultiProcessCollector(registry)
        start_http_server(port=self.metrics_port, addr=self.metrics_address, registry=registry)",not os.path.exists(self.metrics_dir),74,path is None,False,0.9816077559181531,N/A
"def _post_preparation(self):
    """"""
        The steps for post-preparation (when the logs, and everything is
        already setup).
        """"""
    self.opts['hwm'] = CONFIG.ZMQ_INTERNAL_HWM if self.hwm is None else self.hwm
    self.opts['_server_send_unknown'] = False
    for pub in self.publisher:
        pub_name = list(pub.keys())[0]
        pub_opts = list(pub.values())[0]
        error_whitelist = pub_opts.get('error_whitelist', [])
        error_blacklist = pub_opts.get('error_blacklist', [])
<mask>:
            error_blacklist.append('UNKNOWN')
        if 'RAW' not in error_blacklist:
            error_blacklist.append('RAW')
        if 'only_unknown' in pub_opts and pub[pub_name]['only_unknown']:
            pub[pub_name]['send_unknown'] = True
            error_whitelist = ['UNKNOWN']
            error_blacklist = []
        if 'only_raw' in pub_opts and pub[pub_name]['only_raw']:
            pub[pub_name]['send_raw'] = True
            error_whitelist = ['RAW']
            error_blacklist = []
        if 'send_unknown' in pub_opts and 'UNKNOWN' in error_blacklist:
            error_blacklist.remove('UNKNOWN')
        if 'send_raw' in pub_opts and 'RAW' in error_blacklist:
            error_blacklist.remove('RAW')
        self.opts['_server_send_unknown'] |= 'UNKNOWN' in error_whitelist or 'UNKNOWN' not in error_blacklist
        pub[pub_name]['error_whitelist'] = error_whitelist
        pub[pub_name]['error_blacklist'] = error_blacklist",'UNKNOWN' not in error_blacklist,118,'UNKNOWN' not in error_whitelist,False,75.98356856515926,N/A
"def __init__(self, address, port, **kwargs):
<mask>:
        address = kwargs['address']
    if kwargs.get('port'):
        port = kwargs['port']
    self.address = address
    self.port = port
    self.hwm = kwargs.get('hwm')
    self.keepalive = kwargs.get('keepalive', 1)
    self.keepalive_idle = kwargs.get('keepalive_idle', 300)
    self.keepalive_interval = kwargs.get('keepalive_interval', -1)
    self.recvtimeout = kwargs.get('timeout')
    self.protocol = kwargs.get('protocol', 'tcp')
    self.type = kwargs.get('socket_type', 'PULL')",kwargs.get('address'),46,kwargs.get('address'),True,100.00000000000004,N/A
"def start(self):
    """"""
        Startup the zmq consumer.
        """"""
    zmq_uri = '{protocol}://{address}:{port}'.format(protocol=self.protocol, address=self.address, port=self.port) if self.port else '{protocol}://{address}'.format(protocol=self.protocol, address=self.address)
    log.debug('ZMQ URI: %s', zmq_uri)
    self.ctx = zmq.Context()
<mask>:
        skt_type = getattr(zmq, self.type)
    else:
        skt_type = zmq.PULL
    self.sub = self.ctx.socket(skt_type)
    self.sub.connect(zmq_uri)
    if self.hwm is not None:
        self.sub.setsockopt(zmq.RCVHWM, self.hwm)
    if self.recvtimeout is not None:
        log.debug('Setting RCVTIMEO to %d', self.recvtimeout)
        self.sub.setsockopt(zmq.RCVTIMEO, self.recvtimeout)
    if self.keepalive is not None:
        log.debug('Setting TCP_KEEPALIVE to %d', self.keepalive)
        self.sub.setsockopt(zmq.TCP_KEEPALIVE, self.keepalive)
    if self.keepalive_idle is not None:
        log.debug('Setting TCP_KEEPALIVE_IDLE to %d', self.keepalive_idle)
        self.sub.setsockopt(zmq.TCP_KEEPALIVE_IDLE, self.keepalive_idle)
    if self.keepalive_interval is not None:
        log.debug('Setting TCP_KEEPALIVE_INTVL to %d', self.keepalive_interval)
        self.sub.setsockopt(zmq.TCP_KEEPALIVE_INTVL, self.keepalive_interval)","hasattr(zmq, self.type)",93,self.type is not None,False,21.64910073203448,N/A
"def __init__(self, address, port, **kwargs):
<mask>:
        self.address = kwargs['address']
    else:
        self.address = address
    if kwargs.get('port'):
        self.port = kwargs['port']
    else:
        self.port = port
    self.buffer_size = kwargs.get('buffer_size', BUFFER_SIZE)
    self.reuse_port = kwargs.get('reuse_port', REUSE_PORT)
    self.socket_timeout = kwargs.get('socket_timeout', TIMEOUT)
    self.max_clients = kwargs.get('max_clients', MAX_TCP_CLIENTS)
    self.framing = kwargs.get('framing', 'traditional')
    self.frame_delimiter = kwargs.get('frame_delimiter', '\n')
    self.buffer = queue.Queue()",kwargs.get('address'),49,kwargs.get('address'),True,100.00000000000004,N/A
"def _client_connection(self, conn, addr):
    """"""
        Handle the connecition with one client.
        """"""
    log.debug('Established connection with %s:%d', addr[0], addr[1])
    conn.settimeout(self.socket_timeout)
    try:
        prev_msg = ''
        while self.__up:
            msg = conn.recv(self.buffer_size)
<mask>:
                continue
            log.debug('[%s] Received %s from %s', time.time(), msg, addr)
            messages = []
            if isinstance(msg, bytes):
                msg = msg.decode('utf-8')
            if self.framing == 'traditional':
                msg = prev_msg + msg
                msg_lines = msg.split(self.frame_delimiter)
                if len(msg_lines) > 1:
                    for line in msg_lines[:-1]:
                        messages.append(line)
                    prev_msg = msg_lines[-1]
                else:
                    messages = [msg]
            elif self.framing == 'octet-counted':
                msg_chunks = re.split(OCTET_FRAMING_RGX, msg)
                messages = ['{}{}'.format(pri, body).strip() for pri, body in zip(msg_chunks[1::2], msg_chunks[2::2])]
            for message in messages:
                log.debug('[%s] Queueing %s', time.time(), message)
                self.buffer.put((message, '{}:{}'.format(addr[0], addr[1])))
    except socket.timeout:
        if not self.__up:
            return
        log.info('Connection %s:%d timed out', addr[0], addr[1])
    finally:
        log.debug('Closing connection with %s', addr)
        conn.close()",not msg,124,not msg,True,100.00000000000004,N/A
"def _serve_clients(self):
    """"""
        Accept cients and serve, one separate thread per client.
        """"""
    self.__up = True
    while self.__up:
        log.debug('Waiting for a client to connect')
        try:
            conn, addr = self.skt.accept()
            log.debug('Received connection from %s:%d', addr[0], addr[1])
        except socket.error as error:
<mask>:
                return
            msg = 'Received listener socket error: {}'.format(error)
            log.error(msg, exc_info=True)
            raise ListenerException(msg)
        client_thread = threading.Thread(target=self._client_connection, args=(conn, addr))
        client_thread.start()",not self.__up,58,self.__up,False,81.87307530779823,N/A
"def start(self):
    """"""
        Start listening for messages.
        """"""
    log.debug('Creating the TCP server')
<mask>:
        self.skt = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)
    else:
        self.skt = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    if self.reuse_port:
        self.skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        if hasattr(socket, 'SO_REUSEPORT'):
            self.skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)
        else:
            log.error('SO_REUSEPORT not supported')
    try:
        self.skt.bind((self.address, int(self.port)))
    except socket.error as msg:
        error_string = 'Unable to bind to port {} on {}: {}'.format(self.port, self.address, msg)
        log.error(error_string, exc_info=True)
        raise BindException(error_string)
    log.debug('Accepting max %d parallel connections', self.max_clients)
    self.skt.listen(self.max_clients)
    self.thread_serve = threading.Thread(target=self._serve_clients)
    self.thread_serve.start()",':' in self.address,72,self.use_udp,False,14.320952289897704,N/A
"def receive(self):
    """"""
        Return one message dequeued from the listen buffer.
        """"""
    while self.buffer.empty() and self.__up:
        sleep_ms = random.randint(0, 1000)
        time.sleep(sleep_ms / 1000.0)
<mask>:
        return self.buffer.get(block=False)
    return ('', '')",not self.buffer.empty(),29,self.__up,False,11.725004053101795,N/A
"def __init__(self, address, port, **kwargs):
<mask>:
        self.address = kwargs['address']
    else:
        self.address = address
    if kwargs.get('port'):
        self.port = kwargs['port']
    else:
        self.port = port
    self.buffer_size = kwargs.get('buffer_size', BUFFER_SIZE)
    self.reuse_port = kwargs.get('reuse_port', REUSE_PORT)
    log.debug('Buffer size: %d', self.buffer_size)",kwargs.get('address'),34,kwargs.get('address'),True,100.00000000000004,N/A
"def start(self):
    """"""
        Create the UDP listener socket.
        """"""
<mask>:
        self.skt = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)
    else:
        self.skt = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    if self.reuse_port:
        self.skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        if hasattr(socket, 'SO_REUSEPORT'):
            self.skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)
        else:
            log.error('SO_REUSEPORT not supported')
    try:
        self.skt.bind((self.address, int(self.port)))
    except socket.error as msg:
        error_string = 'Unable to bind to port {} on {}: {}'.format(self.port, self.address, msg)
        log.error(error_string, exc_info=True)
        raise BindException(error_string)",':' in self.address,58,self.use_udp,False,14.320952289897704,N/A
"def __init__(self, address, port, **kwargs):
<mask>:
        address = kwargs['address']
    if kwargs.get('port'):
        port = kwargs['port']
    self.bootstrap_servers = kwargs.get('bootstrap_servers', '{}:{}'.format(address, port))
    self.group_id = kwargs.get('group_id', 'napalm-logs')
    self.topic = kwargs.get('topic', 'syslog.net')",kwargs.get('address'),27,kwargs.get('address'),True,100.00000000000004,N/A
"def __init__(self, address, port, **kwargs):
<mask>:
        self.address = kwargs['address']
    else:
        self.address = address
    if kwargs.get('port'):
        self.port = kwargs['port']
    else:
        self.port = port
    if kwargs.get('no_encrypt'):
        self.NO_ENCRYPT = kwargs['no_encrypt']",kwargs.get('address'),27,kwargs.get('address'),True,100.00000000000004,N/A
"def start(self):
    self.context = zmq.Context()
    self.socket = self.context.socket(zmq.PUB)
<mask>:
        self.socket.ipv6 = True
    try:
        self.socket.bind('tcp://{addr}:{port}'.format(addr=self.address, port=self.port))
    except zmq.error.ZMQError as err:
        log.error(err, exc_info=True)
        raise BindException(err)",':' in self.address,23,self.address.startswith('udp:'),False,19.64073254502565,N/A
"def __init__(self, address, port, **kwargs):
<mask>:
        self.address = kwargs['address']
    else:
        self.address = address
    if kwargs.get('port'):
        self.port = kwargs['port']
    else:
        self.port = port",kwargs.get('address'),22,kwargs.get('address'),True,100.00000000000004,N/A
"def __init__(self, address, port, **kwargs):
    super().__init__(address, port, **kwargs)
<mask>:
        self.address = '{}/alert'.format(self.address)
    self.method = 'POST'
    self.headers['Content-type'] = 'application/json'
    key = kwargs.get('key')
    if key and 'Authorization' not in self.headers:
        self.headers.update({'Authorization': 'Key {}'.format(key)})
    token = kwargs.get('token')
    if token and 'Authorization' not in self.headers:
        self.headers.update({'Authorization': 'Bearer {}'.format(token)})
    self.environment = kwargs.get('environment')
    self.pairs = kwargs.get('pairs')
    if not self.pairs:
        self.pairs = {'INTERFACE_UP': 'INTERFACE_DOWN', 'OSPF_NEIGHBOR_UP': 'OSPF_NEIGHBOR_DOWN', 'ISIS_NEIGHBOR_UP': 'ISIS_NEIGHBOR_DOWN'}",not self.address.endswith('/alert') and (not self.address.endswith('/alert/')),61,'://' not in self.address,False,4.035404353272006,N/A
"def publish(self, obj):
    data = napalm_logs.utils.unserialize(obj)
    error = data['error']
    status = 'open'
<mask>:
        error = self.pairs[error]
        status = 'closed'
    alerta_data = {'resource': '{host}::{msg}'.format(host=data['host'], msg=error), 'event': data['error'], 'service': ['napalm-logs'], 'text': data['message_details']['message'].strip(), 'attributes': data, 'status': status}
    if self.environment:
        alerta_data['environment'] = self.environment
    alerta_data['severity'] = ALERTA_SEVERITY.get(data['severity'], 'unknown')
    if self.backend == 'tornado':
        self.tornado_client.fetch(self.address, callback=self._handle_tornado_response, method=self.method, headers=self.headers, auth_username=self.username, auth_password=self.password, body=str(alerta_data), validate_cert=self.verify_ssl, allow_nonstandard_methods=True, decompress_response=False)
    elif self.backend == 'requests':
        self._publish_queue.put_nowait(alerta_data)",error in self.pairs,62,error in self.pairs,True,100.00000000000004,N/A
"def __parse_without_details(self, msg):
    """"""
        Helper to generate Counter metrics that only provide the host label
        from the structured message.
        """"""
    error = msg['error']
<mask>:
        self.metrics[error] = Counter('napalm_logs_{error}'.format(error=error.lower()), 'Counter for {error} notifications'.format(error=error), ['host'])
    self.metrics[error].labels(host=msg['host']).inc()
    if msg.get('state') is not None:
        base = error.split('_')[:-1]
        metric = msg.get('state_tag', '_'.join(base + ['state']).lower())
        if metric not in self.metrics:
            self.metrics[metric] = Gauge('napalm_logs_{}'.format(metric), 'State for {} type notifications'.format('_'.join(base)), ['host'])
        self.metrics[metric].labels(host=msg['host']).set(msg['state'])",error not in self.metrics,62,error not in self.metrics,True,100.00000000000004,N/A
"def __parse_user_action(self, msg):
    """"""
        Helper to generate Counter metrics that provide the host label, together
        with the username under a YANG structure users > user > [USER].
        """"""
    error = msg['error']
<mask>:
        self.metrics[error] = Counter('napalm_logs_{error}'.format(error=error.lower()), 'Counter for {error} notifications'.format(error=error), ['host', 'user'])
    self.metrics[error].labels(host=msg['host'], user=list(msg['yang_message']['users']['user'].keys())[0]).inc()",error not in self.metrics,43,error not in self.metrics,True,100.00000000000004,N/A
"def __parse_interface_basic(self, msg):
    """"""
        Helper to generate Counter metrics for interface notifications.
        """"""
    error = msg['error']
<mask>:
        self.metrics[error] = Counter('napalm_logs_{error}'.format(error=error.lower()), 'Counter for {error} notifications'.format(error=error), ['host', 'interface'])
    if 'interface_state' not in self.metrics:
        self.metrics['interface_state'] = Gauge('napalm_logs_interface_state', 'State of this interface. 0=DOWN, 1=UP', ['host', 'interface'])
    labels = {'host': msg['host'], 'interface': list(msg['yang_message']['interfaces']['interface'].keys())[0]}
    self.metrics[error].labels(**labels).inc()
    state = 1 if error == 'INTERFACE_UP' else 0
    self.metrics['interface_state'].labels(**labels).set(state)",error not in self.metrics,59,error not in self.metrics,True,100.00000000000004,N/A
"def __parse_lacp(self, msg):
    """"""
        Helper to generate Counter metrics for LACP notifications.
        """"""
    error = msg['error']
<mask>:
        self.metrics[error] = Counter('napalm_logs_{error}'.format(error=error.lower()), 'Counter for {error} notifications'.format(error=error), ['host', 'interface', 'member'])
    lacp_dict = msg['yang_message']['lacp']['interfaces']['interface']
    if_name = list(lacp_dict.keys())[0]
    members_dict = lacp_dict[if_name]['members']['member']
    member_name = list(members_dict.keys())[0]
    self.metrics[error].labels(host=msg['host'], interface=if_name, member=member_name).inc()",error not in self.metrics,42,error not in self.metrics,True,100.00000000000004,N/A
"def __parse_bgp_basic(self, msg):
    """"""
        Helper to generate Counter metrics for simple BGP notifications,
        providing the neighbor address and peer AS number.
        """"""
    error = msg['error']
<mask>:
        self.metrics[error] = Counter('napalm_logs_{error}'.format(error=error.lower()), 'Counter for {error} notifications'.format(error=error), ['host', 'neighbor', 'peer_as'])
    neigh_dict = msg['yang_message']['bgp']['neighbors']['neighbor']
    neighbor = list(neigh_dict.keys())[0]
    self.metrics[error].labels(host=msg['host'], neighbor=neighbor, peer_as=neigh_dict[neighbor]['state']['peer_as']).inc()",error not in self.metrics,45,error not in self.metrics,True,100.00000000000004,N/A
"def __init__(self, address, port, **kwargs):
<mask>:
        self.address = kwargs['address']
    else:
        self.address = address
    self.method = kwargs.get('method', 'POST')
    log.debug('Publishing to %s using method %s', self.address, self.method)
    self.auth = kwargs.get('auth')
    self.username = kwargs.get('username')
    self.password = kwargs.get('password')
    if not self.auth:
        if self.username and self.password:
            self.auth = (self.username, self.password)
    self.headers = kwargs.get('headers', {})
    self.verify_ssl = kwargs.get('verify_ssl', True)
    self.params = kwargs.get('params')
    self.max_clients = kwargs.get('max_clients', 10)
    self.backend = kwargs.get('backend')
    if not self.backend:
        log.info('No explicit backend requested')
        if HAS_TORNADO:
            self.backend = 'tornado'
            log.info('Tornado seems to be installed, so will use')
        elif HAS_REQUESTS:
            self.backend = 'requests'
            log.info('Requests seems to be installed, so will use')",kwargs.get('address'),96,'address' in kwargs,False,12.753667906901528,N/A
"def start(self):
<mask>:
        raise TransportException('Invalid HTTP backend: %s', self.backend)
    if self.backend == 'requests' and (not HAS_REQUESTS):
        raise TransportException('Trying to use Requests as backend, but it is not installed')
    if self.backend == 'tornado' and (not HAS_TORNADO):
        raise TransportException('Trying to use Tornado as backend, but it is not installed')
    if self.backend == 'tornado':
        self.tornado_client = tornado.httpclient.AsyncHTTPClient(max_clients=self.max_clients)
    elif self.backend == 'requests':
        self._publish_queue = queue.Queue()
        self._pool = []
        for index in range(self.max_clients):
            thread = threading.Thread(target=self._publish_requests)
            thread.daemon = True
            thread.start()
            self._pool.append(thread)","self.backend not in ('requests', 'tornado')",76,"self.backend not in ['http', 'https']",False,41.11336169005198,N/A
"def publish(self, obj):
    data = napalm_logs.utils.unserialize(obj)
<mask>:
        self.tornado_client.fetch(self.address, callback=self._handle_tornado_response, method=self.method, headers=self.headers, auth_username=self.username, auth_password=self.password, body=str(data), validate_cert=self.verify_ssl, allow_nonstandard_methods=True, decompress_response=False)
    elif self.backend == 'requests':
        self._publish_queue.put_nowait(data)",self.backend == 'tornado',22,self.backend == 'tornado',True,100.00000000000004,N/A
"def _publish_requests(self):
    while True:
        try:
            try:
                data = self._publish_queue.get(timeout=1)
                self._publish_queue.task_done()
            except queue.Empty:
                continue
        except AttributeError:
            continue
        session = requests.Session()
        session.auth = self.auth
        session.headers.update(self.headers)
        session.verify = self.verify_ssl
        try:
            result = session.request(self.method, self.address, params=self.params, data=json.dumps(data))
<mask>:
                log.error('Unable to publish to %s', self.address)
                log.error('Status code: %d', result.status_code)
                log.error(result.text)
            else:
                log.debug(result.text)
            del result
        except requests.ConnectionError as conn_err:
            log.error(conn_err)
        del session",not result.ok,56,result.status_code != 200,False,11.044795567078939,N/A
"def _handle_tornado_response(self, response):
<mask>:
        log.error('Unable to publish to %s', self.address)
        log.error(response.error)
    else:
        log.debug(response.body)",response.error,13,response.error,True,100.00000000000004,N/A
"def __init__(self, address, port, **kwargs):
<mask>:
        address = kwargs['address']
    if kwargs.get('port'):
        address = kwargs['port']
    if kwargs.get('no_encrypt'):
        self.NO_ENCRYPT = kwargs['no_encrypt']
    if kwargs.get('bootstrap_servers'):
        self.bootstrap_servers = kwargs['bootstrap_servers']
    else:
        self.bootstrap_servers = '{}:{}'.format(address, port)
    self.kafka_topic = kwargs.get('topic', 'napalm-logs')",kwargs.get('address'),33,kwargs.get('address'),True,100.00000000000004,N/A
"def emit(msg_dict):
    """"""
    Extracts the details from the syslog message
    and returns an object having the following structure:

    .. code-block:: python

        {
            u'users': {
                u'user': {
                    u'luke': {
                        u'action': {
                            u'login': True
                        },
                        u'uid': 0
                    }
                }
            }
        }
    """"""
    log.debug('Evaluating the message dict:')
    log.debug(msg_dict)
    ret = {}
    extracted = napalm_logs.utils.extract(_RGX, msg_dict['message'], _RGX_PARTS)
<mask>:
        return ret
    uid_key_path = 'users//user//{0[user]}//uid'.format(extracted)
    uid_value = int(extracted['uid'])
    log.debug('Setting %d under key path %s', uid_value, uid_key_path)
    ret.update(napalm_logs.utils.setval(uid_key_path, uid_value, dict_=ret))
    login_key_path = 'users//user//{0[user]}//action//login'.format(extracted)
    ret.update(napalm_logs.utils.setval(login_key_path, True, dict_=ret))
    return ret",not extracted,81,not extracted,True,100.00000000000004,N/A
"def keep_alive(self):
    """"""
        Send a keep alive request periodically to make sure that the server
        is still alive. If not then try to reconnect.
        """"""
    self.ssl_skt.settimeout(defaults.AUTH_KEEP_ALIVE_INTERVAL)
    while self.__up:
        try:
            log.debug('Sending keep-alive message to the server')
            self.ssl_skt.send(defaults.AUTH_KEEP_ALIVE)
        except socket.error:
            log.error('Unable to send keep-alive message to the server.')
            log.error('Re-init the SSL socket.')
            self.reconnect()
            log.debug('Trying to re-send the keep-alive message to the server.')
            self.ssl_skt.send(defaults.AUTH_KEEP_ALIVE)
        msg = self.ssl_skt.recv(len(defaults.AUTH_KEEP_ALIVE_ACK))
        log.debug('Received %s from the keep-alive server', msg)
<mask>:
            log.error('Received %s instead of %s form the auth keep-alive server', msg, defaults.AUTH_KEEP_ALIVE_ACK)
            log.error('Re-init the SSL socket.')
            self.reconnect()
        time.sleep(defaults.AUTH_KEEP_ALIVE_INTERVAL)",msg != defaults.AUTH_KEEP_ALIVE_ACK,90,msg != defaults.AUTH_KEEP_ALIVE_ACK,True,100.00000000000004,N/A
"def authenticate(self):
    """"""
        Authenticate the client and return the private
        and signature keys.

        Establish a connection through a secured socket,
        then do the handshake using the napalm-logs
        auth algorithm.
        """"""
    log.debug('Authenticate to %s:%d, using the certificate %s', self.address, self.port, self.certificate)
<mask>:
        skt_ver = socket.AF_INET6
    else:
        skt_ver = socket.AF_INET
    skt = socket.socket(skt_ver, socket.SOCK_STREAM)
    self.ssl_skt = ssl.wrap_socket(skt, ca_certs=self.certificate, cert_reqs=ssl.CERT_REQUIRED)
    try:
        self.ssl_skt.connect((self.address, self.port))
        self.auth_try_id = 0
    except socket.error as err:
        log.error('Unable to open the SSL socket.')
        self.auth_try_id += 1
        if not self.max_try or self.auth_try_id < self.max_try:
            log.error('Trying to authenticate again in %d seconds', self.timeout)
            time.sleep(self.timeout)
            self.authenticate()
        log.critical('Giving up, unable to authenticate to %s:%d using the certificate %s', self.address, self.port, self.certificate)
        raise ClientConnectException(err)
    self.ssl_skt.write(defaults.MAGIC_REQ)
    private_key = self.ssl_skt.recv(defaults.BUFFER_SIZE)
    self.ssl_skt.write(defaults.MAGIC_ACK)
    verify_key_hex = self.ssl_skt.recv(defaults.BUFFER_SIZE)
    self.ssl_skt.write(defaults.MAGIC_ACK)
    self.priv_key = nacl.secret.SecretBox(private_key)
    self.verify_key = nacl.signing.VerifyKey(verify_key_hex, encoder=nacl.encoding.HexEncoder)",':' in self.address,125,self.ssl_ssl_ssl_verify_key,False,7.495553473355842,N/A
"def cast(var, function):
<mask>:
        try:
            return locate(function)(var)
        except ValueError:
            log.error('Unable to use function %s on value %s', function, var, exc_info=True)
    if hasattr(str, function) and hasattr(getattr(str, function), '__call__'):
        return getattr(str, function)(var)
    glob = globals()
    if function in glob and hasattr(glob[function], '__call__'):
        return glob[function](var)
    return var","locate(function) and hasattr(locate(function), '__call__')",44,"function in ('find', 'replace')",False,1.4499508732277653,N/A
"def extract(rgx, msg, mapping, time_format=None):
    ret = {}
    log.debug('Matching regex ""%s"" on ""%s""', rgx, msg)
    matched = re.search(rgx, msg, re.I)
<mask>:
        log.info('The regex didnt match')
        return None
    else:
        group_index = 0
        for group_value in matched.groups():
            group_name = list(mapping.keys())[group_index]
            ret[group_name] = group_value
            group_index += 1
        log.debug('Regex matched')
        log.debug(ret)
    if time_format:
        try:
            parsed_time = datetime.strptime(time_format[0].format(**ret), time_format[1])
        except ValueError as error:
            log.error('Unable to convert date and time into a timestamp: %s', error)
        ret['timestamp'] = int((parsed_time - datetime(1970, 1, 1)).total_seconds())
    return ret",not matched,78,matched is None,False,27.516060407455225,N/A
"def setval(key, val, dict_=None, delim=defaults.DEFAULT_DELIM):
    """"""
    Set a value under the dictionary hierarchy identified
    under the key. The target 'foo/bar/baz' returns the
    dictionary hierarchy {'foo': {'bar': {'baz': {}}}}.

    .. note::

        Currently this doesn't work with integers, i.e.
        cannot build lists dynamically.
        TODO
    """"""
<mask>:
        dict_ = {}
    prev_hier = dict_
    dict_hier = key.split(delim)
    for each in dict_hier[:-1]:
        if isinstance(each, str):
            if each not in prev_hier:
                prev_hier[each] = {}
            prev_hier = prev_hier[each]
        else:
            prev_hier[each] = [{}]
            prev_hier = prev_hier[each]
    prev_hier[dict_hier[-1]] = val
    return dict_",not dict_,83,dict_ is None,False,31.947155212313625,N/A
"def parse_args(self, args=None, values=None):
    options, args = optparse.OptionParser.parse_args(self, args, values)
<mask>:
        new_inargs = sys.stdin.readlines()
        new_inargs = [arg.rstrip('\r\n') for arg in new_inargs]
        new_options, new_args = optparse.OptionParser.parse_args(self, new_inargs)
        options.__dict__.update(new_options.__dict__)
        args.extend(new_args)
    self.options, self.args = (options, args)",'args_stdin' in options.__dict__ and options.args_stdin is True,33,options is None,False,0.16737567801120534,N/A
"def convert_env_dict(self, d):
    for k, v in d.items():
<mask>:
            if not v.startswith('${') or not v.endswith('}'):
                continue
            if not os.environ.get(v[2:-1]):
                log.error('No env variable found for %s, please check your config file', v[2:-1])
                sys.exit(1)
            d[k] = os.environ[v[2:-1]]
        if isinstance(v, dict):
            self.convert_env_dict(v)
        if isinstance(v, list):
            self.convert_env_list(v)","isinstance(v, str)",43,v.startswith('ENV'),False,10.682175159905853,N/A
"def convert_env_list(self, lst):
    for name, value in enumerate(lst):
<mask>:
            if not value.startswith('${') or not value.endswith('}'):
                continue
            if not os.environ.get(value[2:-1]):
                log.error('No env variable found for %s, please check your config file', value[2:-1])
                sys.exit(1)
            lst[name] = os.environ[value[2:-1]]
        if isinstance(value, dict):
            self.convert_env_dict(value)
        if isinstance(value, list):
            self.convert_env_list(value)","isinstance(value, str)",43,not name.startswith('ENV'),False,7.809849842300637,N/A
"def napalm_logs_engine():
<mask>:
        sys.path.remove('')
    screen_logger = logging.StreamHandler(sys.stdout)
    screen_logger.setFormatter(logging.Formatter(defaults.LOG_FORMAT))
    log.addHandler(screen_logger)
    nlop = NLOptionParser()
    config = nlop.parse(log, screen_logger)
    signal.signal(signal.SIGINT, signal.SIG_IGN)
    signal.signal(signal.SIGTERM, signal.SIG_IGN)
    nl = napalm_logs.NapalmLogs(**config)
    nl.start_engine()
    signal.signal(signal.SIGINT, _exit_gracefully)
    signal.signal(signal.SIGTERM, _exit_gracefully)
    while _up is True and nl.up is True:
        time.sleep(1)
    nl.stop_engine()",'' in sys.path,38,os.path.exists(''),False,12.22307556087252,N/A
"def __getitem__(self, key):
    try:
        item = self._cache[key]
    except KeyError:
        return None
<mask>:
        return item['data']
    else:
        del self._cache[key]
        return None",datetime.datetime.utcnow() - item['timestamp'] < self.expire_time_delta,19,item['type'] == 'object',False,2.269295753552573,N/A
"def items(self):
    keys = list(self._cache)
    for key in keys:
        val = self[key]
<mask>:
            yield key",val,15,val is not None,False,15.97357760615681,N/A
"def __getitem__(self, key):
    key = '{prefix}{key}'.format(prefix=self._key_prefix, key=key)
    val = self._redis.get(key)
<mask>:
        self._redis.srem(self._keys_set_name, key)
    return val",val is None,15,val is None,True,100.00000000000004,N/A
"def items(self):
    keys = self._redis.smembers(self._keys_set_name)
    for key in keys:
        self._redis_pipeline.get(key)
    get_results = self._redis_pipeline.execute()
    key_vals = dict(zip(keys, get_results))
    for key, value in key_vals.items():
<mask>:
            yield key
        else:
            self._redis_pipeline.srem(self._keys_set_name, key)
    self._redis_pipeline.execute()",value,29,value,True,100.00000000000004,N/A
"@staticmethod
def _parse_sw_information(swinfo: str) -> Tuple[str, str]:
<mask>:
        version, build_date = swinfo.split(',')
    else:
        version, build_date = swinfo.split()
    _, _, version = version.rpartition('=')
    _, _, build_date = build_date.rpartition(':')
    return (version, build_date)","',' in swinfo",30,':' in swinfo,False,42.72870063962342,N/A
"def config_backup(self, filename: Optional[str]=None) -> Optional[str]:
    ret = self.command('Config.backup?action=All')
<mask>:
        return None
    if filename:
        with open(filename, 'w+') as cfg:
            cfg.write(ret.content.decode())
        return None
    return ret.content.decode()",not ret,24,ret.status_code != 200,False,5.522397783539471,N/A
"def reboot(self, delay: Optional[int]=None) -> str:
    cmd = 'reboot'
<mask>:
        cmd += f'&delay={delay}'
    return self._magic_box(cmd)",delay,15,delay is not None,False,15.97357760615681,N/A
"def _event_lines(ret: Iterable[str]) -> Iterator[str]:
    line = []
    for char in ret:
        line.append(char)
<mask>:
            yield ''.join(line).strip()
            line.clear()","line[-2:] == ['\r', '\n']",17,line,False,3.0590232050182594e-05,N/A
"def event_channels_happened(self, eventcode: str) -> List[int]:
    """"""
        Params:

        VideoMotion: motion detection event
        VideoLoss: video loss detection event
        VideoBlind: video blind detection event
        AlarmLocal: alarm detection event
        StorageNotExist: storage not exist event
        StorageFailure: storage failure event
        StorageLowSpace: storage low space event
        AlarmOutput: alarm output event
        SmartMotionHuman: human detection event
        SmartMotionVehicle: vehicle detection event
        """"""
    ret = self.command(f'eventManager.cgi?action=getEventIndexes&code={eventcode}')
    output = ret.content.decode()
<mask>:
        return []
    return [int(pretty(channel)) for channel in output.split()]",'Error' in output,68,not output,False,30.326532985631665,N/A
"def event_stream(self, eventcodes: str, *, retries: Optional[int]=None, timeout_cmd: TimeoutT=None) -> Iterator[str]:
    """"""
        Return a stream of event info lines.

        eventcodes: One or more event codes separated by commas with no spaces

        VideoMotion: motion detection event
        VideoLoss: video loss detection event
        VideoBlind: video blind detection event
        AlarmLocal: alarm detection event
        StorageNotExist: storage not exist event
        StorageFailure: storage failure event
        StorageLowSpace: storage low space event
        AlarmOutput: alarm output event
        SmartMotionHuman: human detection event
        SmartMotionVehicle: vehicle detection event
        """"""
    urllib3_logger = logging.getLogger('urllib3.connectionpool')
<mask>:
        urllib3_logger.addFilter(NoHeaderErrorFilter())
    if timeout_cmd is None:
        if isinstance(self._timeout_default, tuple):
            timeout_cmd = (self._timeout_default[0], None)
        else:
            timeout_cmd = (self._timeout_default, None)
    ret = self.command(f'eventManager.cgi?action=attach&codes=[{eventcodes}]', retries=retries, timeout_cmd=timeout_cmd, stream=True)
    if ret.encoding is None:
        ret.encoding = 'utf-8'
    try:
        for line in _event_lines(ret.iter_content(decode_unicode=True)):
            if line.lower().startswith('content-length:'):
                chunk_size = int(line.split(':')[1]) + 2
                try:
                    yield next(ret.iter_content(chunk_size=chunk_size, decode_unicode=True))
                except StopIteration:
                    return
    except (RequestException, HTTPError) as error:
        _LOGGER.debug('%s Error during event streaming: %r', self, error)
        raise CommError(error) from error
    finally:
        ret.close()","not any((isinstance(x, NoHeaderErrorFilter) for x in urllib3_logger.filters))",148,not urllib3_logger.hasFilter(NoHeaderErrorFilter),False,11.235571815872339,N/A
"def _build_payload(self, event_info: str) -> Dict[str, Any]:
    _LOGGER.debug('%s event info: %r', self, event_info)
    payload = {}
    for key, value in _REG_PARSE_KEY_VALUE.findall(event_info.strip().replace('\n', '')):
<mask>:
            try:
                value = json.loads(value)
            except json.JSONDecodeError:
                pass
        payload[key] = value
    _LOGGER.debug('%s generate new event, code: %s , payload: %s', self, payload['Code'], payload)
    return payload",key == 'data',47,key == 'Message',False,59.460355750136046,N/A
"def find_files(self, start_time: datetime, end_time: datetime, channel: int=0, directories: Sequence[str]=(), types: Sequence[str]=(), flags: Sequence[str]=(), events: Sequence[str]=(), stream: Optional[str]=None) -> Iterator[str]:
    """"""
        https://s3.amazonaws.com/amcrest-files/Amcrest+HTTP+API+3.2017.pdf

        dir : in which directories you want to find the file. It is an array.
                The index starts from 1. The range of dir is {""/mnt/dvr/sda0"",
                ""/mnt/dvr/sda1""}. This condition can be omitted. If omitted,
                find files in all the directories.

        type : which types of the file you want to find. It is an array. The
                index starts from 0. The range of type is {""dav"", ""jpg"", ""mp4""}
                If omitted, find files with all the types.

        flag : which flags of the file you want to find. It is an array. The
                index starts from 0. The range of flag is {""Timing"", ""Manual"",
                ""Marker"", ""Event"", ""Mosaic"", ""Cutout""}. If omitted, find files
                with all the flags.

        event : by which event the record file is triggered. It is an array.
                The index starts from 0. The range of event is {""AlarmLocal"",
                ""VideoMotion"", ""VideoLoss"", ""VideoBlind"", ""Traffic*""}. This
                condition can be omitted. If omitted, find files of all the
                events. stream : which video stream type you want to find.
                The range of stream is {""Main"", ""Extra1"", ""Extra2"", ""Extra3""}.
                If omitted, find files with all the stream types.
        """"""
    factory_id = self.factory_create().strip().split('=')[1]
    _LOGGER.debug('%s findFile for factory_id=%s', self, factory_id)
    search = self.media_file_find_start(factory_id=factory_id, start_time=start_time, end_time=end_time, channel=channel, directories=directories, types=types, flags=flags, events=events, stream=stream)
<mask>:
        while True:
            _LOGGER.debug('%s findNextFile', self)
            content = self.media_file_find_next(factory_id)
            tag, _, str_count = content.split('\r\n', 1)[0].partition('=')
            if tag == 'found':
                count = int(str_count)
            else:
                break
            _LOGGER.debug('%s returned %s %s', self, tag, count)
            if count == 0:
                break
            yield content
        self.factory_close(factory_id)
        self.factory_destroy(factory_id)
    else:
        _LOGGER.debug('%s returned error: %s', self, search)",'ok' in search.lower(),275,not search,False,4.104249931194939,N/A
"def log_find(self, start_time: datetime, end_time: datetime) -> Iterator[str]:
    token = self.log_find_start(start_time, end_time).strip().split('=')[1]
    while True:
        content = self.log_find_next(token)
        tag, _, count = content.split('\r\n', 1)[0].partition('=')
        yield content
<mask>:
            break
    self.log_find_stop(token)",tag != 'found' or int(count) == 0,28,count > 0,False,1.7260212584862158,N/A
"def str2bool(value: Union[str, int]) -> bool:
    """"""
    Args:
        value - text to be converted to boolean
         True values: y, yes, true, t, on, 1
         False values: n, no, false, off, 0
    """"""
<mask>:
        return bool(util.strtobool(value))
    return bool(value)","isinstance(value, str)",37,"isinstance(value, str)",True,100.00000000000004,N/A
"def to_unit(value: Union[None, str, int, float], unit: str='B') -> Tuple[str, str]:
    """"""Convert bytes to give unit.""""""
    byte_array = ['B', 'KB', 'MB', 'GB', 'TB']
<mask>:
        raise ValueError(f'Unit {unit} missing from known units')
    if value is None:
        return ('unknown', unit)
    if isinstance(value, (int, float)):
        value_f = value
    else:
        try:
            value_f = float(value)
        except (TypeError, ValueError):
            return ('unknown', unit)
    result = value_f / 1024 ** byte_array.index(unit)
    return (str(round(result, PRECISION)), unit)",unit not in byte_array,67,unit not in byte_array,True,100.00000000000004,N/A
"def enable_audio_video_cmd(param: str, enable: bool, channel: int, *, stream: str) -> str:
    """"""Return command to enable/disable all audio/video streams.""""""
    formats = [('Extra', 3), ('Main', 4)]
<mask>:
        formats.append(('Snap', 3))
    if stream is not None:
        formats = [x for x in formats if x[0] == stream]
        if not formats:
            raise RuntimeError(f'Bad stream specified: {stream}')
    set_enable = str(enable).lower()
    cmds = ['configManager.cgi?action=setConfig']
    cmds.extend((f'Encode[{channel}].{fmt}Format[{i}].{param}Enable={set_enable}' for fmt, num in formats for i in range(num)))
    return '&'.join(cmds)",param == 'Video',70,enable,False,0.0,N/A
"def init_poolmanager(self, *args, **kwargs) -> None:
<mask>:
        kwargs['socket_options'] = self.socket_options
    super().init_poolmanager(*args, **kwargs)",self.socket_options is not None,12,'socket_options' not in kwargs,False,6.916271812933183,N/A
"def _generate_token(self) -> None:
    """"""Create authentation to use with requests.""""""
    cmd = 'magicBox.cgi?action=getMachineName'
    _LOGGER.debug('%s Trying Basic Authentication', self)
    self._token = requests.auth.HTTPBasicAuth(self._user, self._password)
    try:
        try:
            resp = self._command(cmd).content.decode()
        except LoginError:
            _LOGGER.debug('%s Trying Digest Authentication', self)
            self._token = requests.auth.HTTPDigestAuth(self._user, self._password)
            resp = self._command(cmd).content.decode()
    except CommError:
        self._token = None
        raise
    result = resp.lower()
<mask>:
        _LOGGER.debug('%s Result from camera: %s', self, resp.strip().replace('\r\n', ': '))
        self._token = None
        raise LoginError('Invalid credentials')
    self._name = pretty(resp.strip())
    _LOGGER.debug('%s Retrieving serial number', self)
    self._serial = pretty(self._command('magicBox.cgi?action=getSerialNo').content.decode().strip())",'invalid' in result or 'error' in result,77,result != 'OK',False,7.545383788761362,N/A
"def __repr__(self) -> str:
    """"""Default object representation.""""""
<mask>:
        return f'<Unconnected @ {self._host}>'
    if self._serial is None:
        return f'<{self._name}:CONNECTING>'
    return f'<{self._name}:{self._serial}>'",self._name is None,20,self._host is not None,False,27.77619034011791,N/A
"def as_dict(self) -> dict:
    """"""Callback for __dict__.""""""
    cdict = self.__dict__.copy()
    redacted = '**********'
<mask>:
        cdict['_token'] = redacted
    cdict['_password'] = redacted
    return cdict",cdict['_token'] is not None,22,self.token,False,0.0,N/A
"def command(self, *args, **kwargs) -> requests.Response:
    """"""
        Args:
            cmd - command to execute via http
            retries - maximum number of retries each connection should attempt
            timeout_cmd - timeout
            stream - if True do not download entire response immediately
        """"""
    with self._token_lock:
<mask>:
            self._generate_token()
    return self._command(*args, **kwargs)",not self._token,46,not self._token,True,100.00000000000004,N/A
"def _get_storage_values(self, info: str, *params) -> List[Optional[float]]:
    ret: List[Optional[float]] = []
    for param in params:
        match = re.search(f'.{param}=([0-9.]+)', info)
<mask>:
            ret.append(None)
        else:
            ret.append(float(match.group(1)))
    return ret",match is None,25,match is None,True,100.00000000000004,N/A
"def _build_storage_type(self, used: Optional[float], total: Optional[float]) -> StorageT:
<mask>:
        return {'used_percent': 'unknown', 'used': to_unit(used), 'total': to_unit(total)}
    try:
        used_percent = percent(float(used), float(total))
    except (TypeError, ValueError, ZeroDivisionError):
        used_percent = 'unknown'
    return {'used_percent': used_percent, 'used': to_unit(used), 'total': to_unit(total)}",used is None or total is None,35,used is None or total is None,True,100.00000000000004,N/A
"def __raw_scan(self, ipaddr: str, timeout: Optional[float]=None) -> None:
<mask>:
        socket.setdefaulttimeout(timeout)
    else:
        socket.setdefaulttimeout(0.2)
    with closing(socket.socket()) as sock:
        try:
            sock.connect((ipaddr, self.__RTSP_PORT))
            sock.connect((ipaddr, self.__PWGPSI_PORT))
            self.amcrest_ips.append(ipaddr)
        except:
            pass",timeout,24,timeout is not None,False,15.97357760615681,N/A
"def scan_devices(self, subnet: str, timeout: Optional[float]=None) -> List[str]:
    """"""
        Scan cameras in a range of ips

        Params:
        subnet - subnet, i.e: 192.168.1.0/24
                 if mask not used, assuming mask 24

        timeout - timeout in sec

        Returns:
        """"""
    max_range = {16: 256, 24: 256, 25: 128, 27: 32, 28: 16, 29: 8, 30: 4, 31: 2}
<mask>:
        mask = int(24)
        network = subnet
    else:
        network, mask_str = subnet.split('/')
        mask = int(mask_str)
    if mask not in max_range:
        raise RuntimeError('Cannot determine the subnet mask!')
    network = network.rpartition('.')[0]
    if mask == 16:
        for i in range(0, 1):
            network = network.rpartition('.')[0]
    if mask == 16:
        for seq1 in range(max_range[mask]):
            for seq2 in range(max_range[mask]):
                ipaddr = f'{network}.{seq1}.{seq2}'
                thd = threading.Thread(target=self.__raw_scan, args=(ipaddr, timeout))
                thd.start()
    else:
        for seq1 in range(max_range[mask]):
            ipaddr = f'{network}.{seq1}'
            thd = threading.Thread(target=self.__raw_scan, args=(ipaddr, timeout))
            thd.start()
    return self.amcrest_ips",'/' not in subnet,132,subnet.startswith('192.168.1.0'),False,8.116697886877475,N/A
"def _process_record_mode(self, record_mode: str, channel: int) -> str:
    status_code = {0: 'Automatic', 1: 'Manual', 2: 'Stop'}
    statuses = [pretty(s) for s in record_mode.split() if f'[{channel}].Mode=' in s]
<mask>:
        return 'Unknown'
    status = int(statuses[0])
    if status not in status_code:
        return 'Unknown'
    return status_code[status]",len(statuses) != 1,42,not statuses,False,4.104249931194939,N/A
"def play_wav(self, httptype: Optional[str]=None, channel: Optional[int]=None, path_file: Optional[str]=None, encoding: str='G.711A') -> None:
<mask>:
        httptype = 'singlepart'
    if channel is None:
        channel = 1
    if path_file is None:
        raise RuntimeError('filename is required')
    self.audio_send_stream(httptype, channel, path_file, encoding)",httptype is None,35,httptype is None,True,100.00000000000004,N/A
"def audio_send_stream(self, httptype: Optional[str]=None, channel: Optional[int]=None, path_file: Optional[str]=None, encode: Optional[str]=None) -> None:
    """"""
        Params:

            path_file - path to audio file
            channel: - integer
            httptype - type string (singlepart or multipart)

                singlepart: HTTP content is a continuos flow of audio packets
                multipart: HTTP content type is multipart/x-mixed-replace, and
                           each audio packet ends with a boundary string

            Supported audio encode type according with documentation:
                PCM
                ADPCM
                G.711A
                G.711.Mu
                G.726
                G.729
                MPEG2
                AMR
                AAC

        """"""
<mask>:
        raise RuntimeError('Requires htttype and channel')
    if encode is None:
        raise RuntimeError('Requires encode')
    if path_file is None:
        raise RuntimeError('Requires path_file')
    header = {'content-type': 'Audio/' + encode, 'content-length': '9999999'}
    cmd = f'audio.cgi?action=postAudio&httptype={httptype}&channel={channel}'
    with open(path_file, 'rb') as f:
        file_audio = {'file': f}
        self.command_audio(cmd, file_content=file_audio, http_header=header)",httptype is None or channel is None,115,httptype is None,False,26.359713811572682,N/A
"def audio_stream_capture(self, httptype: Optional[str]=None, channel: Optional[int]=None, path_file: Optional[str]=None) -> bytes:
    """"""
        Params:

            httptype - type string (singlepart or multipart)
                singlepart: HTTP content is a continuos flow of audio packets
                multipart: HTTP content type is multipart/x-mixed-replace, and
                           each audio packet ends with a boundary string
            channel - integer
            path_file - path to output file
        """"""
<mask>:
        raise RuntimeError('Requires htttype and channel')
    ret = self.command(f'audio.cgi?action=getAudio&httptype={httptype}&channel={channel}', stream=True)
    if path_file:
        try:
            with open(path_file, 'wb') as out_file:
                shutil.copyfileobj(ret.raw, out_file)
        except HTTPError as error:
            _LOGGER.debug('%s Audio stream capture to file failed due to error: %s', self, repr(error))
            raise CommError(error) from error
    return ret.raw",httptype is None and channel is None,97,not httptype or not channel,False,8.51528917838043,N/A
"def add_user(self, username: str, password: str, group: str, sharable: bool=True, reserved: bool=False, memo: Optional[str]=None) -> str:
    """"""
        Params:
            username - username for user
            password - password for user
            group - string the range is ""admin"" and ""user"". In different group,
                    the user has different authorities.

            sharable - bool, true means allow multi-point login

            reserved - bool, true means this user can't be deleted

            memo - memo to user
        """"""
    cmd = f'addUser&user.Name={username}&user.Password={password}&user.Group={group.lower()}&user.Sharable={str(sharable).lower()}&user.Reserved={str(reserved).lower()}'
<mask>:
        cmd += f'&user.Memo={memo}'
    return self._user_manager(cmd)",memo,78,memo,True,100.00000000000004,N/A
"def modify_user(self, username: str, attribute: str, value: str) -> str:
    """"""
        Params:
            username - username for user
            attribute - the attribute name that will change:
                        group, sharable, reserved, memo

            value - the new value for attribute
        """"""
    cmd = f'modifyUser&name={username}'
<mask>:
        cmd += f'&user.Group={value.lower()}'
    elif attribute.lower() == 'sharable':
        cmd += f'&user.Sharable={value.lower()}'
    elif attribute.lower() == 'reserved':
        cmd += f'&user.Reserved={value.lower()}'
    elif attribute == 'memo':
        cmd += f'&user.Memo={value.lower()}'
    return self._user_manager(cmd)",attribute.lower() == 'group',67,attribute.lower() == 'group',True,100.00000000000004,N/A
"def video_in_option(self, param: str, *, profile: str='Day') -> List[str]:
    """"""
        Return video input option.

        Params:
            param - parameter, such as 'DayNightColor'
            profile - 'Day', 'Night' or 'Normal'
        """"""
<mask>:
        field = param
    else:
        field = f'{profile}Options.{param}'
    return [utils.pretty(opt) for opt in self.video_in_options.split() if f'].{field}=' in opt]",profile == 'Day',46,profile == 'Day',True,100.00000000000004,N/A
"def set_video_in_option(self, param: str, value: str, *, profile: str='Day', channel: int=0) -> str:
<mask>:
        field = param
    else:
        field = f'{profile}Options.{param}'
    ret = self.command(f'configManager.cgi?action=setConfig&VideoInOptions[{channel}].{field}={value}')
    return ret.content.decode()",profile == 'Day',26,profile == 'Day',True,100.00000000000004,N/A
"def realtime_stream(self, *, channel: int=1, typeno: int=0, path_file: Optional[str]=None) -> bytes:
    """"""
        If the stream is redirect to a file, use mplayer tool to
        visualize the video record

        camera.realtime_stream(path_file=""/home/user/Desktop/myvideo)
        $ mplayer /home/user/Desktop/myvideo
        """"""
    ret = self.command(f'realmonitor.cgi?action=getStream&channel={channel}&subtype={typeno}', stream=True)
<mask>:
        try:
            with open(path_file, 'wb') as out_file:
                shutil.copyfileobj(ret.raw, out_file)
        except HTTPError as error:
            _LOGGER.debug('%s Realtime stream capture to file failed due to error: %s', self, repr(error))
            raise CommError(error) from error
    return ret.raw",path_file,69,path_file,True,100.00000000000004,N/A
"def mjpg_stream(self, *, channel: int=1, typeno: int=0, path_file: Optional[str]=None) -> bytes:
    """"""
        Params:
            channel: integer, the video channel index which starts from 1,
                     default 1 if not specified.

            typeno: the stream type, default 0 if not specified. It can be
                    the following value:

                    0-Main Stream
                    1-Extra Stream 1 (Sub Stream)
                    2-Extra Stream 2 (Sub Stream)
        """"""
    cmd = self.mjpeg_url(channel=channel, typeno=typeno)
    ret = self.command(cmd, stream=True)
<mask>:
        try:
            with open(path_file, 'wb') as out_file:
                shutil.copyfileobj(ret.raw, out_file)
        except HTTPError as error:
            _LOGGER.debug('%s MJPEG stream capture to file failed due to error: %s', self, repr(error))
            raise CommError(error) from error
    return ret.raw",path_file,96,path_file,True,100.00000000000004,N/A
"def snapshot(self, *, channel: Optional[int]=None, path_file: Optional[str]=None, timeout: TimeoutT=None, stream: bool=True) -> Union[bytes, HTTPResponse]:
    """"""
        Args:

            channel:
                Video input channel number

                If no channel param is used, don't send channel parameter
                so camera will use its default channel

            path_file:
                If path_file is provided, save the snapshot
                in the path

        Return:
            raw from http request if stream is True
            response content if stream is False
        """"""
    cmd = 'snapshot.cgi'
<mask>:
        cmd += f'?channel={channel}'
    ret = self.command(cmd, timeout_cmd=timeout, stream=stream)
    if path_file:
        with open(path_file, 'wb') as out_file:
            if stream:
                try:
                    shutil.copyfileobj(ret.raw, out_file)
                except HTTPError as error:
                    _LOGGER.debug('%s Snapshot to file failed due to error: %s', self, repr(error))
                    raise CommError(error) from error
            else:
                out_file.write(ret.content)
    return ret.raw if stream else ret.content",channel is not None,116,channel,False,4.9787068367863965,N/A
"def test_generate_hash(self):
    input_string = self.data.joinpath('logic_collision_ruleset.yar').read_text()
    result = plyara.core.Plyara().parse_string(input_string)
    rule_mapping = {}
    for entry in result:
        rulename = entry['rule_name']
        setname, _ = rulename.split('_')
        rulehash = generate_hash(entry)
<mask>:
            rule_mapping[setname] = [rulehash]
        else:
            rule_mapping[setname].append(rulehash)
    for setname, hashvalues in rule_mapping.items():
        self.assertTrue(len(set(hashvalues)) == 1, 'Collision detection failure for {}'.format(setname))",setname not in rule_mapping,44,setname not in rule_mapping,True,100.00000000000004,N/A
"def test_scopes(self):
    input_string = self.data.joinpath('scope_ruleset.yar').read_text()
    result = self.parser.parse_string(input_string)
    for entry in result:
        rulename = entry['rule_name']
<mask>:
            self.assertIn('global', entry['scopes'])
        elif rulename == 'PrivateScope':
            self.assertIn('private', entry['scopes'])
        elif rulename == 'PrivateGlobalScope':
            self.assertIn('global', entry['scopes'])
            self.assertIn('private', entry['scopes'])
        else:
            raise AssertionError(self.unhandled_rule.format(rulename))",rulename == 'GlobalScope',35,rulename == 'GlobalScope',True,100.00000000000004,N/A
"def test_tags(self):
    input_string = self.data.joinpath('tag_ruleset.yar').read_text()
    result = self.parser.parse_string(input_string)
    for entry in result:
        rulename = entry['rule_name']
<mask>:
            self.assertEqual(len(entry['tags']), 1)
            self.assertIn('tag1', entry['tags'])
        elif rulename == 'TwoTags':
            self.assertEqual(len(entry['tags']), 2)
            self.assertIn('tag1', entry['tags'])
            self.assertIn('tag2', entry['tags'])
        elif rulename == 'ThreeTags':
            self.assertTrue(len(entry['tags']), 3)
            self.assertIn('tag1', entry['tags'])
            self.assertIn('tag2', entry['tags'])
            self.assertIn('tag3', entry['tags'])
        else:
            raise AssertionError(self.unhandled_rule.format(rulename))",rulename == 'OneTag',45,rulename == 'OneTags',False,59.460355750136046,N/A
"def test_metadata(self):
    input_string = self.data.joinpath('metadata_ruleset.yar').read_text()
    result = self.parser.parse_string(input_string)
    for entry in result:
        rulename = entry['rule_name']
        kv = entry['metadata']
        kv_list = [(k,) + (v,) for dic in kv for k, v in dic.items()]
<mask>:
            self.assertEqual(len(kv), 2)
            self.assertEqual(kv_list[0][0], 'string_value')
            self.assertEqual(kv_list[0][1], 'String Metadata')
            self.assertEqual(kv_list[1][0], 'string_value2')
            self.assertEqual(kv_list[1][1], 'String Metadata with \\""ending quotes\\""')
        elif rulename == 'IntegerTypeMetadata':
            self.assertEqual(len(kv), 1)
            self.assertEqual(kv_list[0][0], 'integer_value')
            self.assertIs(kv_list[0][1], 100)
        elif rulename == 'BooleanTypeMetadata':
            self.assertEqual(len(kv), 1)
            self.assertEqual(kv_list[0][0], 'boolean_value')
            self.assertIs(kv_list[0][1], True)
        elif rulename == 'AllTypesMetadata':
            self.assertEqual(len(kv), 3)
            self.assertEqual(kv_list[0][0], 'string_value')
            self.assertEqual(kv_list[1][0], 'integer_value')
            self.assertEqual(kv_list[2][0], 'boolean_value')
            self.assertEqual(kv_list[0][1], 'Different String Metadata')
            self.assertIs(kv_list[1][1], 33)
            self.assertIs(kv_list[2][1], False)
        elif rulename == 'NegativeNumberMetadata':
            self.assertEqual(len(kv), 1)
            self.assertEqual(kv_list[0][0], 'negative')
            self.assertIs(kv_list[0][1], -5)
        else:
            raise AssertionError(self.unhandled_rule.format(rulename))",rulename == 'StringTypeMetadata',101,rulename == 'StringTypeMetadata',True,100.00000000000004,N/A
"def test_strings(self):
    input_string = self.data.joinpath('string_ruleset.yar').read_text()
    result = self.parser.parse_string(input_string)
    for entry in result:
        rulename = entry['rule_name']
        kv = entry['strings']
<mask>:
            self.assertEqual(kv, [{'name': '$text_string', 'value': 'foobar', 'type': 'text'}])
        elif rulename == 'FullwordText':
            self.assertEqual(kv, [{'name': '$text_string', 'value': 'foobar', 'type': 'text', 'modifiers': ['fullword']}])
        elif rulename == 'CaseInsensitiveText':
            self.assertEqual(kv, [{'name': '$text_string', 'value': 'foobar', 'type': 'text', 'modifiers': ['nocase']}])
        elif rulename == 'WideCharText':
            self.assertEqual(kv, [{'name': '$wide_string', 'value': 'Borland', 'type': 'text', 'modifiers': ['wide']}])
        elif rulename == 'WideCharAsciiText':
            self.assertEqual(kv, [{'name': '$wide_and_ascii_string', 'value': 'Borland', 'type': 'text', 'modifiers': ['wide', 'ascii']}])
        elif rulename == 'HexWildcard':
            self.assertEqual(kv, [{'name': '$hex_string', 'value': '{ E2 34 ?? C8 A? FB }', 'type': 'byte'}])
        elif rulename == 'HexJump':
            self.assertEqual(kv, [{'name': '$hex_string', 'value': '{ F4 23 [4-6] 62 B4 }', 'type': 'byte'}])
        elif rulename == 'HexAlternatives':
            self.assertEqual(kv, [{'name': '$hex_string', 'value': '{ F4 23 ( 62 B4 | 56 ) 45 }', 'type': 'byte'}])
        elif rulename == 'HexMultipleAlternatives':
            self.assertEqual(kv, [{'name': '$hex_string', 'value': '{ F4 23 ( 62 B4 | 56 | 45 ?? 67 ) 45 }', 'type': 'byte'}])
        elif rulename == 'RegExp':
            self.assertEqual(kv, [{'name': '$re1', 'value': '/md5: [0-9a-fA-F]{32}/', 'type': 'regex', 'modifiers': ['nocase']}, {'name': '$re2', 'value': '/state: (on|off)/i', 'type': 'regex'}, {'name': '$re3', 'value': '/\\x00https?:\\/\\/[^\\x00]{4,500}\\x00\\x00\\x00/', 'type': 'regex'}])
        elif rulename == 'Xor':
            self.assertEqual(kv, [{'name': '$xor_string', 'value': 'This program cannot', 'type': 'text', 'modifiers': ['xor']}])
        elif rulename == 'WideXorAscii':
            self.assertEqual(kv, [{'name': '$xor_string', 'value': 'This program cannot', 'type': 'text', 'modifiers': ['xor', 'wide', 'ascii']}])
        elif rulename == 'WideXor':
            self.assertEqual(kv, [{'name': '$xor_string', 'value': 'This program cannot', 'type': 'text', 'modifiers': ['xor', 'wide']}])
        elif rulename == 'DoubleBackslash':
            self.assertEqual(kv, [{'name': '$bs', 'value': '\\""\\\\\\\\\\\\\\""', 'type': 'text'}])
        elif rulename == 'DoubleQuote':
            self.assertEqual(kv, [{'name': '$text_string', 'value': 'foobar\\""', 'type': 'text'}])
        elif rulename == 'HorizontalTab':
            self.assertEqual(kv, [{'name': '$text_string', 'value': 'foo\\tbar', 'type': 'text'}])
        elif rulename == 'Newline':
            self.assertEqual(kv, [{'name': '$text_string', 'value': 'foo\\nbar', 'type': 'text'}])
        elif rulename == 'HexEscape':
            self.assertEqual(kv, [{'name': '$text_string', 'value': 'foo\\x00bar', 'type': 'text'}])
        else:
            raise AssertionError(self.unhandled_rule.format(rulename))",rulename == 'Text',293,rulename == 'TextText',False,59.460355750136046,N/A
"def disable_test_rule_name_imports_and_scopes(self):
    input_string_nis = '\n        rule four {meta: i = ""j"" strings: $a = ""b"" condition: true }\n\n        global rule five {meta: i = ""j"" strings: $a = ""b"" condition: false }\n\n        private rule six {meta: i = ""j"" strings: $a = ""b"" condition: true }\n\n        global private rule seven {meta: i = ""j"" strings: $a = ""b"" condition: true }\n\n        import ""lib1""\n        rule eight {meta: i = ""j"" strings: $a = ""b"" condition: true }\n\n        import ""lib1""\n        import ""lib2""\n        rule nine {meta: i = ""j"" strings: $a = ""b"" condition: true }\n\n        import ""lib2""\n        private global rule ten {meta: i = ""j"" strings: $a = ""b"" condition: true }\n        '
    plyara = Plyara()
    result = plyara.parse_string(input_string_nis)
    self.assertEqual(len(result), 7)
    for rule in result:
        rule_name = rule['rule_name']
<mask>:
            self.assertNotIn('scopes', rule)
            self.assertIn('imports', rule)
        if rule_name == 'five':
            self.assertIn('imports', rule)
            self.assertIn('global', rule['scopes'])
        if rule_name == 'six':
            self.assertIn('imports', rule)
            self.assertIn('private', rule['scopes'])
        if rule_name == 'seven':
            self.assertIn('imports', rule)
            self.assertTrue('private' in rule['scopes'] and 'global' in rule['scopes'])
        if rule_name == 'eight':
            self.assertIn('lib1', rule['imports'])
            self.assertNotIn('scopes', rule)
        if rule_name == 'nine':
            self.assertTrue('lib1' in rule['imports'] and 'lib2' in rule['imports'])
            self.assertNotIn('scopes', rule)
        if rule_name == 'ten':
            self.assertTrue('lib1' in rule['imports'] and 'lib2' in rule['imports'])
            self.assertTrue('global' in rule['scopes'] and 'private' in rule['scopes'])",rule_name == 'four',199,rule_name == 'five',False,75.98356856515926,N/A
"def test_issue_141_store_raw_sections_true(self):
    """"""Check when store_raw_sections at the default.""""""
    input_string = self.data.joinpath('issue141.yar').read_text()
    parsed_rules = self.parser.parse_string(input_string)
    for i, rule in enumerate(parsed_rules):
        with self.subTest(rulenum=i):
<mask>:
                self.assertIsNone(rule.get('imports'))
            elif i == 1:
                self.assertEqual(rule.get('imports'), ['pe'])",i == 0,29,i == 0,True,100.00000000000004,N/A
"def test_issue_141_store_raw_sections_false(self):
    """"""Check when store_raw_sections set to False.""""""
    input_string = self.data.joinpath('issue141.yar').read_text()
    parser = plyara.core.Plyara(store_raw_sections=False)
    parsed_rules = parser.parse_string(input_string)
    for i, rule in enumerate(parsed_rules):
        with self.subTest(rulenum=i):
<mask>:
                self.assertIsNone(rule.get('imports'))
            elif i == 1:
                self.assertEqual(rule.get('imports'), ['pe'])",i == 0,32,i == 0,True,100.00000000000004,N/A
"def test_issue_145(self):
    """"""Check correct parsing for PR#130 changes.""""""
    input_string = self.data.joinpath('issue145.yar').read_text()
    parsed_rules = self.parser.parse_string(input_string)
    for rule in parsed_rules:
        rulename = rule.get('rule_name')
        with self.subTest(rulenum=rulename):
<mask>:
                bv = rule['strings'][0]['value']
                self.assertEqual(bv, '{ AA AA ~AA }')
            elif rulename == 'test2':
                bv = rule['strings'][0]['value']
                self.assertEqual(bv, '{ AA AA~AA }')
            elif rulename == 'test3':
                md = rule['metadata'][0]['one']
                self.assertEqual(md, 0)
            elif rulename == 'test4':
                ct = rule['condition_terms']
                self.assertListEqual(ct, ['-', '0.5'])
            elif rulename == 'test5':
                ct = rule['condition_terms']
                self.assertListEqual(ct, ['-', '1.5'])",rulename == 'test1',74,rulename == 'test1',True,100.00000000000004,N/A
"def test_third_party_rules(self):
    projects = ['bartblaze/Yara-rules', 'The-DFIR-Report/Yara-Rules', 'ditekshen/detection', 'elastic/protections-artifacts', 'eset/malware-ioc', 'Neo23x0/signature-base', 'intezer/yara-rules', 'JPCERTCC/jpcert-yara', 'malpedia/signator-rules', 'kevoreilly/CAPE', 'reversinglabs/reversinglabs-yara-rules', 'stratosphereips/yara-rules', 'advanced-threat-research/Yara-Rules', 'volexity/threat-intel']
    for project in projects:
        with TemporaryDirectory() as rules_directory:
            subprocess.run(['git', 'clone', '--depth', '1', f'https://github.com/{project}.git'], cwd=rules_directory, capture_output=True)
            for yara_file in Path(rules_directory).rglob('*.yar*'):
<mask>:
                    with self.subTest(msg=project, yara_file=yara_file):
                        plyara = Plyara()
                        plyara.parse_string(yara_file.read_text())",'.yar' in yara_file.suffix,45,yara_file.name == 'yara-rules',False,30.509752160562883,N/A
"def setUp(self):
    """"""Set path object to package directory and build list of Python files.""""""
    self.package_dir = pathlib.Path(__file__).parent.parent
    self.to_check = list()
    for path in self.package_dir.rglob('*.py'):
<mask>:
            self.to_check.append(path)",not exclude_paths.intersection(set(path.parts)),26,path.endswith('.py'),False,6.560271639619885,N/A
"def test_pydocstyle(self):
    """"""Test that docstrings conform to PEP-257.""""""
    for path in self.to_check:
        relative_path = path.relative_to(self.package_dir).as_posix()
        with self.subTest(file=relative_path):
            msg = str()
            source = path.read_text()
            try:
                for error in self.cc.check_source(source, path.name):
<mask>:
                        continue
                    if error.code in ply_ignore and path.name in ply_files:
                        continue
                    msg += f'\n{error}'
            except pydocstyle.parser.ParseError:
                self.skipTest(f'Cannot parse file: {relative_path}')
            self.assertFalse(any(msg), msg)",error.code in pydocstyle_ignore,51,error.code in ply_ignore and path.name in ply_files,False,19.56475149792291,N/A
"def example():
    """"""Execute the example code.""""""
    parser = argparse.ArgumentParser()
    parser.add_argument('file', metavar='FILE', help='File containing YARA rules to parse.')
    args = parser.parse_args()
    print('Parsing file...')
    with open(args.file, 'r') as fh:
        data = fh.read()
    parser = plyara.Plyara()
    rules_dict = parser.parse_string(data)
    print('Analyzing dictionary...')
    imps = {}
    tags = {}
    rule_count = 0
    for rule in rules_dict:
        rule_count += 1
<mask>:
            for imp in rule['imports']:
                imp = imp.replace('""', '')
                if imp in imps:
                    imps[imp] += 1
                else:
                    imps[imp] = 1
        if 'tags' in rule:
            for tag in rule['tags']:
                if tag in tags:
                    tags[tag] += 1
                else:
                    tags[tag] = 1
    print('\n======================\n')
    print('Number of rules in file: {}'.format(rule_count))
    ordered_imps = sorted(imps.items(), key=operator.itemgetter(1), reverse=True)
    ordered_tags = sorted(tags.items(), key=operator.itemgetter(1), reverse=True)
    print('\n======================\n')
    print('Top imports:')
    for i in range(5):
        if i < len(ordered_imps):
            print(ordered_imps[i])
    print('\n======================\n')
    print('Top tags:')
    for i in range(5):
        if i < len(ordered_tags):
            print(ordered_tags[i])",'imports' in rule,134,'imports' in rule,True,100.00000000000004,N/A
"def main():
    """"""Run the command line process to parse a yara rule file and output pretty printed JSON.""""""
    parser = argparse.ArgumentParser(description='Parse YARA rules into a JSON representation.')
    parser.add_argument('file', metavar='FILE', help='File containing YARA rules to parse.')
    parser.add_argument('--log', help='Enable debug logging to the console.', action='store_true')
    args = parser.parse_args()
    try:
        input_string = pathlib.Path(args.file).read_text(encoding='utf-8')
    except FileNotFoundError as e:
        sys.exit(e)
    parser = plyara.core.Plyara()
<mask>:
        _set_logging()
    rules = parser.parse_string(input_string)
    print(json.dumps(rules, sort_keys=True, indent=4))",args.log,66,args.log,True,100.00000000000004,N/A
"def __init__(self, store_raw_sections=True, meta_as_kv=False, import_effects=False, testmode=False):
    """"""Initialize the parser object.

        Args:
            store_raw_sections: Enable attribute storage of raw section input. (default True)
            meta_as_kv: Enable alternate structure for meta section as dictionary. (default False)
            import_effects: Enable including imports in all rules affected by the import. (default False)
            testmode: Enable permanent accumulators for unit testing purposes. (default: False)
        """"""
    self.rules = list()
    self.current_rule = dict()
    self.string_modifiers = dict()
    self.imports = set()
    self.includes = list()
    self.terms = list()
    self.scopes = list()
    self.tags = list()
    self.comments = list()
    self.store_raw_sections = store_raw_sections
    self._raw_input = None
    self._meta_start = None
    self._meta_end = None
    self._strings_start = None
    self._strings_end = None
    self._condition_start = None
    self._condition_end = None
    self._rule_comments = list()
    self._stringnames = set()
    self.meta_as_kv = meta_as_kv
    self.import_effects = import_effects
    self.lexer = lex.lex(module=self, debug=False)
    self.parser = yacc.yacc(module=self, debug=False)
    self._testmode = testmode
<mask>:
        self._comment_record = list()",testmode,134,self._testmode,False,15.97357760615681,N/A
"def _add_element(self, element_type, element_value):
    """"""Accept elements from the parser and uses them to construct a representation of the YARA rule.

        Args:
            element_type: The element type determined by the parser. Input is one of ElementTypes.
            element_value: This is the contents of the element as parsed from the rule.
        """"""
<mask>:
        rule_name, start_line, stop_line = element_value
        self.current_rule['rule_name'] = rule_name
        self.current_rule['start_line'] = start_line
        self.current_rule['stop_line'] = stop_line
        if self.store_raw_sections:
            if self._meta_start:
                self.current_rule['raw_meta'] = self._raw_input[self._meta_start:self._meta_end]
            if self._strings_start:
                self.current_rule['raw_strings'] = self._raw_input[self._strings_start:self._strings_end]
            if self._condition_start:
                self.current_rule['raw_condition'] = self._raw_input[self._condition_start:self._condition_end]
        self._flush_accumulators()
        self.rules.append(self.current_rule)
        logger.debug('Adding Rule: {}'.format(self.current_rule['rule_name']))
        self.current_rule = dict()
        self._stringnames.clear()
    elif element_type == ElementTypes.METADATA_KEY_VALUE:
        key, value = element_value
        if 'metadata' not in self.current_rule:
            self.current_rule['metadata'] = [{key: value}]
            if self.meta_as_kv:
                self.current_rule['metadata_kv'] = {key: value}
        else:
            self.current_rule['metadata'].append({key: value})
            if self.meta_as_kv:
                self.current_rule['metadata_kv'][key] = value
    elif element_type == ElementTypes.STRINGS_KEY_VALUE:
        key, value, string_type = element_value
        string_dict = {'name': key, 'value': value, 'type': string_type.name.lower()}
        if any(self.string_modifiers):
            string_dict['modifiers'] = [k + v for k, v in self.string_modifiers.items()]
            self.string_modifiers.clear()
        if 'strings' not in self.current_rule:
            self.current_rule['strings'] = [string_dict]
        else:
            self.current_rule['strings'].append(string_dict)
    elif element_type == ElementTypes.STRINGS_MODIFIER:
        modifier, predicate = element_value
        self.string_modifiers[modifier] = predicate
    elif element_type == ElementTypes.IMPORT:
        self.imports.add(element_value)
    elif element_type == ElementTypes.INCLUDE:
        self.includes.append(element_value)
    elif element_type == ElementTypes.TERM:
        self.terms.append(element_value)
    elif element_type == ElementTypes.SCOPE:
        self.scopes.append(element_value)
    elif element_type == ElementTypes.TAG:
        self.tags.append(element_value)
    elif element_type == ElementTypes.COMMENT:
        self.comments.append(element_value)
    else:
        self.comments.append(element_value)",element_type == ElementTypes.RULE_NAME,203,element_type == ElementTypes.RULE_KEY_VALUE,False,71.02992180127417,N/A
"def _flush_accumulators(self):
    """"""Add accumulated elements to the current rule and resets the accumulators.""""""
<mask>:
        self.current_rule['condition_terms'] = self.terms
        self.terms = list()
    if any(self.scopes):
        self.current_rule['scopes'] = self.scopes
        self.scopes = list()
    if any(self.tags):
        self.current_rule['tags'] = self.tags
        self.tags = list()
    if any(self.comments):
        self.current_rule['comments'] = self.comments
        self.comments = list()
    self._meta_start = None
    self._meta_end = None
    self._strings_start = None
    self._strings_end = None
    self._condition_start = None
    self._condition_end = None",any(self.terms),62,any(self.terms),True,100.00000000000004,N/A
"@staticmethod
def _find_imports_in_condition(imports, condition_terms):
    """"""Search a list of condition terms for any that use the given imports.""""""
    used_imports = set()
    for imp in imports:
        for term in condition_terms:
<mask>:
                used_imports.add(imp)
                break
    return list(used_imports)",term.startswith(f'{imp}.'),33,term in imp,False,2.4088567143060917,N/A
"def parse_string(self, input_string):
    """"""Take a string input expected to consist of YARA rules, and return list of dictionaries representing them.

        Args:
            input_string: String input expected to consist of YARA rules.

        Returns:
            dict: All the parsed components of a YARA rule.
        """"""
    self._raw_input = input_string
    self.parser.parse(input_string, lexer=self.lexer)
    for rule in self.rules:
<mask>:
            if self.import_effects:
                rule['imports'] = list(self.imports)
            elif (imports := self._find_imports_in_condition(self.imports, rule['condition_terms'])):
                rule['imports'] = imports
        if any(self.includes):
            rule['includes'] = self.includes
    return self.rules",any(self.imports),71,any(self.imports),True,100.00000000000004,N/A
"def is_valid_rule_name(entry):
    """"""Check to see if entry is a valid rule name.

    Args:
        entry: String containing rule name.

    Returns:
        bool
    """"""
    warnings.warn('Rule name validity checked by parser. is_valid_rule_name will be removed in plyara version 2.3.0', DeprecationWarning)
<mask>:
        return False
    if len(entry) > 128:
        return False
    if entry[0].isdigit():
        return False
    if not re.match('[a-zA-Z_][a-zA-Z_0-9]*$', entry):
        return False
    if entry in Parser.KEYWORDS:
        return False
    return True",not entry,63,entry.startswith('#'),False,5.522397783539471,N/A
"def detect_imports(rule):
    """"""Take a parsed yararule and provide a list of required imports based on condition.

    Args:
        rule: Dict output from a parsed rule.

    Returns:
        list: Imports that are required.
    """"""
    warnings.warn('Imports now parsed for all rules. detect_imports will be removed in plyara version 2.3.0', DeprecationWarning)
    detected_imports = list()
    condition_terms = rule['condition_terms']
    for imp in Parser.IMPORT_OPTIONS:
        imp_module = '{}.'.format(imp)
        for term in condition_terms:
<mask>:
                detected_imports.append(imp)
                break
    return detected_imports",term.startswith(imp_module),68,term == imp_module,False,23.263472697663296,N/A
"def detect_dependencies(rule):
    """"""Take a parsed yararule and provide a list of external rule dependencies.

    Args:
        rule: Dict output from a parsed rule.

    Returns:
        list: External rule dependencies.
    """"""
    warnings.warn('Deprecation: detect_dependencies will be removed in plyara version 2.3.0', DeprecationWarning)
    dependencies = list()
    string_iteration_variables = list()
    condition_terms = rule['condition_terms']
    term_count = len(condition_terms)
    for index in range(0, term_count):
        term = condition_terms[index]
<mask>:
            if index > 0:
                previous_term = condition_terms[index - 1]
            else:
                previous_term = None
            if index < term_count - 1:
                next_term = condition_terms[index + 1]
            else:
                next_term = None
            if previous_term == '(' and next_term == ')':
                if index - 2 >= 0:
                    previous_term = condition_terms[index - 2]
                else:
                    previous_term = None
                if index + 2 < term_count:
                    next_term = condition_terms[index + 2]
                else:
                    next_term = None
            if term in string_iteration_variables:
                continue
            if previous_term in ('any', 'all') and next_term == 'in':
                string_iteration_variables.append(term)
                continue
            if next_term in ('matches', 'contains') or previous_term in ('matches', 'contains'):
                continue
            if next_term in Parser.COMPARISON_OPERATORS or previous_term in Parser.COMPARISON_OPERATORS:
                continue
            if previous_term is None and next_term is None:
                dependencies.append(term)
            elif previous_term in ('and', 'or', 'not') or next_term in ('and', 'or', 'not'):
                dependencies.append(term)
    return dependencies",is_valid_rule_name(term) and term not in Parser.IMPORT_OPTIONS,185,term in Parser.COMPARISON_OPERATORS,False,5.0022783410134535,N/A
"def _validate_instance_id(instance_id):
    """"""
    Validate instance ID
    """"""
    instance_id_regex = '^(i-\\S{17})'
    compiled_regex = re.compile(instance_id_regex)
    match = compiled_regex.match(instance_id)
<mask>:
        return None
    return match.group(1)",not match,21,not match,True,100.00000000000004,N/A
"def _retrieve_instance_id():
    """"""
    Retrieve instance ID from instance metadata service
    """"""
    instance_id = None
    url = 'http://169.254.169.254/latest/meta-data/instance-id'
    response = requests_helper(url, timeout=0.1)
<mask>:
        instance_id = _validate_instance_id(response.text)
    return instance_id",response is not None,27,response.status_code == 200,False,5.522397783539471,N/A
"def _retrieve_instance_region():
    """"""
    Retrieve instance region from instance metadata service
    """"""
    region = None
    valid_regions = ['ap-northeast-1', 'ap-northeast-2', 'ap-southeast-1', 'ap-southeast-2', 'ap-south-1', 'ca-central-1', 'eu-central-1', 'eu-north-1', 'eu-west-1', 'eu-west-2', 'eu-west-3', 'sa-east-1', 'us-east-1', 'us-east-2', 'us-west-1', 'us-west-2']
    url = 'http://169.254.169.254/latest/dynamic/instance-identity/document'
    response = requests_helper(url, timeout=0.1)
<mask>:
        response_json = json.loads(response.text)
        if response_json['region'] in valid_regions:
            region = response_json['region']
    return region",response is not None,52,response.status_code == 200,False,5.522397783539471,N/A
"def query_bucket():
    """"""
    GET request on an empty object from an Amazon S3 bucket
    """"""
    response = None
    instance_id = _retrieve_instance_id()
    region = _retrieve_instance_region()
<mask>:
        url = 'https://aws-deep-learning-containers-{0}.s3.{0}.amazonaws.com/dlc-containers.txt?x-instance-id={1}'.format(region, instance_id)
        response = requests_helper(url, timeout=0.2)
    logging.debug('Query bucket finished: {}'.format(response))
    return response",instance_id is not None and region is not None,39,instance_id,False,6.948345122280157,N/A
"def parse_request(req, rest_port, grpc_port, default_model_name, model_name=None, channel=None):
    tfs_attributes = parse_tfs_custom_attributes(req)
    tfs_uri = make_tfs_uri(rest_port, tfs_attributes, default_model_name, model_name)
<mask>:
        model_name = tfs_attributes.get('tfs-model-name')
    context = Context(model_name, tfs_attributes.get('tfs-model-version'), tfs_attributes.get('tfs-method'), tfs_uri, grpc_port, channel, req.get_header(CUSTOM_ATTRIBUTES_HEADER), req.get_header('Content-Type') or DEFAULT_CONTENT_TYPE, req.get_header('Accept') or DEFAULT_ACCEPT_HEADER, req.content_length)
    data = req.stream
    return (data, context)",not model_name,42,model_name is None,False,39.76353643835252,N/A
"def make_tfs_uri(port, attributes, default_model_name, model_name=None):
    log.info('sagemaker tfs attributes: \n{}'.format(attributes))
    tfs_model_name = model_name or attributes.get('tfs-model-name', default_model_name)
    tfs_model_version = attributes.get('tfs-model-version')
    tfs_method = attributes.get('tfs-method', 'predict')
    uri = 'http://localhost:{}/v1/models/{}'.format(port, tfs_model_name)
<mask>:
        uri += '/versions/' + tfs_model_version
    uri += ':' + tfs_method
    return uri",tfs_model_version,39,tfs_model_version,True,100.00000000000004,N/A
"def parse_tfs_custom_attributes(req):
    attributes = {}
    header = req.get_header(CUSTOM_ATTRIBUTES_HEADER)
<mask>:
        matches = re.findall('(tfs-[a-z\\-]+=[^,]+)', header)
        attributes = dict((attribute.split('=') for attribute in matches))
    return attributes",header,22,header,True,100.00000000000004,N/A
"def find_models():
    base_path = '/opt/ml/model'
    models = []
    for f in _find_saved_model_files(base_path):
        parts = f.split('/')
<mask>:
            model_path = '/'.join(parts[0:-2])
            if model_path not in models:
                models.append(model_path)
    return models","len(parts) >= 6 and re.match('^\\d+$', parts[-2])",27,len(parts) > 2 and parts[0] == 'model',False,14.186216087246452,N/A
"def _find_saved_model_files(path):
    for e in os.scandir(path):
<mask>:
            yield from _find_saved_model_files(os.path.join(path, e.name))
        elif e.name == 'saved_model.pb':
            yield os.path.join(path, e.name)",e.is_dir(),18,e.name == 'saved_model.pb_file',False,7.495553473355845,N/A
"def __init__(self):
    self._state = 'initializing'
    self._nginx = None
    self._tfs = []
    self._gunicorn = None
    self._gunicorn_command = None
    self._enable_python_service = False
    self._tfs_version = os.environ.get('SAGEMAKER_TFS_VERSION', '1.13')
    self._nginx_http_port = os.environ.get('SAGEMAKER_BIND_TO_PORT', '8080')
    self._nginx_loglevel = os.environ.get('SAGEMAKER_TFS_NGINX_LOGLEVEL', 'error')
    self._tfs_default_model_name = os.environ.get('SAGEMAKER_TFS_DEFAULT_MODEL_NAME', 'None')
    self._sagemaker_port_range = os.environ.get('SAGEMAKER_SAFE_PORT_RANGE', None)
    self._gunicorn_workers = os.environ.get('SAGEMAKER_GUNICORN_WORKERS', 1)
    self._gunicorn_threads = os.environ.get('SAGEMAKER_GUNICORN_THREADS', 1)
    self._gunicorn_loglevel = os.environ.get('SAGEMAKER_GUNICORN_LOGLEVEL', 'info')
    self._tfs_config_path = '/sagemaker/model-config.cfg'
    self._tfs_batching_config_path = '/sagemaker/batching-config.cfg'
    _enable_batching = os.environ.get('SAGEMAKER_TFS_ENABLE_BATCHING', 'false').lower()
    _enable_multi_model_endpoint = os.environ.get('SAGEMAKER_MULTI_MODEL', 'false').lower()
    self._tfs_gpu_margin = float(os.environ.get('SAGEMAKER_TFS_FRACTIONAL_GPU_MEM_MARGIN', 0.2))
    self._tfs_instance_count = int(os.environ.get('SAGEMAKER_TFS_INSTANCE_COUNT', 1))
    self._tfs_wait_time_seconds = int(os.environ.get('SAGEMAKER_TFS_WAIT_TIME_SECONDS', 300))
    self._tfs_inter_op_parallelism = os.environ.get('SAGEMAKER_TFS_INTER_OP_PARALLELISM', 0)
    self._tfs_intra_op_parallelism = os.environ.get('SAGEMAKER_TFS_INTRA_OP_PARALLELISM', 0)
    self._gunicorn_worker_class = os.environ.get('SAGEMAKER_GUNICORN_WORKER_CLASS', 'gevent')
    self._gunicorn_timeout_seconds = int(os.environ.get('SAGEMAKER_GUNICORN_TIMEOUT_SECONDS', 30))
    self._nginx_proxy_read_timeout_seconds = int(os.environ.get('SAGEMAKER_NGINX_PROXY_READ_TIMEOUT_SECONDS', 60))
<mask>:
        log.info('GUnicorn timeout was higher than Nginx proxy read timeout. Setting Nginx proxy read timeout from {} seconds to {} seconds to match GUnicorn timeout.'.format(self._nginx_proxy_read_timeout_seconds, self._gunicorn_timeout_seconds))
        self._nginx_proxy_read_timeout_seconds = self._gunicorn_timeout_seconds
    if os.environ.get('OMP_NUM_THREADS') is None:
        os.environ['OMP_NUM_THREADS'] = '1'
    if _enable_multi_model_endpoint not in ['true', 'false']:
        raise ValueError(""SAGEMAKER_MULTI_MODEL must be 'true' or 'false'"")
    self._tfs_enable_multi_model_endpoint = _enable_multi_model_endpoint == 'true'
    self._need_python_service()
    log.info('PYTHON SERVICE: {}'.format(str(self._enable_python_service)))
    if _enable_batching not in ['true', 'false']:
        raise ValueError(""SAGEMAKER_TFS_ENABLE_BATCHING must be 'true' or 'false'"")
    self._tfs_enable_batching = _enable_batching == 'true'
    if _enable_multi_model_endpoint not in ['true', 'false']:
        raise ValueError(""SAGEMAKER_MULTI_MODEL must be 'true' or 'false'"")
    self._tfs_enable_multi_model_endpoint = _enable_multi_model_endpoint == 'true'
    self._use_gunicorn = self._enable_python_service or self._tfs_enable_multi_model_endpoint
    if self._sagemaker_port_range is not None:
        parts = self._sagemaker_port_range.split('-')
        low = int(parts[0])
        hi = int(parts[1])
        self._tfs_grpc_ports = []
        self._tfs_rest_ports = []
        if low + 2 * self._tfs_instance_count > hi:
            raise ValueError('not enough ports available in SAGEMAKER_SAFE_PORT_RANGE ({})'.format(self._sagemaker_port_range))
        for i in range(self._tfs_instance_count):
            self._tfs_grpc_ports.append(str(low + 2 * i))
            self._tfs_rest_ports.append(str(low + 2 * i + 1))
        self._tfs_grpc_concat_ports = self._concat_ports(self._tfs_grpc_ports)
        self._tfs_rest_concat_ports = self._concat_ports(self._tfs_rest_ports)
    else:
        self._tfs_grpc_ports = ['9000']
        self._tfs_rest_ports = ['8501']
        self._tfs_grpc_concat_ports = '9000'
        self._tfs_rest_concat_ports = '8501'
    os.environ['TFS_GRPC_PORTS'] = self._tfs_grpc_concat_ports
    os.environ['TFS_REST_PORTS'] = self._tfs_rest_concat_ports",self._gunicorn_timeout_seconds > self._nginx_proxy_read_timeout_seconds,274,self._enable_python_service,False,4.599246087297971,N/A
"def _need_python_service(self):
<mask>:
        self._enable_python_service = True
    if os.environ.get('SAGEMAKER_MULTI_MODEL_UNIVERSAL_BUCKET') and os.environ.get('SAGEMAKER_MULTI_MODEL_UNIVERSAL_PREFIX'):
        self._enable_python_service = True",os.path.exists(INFERENCE_PATH),13,os.environ.get('SAGEMAKER_MULTI_MODEL_BUCKET') and os.environ.get('SAGEMAKER_MULTI_MODEL_UNIVERSAL_PREFIX'),False,3.1568618107860202,N/A
"def _create_tfs_config(self):
    models = tfs_utils.find_models()
<mask>:
        raise ValueError('no SavedModel bundles found!')
    if self._tfs_default_model_name == 'None':
        default_model = os.path.basename(models[0])
        if default_model:
            self._tfs_default_model_name = default_model
            log.info('using default model name: {}'.format(self._tfs_default_model_name))
        else:
            log.info('no default model detected')
    config = 'model_config_list: {\n'
    for m in models:
        config += '  config: {\n'
        config += ""    name: '{}'\n"".format(os.path.basename(m))
        config += ""    base_path: '{}'\n"".format(m)
        config += ""    model_platform: 'tensorflow'\n""
        config += '    model_version_policy: {\n'
        config += '      specific: {\n'
        for version in tfs_utils.find_model_versions(m):
            config += '        versions: {}\n'.format(version)
        config += '      }\n'
        config += '    }\n'
        config += '  }\n'
    config += '}\n'
    log.info('tensorflow serving model config: \n%s\n', config)
    with open(self._tfs_config_path, 'w', encoding='utf8') as f:
        f.write(config)",not models,108,models is None,False,27.516060407455225,N/A
"def _setup_gunicorn(self):
    python_path_content = []
    python_path_option = ''
    bucket = os.environ.get('SAGEMAKER_MULTI_MODEL_UNIVERSAL_BUCKET', None)
    prefix = os.environ.get('SAGEMAKER_MULTI_MODEL_UNIVERSAL_PREFIX', None)
<mask>:
        self._download_scripts(bucket, prefix)
    if self._enable_python_service:
        lib_path_exists = os.path.exists(PYTHON_LIB_PATH)
        requirements_exists = os.path.exists(REQUIREMENTS_PATH)
        python_path_content = ['/opt/ml/model/code']
        python_path_option = '--pythonpath '
        if lib_path_exists:
            python_path_content.append(PYTHON_LIB_PATH)
        if requirements_exists:
            if lib_path_exists:
                log.warning(""loading modules in '{}', ignoring requirements.txt"".format(PYTHON_LIB_PATH))
            else:
                log.info('installing packages from requirements.txt...')
                pip_install_cmd = 'pip3 install -r {}'.format(REQUIREMENTS_PATH)
                try:
                    subprocess.check_call(pip_install_cmd.split())
                except subprocess.CalledProcessError:
                    log.error('failed to install required packages, exiting.')
                    self._stop()
                    raise ChildProcessError('failed to install required packages.')
    gunicorn_command = 'gunicorn -b unix:/tmp/gunicorn.sock -k {} --chdir /sagemaker --workers {} --threads {} --log-level {} --timeout {} {}{} -e TFS_GRPC_PORTS={} -e TFS_REST_PORTS={} -e SAGEMAKER_MULTI_MODEL={} -e SAGEMAKER_SAFE_PORT_RANGE={} -e SAGEMAKER_TFS_WAIT_TIME_SECONDS={} python_service:app'.format(self._gunicorn_worker_class, self._gunicorn_workers, self._gunicorn_threads, self._gunicorn_loglevel, self._gunicorn_timeout_seconds, python_path_option, ','.join(python_path_content), self._tfs_grpc_concat_ports, self._tfs_rest_concat_ports, self._tfs_enable_multi_model_endpoint, self._sagemaker_port_range, self._tfs_wait_time_seconds)
    log.info('gunicorn command: {}'.format(gunicorn_command))
    self._gunicorn_command = gunicorn_command",not os.path.exists(CODE_DIR) and bucket and prefix,121,bucket and prefix,False,1.8315638888734187,N/A
"def _download_scripts(self, bucket, prefix):
    log.info('checking boto session region ...')
    boto_session = boto3.session.Session()
    boto_region = boto_session.region_name
<mask>:
        raise ValueError('Universal scripts is not supported in us-iso-east-1 or us-gov-west-1')
    log.info('downloading universal scripts ...')
    client = boto3.client('s3')
    resource = boto3.resource('s3')
    paginator = client.get_paginator('list_objects')
    for result in paginator.paginate(Bucket=bucket, Delimiter='/', Prefix=prefix):
        for file in result.get('Contents', []):
            destination = os.path.join(CODE_DIR, file.get('Key'))
            if not os.path.exists(os.path.dirname(destination)):
                os.makedirs(os.path.dirname(destination))
            resource.meta.client.download_file(bucket, file.get('Key'), destination)","boto_region in ('us-iso-east-1', 'us-gov-west-1')",61,"boto_region not in ['us-iso-east-1', 'us-gov-west-1']",False,27.301208627090666,N/A
"def default_handler(data, context):
    """"""A default inference request handler that directly send post request to TFS rest port with
    un-processed data and return un-processed response

    :param data: input data
    :param context: context instance that contains tfs_rest_uri
    :return: inference response from TFS model server
    """"""
    data = data.read().decode('utf-8')
<mask>:
        data = json.loads(data)
    response = requests.post(context.rest_uri, data=data)
    return (response.content, context.accept_header)","not isinstance(data, str)",57,context.accept_header is None,False,0.0,N/A
"def __init__(self):
<mask>:
        self._model_tfs_rest_port = {}
        self._model_tfs_grpc_port = {}
        self._model_tfs_pid = {}
        self._tfs_ports = self._parse_sagemaker_port_range_mme(SAGEMAKER_TFS_PORT_RANGE)
        self.model_handlers = {}
    else:
        self._tfs_grpc_ports = self._parse_concat_ports(TFS_GRPC_PORTS)
        self._tfs_rest_ports = self._parse_concat_ports(TFS_REST_PORTS)
        self._channels = {}
        for grpc_port in self._tfs_grpc_ports:
            self._setup_channel(grpc_port)
    if os.path.exists(INFERENCE_SCRIPT_PATH):
        self._handler, self._input_handler, self._output_handler = self._import_handlers()
        self._handlers = self._make_handler(self._handler, self._input_handler, self._output_handler)
    else:
        self._handlers = default_handler
    self._tfs_enable_batching = SAGEMAKER_BATCHING_ENABLED == 'true'
    self._tfs_default_model_name = os.environ.get('TFS_DEFAULT_MODEL_NAME', 'None')
    self._tfs_wait_time_seconds = int(os.environ.get('SAGEMAKER_TFS_WAIT_TIME_SECONDS', 300))",SAGEMAKER_MULTI_MODEL_ENABLED,62,SAGEMAKER_TFS_PORT_RANGE != 'all',False,9.980099403873663,N/A
"def on_post(self, req, res, model_name=None):
<mask>:
        self._handle_invocation_post(req, res, model_name)
    else:
        data = json.loads(req.stream.read().decode('utf-8'))
        self._handle_load_model_post(res, data)",model_name or 'invocations' in req.uri,15,model_name,False,13.533528323661276,N/A
"def _handle_load_model_post(self, res, data):
    model_name = data['model_name']
    base_path = data['url']
<mask>:
        res.status = falcon.HTTP_409
        res.body = json.dumps({'error': 'Model {} is already loaded.'.format(model_name)})
    if not self._ports_available():
        res.status = falcon.HTTP_507
        res.body = json.dumps({'error': 'Memory exhausted: no available ports to load the model.'})
    with lock():
        self._model_tfs_rest_port[model_name] = self._tfs_ports['rest_port'].pop()
        self._model_tfs_grpc_port[model_name] = self._tfs_ports['grpc_port'].pop()
    if self.validate_model_dir(base_path):
        try:
            tfs_config = tfs_utils.create_tfs_config_individual_model(model_name, base_path)
            tfs_config_file = '/sagemaker/tfs-config/{}/model-config.cfg'.format(model_name)
            log.info('tensorflow serving model config: \n%s\n', tfs_config)
            os.makedirs(os.path.dirname(tfs_config_file))
            with open(tfs_config_file, 'w', encoding='utf8') as f:
                f.write(tfs_config)
            batching_config_file = '/sagemaker/batching/{}/batching-config.cfg'.format(model_name)
            if self._tfs_enable_batching:
                tfs_utils.create_batching_config(batching_config_file)
            cmd = tfs_utils.tfs_command(self._model_tfs_grpc_port[model_name], self._model_tfs_rest_port[model_name], tfs_config_file, self._tfs_enable_batching, batching_config_file)
            p = subprocess.Popen(cmd.split())
            tfs_utils.wait_for_model(self._model_tfs_rest_port[model_name], model_name, self._tfs_wait_time_seconds)
            log.info('started tensorflow serving (pid: %d)', p.pid)
            self._model_tfs_pid[model_name] = p
            res.status = falcon.HTTP_200
            res.body = json.dumps({'success': 'Successfully loaded model {}, listening on rest port {} and grpc port {}.'.format(model_name, self._model_tfs_rest_port, self._model_tfs_grpc_port)})
        except MultiModelException as multi_model_exception:
            self._cleanup_config_file(tfs_config_file)
            self._cleanup_config_file(batching_config_file)
            if multi_model_exception.code == 409:
                res.status = falcon.HTTP_409
                res.body = multi_model_exception.msg
            elif multi_model_exception.code == 408:
                res.status = falcon.HTTP_408
                res.body = multi_model_exception.msg
            else:
                raise MultiModelException(falcon.HTTP_500, multi_model_exception.msg)
        except FileExistsError as e:
            res.status = falcon.HTTP_409
            res.body = json.dumps({'error': 'Model {} is already loaded. {}'.format(model_name, str(e))})
        except OSError as os_error:
            self._cleanup_config_file(tfs_config_file)
            self._cleanup_config_file(batching_config_file)
            if os_error.errno == 12:
                raise MultiModelException(falcon.HTTP_507, 'Memory exhausted: not enough memory to start TFS instance')
            else:
                raise MultiModelException(falcon.HTTP_500, os_error.strerror)
    else:
        res.status = falcon.HTTP_404
        res.body = json.dumps({'error': 'Could not find valid base path {} for servable {}'.format(base_path, model_name)})",model_name in self._model_tfs_pid,211,model_name in self._model_tfs_rest_port.keys(),False,57.30574043798692,N/A
"@pytest.fixture(scope='module')
def runtime_config(request, processor):
<mask>:
        return '--runtime=nvidia '
    else:
        return ''",processor == 'gpu',11,request.user.is_authenticated,False,0.0,N/A
"@pytest.fixture(scope='module')
def tag(request, framework_version, processor):
    image_tag = request.config.getoption('--tag')
<mask>:
        image_tag = '{}-{}'.format(framework_version, processor)
    return image_tag",not image_tag,15,image_tag is None,False,39.76353643835252,N/A
"@pytest.fixture(autouse=True)
def skip_by_device_type(request, processor):
    is_gpu = processor == 'gpu'
<mask>:
        pytest.skip('Skipping because running on ""{}"" instance'.format(processor))",request.node.get_closest_marker('skip_gpu') and is_gpu or (request.node.get_closest_marker('skip_cpu') and (not is_gpu)),16,is_gpu,False,0.00022603294069810552,N/A
"@pytest.fixture(scope='module', autouse=True)
def container(request, docker_base_name, tag, runtime_config):
    try:
        command = 'docker run {}--name sagemaker-tensorflow-serving-test -p 8080:8080 --mount type=volume,source=dynamic_endpoint_model_volume,target=/opt/ml/models,readonly -e SAGEMAKER_TFS_NGINX_LOGLEVEL=info -e SAGEMAKER_BIND_TO_PORT=8080 -e SAGEMAKER_SAFE_PORT_RANGE=9000-9999 -e SAGEMAKER_MULTI_MODEL=true {}:{} serve'.format(runtime_config, docker_base_name, tag)
        proc = subprocess.Popen(command.split(), stdout=sys.stdout, stderr=subprocess.STDOUT)
        attempts = 0
        while attempts < 40:
            time.sleep(3)
            try:
                res_code = requests.get('http://localhost:8080/ping').status_code
<mask>:
                    break
            except:
                attempts += 1
                pass
        yield proc.pid
    finally:
        subprocess.check_call('docker rm -f sagemaker-tensorflow-serving-test'.split())",res_code == 200,61,res_code == 503,False,75.98356856515926,N/A
"@pytest.fixture(scope='module', autouse=True, params=[True, False])
def container(request, docker_base_name, tag, runtime_config):
    try:
<mask>:
            batching_config = ' -e SAGEMAKER_TFS_ENABLE_BATCHING=true'
            port_config = ' -e SAGEMAKER_SAFE_PORT_RANGE=9000-9999'
        else:
            batching_config = ''
            port_config = ''
        command = 'docker run {}--name sagemaker-tensorflow-serving-test -p 8080:8080 --mount type=volume,source=model_volume,target=/opt/ml/model,readonly -e SAGEMAKER_TFS_NGINX_LOGLEVEL=info -e SAGEMAKER_BIND_TO_PORT=8080 {} {} {}:{} serve'.format(runtime_config, batching_config, port_config, docker_base_name, tag)
        proc = subprocess.Popen(command.split(), stdout=sys.stdout, stderr=subprocess.STDOUT)
        attempts = 0
        while attempts < 40:
            time.sleep(3)
            try:
                res_code = requests.get('http://localhost:8080/ping').status_code
                if res_code == 200:
                    break
            except:
                attempts += 1
                pass
        yield proc.pid
    finally:
        subprocess.check_call('docker rm -f sagemaker-tensorflow-serving-test'.split())",request.param,84,request.param,True,100.00000000000004,N/A
"def make_request(data, content_type='application/json', method='predict', version=None):
    custom_attributes = 'tfs-model-name=half_plus_three,tfs-method={}'.format(method)
<mask>:
        custom_attributes += ',tfs-model-version={}'.format(version)
    headers = {'Content-Type': content_type, 'X-Amzn-SageMaker-Custom-Attributes': custom_attributes}
    response = requests.post(BASE_URL, data=data, headers=headers)
    return json.loads(response.content.decode('utf-8'))",version,25,version,True,100.00000000000004,N/A
"def make_headers(content_type='application/json', method='predict', version=None):
    custom_attributes = 'tfs-method={}'.format(method)
<mask>:
        custom_attributes += ',tfs-model-version={}'.format(version)
    return {'Content-Type': content_type, 'X-Amzn-SageMaker-Custom-Attributes': custom_attributes}",version,16,version,True,100.00000000000004,N/A
"def test_run_nginx_with_default_parameters(docker_base_name, tag, runtime_config):
    try:
        command = 'docker run {}--name sagemaker-tensorflow-serving-test -p 8080:8080 --mount type=volume,source=nginx_model_volume,target=/opt/ml/model,readonly {}:{} serve'.format(runtime_config, docker_base_name, tag)
        proc = subprocess.Popen(command.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        lines_seen = {'error_log  /dev/stderr error;': 0, 'proxy_read_timeout 60;': 0}
        for stdout_line in iter(proc.stdout.readline, ''):
            stdout_line = str(stdout_line)
            for line in lines_seen.keys():
<mask>:
                    lines_seen[line] += 1
            if 'started nginx' in stdout_line:
                for value in lines_seen.values():
                    assert value == 1
                break
    finally:
        subprocess.check_call('docker rm -f sagemaker-tensorflow-serving-test'.split())",line in stdout_line,68,line != 'error_log',False,9.652434877402245,N/A
"def test_run_nginx_with_env_var_parameters(docker_base_name, tag, runtime_config):
    try:
        command = 'docker run {}--name sagemaker-tensorflow-serving-test -p 8080:8080 --mount type=volume,source=nginx_model_volume,target=/opt/ml/model,readonly -e SAGEMAKER_TFS_NGINX_LOGLEVEL=info -e SAGEMAKER_NGINX_PROXY_READ_TIMEOUT_SECONDS=63 {}:{} serve'.format(runtime_config, docker_base_name, tag)
        proc = subprocess.Popen(command.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        lines_seen = {'error_log  /dev/stderr info;': 0, 'proxy_read_timeout 63;': 0}
        for stdout_line in iter(proc.stdout.readline, ''):
            stdout_line = str(stdout_line)
            for line in lines_seen.keys():
<mask>:
                    lines_seen[line] += 1
            if 'started nginx' in stdout_line:
                for value in lines_seen.values():
                    assert value == 1
                break
    finally:
        subprocess.check_call('docker rm -f sagemaker-tensorflow-serving-test'.split())",line in stdout_line,72,line != 'error_log',False,9.652434877402245,N/A
"def test_run_nginx_with_higher_gunicorn_parameter(docker_base_name, tag, runtime_config):
    try:
        command = 'docker run {}--name sagemaker-tensorflow-serving-test -p 8080:8080 --mount type=volume,source=nginx_model_volume,target=/opt/ml/model,readonly -e SAGEMAKER_NGINX_PROXY_READ_TIMEOUT_SECONDS=60 -e SAGEMAKER_GUNICORN_TIMEOUT_SECONDS=120 {}:{} serve'.format(runtime_config, docker_base_name, tag)
        proc = subprocess.Popen(command.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        lines_seen = {'proxy_read_timeout 120;': 0}
        for stdout_line in iter(proc.stdout.readline, ''):
            stdout_line = str(stdout_line)
            for line in lines_seen.keys():
<mask>:
                    lines_seen[line] += 1
            if 'started nginx' in stdout_line:
                for value in lines_seen.values():
                    assert value == 1
                break
    finally:
        subprocess.check_call('docker rm -f sagemaker-tensorflow-serving-test'.split())",line in stdout_line,68,line not in line,False,16.37226966703825,N/A
"@pytest.fixture(scope='module', autouse=True)
def container(docker_base_name, tag, runtime_config):
    try:
        command = 'docker run {}--name sagemaker-tensorflow-serving-test -p 8080:8080 --mount type=volume,source=model_volume_mme,target=/opt/ml/models,readonly -e SAGEMAKER_TFS_NGINX_LOGLEVEL=info -e SAGEMAKER_BIND_TO_PORT=8080 -e SAGEMAKER_SAFE_PORT_RANGE=9000-9999 -e SAGEMAKER_MULTI_MODEL=True {}:{} serve'.format(runtime_config, docker_base_name, tag)
        proc = subprocess.Popen(command.split(), stdout=sys.stdout, stderr=subprocess.STDOUT)
        attempts = 0
        while attempts < 40:
            time.sleep(3)
            try:
                res_code = requests.get('http://localhost:8080/ping').status_code
<mask>:
                    break
            except:
                attempts += 1
                pass
        yield proc.pid
    finally:
        subprocess.check_call('docker rm -f sagemaker-tensorflow-serving-test'.split())",res_code == 200,60,res_code == 200,True,100.00000000000004,N/A
"@pytest.fixture(scope='module', autouse=True)
def container(volume, docker_base_name, tag, runtime_config):
    try:
        command = 'docker run {}--name sagemaker-tensorflow-serving-test -p 8080:8080 --mount type=volume,source={},target=/opt/ml/model,readonly -e SAGEMAKER_TFS_NGINX_LOGLEVEL=info -e SAGEMAKER_BIND_TO_PORT=8080 -e SAGEMAKER_SAFE_PORT_RANGE=9000-9999 {}:{} serve'.format(runtime_config, volume, docker_base_name, tag)
        proc = subprocess.Popen(command.split(), stdout=sys.stdout, stderr=subprocess.STDOUT)
        attempts = 0
        while attempts < 40:
            time.sleep(3)
            try:
                res_code = requests.get('http://localhost:8080/ping').status_code
<mask>:
                    break
            except:
                attempts += 1
                pass
        yield proc.pid
    finally:
        subprocess.check_call('docker rm -f sagemaker-tensorflow-serving-test'.split())",res_code == 200,60,res_code == 200,True,100.00000000000004,N/A
"def make_headers(content_type, method, version=None):
    custom_attributes = 'tfs-model-name=half_plus_three,tfs-method={}'.format(method)
<mask>:
        custom_attributes += ',tfs-model-version={}'.format(version)
    return {'Content-Type': content_type, 'X-Amzn-SageMaker-Custom-Attributes': custom_attributes}",version,16,version,True,100.00000000000004,N/A
"def test_run_tfs_with_batching_parameters(docker_base_name, tag, runtime_config):
    try:
        command = 'docker run {}--name sagemaker-tensorflow-serving-test -p 8080:8080 --mount type=volume,source=batching_model_volume,target=/opt/ml/model,readonly -e SAGEMAKER_TFS_ENABLE_BATCHING=true -e SAGEMAKER_TFS_MAX_BATCH_SIZE=16 -e SAGEMAKER_TFS_BATCH_TIMEOUT_MICROS=500 -e SAGEMAKER_TFS_NUM_BATCH_THREADS=100 -e SAGEMAKER_TFS_MAX_ENQUEUED_BATCHES=1 -e SAGEMAKER_TFS_NGINX_LOGLEVEL=info -e SAGEMAKER_BIND_TO_PORT=8080 -e SAGEMAKER_SAFE_PORT_RANGE=9000-9999 {}:{} serve'.format(runtime_config, docker_base_name, tag)
        proc = subprocess.Popen(command.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        lines_seen = {'max_batch_size { value: 16 }': 0, 'batch_timeout_micros { value: 500 }': 0, 'num_batch_threads { value: 100 }': 0, 'max_enqueued_batches { value: 1 }': 0}
        for stdout_line in iter(proc.stdout.readline, ''):
            stdout_line = str(stdout_line)
            for line in lines_seen.keys():
<mask>:
                    lines_seen[line] += 1
            if 'Entering the event loop' in stdout_line:
                for value in lines_seen.values():
                    assert value == 1
                break
    finally:
        subprocess.check_call('docker rm -f sagemaker-tensorflow-serving-test'.split())",line in stdout_line,103,line != '',False,12.44023474812678,N/A
"@pytest.fixture(scope='module', autouse=True, params=[True, False])
def container(request, docker_base_name, tag, runtime_config):
    try:
<mask>:
            batching_config = ' -e SAGEMAKER_TFS_ENABLE_BATCHING=true'
        else:
            batching_config = ''
        command = 'docker run {}--name sagemaker-tensorflow-serving-test -p 8080:8080 --mount type=volume,source=multi_tfs_model_volume,target=/opt/ml/model,readonly -e SAGEMAKER_TFS_NGINX_LOGLEVEL=info -e SAGEMAKER_BIND_TO_PORT=8080 -e SAGEMAKER_SAFE_PORT_RANGE=9000-9999 -e SAGEMAKER_TFS_INSTANCE_COUNT=2 -e SAGEMAKER_GUNICORN_WORKERS=4 -e SAGEMAKER_TFS_INTER_OP_PARALLELISM=1 -e SAGEMAKER_TFS_INTRA_OP_PARALLELISM=1 {} {}:{} serve'.format(runtime_config, batching_config, docker_base_name, tag)
        proc = subprocess.Popen(command.split(), stdout=sys.stdout, stderr=subprocess.STDOUT)
        attempts = 0
        while attempts < 40:
            time.sleep(3)
            try:
                res_code = requests.get('http://localhost:8080/ping').status_code
                if res_code == 200:
                    break
            except:
                attempts += 1
                pass
        yield proc.pid
    finally:
        subprocess.check_call('docker rm -f sagemaker-tensorflow-serving-test'.split())",request.param,84,request.param,True,100.00000000000004,N/A
"@pytest.fixture
def tag(request, version, instance_type):
<mask>:
        return request.config.getoption('--tag')
    arch = 'gpu' if instance_type.startswith('ml.p') else 'cpu'
    return f'{version}-{arch}'",request.config.getoption('--tag'),17,request.config.has_option('--tag'),False,37.99178428257963,N/A
"def test_python_model_with_requirements(boto_session, sagemaker_client, sagemaker_runtime_client, model_name, python_model_with_requirements, image_uri, instance_type, accelerator_type):
<mask>:
        pytest.skip('skip for p3 instance')
    input_data = {'x': [1.0, 2.0, 5.0]}
    output_data = util.create_and_invoke_endpoint(boto_session, sagemaker_client, sagemaker_runtime_client, model_name, python_model_with_requirements, image_uri, instance_type, accelerator_type, input_data)
    assert output_data['python'] is True
    assert output_data['pillow'] == '6.0.0'",'p3' in instance_type,39,instance_type != 'p3',False,32.46679154750991,N/A
"def test_python_model_with_lib(boto_session, sagemaker_client, sagemaker_runtime_client, model_name, python_model_with_lib, image_uri, instance_type, accelerator_type):
<mask>:
        pytest.skip('skip for p3 instance')
    input_data = {'x': [1.0, 2.0, 5.0]}
    output_data = util.create_and_invoke_endpoint(boto_session, sagemaker_client, sagemaker_runtime_client, model_name, python_model_with_lib, image_uri, instance_type, accelerator_type, input_data)
    assert output_data['python'] is True
    assert output_data['dummy_module'] == '0.1'",'p3' in instance_type,39,instance_type != 'p3',False,32.46679154750991,N/A
"def pytest_configure(config):
    os.environ['TEST_REGION'] = config.getoption('--region')
    os.environ['TEST_VERSIONS'] = config.getoption('--versions') or '1.11.1,1.12.0,1.13.0'
    os.environ['TEST_INSTANCE_TYPES'] = config.getoption('--instance-types') or 'ml.m5.xlarge,ml.p2.xlarge'
    os.environ['TEST_EI_VERSIONS'] = config.getoption('--versions') or '1.11,1.12'
    os.environ['TEST_EI_INSTANCE_TYPES'] = config.getoption('--instance-types') or 'ml.m5.xlarge'
<mask>:
        os.environ['TEST_VERSIONS'] = config.getoption('--tag')
        os.environ['TEST_EI_VERSIONS'] = config.getoption('--tag')",config.getoption('--tag'),32,'--tag' in config.getoption('--version'),False,38.260294162784454,N/A
"@pytest.fixture(scope='session')
def registry(request, region):
<mask>:
        return request.config.getoption('--registry')
    sts = boto3.client('sts', region_name=region, endpoint_url='https://sts.{}.amazonaws.com'.format(region))
    return sts.get_caller_identity()['Account']",request.config.getoption('--registry'),14,request.config.getoption('--registry'),True,100.00000000000004,N/A
"@pytest.fixture(autouse=True)
def skip_gpu_instance_restricted_regions(region, instance_type):
<mask>:
        pytest.skip('Skipping GPU test in region {}'.format(region))",region in NO_P2_REGIONS and instance_type.startswith('ml.p2') or (region in NO_P3_REGIONS and instance_type.startswith('ml.p3')),11,instance_type == 'gpu',False,0.12347670678795658,N/A
"@pytest.fixture(autouse=True)
def skip_by_device_type(request, instance_type):
    is_gpu = instance_type[3] in ['g', 'p']
<mask>:
        pytest.skip('Skipping because running on ""{}"" instance'.format(instance_type))",request.node.get_closest_marker('skip_gpu') and is_gpu or (request.node.get_closest_marker('skip_cpu') and (not is_gpu)),17,is_gpu,False,0.00022603294069810552,N/A
"def _production_variants(model_name, instance_type, accelerator_type):
    production_variants = [{'VariantName': 'AllTraffic', 'ModelName': model_name, 'InitialInstanceCount': 1, 'InstanceType': instance_type}]
<mask>:
        production_variants[0]['AcceleratorType'] = accelerator_type
    return production_variants",accelerator_type,20,accelerator_type,True,100.00000000000004,N/A
"def find_or_put_model_data(region, boto_session, local_path):
    model_file = os.path.basename(local_path)
    bucket = _test_bucket(region, boto_session)
    key = f'test-tfs/{model_file}'
    s3 = boto_session.client('s3', region)
    try:
        s3.head_bucket(Bucket=bucket)
    except botocore.exceptions.ClientError as e:
<mask>:
            raise
        if region == 'us-east-1':
            s3.create_bucket(Bucket=bucket)
        else:
            s3.create_bucket(Bucket=bucket, CreateBucketConfiguration={'LocationConstraint': region})
    try:
        s3.head_object(Bucket=bucket, Key=key)
    except botocore.exceptions.ClientError as e:
        if e.response['Error']['Code'] != '404':
            raise
        s3.upload_file(local_path, bucket, key)
    return f's3://{bucket}/{key}'",e.response['Error']['Code'] != '404',52,e.response['Error']['Code'] != '404',True,100.00000000000004,N/A
"@contextlib.contextmanager
def sagemaker_endpoint(sagemaker_client, model_name, instance_type, accelerator_type=None):
    logger.info('creating endpoint %s', model_name)
    delay = round(random.random() * 5, 3)
    logger.info('waiting for {} seconds'.format(delay))
    time.sleep(delay)
    production_variants = _production_variants(model_name, instance_type, accelerator_type)
    sagemaker_client.create_endpoint_config(EndpointConfigName=model_name, ProductionVariants=production_variants)
    sagemaker_client.create_endpoint(EndpointName=model_name, EndpointConfigName=model_name)
    try:
        sagemaker_client.get_waiter('endpoint_in_service').wait(EndpointName=model_name)
    finally:
        status = sagemaker_client.describe_endpoint(EndpointName=model_name)['EndpointStatus']
<mask>:
            raise ValueError(f'failed to create endpoint {model_name}')
    try:
        yield model_name
    finally:
        logger.info('deleting endpoint and endpoint config %s', model_name)
        sagemaker_client.delete_endpoint(EndpointName=model_name)
        sagemaker_client.delete_endpoint_config(EndpointConfigName=model_name)",status != 'InService',56,status != 'created',False,59.460355750136046,N/A
"def _wait_for_transform_job(region, boto_session, sagemaker_client, model_name, poll, timeout):
    status = sagemaker_client.describe_transform_job(TransformJobName=model_name)['TransformJobStatus']
    job_runtime = 0
    while status == 'InProgress':
        logger.info(f'Waiting for batch transform job {model_name} to finish')
        time.sleep(poll)
        job_runtime += poll
<mask>:
            raise ValueError(f'Batch transform job {model_name} exceeded maximum runtime {timeout} seconds')
        status = sagemaker_client.describe_transform_job(TransformJobName=model_name)['TransformJobStatus']
        if status == 'Completed':
            return _read_batch_output(region=region, boto_session=boto_session, bucket=_test_bucket(region, boto_session), model_name=model_name)
        if status == 'Failed':
            raise ValueError(f'Failed to execute batch transform job {model_name}')
        if status in ['Stopped', 'Stopping']:
            raise ValueError(f'Batch transform job {model_name} was stopped')",job_runtime > timeout,77,timeout > 0 and job_runtime > MAX_RETRIES,False,24.808415001701817,N/A
"@pytest.fixture
def skip_if_no_accelerator(accelerator_type):
<mask>:
        pytest.skip('Skipping because accelerator type was not provided')",accelerator_type is None,11,accelerator_type is None,True,100.00000000000004,N/A
"@pytest.fixture
def skip_if_non_supported_ei_region(region):
<mask>:
        pytest.skip('EI is not supported in {}'.format(region))",region not in EI_SUPPORTED_REGIONS,10,region not in SUPPORTED_EI_REGIONS,False,33.03164318013809,N/A
"def input_handler(data, context):
    """""" Pre-process request input before it is sent to TensorFlow Serving REST API

    Args:
        data (obj): the request data, in format of dict or string
        context (Context): an object containing request and configuration details

    Returns:
        (dict): a JSON-serializable dict that contains request body and headers
    """"""
<mask>:
        d = data.read().decode('utf-8')
        return d if len(d) else ''
    if context.request_content_type == 'text/csv':
        return json.dumps({'instances': [float(x) for x in data.read().decode('utf-8').split(',')]})
    raise ValueError('{{""error"": ""unsupported content type {}""}}'.format(context.request_content_type or 'unknown'))",context.request_content_type == 'application/json',78,context.request_content_type == 'text/plain',False,72.92571723872932,N/A
"def output_handler(data, context):
    """"""Post-process TensorFlow Serving output before it is returned to the client.

    Args:
        data (obj): the TensorFlow serving response
        context (Context): an object containing request and configuration details

    Returns:
        (bytes, string): data to return to client, response content type
    """"""
<mask>:
        raise ValueError(data.content.decode('utf-8'))
    response_content_type = context.accept_header
    prediction = data.content
    return (prediction, response_content_type)",data.status_code != 200,54,"not isinstance(data.content, bytes)",False,9.535414040914192,N/A
"def _process_input(data, context):
<mask>:
        d = data.read().decode('utf-8')
        return d if len(d) else ''
    if context.request_content_type == 'text/csv':
        return json.dumps({'instances': [float(x) for x in data.read().decode('utf-8').split(',')]})
    raise ValueError('{{""error"": ""unsupported content type {}""}}'.format(context.request_content_type or 'unknown'))",context.request_content_type == 'application/json',32,context.request_content_type == 'text/plain',False,72.92571723872932,N/A
"def _process_output(data, context):
<mask>:
        raise ValueError(data.content.decode('utf-8'))
    response_content_type = context.accept_header
    prediction = data.content
    return (prediction, response_content_type)",data.status_code != 200,15,"not isinstance(data.content, bytes)",False,9.535414040914192,N/A
"def generate_json(shape, payload_size):
    one_record = _generate_json_recursively(shape)
<mask>:
        per_record_size = len(str(one_record))
        num_records = _get_num_records_for_json_payload(payload_size, per_record_size)
        records = []
        for record in range(0, num_records):
            records.append(one_record)
        return str(records)
    else:
        return str(one_record)",payload_size,28,len(one_record) > 0,False,5.522397783539471,N/A
"def _generate_json_recursively(shape):
<mask>:
        input = list(_random_input(shape[0]))
        return input
    else:
        inner_list = _generate_json_recursively(shape[1:])
        return [inner_list for _ in range(0, shape[0])]",len(shape) == 1,19,len(shape) == 1,True,100.00000000000004,N/A
"def generate_jsonlines(shape, payload_size):
    one_row = _generate_json_recursively(shape)
<mask>:
        one_row_string = str(one_row)
        num_records = _get_num_records_for_json_payload(payload_size, len(one_row_string))
        return '\n'.join([one_row_string for _ in range(0, num_records)])
    else:
        return one_row",payload_size,24,len(one_row) > 0,False,5.522397783539471,N/A
"def generate_csv(shape, payload_size):
    try:
        rows, columns = shape
    except ValueError:
        rows = 1
        columns = shape[0]
<mask>:
        rows = int(math.ceil(float(payload_size) / columns / 2.0))
    row = ','.join(map(lambda x: str(x), _random_input(columns)))
    return '\n'.join([row for _ in range(0, rows)])",payload_size,37,rows % columns == 0,False,0.0,N/A
"def generate_data(content_type, shape, payload_size, unit_of_payload='B'):
    assert unit_of_payload in _UNIT_FOR_PAYLOAD_SIZE.keys()
    payload_size = _map_payload_size_given_unit(payload_size, unit_of_payload)
<mask>:
        return generate_jsonlines(shape, payload_size)
    elif content_type == _CONTENT_TYPE_JSON:
        return generate_json(shape, payload_size)
    elif content_type == _CONTENT_TYPE_CSV:
        return generate_csv(shape, payload_size)
    else:
        raise ValueError('Content-type {} must be in {}'.format(content_type, _VALID_CONTENT_TYPES))",content_type == _CONTENT_TYPE_JSONLINES,40,content_type == _CONTENT_TYPE_JSONLINE,False,89.31539818068698,N/A
"def search_code(py_file_name, section_idx):
    """"""
    给定py文件名和section序号，返回一个list，内容是py文件中的code（markdown格式）
    :param py_file_name:
    :param section_idx:
    :return:
    """"""
    with open('../' + py_file_name, encoding='utf-8', mode='r') as f:
        content = f.readlines()
    content_new, i, search_idx, idx_first_match = ([], 0, 0, None)
    while i < len(content) and search_idx <= section_idx:
<mask>:
            search_idx += 1
            i += 1
        if search_idx < section_idx:
            pass
        elif search_idx == section_idx:
            idx_first_match = idx_first_match or i
            content_new.append(content[i])
        i += 1
    return ['-> Demo code: [{py_file_name}#s{section_idx}](https://github.com/guofei9987/blind_watermark/blob/master/{py_file_name}#L{idx_first_match})\n'.format(py_file_name=py_file_name, section_idx=section_idx + 1, idx_first_match=idx_first_match), '```python\n'] + content_new + ['```\n']",content[i].startswith('# %%'),78,content[i] == 'code',False,17.447394295753057,N/A
"def make_doc(origin_file):
    with open(origin_file, encoding='utf-8', mode='r') as f_readme:
        readme = f_readme.readlines()
    regex = re.compile('\\[examples/[\\w#.]+\\]')
    readme_idx = 0
    readme_new = []
    while readme_idx < len(readme):
        readme_line = readme[readme_idx]
<mask>:
            py_file_name, section_idx = regex.findall(readme[readme_idx])[0][1:-1].split('#s')
            section_idx = int(section_idx) - 1
            print('插入代码: ', py_file_name, section_idx)
            content_new = search_code(py_file_name, section_idx)
            readme_new.extend(content_new)
            while readme[readme_idx] != '```\n':
                readme_idx += 1
        else:
            readme_new.append(readme_line)
        readme_idx += 1
    return readme_new",readme_line.startswith('-> Demo code: ['),60,regex.search(readme[readme_idx]),False,7.510002314354895,N/A
"def print_notes(self):
<mask>:
        print(f'\nWelcome to use blind-watermark, version = {__version__}\nMake sure the version is the same when encode and decode\nYour star means a lot: https://github.com/guofei9987/blind_watermark\nThis message only show once. To close it: `blind_watermark.bw_notes.close()`\n            ')
        self.close()",self.show,35,self.notes,False,55.03212081491043,N/A
"def read_img(self, filename=None, img=None):
<mask>:
        img = cv2.imread(filename, flags=cv2.IMREAD_UNCHANGED)
        assert img is not None, ""image file '{filename}' not read"".format(filename=filename)
    self.bwm_core.read_img_arr(img=img)
    return img",img is None,22,filename is not None,False,18.99589214128981,N/A
"def read_wm(self, wm_content, mode='img'):
    assert mode in ('img', 'str', 'bit'), ""mode in ('img','str','bit')""
<mask>:
        wm = cv2.imread(filename=wm_content, flags=cv2.IMREAD_GRAYSCALE)
        assert wm is not None, 'file ""{filename}"" not read'.format(filename=wm_content)
        self.wm_bit = wm.flatten() > 128
    elif mode == 'str':
        byte = bin(int(wm_content.encode('utf-8').hex(), base=16))[2:]
        self.wm_bit = np.array(list(byte)) == '1'
    else:
        self.wm_bit = np.array(wm_content)
    self.wm_size = self.wm_bit.size
    np.random.RandomState(self.password_wm).shuffle(self.wm_bit)
    self.bwm_core.read_wm(self.wm_bit)",mode == 'img',54,mode == 'img',True,100.00000000000004,N/A
"def embed(self, filename=None, compression_ratio=None):
    """"""
        :param filename: string
            Save the image file as filename
        :param compression_ratio: int or None
            If compression_ratio = None, do not compression,
            If compression_ratio is integer between 0 and 100, the smaller, the output file is smaller.
        :return:
        """"""
    embed_img = self.bwm_core.embed()
<mask>:
        if compression_ratio is None:
            cv2.imwrite(filename=filename, img=embed_img)
        elif filename.endswith('.jpg'):
            cv2.imwrite(filename=filename, img=embed_img, params=[cv2.IMWRITE_JPEG_QUALITY, compression_ratio])
        elif filename.endswith('.png'):
            cv2.imwrite(filename=filename, img=embed_img, params=[cv2.IMWRITE_PNG_COMPRESSION, compression_ratio])
        else:
            cv2.imwrite(filename=filename, img=embed_img)
    return embed_img",filename is not None,70,filename is not None,True,100.00000000000004,N/A
"def extract(self, filename=None, embed_img=None, wm_shape=None, out_wm_name=None, mode='img'):
    assert wm_shape is not None, 'wm_shape needed'
<mask>:
        embed_img = cv2.imread(filename, flags=cv2.IMREAD_COLOR)
        assert embed_img is not None, '{filename} not read'.format(filename=filename)
    self.wm_size = np.array(wm_shape).prod()
    if mode in ('str', 'bit'):
        wm_avg = self.bwm_core.extract_with_kmeans(img=embed_img, wm_shape=wm_shape)
    else:
        wm_avg = self.bwm_core.extract(img=embed_img, wm_shape=wm_shape)
    wm = self.extract_decrypt(wm_avg=wm_avg)
    if mode == 'img':
        wm = 255 * wm.reshape(wm_shape[0], wm_shape[1])
        cv2.imwrite(out_wm_name, wm)
    elif mode == 'str':
        byte = ''.join((str((i >= 0.5) * 1) for i in wm))
        wm = bytes.fromhex(hex(int(byte, base=2))[2:]).decode('utf-8', errors='replace')
    return wm",filename is not None,81,filename is not None,True,100.00000000000004,N/A
"def cut_att3(input_filename=None, input_img=None, output_file_name=None, loc_r=None, loc=None, scale=None):
<mask>:
        input_img = cv2.imread(input_filename)
    if loc is None:
        h, w, _ = input_img.shape
        x1, y1, x2, y2 = (int(w * loc_r[0][0]), int(h * loc_r[0][1]), int(w * loc_r[1][0]), int(h * loc_r[1][1]))
    else:
        x1, y1, x2, y2 = loc
    output_img = input_img[y1:y2, x1:x2].copy()
    if scale and scale != 1:
        h, w, _ = output_img.shape
        output_img = cv2.resize(output_img, dsize=(round(w * scale), round(h * scale)))
    else:
        output_img = output_img
    if output_file_name:
        cv2.imwrite(output_file_name, output_img)
    return output_img",input_filename,78,input_filename,True,100.00000000000004,N/A
"def resize_att(input_filename=None, input_img=None, output_file_name=None, out_shape=(500, 500)):
<mask>:
        input_img = cv2.imread(input_filename)
    output_img = cv2.resize(input_img, dsize=out_shape)
    if output_file_name:
        cv2.imwrite(output_file_name, output_img)
    return output_img",input_filename,20,input_filename,True,100.00000000000004,N/A
"def bright_att(input_filename=None, input_img=None, output_file_name=None, ratio=0.8):
<mask>:
        input_img = cv2.imread(input_filename)
    output_img = input_img * ratio
    output_img[output_img > 255] = 255
    if output_file_name:
        cv2.imwrite(output_file_name, output_img)
    return output_img",input_filename,25,input_filename,True,100.00000000000004,N/A
"def shelter_att(input_filename=None, input_img=None, output_file_name=None, ratio=0.1, n=3):
<mask>:
        output_img = cv2.imread(input_filename)
    else:
        output_img = input_img.copy()
    input_img_shape = output_img.shape
    for i in range(n):
        tmp = np.random.rand() * (1 - ratio)
        start_height, end_height = (int(tmp * input_img_shape[0]), int((tmp + ratio) * input_img_shape[0]))
        tmp = np.random.rand() * (1 - ratio)
        start_width, end_width = (int(tmp * input_img_shape[1]), int((tmp + ratio) * input_img_shape[1]))
        output_img[start_height:end_height, start_width:end_width, :] = 255
    if output_file_name:
        cv2.imwrite(output_file_name, output_img)
    return output_img",input_filename,68,input_filename,True,100.00000000000004,N/A
"def salt_pepper_att(input_filename=None, input_img=None, output_file_name=None, ratio=0.01):
<mask>:
        input_img = cv2.imread(input_filename)
    input_img_shape = input_img.shape
    output_img = input_img.copy()
    for i in range(input_img_shape[0]):
        for j in range(input_img_shape[1]):
            if np.random.rand() < ratio:
                output_img[i, j, :] = 255
    if output_file_name:
        cv2.imwrite(output_file_name, output_img)
    return output_img",input_filename,38,input_filename,True,100.00000000000004,N/A
"def read_img_arr(self, img):
    self.alpha = None
<mask>:
        if img[:, :, 3].min() < 255:
            self.alpha = img[:, :, 3]
            img = img[:, :, :3]
    self.img = img.astype(np.float32)
    self.img_shape = self.img.shape[:2]
    self.img_YUV = cv2.copyMakeBorder(cv2.cvtColor(self.img, cv2.COLOR_BGR2YUV), 0, self.img.shape[0] % 2, 0, self.img.shape[1] % 2, cv2.BORDER_CONSTANT, value=(0, 0, 0))
    self.ca_shape = [(i + 1) // 2 for i in self.img_shape]
    self.ca_block_shape = (self.ca_shape[0] // self.block_shape[0], self.ca_shape[1] // self.block_shape[1], self.block_shape[0], self.block_shape[1])
    strides = 4 * np.array([self.ca_shape[1] * self.block_shape[0], self.block_shape[1], self.ca_shape[1], 1])
    for channel in range(3):
        self.ca[channel], self.hvd[channel] = dwt2(self.img_YUV[:, :, channel], 'haar')
        self.ca_block[channel] = np.lib.stride_tricks.as_strided(self.ca[channel].astype(np.float32), self.ca_block_shape, strides)",img.shape[2] == 4,92,img.ndim == 2,False,14.723282228934908,N/A
"def block_add_wm_slow(self, arg):
    block, shuffler, i = arg
    wm_1 = self.wm_bit[i % self.wm_size]
    block_dct = dct(block)
    block_dct_shuffled = block_dct.flatten()[shuffler].reshape(self.block_shape)
    u, s, v = svd(block_dct_shuffled)
    s[0] = (s[0] // self.d1 + 1 / 4 + 1 / 2 * wm_1) * self.d1
<mask>:
        s[1] = (s[1] // self.d2 + 1 / 4 + 1 / 2 * wm_1) * self.d2
    block_dct_flatten = np.dot(u, np.dot(np.diag(s), v)).flatten()
    block_dct_flatten[shuffler] = block_dct_flatten.copy()
    return idct(block_dct_flatten.reshape(self.block_shape))",self.d2,69,self.wm_size > 1,False,13.134549472120788,N/A
"def embed(self):
    self.init_block_index()
    embed_ca = copy.deepcopy(self.ca)
    embed_YUV = [np.array([])] * 3
    self.idx_shuffle = random_strategy1(self.password_img, self.block_num, self.block_shape[0] * self.block_shape[1])
    for channel in range(3):
        tmp = self.pool.map(self.block_add_wm, [(self.ca_block[channel][self.block_index[i]], self.idx_shuffle[i], i) for i in range(self.block_num)])
        for i in range(self.block_num):
            self.ca_block[channel][self.block_index[i]] = tmp[i]
        self.ca_part[channel] = np.concatenate(np.concatenate(self.ca_block[channel], 1), 1)
        embed_ca[channel][:self.part_shape[0], :self.part_shape[1]] = self.ca_part[channel]
        embed_YUV[channel] = idwt2((embed_ca[channel], self.hvd[channel]), 'haar')
    embed_img_YUV = np.stack(embed_YUV, axis=2)
    embed_img_YUV = embed_img_YUV[:self.img_shape[0], :self.img_shape[1]]
    embed_img = cv2.cvtColor(embed_img_YUV, cv2.COLOR_YUV2BGR)
    embed_img = np.clip(embed_img, a_min=0, a_max=255)
<mask>:
        embed_img = cv2.merge([embed_img.astype(np.uint8), self.alpha])
    return embed_img",self.alpha is not None,77,self.alpha is not None,True,100.00000000000004,N/A
"def __init__(self, mode, processes):
<mask>:
        warnings.warn('multiprocessing not support in windows, turning to multithreading')
        mode = 'multithreading'
    self.mode = mode
    self.processes = processes
    if mode == 'vectorization':
        pass
    elif mode == 'cached':
        pass
    elif mode == 'multithreading':
        from multiprocessing.dummy import Pool as ThreadPool
        self.pool = ThreadPool(processes=processes)
    elif mode == 'multiprocessing':
        from multiprocessing import Pool
        self.pool = Pool(processes=processes)
    else:
        self.pool = CommonPool()",mode == 'multiprocessing' and sys.platform == 'win32',60,sys.platform.startswith('win'),False,14.128386352314104,N/A
"def main():
    bwm1 = WaterMark(password_img=int(opts.password))
<mask>:
        if not len(args) == 3:
            print('Error! Usage: ')
            print(usage1)
            return
        else:
            bwm1.read_img(args[0])
            bwm1.read_wm(args[1], mode='str')
            bwm1.embed(args[2])
            print('Embed succeed! to file ', args[2])
            print('Put down watermark size:', len(bwm1.wm_bit))
    if opts.work_mode == 'extract':
        if not len(args) == 1:
            print('Error! Usage: ')
            print(usage2)
            return
        else:
            wm_str = bwm1.extract(filename=args[0], wm_shape=int(opts.wm_shape), mode='str')
            print('Extract succeed! watermark is:')
            print(wm_str)",opts.work_mode == 'embed',57,opts.work_mode == 'embed',True,100.00000000000004,N/A
"def search_template(scale=(0.5, 2), search_num=200):
    image, template = (my_value.image, my_value.template)
    tmp = []
    min_scale, max_scale = scale
    max_scale = min(max_scale, image.shape[0] / template.shape[0], image.shape[1] / template.shape[1])
    max_idx = 0
    for i in range(2):
        for scale in np.linspace(min_scale, max_scale, search_num):
            ind, score, scale = match_template_by_scale(scale)
            tmp.append([ind, score, scale])
        max_idx = 0
        max_score = 0
        for idx, (ind, score, scale) in enumerate(tmp):
<mask>:
                max_idx, max_score = (idx, score)
        min_scale, max_scale = (tmp[max(0, max_idx - 1)][2], tmp[min(len(tmp) - 1, max_idx + 1)][2])
        search_num = 2 * int((max_scale - min_scale) * max(template.shape[1], template.shape[0])) + 1
    return tmp[max_idx]",score > max_score,92,score > max_score,True,100.00000000000004,N/A
"def estimate_crop_parameters(original_file=None, template_file=None, ori_img=None, tem_img=None, scale=(0.5, 2), search_num=200):
<mask>:
        tem_img = cv2.imread(template_file, cv2.IMREAD_GRAYSCALE)
    if original_file:
        ori_img = cv2.imread(original_file, cv2.IMREAD_GRAYSCALE)
    if scale[0] == scale[1] == 1:
        scale_infer = 1
        scores = cv2.matchTemplate(ori_img, tem_img, cv2.TM_CCOEFF_NORMED)
        ind = np.unravel_index(np.argmax(scores, axis=None), scores.shape)
        ind, score = (ind, scores[ind])
    else:
        my_value.set_val(image=ori_img, template=tem_img)
        ind, score, scale_infer = search_template(scale=scale, search_num=search_num)
    w, h = (int(tem_img.shape[1] * scale_infer), int(tem_img.shape[0] * scale_infer))
    x1, y1, x2, y2 = (ind[1], ind[0], ind[1] + w, ind[0] + h)
    return ((x1, y1, x2, y2), ori_img.shape, score, scale_infer)",template_file,82,template_file,True,100.00000000000004,N/A
"def recover_crop(template_file=None, tem_img=None, output_file_name=None, loc=None, image_o_shape=None):
<mask>:
        tem_img = cv2.imread(template_file)
    x1, y1, x2, y2 = loc
    img_recovered = np.zeros((image_o_shape[0], image_o_shape[1], 3))
    img_recovered[y1:y2, x1:x2, :] = cv2.resize(tem_img, dsize=(x2 - x1, y2 - y1))
    if output_file_name:
        cv2.imwrite(output_file_name, img_recovered)
    return img_recovered",template_file,38,template_file,True,100.00000000000004,N/A
"def readkey() -> str:
    """"""Reads the next keypress. If an escaped key is pressed, the full
    sequence is read and returned as noted in `_win_key.py`.""""""
    ch = readchar()
<mask>:
        raise KeyboardInterrupt
    if ch in '\x00à':
        ch = '\x00' + readchar()
    if '\ud800' <= ch <= '\udfff':
        ch += readchar()
        ch = ch.encode('utf-16', errors='surrogatepass').decode('utf-16')
    return ch",ch in config.INTERRUPT_KEYS,55,ch in '\x00',False,14.320952289897704,N/A
"def readkey() -> str:
    """"""Get a keypress. If an escaped key is pressed, the full sequence is
    read and returned as noted in `_posix_key.py`.""""""
    c1 = readchar()
<mask>:
        raise KeyboardInterrupt
    if c1 != '\x1b':
        return c1
    c2 = readchar()
    if c2 not in 'O[':
        return c1 + c2
    c3 = readchar()
    if c3 not in '12356':
        return c1 + c2 + c3
    c4 = readchar()
    if c4 not in '01345789':
        return c1 + c2 + c3 + c4
    c5 = readchar()
    return c1 + c2 + c3 + c4 + c5",c1 in config.INTERRUPT_KEYS,91,c1 in 'C',False,14.506309551249304,N/A
"def parse(lines: typing.Sequence[str]) -> typing.List[Caption]:
    """"""
    Parse SBV captions from lines of text.

    :param lines: lines of text
    :returns: list of `Caption` objects
    """"""
<mask>:
        raise MalformedFileError('Invalid format')
    return _parse_captions(lines)",not _is_valid_content(lines),30,not _validate_captions(lines),False,21.069764742263047,N/A
"def _is_valid_content(lines: typing.Sequence[str]) -> bool:
    """"""
    Validate lines of text for valid SBV content.

    :param lines: lines of text
    :returns: true for a valid SBV content
    """"""
<mask>:
        return False
    first_block = next(utils.iter_blocks_of_lines(lines))
    return bool(first_block and SBVCueBlock.is_valid(first_block))",len(lines) < 2,37,not utils.is_valid_lines(lines),False,13.950796967929138,N/A
"def _parse_captions(lines: typing.Sequence[str]) -> typing.List[Caption]:
    """"""
    Parse captions from the text.

    :param lines: lines of text
    :returns: list of `Caption` objects
    """"""
    captions = []
    for block_lines in utils.iter_blocks_of_lines(lines):
<mask>:
            continue
        cue_block = SBVCueBlock.from_lines(block_lines)
        captions.append(Caption(cue_block.start, cue_block.end, cue_block.payload))
    return captions",not SBVCueBlock.is_valid(block_lines),39,block_lines is None,False,12.869637315183779,N/A
"@classmethod
def from_string(cls, value: str) -> 'Timestamp':
    """"""Return a `Timestamp` instance from a string value.""""""
<mask>:
        raise MalformedCaptionError(f'Invalid timestamp {value!r}')
    match = re.match(cls.PATTERN, value)
    if not match:
        raise MalformedCaptionError(f'Invalid timestamp {value!r}')
    hours = int(match.group(1) or 0)
    minutes = int(match.group(2))
    seconds = int(match.group(3))
    milliseconds = int(match.group(4))
    if minutes > 59 or seconds > 59:
        raise MalformedCaptionError(f'Invalid timestamp {value!r}')
    return cls(hours, minutes, seconds, milliseconds)",type(value) is not str,62,not value,False,5.804285916064729,N/A
"def __eq__(self, other):
    """"""Compare equality with another object.""""""
<mask>:
        return False
    return self.start == other.start and self.end == other.end and (self.raw_text == other.raw_text) and (self.identifier == other.identifier)","not isinstance(other, type(self))",27,"not isinstance(other, Source)",False,41.91742490576712,N/A
"@text.setter
def text(self, value: str):
    """"""Set the text of the captions.""""""
<mask>:
        raise AttributeError(f'String value expected but received {value}.')
    self.lines = value.splitlines()","not isinstance(value, str)",22,"not isinstance(value, str)",True,100.00000000000004,N/A
"@property
def voice(self) -> typing.Optional[str]:
    """"""Return the voice span if present.""""""
<mask>:
        match = re.match(self.VOICE_SPAN_PATTERN, self.lines[0])
        if match:
            return match.group(1)
    return None",self.lines and self.lines[0].startswith('<v'),22,self.lines,False,0.940356255149521,N/A
"@classmethod
def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTCueBlock':
    """"""
        Create a `WebVTTCueBlock` from lines of text.

        :param lines: the lines of text
        :returns: `WebVTTCueBlock` instance
        """"""
    identifier = None
    start = None
    end = None
    payload = []
    for line in lines:
        timing_match = re.match(cls.CUE_TIMINGS_PATTERN, line)
<mask>:
            start = timing_match.group(1)
            end = timing_match.group(2)
        elif not start:
            identifier = line
        else:
            payload.append(line)
    return cls(identifier, start, end, payload)",timing_match,65,timing_match,True,100.00000000000004,N/A
"@staticmethod
def format_lines(lines: str) -> typing.List[str]:
    """"""
        Return the lines for a comment block.

        :param lines: comment lines
        :returns: list of lines for a comment block
        """"""
    list_of_lines = lines.split('\n')
<mask>:
        return [f'NOTE {lines}']
    return ['NOTE', *list_of_lines]",len(list_of_lines) == 1,37,len(list_of_lines) == 1,True,100.00000000000004,N/A
"def parse(lines: typing.Sequence[str]) -> ParserOutput:
    """"""
    Parse VTT captions from lines of text.

    :param lines: lines of text
    :returns: object `ParserOutput` with all parsed items
    """"""
<mask>:
        raise MalformedFileError('Invalid format')
    return parse_items(lines)",not is_valid_content(lines),32,not lines,False,2.1352774592011654,N/A
"def parse_items(lines: typing.Sequence[str]) -> ParserOutput:
    """"""
    Parse items from the text.

    :param lines: lines of text
    :returns: an object `ParserOutput` with all parsed items
    """"""
    header_comments: typing.List[str] = []
    items: typing.List[typing.Union[Caption, Style]] = []
    comments: typing.List[WebVTTCommentBlock] = []
    for block_lines in utils.iter_blocks_of_lines(lines):
        item = parse_item(block_lines)
<mask>:
            item.comments = [comment.text for comment in comments]
            comments = []
            items.append(item)
        elif WebVTTCommentBlock.is_valid(block_lines):
            comments.append(WebVTTCommentBlock.from_lines(block_lines))
    if items:
        header_comments, items[0].comments = (items[0].comments, header_comments)
    return ParserOutput.from_data({'items': items, 'header_comments': header_comments, 'footer_comments': [comment.text for comment in comments]})",item,78,item,True,100.00000000000004,N/A
"def parse_item(lines: typing.Sequence[str]) -> typing.Union[Caption, Style, None]:
    """"""
    Parse an item from lines of text.

    :param lines: lines of text
    :returns: An item (Caption or Style) if found, otherwise None
    """"""
<mask>:
        cue_block = WebVTTCueBlock.from_lines(lines)
        return Caption(cue_block.start, cue_block.end, cue_block.payload, cue_block.identifier)
    if WebVTTStyleBlock.is_valid(lines):
        return Style(WebVTTStyleBlock.from_lines(lines).text)
    return None",WebVTTCueBlock.is_valid(lines),46,WebVTTCueBlock.is_valid(lines),True,100.00000000000004,N/A
"def __enter__(self):
    """"""Enter context.""""""
    self.file = open(file=self.file_path, mode=self.mode, encoding=self.encoding)
<mask>:
        self.file.seek(len(CODEC_BOMS[self.bom_encoding]))
    return self",self.bom_encoding,13,self.bom_encoding,True,100.00000000000004,N/A
"@staticmethod
def detect_bom_encoding(file_path: str) -> typing.Optional[str]:
    """"""
        Detect the encoding of a file based on the presence of the BOM.

        :param file_path: path to the file
        :returns: the encoding if BOM is found or None.
        """"""
    with open(file_path, mode='rb') as f:
        first_bytes = f.read(4)
        for encoding, bom in CODEC_BOMS.items():
<mask>:
                return encoding
    return None",first_bytes.startswith(bom),54,first_bytes == bom,False,23.263472697663296,N/A
"def iter_blocks_of_lines(lines: typing.Iterable[str]) -> typing.Generator[typing.List[str], None, None]:
    """"""
    Iterate blocks of text.

    :param lines: lines of text.
    """"""
    current_text_block = []
    for line in lines:
<mask>:
            current_text_block.append(line)
        elif current_text_block:
            yield current_text_block
            current_text_block = []
    if current_text_block:
        yield current_text_block",line.strip(),38,line,False,1.8315638888734187,N/A
"def slice_segments(captions: typing.Sequence[Caption], seconds: int) -> typing.List[typing.List[Caption]]:
    """"""
    Slice segments of captions based on seconds per segment.

    :param captions: the captions
    :param seconds: seconds per segment
    :returns: list of lists of `Caption` objects
    """"""
    total_segments = 0 if not captions else int(ceil(captions[-1].end_in_seconds / seconds))
    segments: typing.List[typing.List[Caption]] = [[] for _ in range(total_segments)]
    for c in captions:
        segment_index_start = floor(c.start_in_seconds / seconds)
        segments[segment_index_start].append(c)
        segment_index_end = floor(c.end_in_seconds / seconds)
<mask>:
            for i in range(segment_index_start + 1, segment_index_end + 1):
                segments[i].append(c)
    return segments",segment_index_end > segment_index_start,80,seconds > 0,False,1.9119108411650758,N/A
"@classmethod
def read(cls, file: str, encoding: typing.Optional[str]=None) -> 'WebVTT':
    """"""
        Read a WebVTT captions file.

        :param file: the file path
        :param encoding: encoding of the file
        :returns: a `WebVTT` instance
        """"""
    with utils.FileWrapper.open(file, encoding=encoding) as fw:
        instance = cls.from_buffer(fw.file)
<mask>:
            instance.encoding = fw.bom_encoding
            instance._has_bom = True
        return instance",fw.bom_encoding,48,fw.bom_encoding is not None,False,51.697315395717055,N/A
"@classmethod
def from_buffer(cls, buffer: typing.Union[typing.Iterable[str], io.BytesIO], format: str='vtt') -> 'WebVTT':
    """"""
        Read WebVTT captions from a file-like object.

        Such file-like object may be the return of an io.open call,
        io.StringIO object, tempfile.TemporaryFile object, etc.

        :param buffer: the file-like object to read captions from
        :param format: the format of the data (vtt, srt or sbv)
        :returns: a `WebVTT` instance
        """"""
<mask>:
        buffer = (line.decode('utf-8') for line in buffer)
    _cls = partial(cls, file=getattr(buffer, 'name', None))
    if format == 'vtt':
        output = vtt.parse(cls._get_lines(buffer))
        return _cls(captions=output.captions, styles=output.styles, header_comments=output.header_comments, footer_comments=output.footer_comments)
    if format == 'srt':
        return _cls(captions=srt.parse(cls._get_lines(buffer)))
    if format == 'sbv':
        return _cls(captions=sbv.parse(cls._get_lines(buffer)))
    raise ValueError(f'Format {format} is not supported.')","isinstance(buffer, io.BytesIO)",103,"isinstance(buffer, bytes)",False,38.49815007763549,N/A
"def _get_destination_file(self, destination_path: typing.Optional[str]=None, extension: str='vtt') -> str:
    """"""
        Return the destination file based on the provided params.

        :param destination_path: optional destination path
        :param extension: the extension of the file
        :returns: the destination file

        :raises MissingFilenameError: if destination path cannot be determined
        """"""
<mask>:
        raise MissingFilenameError
    if not destination_path and self.file:
        destination_path = f'{os.path.splitext(self.file)[0]}.{extension}'
    assert destination_path is not None
    target = os.path.join(os.getcwd(), destination_path)
    if os.path.isdir(target):
        if not self.file:
            raise MissingFilenameError
        base_name = os.path.splitext(os.path.basename(self.file))[0]
        new_filename = f'{base_name}.{extension}'
        return os.path.join(target, new_filename)
    if target[-4:].lower() != f'.{extension}':
        target = f'{target}.{extension}'
    return target",not destination_path and (not self.file),88,not self.file,False,17.37739434504452,N/A
"def save(self, output: typing.Optional[str]=None, encoding: typing.Optional[str]=None, add_bom: typing.Optional[bool]=None):
    """"""
        Save the WebVTT captions to a file.

        :param output: destination path of the file
        :param encoding: encoding of the file
        :param add_bom: save the file with Byte Order Mark

        :raises MissingFilenameError: if output cannot be determined
        """"""
    self.file = self._get_destination_file(output)
    encoding = encoding or self.encoding
<mask>:
        add_bom = True
    with open(self.file, 'w', encoding=encoding) as f:
        if add_bom and encoding in utils.CODEC_BOMS:
            f.write(utils.CODEC_BOMS[encoding].decode(encoding))
        vtt.write(f, self.captions, self.styles, self.header_comments, self.footer_comments)",add_bom is None and self._has_bom,76,not add_bom,False,8.047084086794415,N/A
"def write(self, f: typing.IO[str], format: str='vtt'):
    """"""
        Save the WebVTT captions to a file-like object.

        :param f: destination file-like object
        :param format: the format to use (`vtt` or `srt`)

        :raises MissingFilenameError: if output cannot be determined
        """"""
<mask>:
        return vtt.write(f, self.captions, self.styles, self.header_comments, self.footer_comments)
    if format == 'srt':
        return srt.write(f, self.captions)
    raise ValueError(f'Format {format} is not supported.')",format == 'vtt',57,format == 'vtt',True,100.00000000000004,N/A
"def parse(lines: typing.Sequence[str]) -> typing.List[Caption]:
    """"""
    Parse SRT captions from lines of text.

    :param lines: lines of text
    :returns: list of `Caption` objects
    """"""
<mask>:
        raise MalformedFileError('Invalid format')
    return parse_captions(lines)",not is_valid_content(lines),30,not lines,False,2.1352774592011654,N/A
"def parse_captions(lines: typing.Sequence[str]) -> typing.List[Caption]:
    """"""
    Parse captions from the text.

    :param lines: lines of text
    :returns: list of `Caption` objects
    """"""
    captions: typing.List[Caption] = []
    for block_lines in utils.iter_blocks_of_lines(lines):
<mask>:
            continue
        cue_block = SRTCueBlock.from_lines(block_lines)
        cue_block.start, cue_block.end = map(lambda x: x.replace(',', '.'), (cue_block.start, cue_block.end))
        captions.append(Caption(cue_block.start, cue_block.end, cue_block.payload))
    return captions",not SRTCueBlock.is_valid(block_lines),49,block_lines is None,False,12.869637315183779,N/A
"def main(req: func.HttpRequest) -> func.HttpResponse:
    analyzer = SentimentIntensityAnalyzer()
    text = req.params.get('text')
<mask>:
        return func.HttpResponse(status_code=302, headers={'Location': req.url + '?text=I+Love+PyCon'})
    scores = analyzer.polarity_scores(text)
    sentiment = 'positive' if scores['compound'] > 0 else 'negative'
    return func.HttpResponse(sentiment)",text is None,32,not text,False,30.326532985631665,N/A
"def dapr_save_state_etag(store='statestore', name='hello', value='world', etag=''):
    """"""DaprClient().save_state(store_name=store, key=name, value=value, etag=etag)""""""
    with dapr.clients.DaprClient() as d:
        d.wait(5)
<mask>:
            d.save_state(store_name=store, key=name, value=value)
            print(f'State store has successfully saved {value} with {name} as key')
        else:
            try:
                d.save_state(store_name=store, key=name, value=value, etag=etag)
                print(f'State store has successfully saved {value} with {name} as key with etag {etag}')
            except grpc.RpcError as err:
                print(f'Cannot save due to bad etag. ErrorCode={err.code()}')",etag == '',58,etag == '',True,100.00000000000004,N/A
"def is_prime(number: int) -> bool:
    """"""Tests primeness of number, returns true if prime.""""""
    min_divisor = 2
    max_divisor = math.ceil(math.sqrt(number))
    for divisor in range(min_divisor, max_divisor + 1):
<mask>:
            return False
    return True",number % divisor == 0,31,number % divisor != 0,False,37.99178428257963,N/A
"def log_message(request):
    form = LogMessageForm(request.POST or None)
<mask>:
        if form.is_valid():
            message = form.save(commit=False)
            message.log_date = datetime.now()
            message.save()
            return redirect('home')
        else:
            return render(request, 'hello/log_message.html', {'form': form})
    else:
        return render(request, 'hello/log_message.html', {'form': form})",request.method == 'POST',31,request.method == 'POST',True,100.00000000000004,N/A
"def process_split_ratio(ratio):
    """"""Generate split ratio lists

    Args:
        ratio (float or list): a float number that indicates split ratio or a list of float
        numbers that indicate split ratios (if it is a multi-split).

    Returns:
        tuple: a tuple containing
            bool: A boolean variable multi that indicates if the splitting is multi or single.
            list: A list of normalized split ratios.
    """"""
<mask>:
        if ratio <= 0 or ratio >= 1:
            raise ValueError('Split ratio has to be between 0 and 1')
        multi = False
    elif isinstance(ratio, list):
        if any([x <= 0 for x in ratio]):
            raise ValueError('All split ratios in the ratio list should be larger than 0.')
        if sum(ratio) != 1.0:
            ratio = [x / sum(ratio) for x in ratio]
        multi = True
    else:
        raise TypeError('Split ratio should be either float or a list of floats.')
    return (multi, ratio)","isinstance(ratio, float)",138,"isinstance(ratio, float)",True,100.00000000000004,N/A
"def _check_min_rating_filter(filter_by, min_rating, col_user, col_item):
<mask>:
        raise ValueError(""filter_by should be either 'user' or 'item'."")
    if min_rating < 1:
        raise ValueError('min_rating should be integer and larger than or equal to 1.')
    split_by_column = col_user if filter_by == 'user' else col_item
    split_with_column = col_item if filter_by == 'user' else col_user
    return (split_by_column, split_with_column)",not (filter_by == 'user' or filter_by == 'item'),51,"filter_by not in ['user', 'item']",False,10.194207971045941,N/A
"def split_pandas_data_with_ratios(data, ratios, seed=42, shuffle=False):
    """"""Helper function to split pandas DataFrame with given ratios

    Note:
        Implementation referenced from
        https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test

    Args:
        data (pd.DataFrame): Pandas data frame to be split.
        ratios (list of floats): list of ratios for split. The ratios have to sum to 1.
        seed (int): random seed.
        shuffle (bool): whether data will be shuffled when being split.

    Returns:
        list: List of pd.DataFrame splitted by the given specifications.
    """"""
<mask>:
        raise ValueError('The ratios have to sum to 1')
    split_index = np.cumsum(ratios).tolist()[:-1]
    if shuffle:
        data = data.sample(frac=1, random_state=seed)
    splits = np.split(data, [round(x * len(data)) for x in split_index])
    for i in range(len(ratios)):
        splits[i]['split_index'] = i
    return splits",sum(ratios) != 1.0,106,len(ratios) != 1,False,61.47881529512643,N/A
"def python_random_split(data, ratio=0.75, seed=42):
    """"""Pandas random splitter
    The splitter randomly splits the input data.

    Args:
        data (pd.DataFrame): Pandas DataFrame to be split.
        ratio (float or list): Ratio for splitting data. If it is a single float number
            it splits data into two halfs and the ratio argument indicates the ratio 
            of training data set; if it is a list of float numbers, the splitter splits 
            data into several portions corresponding to the split ratios. If a list is 
            provided and the ratios are not summed to 1, they will be normalized.
        seed (int): Seed.
        
    Returns:
        list: Splits of the input data as pd.DataFrame.
    """"""
    multi_split, ratio = process_split_ratio(ratio)
<mask>:
        splits = split_pandas_data_with_ratios(data, ratio, shuffle=True, seed=seed)
        splits_new = [x.drop('split_index', axis=1) for x in splits]
        return splits_new
    else:
        return sk_split(data, test_size=None, train_size=ratio, random_state=seed)",multi_split,131,multi_split,True,100.00000000000004,N/A
"def _do_stratification(data, ratio=0.75, min_rating=1, filter_by='user', is_random=True, seed=42, col_user=DEFAULT_USER_COL, col_item=DEFAULT_ITEM_COL, col_timestamp=DEFAULT_TIMESTAMP_COL):
<mask>:
        raise ValueError(""filter_by should be either 'user' or 'item'."")
    if min_rating < 1:
        raise ValueError('min_rating should be integer and larger than or equal to 1.')
    if col_user not in data.columns:
        raise ValueError('Schema of data not valid. Missing User Col')
    if col_item not in data.columns:
        raise ValueError('Schema of data not valid. Missing Item Col')
    if not is_random:
        if col_timestamp not in data.columns:
            raise ValueError('Schema of data not valid. Missing Timestamp Col')
    multi_split, ratio = process_split_ratio(ratio)
    split_by_column = col_user if filter_by == 'user' else col_item
    ratio = ratio if multi_split else [ratio, 1 - ratio]
    if min_rating > 1:
        data = min_rating_filter_pandas(data, min_rating=min_rating, filter_by=filter_by, col_user=col_user, col_item=col_item)
    splits = []
    df_grouped = data.sort_values(col_timestamp).groupby(split_by_column) if is_random is False else data.groupby(split_by_column)
    for name, group in df_grouped:
        group_splits = split_pandas_data_with_ratios(df_grouped.get_group(name), ratio, shuffle=is_random, seed=seed)
        concat_group_splits = pd.concat(group_splits)
        splits.append(concat_group_splits)
    splits_all = pd.concat(splits)
    splits_list = [splits_all[splits_all['split_index'] == x].drop('split_index', axis=1) for x in range(len(ratio))]
    return splits_list",not (filter_by == 'user' or filter_by == 'item'),156,"filter_by not in ['user', 'item']",False,10.194207971045941,N/A
"def user_item_pairs(user_df, item_df, user_col=DEFAULT_USER_COL, item_col=DEFAULT_ITEM_COL, user_item_filter_df=None, shuffle=True):
    """"""Get all pairs of users and items data.

    Args:
        user_df (pd.DataFrame): User data containing unique user ids and maybe their features.
        item_df (pd.DataFrame): Item data containing unique item ids and maybe their features.
        user_col (str): User id column name.
        item_col (str): Item id column name.
        user_item_filter_df (pd.DataFrame): User-item pairs to be used as a filter.
        shuffle (bool): If True, shuffles the result.

    Returns:
        pd.DataFrame: All pairs of user-item from user_df and item_df, excepting the pairs in user_item_filter_df
    """"""
    user_df['key'] = 1
    item_df['key'] = 1
    users_items = user_df.merge(item_df, on='key')
    user_df.drop('key', axis=1, inplace=True)
    item_df.drop('key', axis=1, inplace=True)
    users_items.drop('key', axis=1, inplace=True)
<mask>:
        users_items = filter_by(users_items, user_item_filter_df, [user_col, item_col])
    if shuffle:
        users_items = users_items.sample(frac=1).reset_index(drop=True)
    return users_items",user_item_filter_df is not None,118,user_item_filter_df is not None,True,100.00000000000004,N/A
"def fit(self, df, col_rating=DEFAULT_RATING_COL):
    """"""Fit the dataframe for libffm format. In there method does nothing but check the validity of 
        the input columns

        Args:
            df (pd.DataFrame): input Pandas dataframe.
            col_rating (str): rating of the data.

        Return:
            obj: the instance of the converter
        """"""
    types = df.dtypes
<mask>:
        raise TypeError('Input columns should be only object and/or numeric types.')
    if col_rating not in df.columns:
        raise TypeError('Column of {} is not in input dataframe columns'.format(col_rating))
    self.col_rating = col_rating
    self.field_names = list(df.drop(col_rating, axis=1).columns)
    return self","not all([x == object or np.issubdtype(x, np.integer) or x == np.float for x in types])",81,"not all((isinstance(types, pd.DataFrame) for types in types))",False,5.624749968810754,N/A
"def transform(self, df):
    """"""Tranform an input dataset with the same schema (column names and dtypes) to libffm format 
        by using the fitted converter.

        Args: 
            df (pd.DataFrame): input Pandas dataframe.

        Return:
            pd.DataFrame: output libffm format dataframe.
        """"""
<mask>:
        raise ValueError('Input dataset does not contain the label column {} in the fitting dataset'.format(self.col_rating))
    if not all([x in df.columns for x in self.field_names]):
        raise ValueError('Not all columns in the input dataset appear in the fitting dataset')
    idx = 1
    field_feature_dict = {}
    for field in self.field_names:
        if df[field].dtype == object:
            for feature in df[field].values:
                if (field, feature) not in field_feature_dict:
                    field_feature_dict[field, feature] = idx
                    idx += 1
    self.field_count = len(self.field_names)
    self.feature_count = idx - 1

    def _convert(field, feature, field_index, field_feature_index_dict):
        if isinstance(feature, str):
            field_feature_index = field_feature_index_dict[field, feature]
            feature = 1
        else:
            field_feature_index = field_index
        return '{}:{}:{}'.format(field_index, field_feature_index, feature)
    for col_index, col in enumerate(self.field_names):
        df[col] = df[col].apply(lambda x: _convert(col, x, col_index + 1, field_feature_dict))
    column_names = self.field_names[:]
    column_names.insert(0, self.col_rating)
    df = df[column_names]
    if self.filepath is not None:
        np.savetxt(self.filepath, df.values, delimiter=' ', fmt='%s')
    return df",not self.col_rating in df.columns,170,self.col_rating not in df.columns,False,67.56000774035174,N/A
"def has_columns(df, columns):
    """"""Check if DataFrame has necessary columns

    Args:
        df (pd.DataFrame): DataFrame
        columns (list(str): columns to check for

    Returns:
        bool: True if DataFrame has specified columns
    """"""
    result = True
    for column in columns:
<mask>:
            logger.error('Missing column: {} in DataFrame'.format(column))
            result = False
    return result",column not in df.columns,46,not df.has_column(column),False,11.339582221952005,N/A
"def has_same_base_dtype(df_1, df_2, columns=None):
    """"""Check if specified columns have the same base dtypes across both DataFrames

    Args:
        df_1 (pd.DataFrame): first DataFrame
        df_2 (pd.DataFrame): second DataFrame
        columns (list(str)): columns to check, None checks all columns

    Returns:
        bool: True if DataFrames columns have the same base dtypes
    """"""
<mask>:
        if any(set(df_1.columns).symmetric_difference(set(df_2.columns))):
            logger.error('Cannot test all columns because they are not all shared across DataFrames')
            return False
        columns = df_1.columns
    if not (has_columns(df=df_1, columns=columns) and has_columns(df=df_2, columns=columns)):
        return False
    result = True
    for column in columns:
        if df_1[column].dtype.type.__base__ != df_2[column].dtype.type.__base__:
            logger.error('Columns {} do not have the same base datatype'.format(column))
            result = False
    return result",columns is None,100,columns is None,True,100.00000000000004,N/A
"def update_to(self, b=1, bsize=1, tsize=None):
    """"""A progress bar showing how much is left to finish the opperation
        
        Args:
            b (int): Number of blocks transferred so far.
            bsize (int): Size of each block (in tqdm units).
            tsize (int): Total size (in tqdm units). 
        """"""
<mask>:
        self.total = tsize
    self.update(b * bsize - self.n)",tsize is not None,52,tsize is not None,True,100.00000000000004,N/A
"def maybe_download(url, filename=None, work_directory='.', expected_bytes=None):
    """"""Download a file if it is not already downloaded.
    
    Args:
        filename (str): File name.
        work_directory (str): Working directory.
        url (str): URL of the file to download.
        expected_bytes (int): Expected file size in bytes.

    Returns:
        str: File path of the file downloaded.
    """"""
<mask>:
        filename = url.split('/')[-1]
    filepath = os.path.join(work_directory, filename)
    if not os.path.exists(filepath):
        with TqdmUpTo(unit='B', unit_scale=True) as t:
            filepath, _ = urlretrieve(url, filepath, reporthook=t.update_to)
    else:
        log.debug('File {} already downloaded'.format(filepath))
    if expected_bytes is not None:
        statinfo = os.stat(filepath)
        if statinfo.st_size != expected_bytes:
            os.remove(filepath)
            raise IOError('Failed to verify {}'.format(filepath))
    return filepath",filename is None,94,filename is None,True,100.00000000000004,N/A
"@contextmanager
def download_path(path=None):
    """"""Return a path to download data. If `path=None`, then it yields a temporal path that is eventually deleted, 
    otherwise the real path of the input. 

    Args:
        path (str): Path to download data.

    Returns:
        str: Real path where the data is stored.

    Examples:
        >>> with download_path() as path:
        >>> ... maybe_download(url=""http://example.com/file.zip"", work_directory=path)

    """"""
<mask>:
        tmp_dir = TemporaryDirectory()
        try:
            yield tmp_dir.name
        finally:
            tmp_dir.cleanup()
    else:
        path = os.path.realpath(path)
        yield path",path is None,70,path is None,True,100.00000000000004,N/A
"def load_pandas_df(size='100k', header=DEFAULT_HEADER, local_cache_path=None, title_col=None, genres_col=None, year_col=None):
    """"""Loads the MovieLens dataset as pd.DataFrame.

    Download the dataset from http://files.grouplens.org/datasets/movielens, unzip, and load.
    To load movie information only, you can use load_item_df function. 

    Args:
        size (str): Size of the data to load. One of (""100k"", ""1m"", ""10m"", ""20m"").
        header (list or tuple or None): Rating dataset header.
        local_cache_path (str): Path (directory or a zip file) to cache the downloaded zip file.
            If None, all the intermediate files will be stored in a temporary directory and removed after use.
        title_col (str): Movie title column name. If None, the column will not be loaded.
        genres_col (str): Genres column name. Genres are '|' separated string.
            If None, the column will not be loaded.
        year_col (str): Movie release year column name. If None, the column will not be loaded.

    Returns:
        pd.DataFrame: Movie rating dataset.
        
    Examples:
        To load just user-id, item-id, and ratings from MovieLens-1M dataset,
        >>> df = load_pandas_df('1m', ('UserId', 'ItemId', 'Rating'))

        To load rating's timestamp together,
        >>> df = load_pandas_df('1m', ('UserId', 'ItemId', 'Rating', 'Timestamp'))

        To load movie's title, genres, and released year info along with the ratings data,
        >>> df = load_pandas_df('1m', ('UserId', 'ItemId', 'Rating', 'Timestamp'),
        ...     title_col='Title',
        ...     genres_col='Genres',
        ...     year_col='Year'
        ... )
    """"""
    size = size.lower()
<mask>:
        raise ValueError(ERROR_MOVIE_LENS_SIZE)
    if header is None or len(header) < 2:
        raise ValueError(ERROR_NO_HEADER)
    elif len(header) > 4:
        warnings.warn(WARNING_MOVIE_LENS_HEADER)
        header = header[:4]
    movie_col = header[1]
    with download_path(local_cache_path) as path:
        filepath = os.path.join(path, 'ml-{}.zip'.format(size))
        datapath, item_datapath = _maybe_download_and_extract(size, filepath)
        item_df = _load_item_df(size, item_datapath, movie_col, title_col, genres_col, year_col)
        df = pd.read_csv(datapath, sep=DATA_FORMAT[size].separator, engine='python', names=header, usecols=[*range(len(header))], header=0 if DATA_FORMAT[size].has_header else None)
        if len(header) > 2:
            df[header[2]] = df[header[2]].astype(float)
        if item_df is not None:
            df = df.merge(item_df, on=header[1])
    return df",size not in DATA_FORMAT,278,"size not in ['100k', '1m', '10m', '20m']",False,12.605968092174914,N/A
"def load_item_df(size='100k', local_cache_path=None, movie_col=DEFAULT_ITEM_COL, title_col=None, genres_col=None, year_col=None):
    """"""Loads Movie info.

    Args:
        size (str): Size of the data to load. One of (""100k"", ""1m"", ""10m"", ""20m"").
        local_cache_path (str): Path (directory or a zip file) to cache the downloaded zip file.
            If None, all the intermediate files will be stored in a temporary directory and removed after use.
        movie_col (str): Movie id column name.
        title_col (str): Movie title column name. If None, the column will not be loaded.
        genres_col (str): Genres column name. Genres are '|' separated string.
            If None, the column will not be loaded.
        year_col (str): Movie release year column name. If None, the column will not be loaded.

    Returns:
        pd.DataFrame: Movie information data, such as title, genres, and release year.
    """"""
    size = size.lower()
<mask>:
        raise ValueError(ERROR_MOVIE_LENS_SIZE)
    with download_path(local_cache_path) as path:
        filepath = os.path.join(path, 'ml-{}.zip'.format(size))
        _, item_datapath = _maybe_download_and_extract(size, filepath)
        item_df = _load_item_df(size, item_datapath, movie_col, title_col, genres_col, year_col)
    return item_df",size not in DATA_FORMAT,151,"size not in ['100k', '1m', '10m', '20m']",False,12.605968092174914,N/A
"def _load_item_df(size, item_datapath, movie_col, title_col, genres_col, year_col):
    """"""Loads Movie info""""""
<mask>:
        return None
    item_header = [movie_col]
    usecols = [0]
    if title_col is not None or year_col is not None:
        item_header.append('title_year')
        usecols.append(1)
    genres_header_100k = None
    if genres_col is not None:
        if size == '100k':
            genres_header_100k = [*(str(i) for i in range(19))]
            item_header.extend(genres_header_100k)
            usecols.extend([*range(5, 24)])
        else:
            item_header.append(genres_col)
            usecols.append(2)
    item_df = pd.read_csv(item_datapath, sep=DATA_FORMAT[size].item_separator, engine='python', names=item_header, usecols=usecols, header=0 if DATA_FORMAT[size].item_has_header else None, encoding='ISO-8859-1')
    if genres_header_100k is not None:
        item_df[genres_col] = item_df[genres_header_100k].values.tolist()
        item_df[genres_col] = item_df[genres_col].map(lambda l: '|'.join([GENRES[i] for i, v in enumerate(l) if v == 1]))
        item_df.drop(genres_header_100k, axis=1, inplace=True)
    if year_col is not None:

        def parse_year(t):
            parsed = re.split('[()]', t)
            if len(parsed) > 2 and parsed[-2].isdecimal():
                return parsed[-2]
            else:
                return None
        item_df[year_col] = item_df['title_year'].map(parse_year)
        if title_col is None:
            item_df.drop('title_year', axis=1, inplace=True)
    if title_col is not None:
        item_df.rename(columns={'title_year': title_col}, inplace=True)
    return item_df",title_col is None and genres_col is None and (year_col is None),136,size == 0,False,0.0,N/A
"def _get_schema(header, schema):
<mask>:
        if header is None or len(header) == 0:
            return None
        elif len(header) > 4:
            warnings.warn(WARNING_MOVIE_LENS_HEADER)
            header = header[:4]
        schema = StructType()
        try:
            schema.add(StructField(header[0], IntegerType())).add(StructField(header[1], IntegerType())).add(StructField(header[2], FloatType())).add(StructField(header[3], LongType()))
        except IndexError:
            pass
    else:
        if header is not None:
            warnings.warn(WARNING_HAVE_SCHEMA_AND_HEADER)
        if len(schema) > 4:
            warnings.warn(WARNING_MOVIE_LENS_HEADER)
            schema = schema[:4]
    return schema",schema is None or len(schema) == 0,51,schema is None,False,6.948345122280157,N/A
"def load_spark_df(spark, size='sample', header=DEFAULT_HEADER, local_cache_path=None, dbfs_datapath='dbfs:/FileStore/dac', dbutils=None):
    """"""Loads the Criteo DAC dataset as pySpark.DataFrame.

    The dataset consists of a portion of Criteo’s traffic over a period
    of 24 days. Each row corresponds to a display ad served by Criteo and the first
    column is indicates whether this ad has been clicked or not.

    There are 13 features taking integer values (mostly count features) and 26
    categorical features. The values of the categorical features have been hashed
    onto 32 bits for anonymization purposes.

    The schema is:
    <label> <integer feature 1> ... <integer feature 13> <categorical feature 1> ... <categorical feature 26>

    More details (need to accept user terms to see the information): 
    http://labs.criteo.com/2013/12/download-terabyte-click-logs/ 

    Args:
        spark (pySpark.SparkSession): Spark session.
        size (str): Dataset size. It can be ""sample"" or ""full"".
        local_cache_path (str): Path where to cache the tar.gz file locally.
        header (list): Dataset header names.
        dbfs_datapath (str): Where to store the extracted files on Databricks.
        dbutils (Databricks.dbutils): Databricks utility object.
  
    Returns:
        pySpark.DataFrame: Criteo DAC training dataset.
    """"""
    with download_path(local_cache_path) as path:
        filepath = download_criteo(size, path)
        filepath = extract_criteo(size, filepath)
<mask>:
            try:
                node_path = 'file:' + filepath
                dbutils.fs.cp(node_path, dbfs_datapath, recurse=True)
                path = dbfs_datapath
            except:
                raise ValueError('To use on a Databricks notebook, dbutils object should be passed as an argument')
        else:
            path = filepath
        schema = get_spark_schema(header)
        df = spark.read.csv(path, schema=schema, sep='\t', header=False)
        df.cache().count()
    return df",is_databricks(),221,dbutils,False,0.0,N/A
"def extract_criteo(size, compressed_file, path=None):
    """"""Extract Criteo dataset tar.

    Args:
        size (str): Size of criteo dataset. It can be ""full"" or ""sample"".
        compressed_file (str): Path to compressed file.
        path (str): Path to extract the file.
    
    Returns:
        str: Path to the extracted file.
    
    """"""
<mask>:
        folder = os.path.dirname(compressed_file)
        extracted_dir = os.path.join(folder, 'dac')
    else:
        extracted_dir = path
    with tarfile.open(compressed_file) as tar:
        tar.extractall(extracted_dir)
    filename_selector = {'sample': 'dac_sample.txt', 'full': 'train.txt'}
    return os.path.join(extracted_dir, filename_selector[size])",path is None,68,path is None,True,100.00000000000004,N/A
"def _gen_index(self):
    """"""
        Generate the user/item index:
            map_users, map_items: dictionaries mapping the original user/item index to matrix indices
            map_back_users, map_back_items: dictionaries to map back the matrix elements to the original
            dataframe indices

        Basic mechanics:
            As a first step we retieve the unique elements in the dataset. In this way we can take care
            of either completely missing rows (a user with no ratings) or completely missing columns
            (an item that has not being reviewed by anyone). The original indices in the dataframe are
            then mapped to an ordered, contiguous integer series to generate a compact matrix representation.

            Functions to map back to the original indices are also provided and can be saved in order to use
            a pretrained model.

        """"""
    self.df_ = self.df.sort_values(by=[self.col_user])
    unique_users = self.df_[self.col_user].unique()
    unique_items = self.df_[self.col_item].unique()
    self.Nusers = len(unique_users)
    self.Nitems = len(unique_items)
    self.map_users = {x: i for i, x in enumerate(unique_users)}
    self.map_items = {x: i for i, x in enumerate(unique_items)}
    self.map_back_users = {i: x for i, x in enumerate(unique_users)}
    self.map_back_items = {i: x for i, x in enumerate(unique_items)}
    self.df_.loc[:, 'hashedItems'] = self.df_[self.col_item].map(self.map_items)
    self.df_.loc[:, 'hashedUsers'] = self.df_[self.col_user].map(self.map_users)
<mask>:
        np.save(self.save_path + '/user_dict', self.map_users)
        np.save(self.save_path + '/item_dict', self.map_items)
        np.save(self.save_path + '/user_back_dict', self.map_back_users)
        np.save(self.save_path + '/item_back_dict', self.map_back_items)",self.save_path is not None,196,self.save_path,False,54.88116360940266,N/A
"def map_back_sparse(self, X, kind):
    """"""
        Map back the user/affinity matrix to a pd dataframe

        Args:
            X (np.array, int32): user/item affinity matrix
            kind (string): specify if the output values are ratings or predictions

        Returns:
            out_df (pandas dataframe): the generated pandas dataframe

        """"""
    m, n = X.shape
    items = [np.asanyarray(np.where(X[i, :] != 0)).flatten() for i in range(m)]
    ratings = [X[i, items[i]] for i in range(m)]
    userids = []
    for i in range(0, m):
        userids.extend([i] * len(items[i]))
    items = list(itertools.chain.from_iterable(items))
    ratings = list(itertools.chain.from_iterable(ratings))
<mask>:
        col_out = self.col_rating
    else:
        col_out = self.col_pred
    out_df = pd.DataFrame.from_dict({self.col_user: userids, self.col_item: items, col_out: ratings})
    out_df[self.col_user] = out_df[self.col_user].map(self.map_back_users)
    out_df[self.col_item] = out_df[self.col_item].map(self.map_back_items)
    return out_df",kind == 'ratings',104,kind == 'ratings',True,100.00000000000004,N/A
"def find_collection(client, dbid, id):
    """"""Find whether or not a CosmosDB collection exists.
    Args:
        client (obj): A pydocumentdb client object.
        dbid (str): Database ID.
        id (str): Collection ID.
    Returns:
        bool: True if the collection exists, False otherwise.
    """"""
    database_link = 'dbs/' + dbid
    collections = list(client.QueryCollections(database_link, {'query': 'SELECT * FROM r WHERE r.id=@id', 'parameters': [{'name': '@id', 'value': id}]}))
<mask>:
        return True
    else:
        return False",len(collections) > 0,63,collections,False,0.673794699908547,N/A
"def read_collection(client, dbid, id):
    """"""Read a CosmosDB collection.
    Args:
        client (obj): A pydocumentdb client object.
        dbid (str): Database ID.
        id (str): Collection ID.
    Returns:
        obj: A collection.
    """"""
    try:
        database_link = 'dbs/' + dbid
        collection_link = database_link + '/colls/{0}'.format(id)
        collection = client.ReadCollection(collection_link)
        return collection
    except errors.DocumentDBError as e:
<mask>:
            print(""A collection with id '{0}' does not exist"".format(id))
        else:
            raise errors.HTTPFailure(e.status_code)",e.status_code == 404,60,e.status_code == 404,True,100.00000000000004,N/A
"def read_database(client, id):
    """"""Read a CosmosDB database.
    Args:
        client (obj): A pydocumentdb client object.
        id (str): Database ID.
    Returns:
        obj: A database.
    """"""
    try:
        database_link = 'dbs/' + id
        database = client.ReadDatabase(database_link)
        return database
    except errors.DocumentDBError as e:
<mask>:
            print(""A database with id '{0}' does not exist"".format(id))
        else:
            raise errors.HTTPFailure(e.status_code)",e.status_code == 404,50,e.status_code == 404,True,100.00000000000004,N/A
"def find_database(client, id):
    """"""Find whether or not a CosmosDB database exists.
    Args:
        client (obj): A pydocumentdb client object.
        id (str): Database ID.
    Returns:
        bool: True if the database exists, False otherwise.
    """"""
    databases = list(client.QueryDatabases({'query': 'SELECT * FROM r WHERE r.id=@id', 'parameters': [{'name': '@id', 'value': id}]}))
<mask>:
        return True
    else:
        return False",len(databases) > 0,52,databases,False,0.673794699908547,N/A
"def spark_random_split(data, ratio=0.75, seed=42):
    """"""Spark random splitter
    Randomly split the data into several splits.

    Args:
        data (spark.DataFrame): Spark DataFrame to be split.
        ratio (float or list): Ratio for splitting data. If it is a single float number
            it splits data into two halfs and the ratio argument indicates the ratio of 
            training data set; if it is a list of float numbers, the splitter splits 
            data into several portions corresponding to the split ratios. If a list 
            is provided and the ratios are not summed to 1, they will be normalized.
        seed (int): Seed.

    Returns:
        list: Splits of the input data as spark.DataFrame.
    """"""
    multi_split, ratio = process_split_ratio(ratio)
<mask>:
        return data.randomSplit(ratio, seed=seed)
    else:
        return data.randomSplit([ratio, 1 - ratio], seed=seed)",multi_split,119,multi_split,True,100.00000000000004,N/A
"def spark_chrono_split(data, ratio=0.75, min_rating=1, filter_by='user', col_user=DEFAULT_USER_COL, col_item=DEFAULT_ITEM_COL, col_timestamp=DEFAULT_TIMESTAMP_COL):
    """"""Spark chronological splitter
    This function splits data in a chronological manner. That is, for each user / item, the
    split function takes proportions of ratings which is specified by the split ratio(s).
    The split is stratified.

    Args:
        data (spark.DataFrame): Spark DataFrame to be split.
        ratio (float or list): Ratio for splitting data. If it is a single float number
            it splits data into two sets and the ratio argument indicates the ratio of
            training data set; if it is a list of float numbers, the splitter splits 
            data into several portions corresponding to the split ratios. If a list is 
            provided and the ratios are not summed to 1, they will be normalized.
        seed (int): Seed.
        min_rating (int): minimum number of ratings for user or item.
        filter_by (str): either ""user"" or ""item"", depending on which of the two is to filter
            with min_rating.
        col_user (str): column name of user IDs.
        col_item (str): column name of item IDs.
        col_timestamp (str): column name of timestamps.

    Returns:
        list: Splits of the input data as spark.DataFrame.
    """"""
<mask>:
        raise ValueError(""filter_by should be either 'user' or 'item'."")
    if min_rating < 1:
        raise ValueError('min_rating should be integer and larger than or equal to 1.')
    multi_split, ratio = process_split_ratio(ratio)
    split_by_column = col_user if filter_by == 'user' else col_item
    if min_rating > 1:
        data = min_rating_filter_spark(data, min_rating=min_rating, filter_by=filter_by, col_user=col_user, col_item=col_item)
    ratio = ratio if multi_split else [ratio, 1 - ratio]
    ratio_index = np.cumsum(ratio)
    window_spec = Window.partitionBy(split_by_column).orderBy(col(col_timestamp))
    rating_grouped = data.groupBy(split_by_column).agg({col_timestamp: 'count'}).withColumnRenamed('count(' + col_timestamp + ')', 'count')
    rating_all = data.join(broadcast(rating_grouped), on=split_by_column)
    rating_rank = rating_all.withColumn('rank', row_number().over(window_spec) / col('count'))
    splits = []
    for i, _ in enumerate(ratio_index):
        if i == 0:
            rating_split = rating_rank.filter(col('rank') <= ratio_index[i])
        else:
            rating_split = rating_rank.filter((col('rank') <= ratio_index[i]) & (col('rank') > ratio_index[i - 1]))
        splits.append(rating_split)
    return splits",not (filter_by == 'user' or filter_by == 'item'),297,"filter_by not in ['user', 'item']",False,10.194207971045941,N/A
"def spark_timestamp_split(data, ratio=0.75, col_user=DEFAULT_USER_COL, col_item=DEFAULT_ITEM_COL, col_timestamp=DEFAULT_TIMESTAMP_COL):
    """"""Spark timestamp based splitter
    The splitter splits the data into sets by timestamps without stratification on either
    user or item.
    The ratios are applied on the timestamp column which is divided accordingly into
    several partitions.

    Args:
        data (spark.DataFrame): Spark DataFrame to be split.
        ratio (float or list): Ratio for splitting data. If it is a single float number
            it splits data into two sets and the ratio argument indicates the ratio of
            training data set; if it is a list of float numbers, the splitter splits
            data into several portions corresponding to the split ratios. If a list is
            provided and the ratios are not summed to 1, they will be normalized.
            Earlier indexed splits will have earlier times
            (e.g the latest time in split[0] <= the earliest time in split[1])
        col_user (str): column name of user IDs.
        col_item (str): column name of item IDs.
        col_timestamp (str): column name of timestamps. Float number represented in
        seconds since Epoch.

    Returns:
        list: Splits of the input data as spark.DataFrame.
    """"""
    multi_split, ratio = process_split_ratio(ratio)
    ratio = ratio if multi_split else [ratio, 1 - ratio]
    ratio_index = np.cumsum(ratio)
    window_spec = Window.orderBy(col(col_timestamp))
    rating = data.withColumn('rank', row_number().over(window_spec))
    data_count = rating.count()
    rating_rank = rating.withColumn('rank', row_number().over(window_spec) / data_count)
    splits = []
    for i, _ in enumerate(ratio_index):
<mask>:
            rating_split = rating_rank.filter(col('rank') <= ratio_index[i]).drop('rank')
        else:
            rating_split = rating_rank.filter((col('rank') <= ratio_index[i]) & (col('rank') > ratio_index[i - 1])).drop('rank')
        splits.append(rating_split)
    return splits",i == 0,236,i == 0,True,100.00000000000004,N/A
"@property
def interval(self):
<mask>:
        raise ValueError('Timer has not been stopped, please use stop().')
    else:
        return self._interval",self.running,16,self._interval is None,False,16.233395773754953,N/A
"def watch_memory():
    global previous_call_memory_usage, keep_watching, watching_memory, input_cells
    new_memory_usage = memory_profiler.memory_usage()[0]
    memory_delta = new_memory_usage - previous_call_memory_usage
    keep_watching = False
    total_memory = psutil.virtual_memory()[0] / 1024 / 1024
    time_delta_secs = time.time() - t1
    num_commands = len(input_cells) - 1
    cmd = 'In [{}]'.format(num_commands)
    output_template = '{cmd} used {memory_delta:0.4f} Mb RAM in {time_delta:0.2f}s, total RAM usage {memory_usage:0.2f} Mb, total RAM memory {total_memory:0.2f} Mb'
    output = output_template.format(time_delta=time_delta_secs, cmd=cmd, memory_delta=memory_delta, memory_usage=new_memory_usage, total_memory=total_memory)
<mask>:
        print(str(output))
    previous_call_memory_usage = new_memory_usage",watching_memory,70,not keep_watching,False,18.99589214128981,N/A
"def get_cuda_version(unix_path=DEFAULT_CUDA_PATH_LINUX):
    """"""Get CUDA version
    
    Args:
        unix_path (str): Path to CUDA version file in Linux/Mac.

    Returns:
        str: Version of the library.
    """"""
<mask>:
        raise NotImplementedError('Implement this!')
    elif sys.platform in ['linux', 'darwin']:
        if os.path.isfile(unix_path):
            with open(unix_path, 'r') as f:
                data = f.read().replace('\n', '')
            return data
        else:
            return 'No CUDA in this machine'
    else:
        raise ValueError('Not in Windows, Linux or Mac')",sys.platform == 'win32',59,sys.platform == 'win32',True,100.00000000000004,N/A
"def get_cudnn_version():
    """"""Get the CuDNN version
    
    Returns:
        str: Version of the library.

    """"""

    def find_cudnn_in_headers(candidates):
        for c in candidates:
            file = glob.glob(c)
<mask>:
                break
        if file:
            with open(file[0], 'r') as f:
                version = ''
                for line in f:
                    if '#define CUDNN_MAJOR' in line:
                        version = line.split()[-1]
                    if '#define CUDNN_MINOR' in line:
                        version += '.' + line.split()[-1]
                    if '#define CUDNN_PATCHLEVEL' in line:
                        version += '.' + line.split()[-1]
            if version:
                return version
            else:
                return 'Cannot find CUDNN version'
        else:
            return 'No CUDNN in this machine'
    if sys.platform == 'win32':
        candidates = ['C:\\NVIDIA\\cuda\\include\\cudnn.h', 'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v*\\include\\cudnn.h']
    elif sys.platform == 'linux':
        candidates = ['/usr/include/x86_64-linux-gnu/cudnn_v*.h', '/usr/local/cuda/include/cudnn.h', '/usr/include/cudnn.h']
    elif sys.platform == 'darwin':
        candidates = ['/usr/local/cuda/include/cudnn.h', '/usr/include/cudnn.h']
    else:
        raise ValueError('Not in Windows, Linux or Mac')
    return find_cudnn_in_headers(candidates)",file,122,file[0] == 'CUDNN',False,6.567274736060395,N/A
"def get_top_k_scored_items(scores, top_k, sort_top_k=False):
    """"""Extract top K items from a matrix of scores for each user-item pair, optionally sort results per user

    Args:
        scores (np.array): score matrix (users x items)
        top_k (int): number of top items to recommend
        sort_top_k (bool): flag to sort top k results

    Returns:
        np.array, np.array: indices into score matrix for each users top items, scores corresponding to top items
    """"""
<mask>:
        scores = scores.todense()
    if scores.shape[1] < top_k:
        logger.warning('Number of items is less than top_k, limiting top_k to number of items')
    k = min(top_k, scores.shape[1])
    test_user_idx = np.arange(scores.shape[0])[:, None]
    top_items = np.argpartition(scores, -k, axis=1)[:, -k:]
    top_scores = scores[test_user_idx, top_items]
    if sort_top_k:
        sort_ind = np.argsort(-top_scores)
        top_items = top_items[test_user_idx, sort_ind]
        top_scores = top_scores[test_user_idx, sort_ind]
    return (np.array(top_items), np.array(top_scores))","isinstance(scores, sparse.spmatrix)",119,"not isinstance(scores, np.ndarray)",False,33.03164318013809,N/A
"def start_or_get_spark(app_name='Sample', url='local[*]', memory='10G', packages=None, jars=None, repository=None):
    """"""Start Spark if not started

    Args:
        app_name (str): Set name of the application
        url (str): URL for spark master
        memory (str): Size of memory for spark driver
        packages (list): list of packages to install
        jars (list): list of jar files to add
        repository (str): The maven repository

    Returns:
        obj: Spark context.
    """"""
    submit_args = ''
<mask>:
        submit_args = '--packages {} '.format(','.join(packages))
    if jars is not None:
        submit_args += '--jars {} '.format(','.join(jars))
    if repository is not None:
        submit_args += '--repositories {}'.format(repository)
    if submit_args:
        os.environ['PYSPARK_SUBMIT_ARGS'] = '{} pyspark-shell'.format(submit_args)
    spark = SparkSession.builder.appName(app_name).master(url).config('spark.driver.memory', memory).getOrCreate()
    return spark",packages is not None,99,packages is not None,True,100.00000000000004,N/A
"def pandas_input_fn(df, y_col=None, batch_size=128, num_epochs=1, shuffle=False, num_threads=1):
    """"""Pandas input function for TensorFlow high-level API Estimator.

    tf.estimator.inputs.pandas_input_fn cannot handle array/list column properly.
    If the df does not include any array/list data column, one can simply use TensorFlow's pandas_input_fn.

    For more information, see (https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/numpy_input_fn)

    Args:
        df (pd.DataFrame): Data containing features
        y_col (str): Label column name if df has it.
        batch_size (int): Batch size for the input function
        num_epochs (int): Number of epochs to iterate over data. If None will run forever.
        shuffle (bool): If True, shuffles the data queue.
        num_threads (int): Number of threads used for reading and enqueueing.

    Returns:
        tf.estimator.inputs.numpy_input_fn: Function that has signature of ()->(dict of features, targets)
    """"""
    X_df = df.copy()
<mask>:
        y = X_df.pop(y_col).values
    else:
        y = None
    X = {}
    for col in X_df.columns:
        values = X_df[col].values
        if isinstance(values[0], (list, np.ndarray)):
            values = np.array([l for l in values], dtype=np.float32)
        X[col] = values
    input_fn = tf.estimator.inputs.numpy_input_fn(x=X, y=y, batch_size=batch_size, num_epochs=num_epochs, shuffle=shuffle, num_threads=num_threads)
    return input_fn",y_col is not None,155,y_col is not None,True,100.00000000000004,N/A
"def build_optimizer(name, lr=0.001, **kwargs):
    """"""Get an optimizer for TensorFlow high-level API Estimator.

    Args:
        name (str): Optimizer name. Note, to use 'Momentum', should specify
        lr (float): Learning rate.
        kwargs (dictionary): Optimizer arguments.

    Returns:
        tf.train.Optimizer
    """"""
<mask>:
        optimizer = tf.train.AdadeltaOptimizer(learning_rate=lr, **kwargs)
    elif name == 'Adagrad':
        optimizer = tf.train.AdagradOptimizer(learning_rate=lr, **kwargs)
    elif name == 'Adam':
        optimizer = tf.train.AdamOptimizer(learning_rate=lr, **kwargs)
    elif name == 'Ftrl':
        optimizer = tf.train.FtrlOptimizer(learning_rate=lr, **kwargs)
    elif name == 'Momentum':
        if 'momentum' in kwargs:
            optimizer = tf.train.MomentumOptimizer(learning_rate=lr, **kwargs)
        else:
            optimizer = tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.9, **kwargs)
    elif name == 'RMSProp':
        optimizer = tf.train.RMSPropOptimizer(learning_rate=lr, **kwargs)
    elif name == 'SGD':
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr, **kwargs)
    else:
        raise ValueError(""Optimizer name should be either 'Adadelta', 'Adagrad', 'Adam',\n            'Ftrl', 'Momentum', 'RMSProp', or 'SGD'"")
    return optimizer",name == 'Adadelta',114,name == 'Adadelta',True,100.00000000000004,N/A
"def begin(self):
<mask>:
        self.summary_writer = tf.summary.FileWriterCache.get(self.model_dir)
        self.global_step_tensor = tf.train.get_or_create_global_step()
    else:
        self.step = 0",self.model_dir is not None,13,self.model_dir is not None,True,100.00000000000004,N/A
"def before_run(self, run_context):
<mask>:
        requests = {'global_step': self.global_step_tensor}
        return tf.train.SessionRunArgs(requests)
    else:
        return None",self.global_step_tensor is not None,13,run_context.get('requests'),False,5.11459870708889,N/A
"def is_jupyter():
    """"""Check if the module is running on Jupyter notebook/console

    Returns:
        bool: True if the module is running on Jupyter notebook or Jupyter console,
        False otherwise.
    """"""
    try:
        shell_name = get_ipython().__class__.__name__
<mask>:
            return True
        else:
            return False
    except NameError:
        return False",shell_name == 'ZMQInteractiveShell',42,shell_name.startswith('jupyter'),False,20.556680845025987,N/A
"def is_databricks():
    """"""Check if the module is running on Databricks

    Returns:
        bool: True if the module is running on Databricks notebook,
        False otherwise.
    """"""
    try:
<mask>:
            return True
        else:
            return False
    except NameError:
        return False",os.path.realpath('.') == '/databricks/driver',35,sys.platform.startswith('win') and sys.platform.startswith('cygwin'),False,3.4197980307804725,N/A
"def __init__(self, remove_seen=True, col_user=constants.DEFAULT_USER_COL, col_item=constants.DEFAULT_ITEM_COL, col_rating=constants.DEFAULT_RATING_COL, col_timestamp=constants.DEFAULT_TIMESTAMP_COL, col_prediction=constants.PREDICTION_COL, similarity_type=sar.SIM_JACCARD, time_decay_coefficient=sar.TIME_DECAY_COEFFICIENT, time_now=sar.TIME_NOW, timedecay_formula=sar.TIMEDECAY_FORMULA, threshold=sar.THRESHOLD):
    """"""Initialize model parameters

        Args:
            col_user (str): user column name
            col_item (str): item column name
            col_rating (str): rating column name
            col_timestamp (str): timestamp column name
            col_prediction (str): prediction column name
            similarity_type (str): [None, 'jaccard', 'lift'] option for computing item-item similarity
            time_decay_coefficient (float): number of days till ratings are decayed by 1/2
            time_now (int | None): current time for time decay calculation
            timedecay_formula (bool): flag to apply time decay
            threshold (int): item-item co-occurrences below this threshold will be removed
        """"""
    self.col_rating = col_rating
    self.col_item = col_item
    self.col_user = col_user
    self.col_timestamp = col_timestamp
    self.col_prediction = col_prediction
    self.remove_seen = remove_seen
    self.similarity_type = similarity_type
    self.time_decay_half_life = time_decay_coefficient * 24 * 60 * 60
    self.time_decay_flag = timedecay_formula
    self.time_now = time_now
    self.threshold = threshold
    self.model_str = 'sar_ref'
    self.user_affinity = None
    self.item_similarity = None
    self.item_frequencies = None
<mask>:
        raise ValueError('Threshold cannot be < 1')
    self.col_item_id = sar.INDEXED_ITEMS
    self.col_user_id = sar.INDEXED_USERS
    self.n_users = None
    self.n_items = None
    self.user2index = None
    self.item2index = None
    self.index2item = None
    self.seen_items = None",self.threshold <= 0,173,threshold < 1,False,20.24518585186855,N/A
"def fit(self, df):
    """"""Main fit method for SAR

        Args:
            df (pd.DataFrame): User item rating dataframe
        """"""
<mask>:
        self.set_index(df)
    logger.info('Collecting user affinity matrix')
    if not np.issubdtype(df[self.col_rating].dtype, np.number):
        raise TypeError('Rating column data type must be numeric')
    temp_df = df[[self.col_user, self.col_item, self.col_rating]].copy()
    if self.time_decay_flag:
        logger.info('Calculating time-decayed affinities')
        if not self.time_now:
            self.time_now = df[self.col_timestamp].max()
        temp_df[self.col_rating] *= exponential_decay(value=df[self.col_timestamp], max_val=self.time_now, half_life=self.time_decay_half_life)
        temp_df = temp_df.groupby([self.col_user, self.col_item]).sum().reset_index()
    else:
        logger.info('De-duplicating the user-item counts')
        temp_df = temp_df.drop_duplicates([self.col_user, self.col_item], keep='last')
    logger.info('Creating index columns')
    temp_df.loc[:, self.col_item_id] = temp_df[self.col_item].map(self.item2index)
    temp_df.loc[:, self.col_user_id] = temp_df[self.col_user].map(self.user2index)
    self.seen_items = temp_df[[self.col_user_id, self.col_item_id]].values
    logger.info('Building user affinity sparse matrix')
    self.user_affinity = self.compute_affinity_matrix(temp_df, self.n_users, self.n_items)
    logger.info('Calculating item co-occurrence')
    item_cooccurrence = self.compute_coocurrence_matrix(temp_df, self.n_users, self.n_items)
    del temp_df
    self.item_frequencies = item_cooccurrence.diagonal()
    logger.info('Calculating item similarity')
    if self.similarity_type == sar.SIM_COOCCUR:
        self.item_similarity = item_cooccurrence
    elif self.similarity_type == sar.SIM_JACCARD:
        logger.info('Calculating jaccard')
        self.item_similarity = jaccard(item_cooccurrence).astype(df[self.col_rating].dtype)
    elif self.similarity_type == sar.SIM_LIFT:
        logger.info('Calculating lift')
        self.item_similarity = lift(item_cooccurrence).astype(df[self.col_rating].dtype)
    else:
        raise ValueError('Unknown similarity type: {0}'.format(self.similarity_type))
    del item_cooccurrence
    logger.info('Done training')",self.index2item is None,145,not self.initialized,False,24.880469496253564,N/A
"def score(self, test, remove_seen=False):
    """"""Score all items for test users

        Args:
            test (pd.DataFrame): user to test
            remove_seen (bool): flag to remove items seen in training from recommendation

        Returns:
            np.ndarray
        """"""
    user_ids = test[self.col_user].drop_duplicates().map(self.user2index).values
<mask>:
        raise ValueError('SAR cannot score users that are not in the training set')
    logger.info('Calculating recommendation scores')
    test_scores = self.user_affinity.dot(self.item_similarity)
    if remove_seen:
        logger.info('Removing seen items')
        test_scores[self.seen_items[:, 0], self.seen_items[:, 1]] = -np.inf
    test_scores = test_scores[user_ids, :]
    if isinstance(test_scores, sparse.spmatrix):
        test_scores = test_scores.toarray()
    return test_scores",any(np.isnan(user_ids)),75,user_ids == self.user_affinity,False,15.187207110382285,N/A
"def get_item_based_topk(self, items, top_k=10, sort_top_k=False):
    """"""Get top K similar items to provided seed items based on similarity metric defined.
        This method will take a set of items and use them to recommend the most similar items to that set
        based on the similarity matrix fit during training.
        This allows recommendations for cold-users (unseen during training), note - the model is not updated.

        The following options are possible based on information provided in the items input:
        1. Single user or seed of items: only item column (ratings are assumed to be 1)
        2. Single user or seed of items w/ ratings: item column and rating column
        3. Separate users or seeds of items: item and user column (user ids are only used to separate item sets)
        4. Separate users or seeds of items with ratings: item, user and rating columns provided

        Args:
            items (pd.DataFrame): DataFrame with item, user (optional), and rating (optional) columns
            top_k (int): number of top items to recommend
            sort_top_k (bool): flag to sort top k results

        Returns:
            pd.DataFrame: sorted top k recommendation items
        """"""
    item_ids = items[self.col_item].map(self.item2index)
<mask>:
        ratings = items[self.col_rating]
    else:
        ratings = pd.Series(np.ones_like(item_ids))
    if self.col_user in items.columns:
        test_users = items[self.col_user]
        user2index = {x[1]: x[0] for x in enumerate(items[self.col_user].unique())}
        user_ids = test_users.map(user2index)
    else:
        test_users = pd.Series(np.zeros_like(item_ids))
        user_ids = test_users
    n_users = user_ids.drop_duplicates().shape[0]
    pseudo_affinity = sparse.coo_matrix((ratings, (user_ids, item_ids)), shape=(n_users, self.n_items)).tocsr()
    test_scores = pseudo_affinity.dot(self.item_similarity)
    test_scores[user_ids, item_ids] = -np.inf
    top_items, top_scores = get_top_k_scored_items(scores=test_scores, top_k=top_k, sort_top_k=sort_top_k)
    df = pd.DataFrame({self.col_user: np.repeat(test_users.drop_duplicates().values, top_items.shape[1]), self.col_item: [self.index2item[item] for item in top_items.flatten()], self.col_prediction: top_scores.flatten()})
    return df.replace(-np.inf, np.nan).dropna()",self.col_rating in items.columns,251,self.col_rating in items.columns,True,100.00000000000004,N/A
"def predict(self, test):
    """"""Output SAR scores for only the users-items pairs which are in the test set
        Args:
            test (pd.DataFrame): DataFrame that contains users and items to test

        Returns:
            pd.DataFrame: DataFrame contains the prediction results
        """"""
    test_scores = self.score(test)
    user_ids = test[self.col_user].map(self.user2index).values
    item_ids = test[self.col_item].map(self.item2index).values
    nans = np.isnan(item_ids)
<mask>:
        logger.warning('Items found in test not seen during training, new items will have score of 0')
        test_scores = np.append(test_scores, np.zeros((self.n_users, 1)), axis=1)
        item_ids[nans] = self.n_items
        item_ids = item_ids.astype('int64')
    df = pd.DataFrame({self.col_user: test[self.col_user].values, self.col_item: test[self.col_item].values, self.col_prediction: test_scores[user_ids, item_ids]})
    return df",any(nans),87,nans != 0,False,15.97357760615681,N/A
"def __init__(self, rating_true, rating_pred, col_user=DEFAULT_USER_COL, col_item=DEFAULT_ITEM_COL, col_rating=DEFAULT_RATING_COL, col_prediction=PREDICTION_COL):
    """"""Initializer.
        This is the Spark version of rating metrics evaluator.
        The methods of this class, calculate rating metrics such as root mean squared error, mean absolute error,
        R squared, and explained variance.

        Args:
            rating_true (spark.DataFrame): True labels.
            rating_pred (spark.DataFrame): Predicted labels.
            col_user (str): column name for user.
            col_item (str): column name for item.
            col_rating (str): column name for rating.
            col_prediction (str): column name for prediction.
        """"""
    self.rating_true = rating_true
    self.rating_pred = rating_pred
    self.col_user = col_user
    self.col_item = col_item
    self.col_rating = col_rating
    self.col_prediction = col_prediction
<mask>:
        raise TypeError('rating_true should be but is not a Spark DataFrame')
    if not isinstance(self.rating_pred, DataFrame):
        raise TypeError('rating_pred should be but is not a Spark DataFrame')
    true_columns = self.rating_true.columns
    pred_columns = self.rating_pred.columns
    if rating_true.count() == 0:
        raise ValueError('Empty input dataframe')
    if rating_pred.count() == 0:
        raise ValueError('Empty input dataframe')
    if self.col_user not in true_columns:
        raise ValueError('Schema of rating_true not valid. Missing User Col')
    if self.col_item not in true_columns:
        raise ValueError('Schema of rating_true not valid. Missing Item Col')
    if self.col_rating not in true_columns:
        raise ValueError('Schema of rating_true not valid. Missing Rating Col')
    if self.col_user not in pred_columns:
        raise ValueError('Schema of rating_pred not valid. Missing User Col')
    if self.col_item not in pred_columns:
        raise ValueError('Schema of rating_pred not valid. Missing Item Col')
    if self.col_prediction not in pred_columns:
        raise ValueError('Schema of rating_pred not valid. Missing Prediction Col')
    self.rating_true = self.rating_true.select(col(self.col_user).cast('double'), col(self.col_item).cast('double'), col(self.col_rating).cast('double').alias('label'))
    self.rating_pred = self.rating_pred.select(col(self.col_user).cast('double'), col(self.col_item).cast('double'), col(self.col_prediction).cast('double').alias('prediction'))
    self.y_pred_true = self.rating_true.join(self.rating_pred, [self.col_user, self.col_item], 'inner').drop(self.col_user).drop(self.col_item)
    self.metrics = RegressionMetrics(self.y_pred_true.rdd.map(lambda x: (x.prediction, x.label)))","not isinstance(self.rating_true, DataFrame)",245,"not isinstance(rating_true, DataFrame)",False,59.86908497649472,N/A
"def check_column_dtypes(func):
    """"""Checks columns of DataFrame inputs

    This includes the checks on 
        1. whether the input columns exist in the input DataFrames
        2. whether the data types of col_user as well as col_item are matched in the two input DataFrames.

    Args:
        func (function): function that will be wrapped
    """"""

    @wraps(func)
    def check_column_dtypes_wrapper(rating_true, rating_pred, col_user=DEFAULT_USER_COL, col_item=DEFAULT_ITEM_COL, col_rating=DEFAULT_RATING_COL, col_prediction=PREDICTION_COL, *args, **kwargs):
        """"""Check columns of DataFrame inputs

        Args:
            rating_true (pd.DataFrame): True data
            rating_pred (pd.DataFrame): Predicted data
            col_user (str): column name for user
            col_item (str): column name for item
            col_rating (str): column name for rating
            col_prediction (str): column name for prediction
        """"""
<mask>:
            raise ValueError('Missing columns in true rating DataFrame')
        if not has_columns(rating_pred, [col_user, col_item, col_prediction]):
            raise ValueError('Missing columns in predicted rating DataFrame')
        if not has_same_base_dtype(rating_true, rating_pred, columns=[col_user, col_item]):
            raise ValueError('Columns in provided DataFrames are not the same datatype')
        return func(*args, rating_true=rating_true, rating_pred=rating_pred, col_user=col_user, col_item=col_item, col_rating=col_rating, col_prediction=col_prediction, **kwargs)
    return check_column_dtypes_wrapper","not has_columns(rating_true, [col_user, col_item, col_rating])",146,"not has_columns(rating_true, [col_user, col_item, col_rating])",True,100.00000000000004,N/A
"@check_column_dtypes
@lru_cache_df(maxsize=1)
def merge_rating_true_pred(rating_true, rating_pred, col_user=DEFAULT_USER_COL, col_item=DEFAULT_ITEM_COL, col_rating=DEFAULT_RATING_COL, col_prediction=PREDICTION_COL):
    """"""Join truth and prediction data frames on userID and itemID and return the true
    and predicted rated with the correct index.
    
    Args:
        rating_true (pd.DataFrame): True data
        rating_pred (pd.DataFrame): Predicted data
        col_user (str): column name for user
        col_item (str): column name for item
        col_rating (str): column name for rating
        col_prediction (str): column name for prediction

    Returns:
        np.array: Array with the true ratings
        np.array: Array with the predicted ratings

    """"""
    suffixes = ['_true', '_pred']
    rating_true_pred = pd.merge(rating_true, rating_pred, on=[col_user, col_item], suffixes=suffixes)
<mask>:
        col_rating = col_rating + suffixes[0]
    if col_prediction in rating_true.columns:
        col_prediction = col_prediction + suffixes[1]
    return (rating_true_pred[col_rating], rating_true_pred[col_prediction])",col_rating in rating_pred.columns,106,col_rating in rating_true.columns,False,66.06328636027612,N/A
"@check_column_dtypes
@lru_cache_df(maxsize=1)
def merge_ranking_true_pred(rating_true, rating_pred, col_user, col_item, col_rating, col_prediction, relevancy_method, k=DEFAULT_K, threshold=DEFAULT_THRESHOLD):
    """"""Filter truth and prediction data frames on common users

    Args:
        rating_true (pd.DataFrame): True DataFrame
        rating_pred (pd.DataFrame): Predicted DataFrame
        col_user (str): column name for user
        col_item (str): column name for item
        col_rating (str): column name for rating
        col_prediction (str): column name for prediction
        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold']
        k (int): number of top k items per user (optional)
        threshold (float): threshold of top items per user (optional)

    Returns:
        pd.DataFrame, pd.DataFrame, int:
            DataFrame of recommendation hits
            DataFrmae of hit counts vs actual relevant items per user
            number of unique user ids
    """"""
    common_users = set(rating_true[col_user]).intersection(set(rating_pred[col_user]))
    rating_true_common = rating_true[rating_true[col_user].isin(common_users)]
    rating_pred_common = rating_pred[rating_pred[col_user].isin(common_users)]
    n_users = len(common_users)
<mask>:
        top_k = k
    elif relevancy_method == 'by_threshold':
        top_k = threshold
    else:
        raise NotImplementedError('Invalid relevancy_method')
    df_hit = get_top_k_items(dataframe=rating_pred_common, col_user=col_user, col_rating=col_prediction, k=top_k)
    df_hit['rank'] = df_hit.groupby(col_user)[col_prediction].rank(method='first', ascending=False)
    df_hit = pd.merge(df_hit, rating_true_common, on=[col_user, col_item])[[col_user, col_item, 'rank']]
    df_hit_count = pd.merge(df_hit.groupby(col_user, as_index=False)[col_user].agg({'hit': 'count'}), rating_true_common.groupby(col_user, as_index=False)[col_user].agg({'actual': 'count'}), on=col_user)
    return (df_hit, df_hit_count, n_users)",relevancy_method == 'top_k',163,relevancy_method == 'top_k',True,100.00000000000004,N/A
"def precision_at_k(rating_true, rating_pred, col_user=DEFAULT_USER_COL, col_item=DEFAULT_ITEM_COL, col_rating=DEFAULT_RATING_COL, col_prediction=PREDICTION_COL, relevancy_method='top_k', k=DEFAULT_K, threshold=DEFAULT_THRESHOLD):
    """"""Precision at K.

    Note:
    We use the same formula to calculate precision@k as that in Spark.
    More details can be found at
    http://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.precisionAt
    In particular, the maximum achievable precision may be < 1, if the number of items for a
    user in rating_pred is less than k.

    Args:
        rating_true (pd.DataFrame): True DataFrame
        rating_pred (pd.DataFrame): Predicted DataFrame
        col_user (str): column name for user
        col_item (str): column name for item
        col_rating (str): column name for rating
        col_prediction (str): column name for prediction
        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold']
        k (int): number of top k items per user
        threshold (float): threshold of top items per user (optional)

    Returns:
        float: precision at k (min=0, max=1)
    """"""
    df_hit, df_hit_count, n_users = merge_ranking_true_pred(rating_true=rating_true, rating_pred=rating_pred, col_user=col_user, col_item=col_item, col_rating=col_rating, col_prediction=col_prediction, relevancy_method=relevancy_method, k=k, threshold=threshold)
<mask>:
        return 0.0
    return (df_hit_count['hit'] / k).sum() / n_users",df_hit.shape[0] == 0,146,n_users == 0,False,14.110009442520557,N/A
"def recall_at_k(rating_true, rating_pred, col_user=DEFAULT_USER_COL, col_item=DEFAULT_ITEM_COL, col_rating=DEFAULT_RATING_COL, col_prediction=PREDICTION_COL, relevancy_method='top_k', k=DEFAULT_K, threshold=DEFAULT_THRESHOLD):
    """"""Recall at K.

    Args:
        rating_true (pd.DataFrame): True DataFrame
        rating_pred (pd.DataFrame): Predicted DataFrame
        col_user (str): column name for user
        col_item (str): column name for item
        col_rating (str): column name for rating
        col_prediction (str): column name for prediction
        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold']
        k (int): number of top k items per user
        threshold (float): threshold of top items per user (optional)

    Returns:
        float: recall at k (min=0, max=1). The maximum value is 1 even when fewer than 
            k items exist for a user in rating_true.
    """"""
    df_hit, df_hit_count, n_users = merge_ranking_true_pred(rating_true=rating_true, rating_pred=rating_pred, col_user=col_user, col_item=col_item, col_rating=col_rating, col_prediction=col_prediction, relevancy_method=relevancy_method, k=k, threshold=threshold)
<mask>:
        return 0.0
    return (df_hit_count['hit'] / df_hit_count['actual']).sum() / n_users",df_hit.shape[0] == 0,119,n_users == 0,False,14.110009442520557,N/A
"def generate_param_grid(params):
    """"""Generator of parameter grids
    Generate parameter lists from a paramater dictionary in the form of
    {
        ""param1"": [value1, value2],
        ""param2"": [value1, value2]
    }

    to

    [
        {""param1"": value1, ""param2"": value1},
        {""param1"": value2, ""param2"": value1},
        {""param1"": value1, ""param2"": value2},
        {""param1"": value2, ""param2"": value2}
    ]

    Args:
        param_dict (dict): dictionary of parameters and values (in a list).

    Return:
        list: A list of parameter dictionary string that can be fed directly into
        model builder as keyword arguments.
    """"""
    param_new = {}
    param_fixed = {}
    for key, value in params.items():
<mask>:
            param_new[key] = value
        else:
            param_fixed[key] = value
    items = sorted(param_new.items())
    keys, values = zip(*items)
    params_exp = []
    for v in product(*values):
        param_exp = dict(zip(keys, v))
        param_exp.update(param_fixed)
        params_exp.append(param_exp)
    return params_exp","isinstance(value, list)",116,key not in param_fixed,False,0.0,N/A
"def _log(metric, value):
    """"""AzureML log wrapper.

    Record list of int or float as a list metrics so that we can plot it from AzureML workspace portal.
    Otherwise, record as a single value of the metric.
    """"""
<mask>:
        if isinstance(value, list) and len(value) > 0 and isinstance(value[0], (int, float)):
            run.log_list(metric, value)
        else:
            run.log(metric, str(value))
    print(metric, '=', value)",run is not None,56,value is not None,False,59.460355750136046,N/A
"def svd_training(args):
    """"""
    Train Surprise SVD using the given hyper-parameters
    """"""
    print('Start training...')
    train_data = pd.read_pickle(path=os.path.join(args.datastore, args.train_datapath))
    validation_data = pd.read_pickle(path=os.path.join(args.datastore, args.validation_datapath))
    svd = surprise.SVD(random_state=args.random_state, n_epochs=args.epochs, verbose=args.verbose, biased=args.biased, n_factors=args.n_factors, init_mean=args.init_mean, init_std_dev=args.init_std_dev, lr_all=args.lr_all, reg_all=args.reg_all, lr_bu=args.lr_bu, lr_bi=args.lr_bi, lr_pu=args.lr_pu, lr_qi=args.lr_qi, reg_bu=args.reg_bu, reg_bi=args.reg_bi, reg_pu=args.reg_pu, reg_qi=args.reg_qi)
    train_set = surprise.Dataset.load_from_df(train_data, reader=surprise.Reader(args.surprise_reader)).build_full_trainset()
    svd.fit(train_set)
    print('Evaluating...')
    rating_metrics = args.rating_metrics
<mask>:
        predictions = compute_rating_predictions(svd, validation_data, usercol=args.usercol, itemcol=args.itemcol)
        for metric in rating_metrics:
            result = eval(metric)(validation_data, predictions)
            print(metric, result)
            if HAS_AML:
                run.log(metric, result)
    ranking_metrics = args.ranking_metrics
    if len(ranking_metrics) > 0:
        all_predictions = compute_ranking_predictions(svd, train_data, usercol=args.usercol, itemcol=args.itemcol, recommend_seen=args.recommend_seen)
        k = args.k
        for metric in ranking_metrics:
            result = eval(metric)(validation_data, all_predictions, col_prediction='prediction', k=k)
            print('{}@{}'.format(metric, k), result)
            if HAS_AML:
                run.log(metric, result)
    if len(ranking_metrics) == 0 and len(rating_metrics) == 0:
        raise ValueError('No metrics were specified.')
    return svd",len(rating_metrics) > 0,119,len(rating_metrics) > 0,True,100.00000000000004,N/A
"def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--datastore', type=str, dest='datastore', help='Datastore path')
    parser.add_argument('--train-datapath', type=str, dest='train_datapath')
    parser.add_argument('--validation-datapath', type=str, dest='validation_datapath')
    parser.add_argument('--output_dir', type=str, help='output directory')
    parser.add_argument('--surprise-reader', type=str, dest='surprise_reader')
    parser.add_argument('--usercol', type=str, dest='usercol', default='userID')
    parser.add_argument('--itemcol', type=str, dest='itemcol', default='itemID')
    parser.add_argument('--rating-metrics', type=str, nargs='*', dest='rating_metrics', default=[])
    parser.add_argument('--ranking-metrics', type=str, nargs='*', dest='ranking_metrics', default=[])
    parser.add_argument('--k', type=int, dest='k', default=None)
    parser.add_argument('--recommend-seen', dest='recommend_seen', action='store_true')
    parser.add_argument('--random-state', type=int, dest='random_state', default=0)
    parser.add_argument('--verbose', dest='verbose', action='store_true')
    parser.add_argument('--epochs', type=int, dest='epochs', default=30)
    parser.add_argument('--biased', dest='biased', action='store_true')
    parser.add_argument('--n_factors', type=int, dest='n_factors', default=100)
    parser.add_argument('--init_mean', type=float, dest='init_mean', default=0.0)
    parser.add_argument('--init_std_dev', type=float, dest='init_std_dev', default=0.1)
    parser.add_argument('--lr_all', type=float, dest='lr_all', default=0.005)
    parser.add_argument('--reg_all', type=float, dest='reg_all', default=0.02)
    parser.add_argument('--lr_bu', type=float, dest='lr_bu', default=None)
    parser.add_argument('--lr_bi', type=float, dest='lr_bi', default=None)
    parser.add_argument('--lr_pu', type=float, dest='lr_pu', default=None)
    parser.add_argument('--lr_qi', type=float, dest='lr_qi', default=None)
    parser.add_argument('--reg_bu', type=float, dest='reg_bu', default=None)
    parser.add_argument('--reg_bi', type=float, dest='reg_bi', default=None)
    parser.add_argument('--reg_pu', type=float, dest='reg_pu', default=None)
    parser.add_argument('--reg_qi', type=float, dest='reg_qi', default=None)
    args = parser.parse_args()
    print('Args:', str(vars(args)), sep='\n')
<mask>:
        run.log('Number of epochs', args.epochs)
    svd = svd_training(args)
    os.makedirs(args.output_dir, exist_ok=True)
    surprise.dump.dump(os.path.join(args.output_dir, 'model.dump'), algo=svd)",HAS_AML,133,not args.datastore,False,0.0,N/A
"def get_or_create_workspace(config_path=None, subscription_id=None, resource_group=None, workspace_name=None, workspace_region=None):
    """"""Get or create AzureML Workspace this will save the config to the path specified for later use

    Args:
        config_path (str): optional directory to look for / store config.json file (defaults to current directory)
        subscription_id (str): subscription id
        resource_group (str): resource group
        workspace_name (str): workspace name
        workspace_region (str): region

    Returns:
        Workspace
    """"""
<mask>:
        subscription_id = os.getenv('SUBSCRIPTION_ID')
    if resource_group is None:
        resource_group = os.getenv('RESOURCE_GROUP')
    if workspace_name is None:
        workspace_name = os.getenv('WORKSPACE_NAME')
    if workspace_region is None:
        workspace_region = os.getenv('WORKSPACE_REGION')
    options = [(Workspace, dict(subscription_id=subscription_id, resource_group=resource_group, workspace_name=workspace_name)), (Workspace.from_config, dict(path=config_path)), (Workspace.create, dict(subscription_id=subscription_id, resource_group=resource_group, name=workspace_name, location=workspace_region, create_resource_group=True, exist_ok=True))]
    for function, kwargs in options:
        try:
            ws = function(**kwargs)
            break
        except Exception:
            continue
    else:
        raise ValueError('Failed to get or create AzureML Workspace with the configuration information provided')
    ws.write_config(path=config_path)
    return ws",subscription_id is None,127,subscription_id is None,True,100.00000000000004,N/A
"@classmethod
def stream_sync(cls, expression, options=None, on_start=None, on_error=None, on_version=None, on_history=None, on_set=None):
<mask>:
        on_error = _on_unhandled_error
    return cls.client.stream(expression, options, on_start, on_error, on_version, on_history, on_set)",on_error is None,22,on_error is None,True,100.00000000000004,N/A
"def test_logging(self):
    logged = self.get_logged(lambda client: client.ping())
    read_line = StringIO(logged).readline
    self.assertEqual(read_line(), 'Fauna GET /ping\n')
    self.assertEqual(read_line(), '  Response headers: {\n')
    while True:
        line = read_line()
<mask>:
            self.assertEqual(line, '  }\n')
            break
    self.assertEqual(read_line(), '  Response JSON: {\n')
    self.assertEqual(read_line(), '    ""resource"": ""Scope write is OK""\n')
    self.assertEqual(read_line(), '  }\n')
    self.assertRegexCompat(read_line(), '^  Response \\(200\\): Network latency \\d+ms\\n$')",not line.startswith('    '),51,line.startswith('Response headers:'),False,35.49481056010054,N/A
"@classmethod
def setUpClass(cls):
    super(FaunaTestCase, cls).setUpClass()
    getLogger('requests').setLevel(WARNING)
    cls.root_client = cls._get_client()
    rnd = ''.join((random.choice(string.ascii_lowercase) for _ in range(10)))
    cls.db_name = 'faunadb-python-test' + rnd
    cls.db_ref = query.database(cls.db_name)
<mask>:
        cls.root_client.query(query.delete(cls.db_ref))
    cls.root_client.query(query.create_database({'name': cls.db_name}))
    cls.server_key = cls.root_client.query(query.create_key({'database': cls.db_ref, 'role': 'server'}))['secret']
    cls.client = cls.root_client.new_session_client(secret=cls.server_key)
    cls.admin_key = cls.root_client.query(query.create_key({'database': cls.db_ref, 'role': 'admin'}))['secret']
    cls.admin_client = cls.root_client.new_session_client(secret=cls.admin_key)",cls.root_client.query(query.exists(cls.db_ref)),46,query.delete(cls.db_ref),False,28.748816342728507,N/A
"def _parse_json_hook(dct):
    """"""
    Looks for FaunaDB types in a JSON object and converts to them if possible.
    """"""
<mask>:
        ref = dct['@ref']
        if not 'collection' in ref and (not 'database' in ref):
            return Native.from_name(ref['id'])
        return Ref(ref['id'], ref.get('collection'), ref.get('database'))
    if '@obj' in dct:
        return dct['@obj']
    if '@set' in dct:
        return SetRef(dct['@set'])
    if '@query' in dct:
        return Query(dct['@query'])
    if '@ts' in dct:
        return FaunaTime(dct['@ts'])
    if '@date' in dct:
        return parse_date(dct['@date']).date()
    if '@bytes' in dct:
        return bytearray(urlsafe_b64decode(dct['@bytes'].encode()))
    return dct",'@ref' in dct,76,'@ref' in dct,True,100.00000000000004,N/A
"def to_json(dct, pretty=False, sort_keys=False):
    """"""
    Opposite of parse_json.
    Converts a :any`_Expr` into a request body, calling :any:`to_fauna_json`.
    """"""
<mask>:
        return dumps(dct, cls=_FaunaJSONEncoder, sort_keys=True, indent=2, separators=(', ', ': '))
    return dumps(dct, cls=_FaunaJSONEncoder, sort_keys=sort_keys, separators=(',', ':'))",pretty,34,pretty,True,100.00000000000004,N/A
"def stream_content_to_json(buffer):
    values = []
    try:
        content = parse_json(buffer)
        values.append({'content': content, 'raw': buffer})
        buffer = ''
    except Exception:
        while True:
            pos = buffer.find('\n') + 1
<mask>:
                break
            slice = buffer[0:pos].strip()
            if len(pos) > 0:
                values.append({'content': slice.decode(), 'raw': slice})
                buffer = buffer[pos].encode()
    return {'buffer': buffer, 'values': values}",pos <= 0,46,pos == -1,False,18.99589214128981,N/A
"def default(self, obj):
<mask>:
        return obj.to_fauna_json()
    elif isinstance(obj, datetime):
        return FaunaTime(obj).to_fauna_json()
    elif isinstance(obj, date):
        return {'@date': obj.isoformat()}
    elif isinstance(obj, (bytes, bytearray)):
        return {'@bytes': urlsafe_b64encode(obj).decode('utf-8')}
    else:
        raise UnexpectedError('Unserializable object {} of type {}'.format(obj, type(obj)), None)","isinstance(obj, _Expr)",34,"isinstance(obj, Fauna)",False,45.48019047027906,N/A
"def ref(collection_ref, id=None):
    """"""See the `docs <https://docs.fauna.com/fauna/current/api/fql/functions/ref>`__.""""""
<mask>:
        return _fn({'@ref': collection_ref})
    return _fn({'ref': collection_ref, 'id': id})",id is None,16,id is None,True,100.00000000000004,N/A
"def let(*args, **kwargs):
    """"""See the `docs <https://docs.fauna.com/fauna/current/api/fql/functions/let>`__.""""""
<mask>:
        return LetBindings([_fn({k: v}) for k, v in kwargs.items()])
    else:
        bindings = [_fn({k: v}) for k, v in args[0].items()]
        in_expr = args[1]
        return _fn({'let': bindings, 'in': in_expr})",kwargs,34,kwargs,True,100.00000000000004,N/A
"def lambda_query(func):
    """"""
  See the `docs <https://app.fauna.com/documentation/reference/queryapi#basic-forms>`__.
  This form generates :any:`var` objects for you, and is called like::

      query.lambda_query(lambda a: query.add(a, a))
      # Produces: {
      #  ""lambda"": ""a"",
      #  ""expr"": {""add"": ({""var"": ""a""}, {""var"": ""a""})}
      # }

  You usually don't need to call this directly as lambdas in queries will be converted for you.
  For example: ``query.map_(lambda a: query.add(a, 1), collection)``.

  You can also use :any:`lambda_` directly.

  :param func:
    Takes one or more :any:`var` expressions and uses them to construct an expression.
    If this has more than one argument, the lambda destructures an array argument.
    (To destructure single-element arrays use :any:`lambda_`.)
  """"""
    vars = func.__code__.co_varnames
    n_args = len(vars)
<mask>:
        raise ValueError('Function must take at least 1 argument.')
    elif n_args == 1:
        v = vars[0]
        return lambda_(v, func(var(v)))
    else:
        return lambda_(vars, func(*[var(v) for v in vars]))",n_args == 0,135,n_args < 1,False,32.555630133216134,N/A
"def query(_lambda):
    """"""See the `docs <https://docs.fauna.com/fauna/current/api/fql/functions/query>`__.""""""
<mask>:
        _lambda = lambda_query(_lambda)
    return _fn({'query': _lambda})","isinstance(_lambda, FunctionType)",13,"isinstance(_lambda, str)",False,64.34588841607616,N/A
"def match(index, *terms):
    """"""See the `docs <https://docs.fauna.com/fauna/current/api/fql/functions/match>`__.""""""
    m = {'match': index}
<mask>:
        m['terms'] = _varargs(terms)
    return _fn(m)",len(terms) >= 1,17,terms,False,0.24787521766663595,N/A
"def getRuntimeEnv(self):
    env = [{'name': 'Netlify', 'check': lambda: 'NETLIFY_IMAGES_CDN_DOMAIN' in os.environ}, {'name': 'Vercel', 'check': lambda: 'VERCEL' in os.environ}, {'name': 'Heroku', 'check': lambda: 'PATH' in os.environ and '.heroku' in os.environ['PATH']}, {'name': 'AWS Lambda', 'check': lambda: 'AWS_LAMBDA_FUNCTION_VERSION' in os.environ}, {'name': 'GCP Cloud Functions', 'check': lambda: '_' in os.environ and 'google' in os.environ['_']}, {'name': 'GCP Compute Instances', 'check': lambda: 'GOOGLE_CLOUD_PROJECT' in os.environ}, {'name': 'Azure Cloud Functions', 'check': lambda: 'WEBSITE_FUNCTIONS_AZUREMONITOR_CATEGORIES' in os.environ}, {'name': 'Azure Compute', 'check': lambda: 'ORYX_ENV_TYPE' in os.environ and 'WEBSITE_INSTANCE_ID' in os.environ and (os.environ['ORYX_ENV_TYPE'] == 'AppService')}]
    try:
        recognized = next((e for e in env if e.get('check')()))
<mask>:
            return recognized.get('name')
    except:
        return 'Unknown'",recognized is not None,100,recognized.get('name'),False,8.116697886877475,N/A
"@property
def request_header(self):
    """"""Produces a dictionary with a non-zero `X-Last-Seen-Txn` header; or,
        if one has not yet been set, the empty header dictionary.""""""
    t = self.time
<mask>:
        return {}
    return {'X-Last-Seen-Txn': str(t)}",t is None,32,t == 0,False,15.97357760615681,N/A
"def update_txn_time(self, new_txn_time):
    """"""Updates the internal transaction time.
        In order to maintain a monotonically-increasing value, `newTxnTime`
        is discarded if it is behind the current timestamp.""""""
    with self._lock:
<mask>:
            self._time = new_txn_time
        else:
            self._time = max(self._time, new_txn_time)",self._time is None,36,self._time < new_txn_time,False,26.269098944241588,N/A
"def __init__(self, secret, domain='db.fauna.com', scheme='https', port=None, timeout=60, observer=None, pool_connections=10, pool_maxsize=10, endpoint=None, **kwargs):
    """"""
        :param secret:
          Auth token for the FaunaDB server.
        :param domain:
          Base URL for the FaunaDB server.
        :param scheme:
          ``""http""`` or ``""https""``.
        :param port:
          Port of the FaunaDB server.
        :param timeout:
          Read timeout in seconds.
        :param observer:
          Callback that will be passed a :any:`RequestResult` after every completed request.
        :param pool_connections:
          The number of connection pools to cache.
        :param pool_maxsize:
          The maximum number of connections to save in the pool.
        :param endpoint:
          Full URL for the FaunaDB server.
        """"""
    self.check_new_version()
    self.domain = domain
    self.scheme = scheme
    self.port = (443 if scheme == 'https' else 80) if port is None else port
    self.auth = HTTPBearerAuth(secret)
    constructed_url = '%s://%s:%s' % (self.scheme, self.domain, self.port)
    self.base_url = self._normalize_endpoint(endpoint) if endpoint else constructed_url
    self.observer = observer
    self.pool_connections = pool_connections
    self.pool_maxsize = pool_maxsize
    self._last_txn_time = kwargs.get('last_txn_time') or _LastTxnTime()
    self._query_timeout_ms = kwargs.get('query_timeout_ms')
<mask>:
        self._query_timeout_ms = int(self._query_timeout_ms)
    if 'session' not in kwargs or 'counter' not in kwargs:
        self.session = Session()
        self.session.mount('https://', HTTPAdapter(pool_connections=pool_connections, pool_maxsize=pool_maxsize))
        self.session.mount('http://', HTTPAdapter(pool_connections=pool_connections, pool_maxsize=pool_maxsize))
        self.counter = _Counter(1)
        self.session.headers.update({'Keep-Alive': 'timeout=5', 'Accept-Encoding': 'gzip', 'Content-Type': 'application/json;charset=utf-8', 'X-Fauna-Driver': 'python', 'X-FaunaDB-API-Version': api_version, 'X-Driver-Env': str(RuntimeEnvHeader())})
        if self._query_timeout_ms is not None:
            self.session.headers['X-Query-Timeout'] = str(self._query_timeout_ms)
        self.session.timeout = timeout
    else:
        self.session = kwargs['session']
        self.counter = kwargs['counter']",self._query_timeout_ms is not None,202,self._query_timeout_ms,False,68.72892787909726,N/A
"def check_new_version(self):
    response = get('https://pypi.org/pypi/faunadb/json')
    latest_version = response.json().get('info').get('version')
<mask>:
        msg1 = 'New fauna version available {} => {}'.format(pkg_version, latest_version)
        msg2 = 'Changelog: https://github.com/fauna/faunadb-python/blob/v4/CHANGELOG.md'
        width = 80
        print('+' + '-' * width + '+')
        print('| ' + msg1 + ' ' * (width - len(msg1) - 1) + '|')
        print('| ' + msg2 + ' ' * (width - len(msg2) - 1) + '|')
        print('+' + '-' * width + '+')",latest_version > pkg_version,70,latest_version != pkg_version,False,34.57207846419412,N/A
"@staticmethod
def set_iterator(client, set_query, map_lambda=None, mapper=None, page_size=None):
    """"""
    Iterator that keeps getting new pages of a set.

    :param map_lambda:
      If present, a :any:`lambda_` for mapping set elements.
    :param mapper:
      Mapping Python function used on each page element.
    :param page_size:
      Number of instances to be fetched at a time.
    :return:
      Iterator through all elements in the set.
    """"""

    def get_page(**kwargs):
        queried = query.paginate(set_query, **kwargs)
<mask>:
            queried = query.map_(map_lambda, queried)
        return Page.from_raw(client.query(queried))
    page = get_page(size=page_size)
    for val in page.data:
        yield (val if mapper is None else mapper(val))
    next_cursor = 'after' if page.after is not None else 'before'
    while getattr(page, next_cursor) is not None:
        page = get_page(**{'size': page_size, next_cursor: getattr(page, next_cursor)})
        for val in page.data:
            yield (val if mapper is None else mapper(val))",map_lambda is not None,120,map_lambda,False,36.78794411714425,N/A
"def _get_or_raise(request_result, dct, key):
<mask>:
        return dct[key]
    else:
        raise UnexpectedError('Response JSON does not contain expected key %s' % key, request_result)","isinstance(dct, dict) and key in dct",20,key in dct,False,9.697196786440509,N/A
"@staticmethod
def raise_for_status_code(request_result):
    code = request_result.status_code
<mask>:
        pass
    elif code == codes.bad_request:
        raise BadRequest(request_result)
    elif code == codes.unauthorized:
        raise Unauthorized(request_result)
    elif code == codes.forbidden:
        raise PermissionDenied(request_result)
    elif code == codes.not_found:
        raise NotFound(request_result)
    elif code == codes.conflict:
        raise ContendedTransaction(request_result)
    elif code == codes.internal_server_error:
        raise InternalError(request_result)
    elif code == codes.unavailable:
        raise UnavailableError(request_result)
    else:
        raise UnexpectedError('Unexpected status code.', request_result)",200 <= code <= 299,56,code == codes.ok,False,9.042266054940777,N/A
"@staticmethod
def get_failures(dct, request_result):
<mask>:
        return [Failure.from_dict(failure, request_result) for failure in dct['failures']]
    return None",'failures' in dct,14,'failures' in dct,True,100.00000000000004,N/A
"@staticmethod
def get_cause(dct, request_result):
<mask>:
        return [ErrorData.from_dict(cause, request_result) for cause in dct['cause']]
    return None",'cause' in dct,14,'cause' in dct,True,100.00000000000004,N/A
"def show_request_result(request_result):
    """"""Translates a :any:`RequestResult` to a string suitable for logging.""""""
    rr = request_result
    parts = []
    log = parts.append

    def _indent(s):
        """"""Adds extra spaces to the beginning of every newline.""""""
        indent_str = '  '
        return ('\n' + indent_str).join(s.split('\n'))
<mask>:
        query_string = '?' + '&'.join(('%s=%s' % (k, v) for k, v in sorted(rr.query.items())))
    else:
        query_string = ''
    log('Fauna %s /%s%s\n' % (rr.method, rr.path, query_string))
    if rr.request_content is not None:
        log('  Request JSON: %s\n' % _indent(to_json(rr.request_content, pretty=True)))
    log('  Response headers: %s\n' % _indent(to_json(dict(rr.response_headers), pretty=True)))
    log('  Response JSON: %s\n' % _indent(to_json(rr.response_content, pretty=True)))
    log('  Response (%i): Network latency %ims\n' % (rr.status_code, int(rr.time_taken * 1000)))
    return u''.join(parts)",rr.query,103,rr.query,True,100.00000000000004,N/A
"def __init__(self, id, cls=None, db=None):
<mask>:
        raise ValueError('The Ref must have an id.')
    value = {'id': id}
    if cls != None:
        value['collection'] = cls
    if db != None:
        value['database'] = db
    super(Ref, self).__init__(value)",id is None,33,id == None,False,18.99589214128981,N/A
"def __init__(self, set_ref):
<mask>:
        value = set_ref.value
    else:
        value = set_ref
    super(SetRef, self).__init__(value)","isinstance(set_ref, _Expr)",13,"isinstance(set_ref, Set)",False,62.401954419369176,N/A
"def __init__(self, value):
    """"""
    :param value:
      If a :class:`datetime.datetime` is passed, it is converted to a string.
      Must include an offset.
    """"""
<mask>:
        if value.utcoffset() is None:
            raise ValueError('FaunaTime requires offset-aware datetimes')
        value = value.isoformat()
    super(FaunaTime, self).__init__(value.replace('+00:00', 'Z'))","isinstance(value, datetime)",38,"isinstance(value, datetime.datetime)",False,57.21248424548516,N/A
"def on(self, event_type, callback):
    """"""
        Subscribe to an event.
        """"""
<mask>:
        self.callbacks[event_type] = callback
    elif callback is not None:
        raise Exception('Callback for event `%s` is not callable.' % event_type)",callable(callback),29,callable(callback),True,100.00000000000004,N/A
"def dispatch(self, event, request_result):
    """"""
        Dispatch the given event to the appropriate listeners.
        """"""
    fn = self.callbacks.get(event.type, None)
<mask>:
        return self._noop(event, request_result)
    return fn(event)",fn is None,24,fn is None,True,100.00000000000004,N/A
"def parse_stream_request_result_or_none(request_result):
    """"""
    Parses a stream RequestResult into a stream Event type.
    """"""
    event = None
    parsed = request_result.response_content
<mask>:
        return UnknownEvent(request_result)
    evt_type = parsed.get('type', None)
    if evt_type == 'start':
        event = Start(parsed)
    elif evt_type is None and 'errors' in parsed:
        event = Error(BadRequest(request_result))
    elif evt_type == 'error':
        event = Error(parsed)
    elif evt_type == 'version':
        event = Version(parsed)
    elif evt_type == 'set':
        event = Set(parsed)
    elif evt_type == 'history_rewrite':
        event = HistoryRewrite(parsed)
    else:
        event = UnknownEvent(request_result)
    return event",parsed is None,78,not parsed,False,30.326532985631665,N/A
"def __init__(self, parsed):
    super(Error, self).__init__('error')
    self.error = None
    self.code = None
    self.description = None
<mask>:
        if 'event' in parsed:
            self.error = parsed['event']
            if isinstance(parsed['event'], dict):
                self.code = parsed['event'].get('code', None)
                self.description = parsed['event'].get('description', None)
        elif 'errors' in parsed:
            self.error = parsed['errors']
        else:
            self.error = parsed
    else:
        self.error = parsed","isinstance(parsed, dict)",48,"isinstance(parsed, dict)",True,100.00000000000004,N/A
"def __init__(self, parsed):
    super(HistoryRewrite, self).__init__('history_rewrite')
<mask>:
        self.event = parsed.get('event', None)
        self.txn = parsed.get('txn')

    def __repr__(self):
        return 'stream:event:HistoryRewrite(event=%s, txn=%s)' % (self.event, self.txn)","isinstance(parsed, dict)",21,parsed,False,0.673794699908547,N/A
"def __init__(self, parsed):
    super(Version, self).__init__('version')
<mask>:
        self.event = parsed.get('event', None)
        self.txn = parsed.get('txn')","isinstance(parsed, dict)",13,parsed,False,0.673794699908547,N/A
"def __init__(self, parsed):
    super(Set, self).__init__('set')
<mask>:
        self.event = parsed.get('event', None)
        self.txn = parsed.get('txn')","isinstance(parsed, dict)",13,parsed,False,0.673794699908547,N/A
"def __init__(self, client, expression, options):
    self._client = client
    self.options = options
    self._fields = None
<mask>:
        self._fields = self.options.get('fields', None)
    elif hasattr(self.options, 'fields'):
        self._fields = self.options.field
    if isinstance(self._fields, list):
        union = set(self._fields).union(VALID_FIELDS)
        if union != VALID_FIELDS:
            raise Exception('Valid fields options are %s, provided %s.' % (VALID_FIELDS, self._fields))
    self._state = 'idle'
    self._query = expression
    self._data = to_json(expression).encode()","isinstance(self.options, dict)",55,"hasattr(self.options, 'get')",False,54.10822690539397,N/A
"def close(self):
    """"""
        Closes the stream subscription by aborting its underlying http request.
        """"""
<mask>:
        raise StreamError('Cannot close inactive stream subscription.')
    self._state = 'closed'",self._state == 'closed',24,self._state == 'closed',True,100.00000000000004,N/A
"def subscribe(self, on_event):
    """"""Initiates the stream subscription.""""""
<mask>:
        raise StreamError('Stream subscription already started.')
    try:
        base_url = f'{self._client.scheme}://{self._client.domain}:{self._client.port}'
        timeout = httpx.Timeout(connect=None, read=None, write=None, pool=10)
        conn = httpx.Client(http2=True, http1=False, base_url=base_url, timeout=timeout)
    except Exception as error_msg:
        raise StreamError(error_msg)
    try:
        self._state = 'connecting'
        headers = self._client.session.headers
        headers['Authorization'] = self._client.auth.auth_header()
        if self._client._query_timeout_ms is not None:
            headers['X-Query-Timeout'] = str(self._client._query_timeout_ms)
        headers['X-Last-Seen-Txn'] = str(self._client.get_last_txn_time())
        start_time = time()
        url_params = ''
        if isinstance(self._fields, list):
            url_params = '?%s' % urlencode({'fields': ','.join(self._fields)})
        stream_id = conn.stream('POST', '/stream%s' % url_params, content=self._data, headers=dict(headers))
        self._state = 'open'
        self._event_loop(stream_id, on_event, start_time)
    except Exception as error_msg:
        if callable(on_event):
            on_event(Error(error_msg), None)
    finally:
        conn.close()",self._state != 'idle',95,self._state == 'started',False,43.47208719449914,N/A
"def _event_loop(self, stream_id, on_event, start_time):
    """""" Event loop for the stream. """"""
    with stream_id as response:
<mask>:
            self._client.sync_last_txn_time(int(response.headers['x-txn-time']))
        try:
            buffer = ''
            for push in response.iter_bytes():
                try:
                    chunk = push.decode()
                    buffer += chunk
                except:
                    continue
                result = stream_content_to_json(buffer)
                buffer = result['buffer']
                for value in result['values']:
                    request_result = self._stream_chunk_to_request_result(response, value['raw'], value['content'], start_time, time())
                    event = parse_stream_request_result_or_none(request_result)
                    if event is not None and hasattr(event, 'txn'):
                        self._client.sync_last_txn_time(int(event.txn))
                    on_event(event, request_result)
                    if self._client.observer is not None:
                        self._client.observer(request_result)
                if self._state == 'closed':
                    break
        except Exception as error_msg:
            self.error = error_msg
            on_event(Error(error_msg), None)",'x-txn-time' in response.headers,86,"hasattr(response, 'headers') and 'x-txn-time' in response.headers",False,31.702331385234313,N/A
"def read_config(path):
    config = {}
    with open(path) as f:
        for line in f.readlines():
<mask>:
                k, v = line.strip().split(':', 1)
                if k in ('url', 'key', 'verify'):
                    config[k] = v.strip()
    return config",':' in line,30,line.startswith('#'),False,7.267884212102741,N/A
"def get_url_key_verify(url, key, verify):
<mask>:
        url = os.environ.get('CDSAPI_URL')
    if key is None:
        key = os.environ.get('CDSAPI_KEY')
    dotrc = os.environ.get('CDSAPI_RC', os.path.expanduser('~/.cdsapirc'))
    if url is None or key is None:
        if os.path.exists(dotrc):
            config = read_config(dotrc)
            if key is None:
                key = config.get('key')
            if url is None:
                url = config.get('url')
            if verify is None:
                verify = bool(int(config.get('verify', 1)))
    if url is None or key is None or key is None:
        raise Exception('Missing/incomplete configuration file: %s' % dotrc)
    if verify is None:
        verify = True
    return (url, key, verify)",url is None,84,url is None,True,100.00000000000004,N/A
"def toJSON(obj):
    to_json = getattr(obj, 'toJSON', None)
<mask>:
        return to_json()
    if isinstance(obj, (list, tuple)):
        return [toJSON(x) for x in obj]
    if isinstance(obj, dict):
        r = {}
        for k, v in obj.items():
            r[k] = toJSON(v)
        return r
    return obj",callable(to_json),38,to_json,False,36.78794411714425,N/A
"def _download(self, url, size, target):
<mask>:
        target = url.split('/')[-1]
    self.info('Downloading %s to %s (%s)', url, target, bytes_to_string(size))
    start = time.time()
    mode = 'wb'
    total = 0
    sleep = 10
    tries = 0
    headers = None
    while tries < self.retry_max:
        r = self.robust(self.session.get)(url, stream=True, verify=self.verify, headers=headers, timeout=self.timeout)
        try:
            r.raise_for_status()
            with tqdm(total=size, unit_scale=True, unit_divisor=1024, unit='B', disable=not self.progress, leave=False) as pbar:
                pbar.update(total)
                with open(target, mode) as f:
                    for chunk in r.iter_content(chunk_size=1024):
                        if chunk:
                            f.write(chunk)
                            total += len(chunk)
                            pbar.update(len(chunk))
        except requests.exceptions.ConnectionError as e:
            self.error('Download interupted: %s' % (e,))
        finally:
            r.close()
        if total >= size:
            break
        self.error('Download incomplete, downloaded %s byte(s) out of %s' % (total, size))
        self.warning('Sleeping %s seconds' % (sleep,))
        time.sleep(sleep)
        mode = 'ab'
        total = os.path.getsize(target)
        sleep *= 1.5
        if sleep > self.sleep_max:
            sleep = self.sleep_max
        headers = {'Range': 'bytes=%d-' % total}
        tries += 1
        self.warning('Resuming download at byte %s' % (total,))
    if total != size:
        raise Exception('Download failed: downloaded %s byte(s) out of %s' % (total, size))
    elapsed = time.time() - start
    if elapsed:
        self.info('Download rate %s/s', bytes_to_string(size / elapsed))
    return target",target is None,171,target is None,True,100.00000000000004,N/A
"def update(self, request_id=None):
<mask>:
        request_id = self.reply['request_id']
    task_url = '%s/tasks/%s' % (self._url, request_id)
    self.debug('GET %s', task_url)
    result = self.robust(self.session.get)(task_url, verify=self.verify, timeout=self.timeout)
    result.raise_for_status()
    self.reply = result.json()",request_id is None,25,request_id is None,True,100.00000000000004,N/A
"@pytest.mark.parametrize('key,expected_client', [(':', cdsapi.Client), ('', datapi.legacy_api_client.LegacyApiClient)])
@pytest.mark.parametrize('key_from_env', [True, False])
def test_instantiation(monkeypatch, key, expected_client, key_from_env):
<mask>:
        monkeypatch.setenv('CDSAPI_KEY', key)
        c = cdsapi.Client()
    else:
        c = cdsapi.Client(key=key)
    assert isinstance(c, cdsapi.Client)
    assert isinstance(c, expected_client)
    assert c.key == key",key_from_env,33,key_from_env,True,100.00000000000004,N/A
"def test_requirements_markers(tmpfile):
    tmpfh, tmppath = tmpfile
    tmpfh.write('python3-openid;python_version>=""3.9""\n')
    tmpfh.close()
<mask>:
        assert len(get_packages_info(tmppath)) == 2
    else:
        assert len(get_packages_info(tmppath)) == 0",sys.version_info.minor >= 9,18,"sys.version_info < (3, 9)",False,36.72056269893591,N/A
"@classmethod
def from_config(cls, strategy_file):
    config = ConfigParser()
    config.optionxform = str
    config.read(strategy_file)

    def get_config_list(section, option):
        try:
            value = config.get(section, option)
        except NoOptionError:
            return []
        return [item for item in value.lower().split('\n') if item]
    authorized_packages = dict()
<mask>:
        for name, value in config.items('Authorized Packages'):
            authorized_packages[name] = value
    strategy = cls(authorized_licenses=get_config_list('Licenses', 'authorized_licenses'), unauthorized_licenses=get_config_list('Licenses', 'unauthorized_licenses'), authorized_packages=authorized_packages)
    if config.has_section('Authorized Packages'):
        for name, value in config.items('Authorized Packages'):
            strategy.AUTHORIZED_PACKAGES[name] = value
    return strategy",config.has_section('Authorized Packages'),65,config.has_section('Authorized Packages'),True,100.00000000000004,N/A
"@classmethod
def starting(cls, value):
    """"""Return level starting with value (case-insensitive)""""""
    for member in cls:
<mask>:
            return member
    raise ValueError('No level starting with {!r}'.format(value))",member.name.startswith(value.upper()),23,member.value == value,False,6.60902979597904,N/A
"def get_packages_info(requirement_file, no_deps=False):
    regex_license = re.compile('License(?:-Expression)?: (?P<license>.*)?$', re.M)
    regex_classifier = re.compile('Classifier: License(?: :: OSI Approved)?(?: :: (?P<classifier>.*))?$', re.M)
    requirements = parse_requirements(requirement_file)

    def transform(dist):
        licenses = get_licenses_from_classifiers(dist) or get_license(dist) or []
        licenses = list(set([strip_license_for_windows(l) for l in licenses]))
        licenses = list(set([strip_license(l) for l in licenses]))
        return {'name': dist.project_name, 'version': dist.version, 'location': dist.location, 'dependencies': [dependency.project_name for dependency in dist.requires()], 'licenses': licenses}

    def get_license(dist):
<mask>:
            metadata = dist.get_metadata(dist.PKG_INFO)
            match = regex_license.search(metadata)
            if match:
                license = match.group('license')
                if license != 'UNKNOWN':
                    return [license]
        return []

    def get_licenses_from_classifiers(dist):
        if dist.has_metadata(dist.PKG_INFO):
            metadata = dist.get_metadata(dist.PKG_INFO)
            return [m for m in regex_classifier.findall(metadata) if m]
        return []

    def strip_license_for_windows(license):
        if license.endswith('\r'):
            return license[:-1]
        return license

    def strip_license(license):
        if license.lower().endswith(' license'):
            return license[:-len(' license')]
        return license
    resolve_func = resolve_without_deps if no_deps else resolve
    packages = [transform(dist) for dist in resolve_func(requirements)]
    unique = []
    [unique.append(item) for item in packages if item not in unique]
    return sorted(unique, key=lambda item: item['name'].lower())",dist.has_metadata(dist.PKG_INFO),148,dist.has_metadata(dist.PKG_INFO),True,100.00000000000004,N/A
"def check_package(strategy, pkg, level=Level.STANDARD, as_regex=False):
    whitelisted = pkg['name'] in strategy.AUTHORIZED_PACKAGES and (semantic_version.SimpleSpec(strategy.AUTHORIZED_PACKAGES[pkg['name']]).match(semantic_version.Version.coerce(pkg['version'])) or (level == Level.STANDARD and strategy.AUTHORIZED_PACKAGES[pkg['name']] == ''))
<mask>:
        return Reason.OK

    def check_one(license_str, license_rule='AUTHORIZED', as_regex=False):
        if as_regex:
            license_regex = getattr(strategy, '{}_REGEX'.format(license_rule))
            return license_regex.search(license_str) is not None
        else:
            license_list = getattr(strategy, '{}_LICENSES'.format(license_rule))
            return license_str in license_list
    at_least_one_unauthorized = False
    count_authorized = 0
    licenses = get_license_names(pkg['licenses'])
    for license in licenses:
        if check_one(license, license_rule='UNAUTHORIZED', as_regex=as_regex):
            at_least_one_unauthorized = True
        if check_one(license, license_rule='AUTHORIZED', as_regex=as_regex):
            count_authorized += 1
    if count_authorized and level is Level.STANDARD or (count_authorized and (not at_least_one_unauthorized) and (level is Level.CAUTIOUS)) or (count_authorized and count_authorized == len(licenses) and (level is Level.PARANOID)):
        return Reason.OK
    if at_least_one_unauthorized:
        return Reason.UNAUTHORIZED
    return Reason.UNKNOWN",whitelisted,107,whitelisted,True,100.00000000000004,N/A
"def find_parents(package, all, seen):
<mask>:
        return [package]
    seen.add(package)
    parents = [p['name'] for p in all if package in p['dependencies']]
    if len(parents) == 0:
        return [package]
    dependency_trees = []
    for parent in parents:
        for dependencies in find_parents(parent, all, seen):
            dependency_trees.append(package + ' << ' + dependencies)
    return dependency_trees",package in seen,47,package in seen,True,100.00000000000004,N/A
"def parse_requirements(requirement_file):
    requirements = []
    for req in pip_parse_requirements(requirement_file, session=PipSession()):
        install_req = install_req_from_parsed_requirement(req)
<mask>:
            continue
        elif install_req.editable:
            continue
        requirements.append(pkg_resources.Requirement.parse(str(install_req.req)))
    return requirements",install_req.markers and (not pkg_resources.evaluate_marker(str(install_req.markers))),21,not install_req,False,0.26112144292372363,N/A
"def spelltest(speller, tests, verbose=2):
    n, bad = (0, 0)
    for target, incorrect_spellings in tests.items():
        for incorrect_spelling in incorrect_spellings.split('|'):
            n += 1
            w = speller(incorrect_spelling)
<mask>:
                bad += 1
                if verbose >= 2:
                    print(f'spell({incorrect_spelling}) => {w}; should be {target}')
    if verbose >= 1:
        print(f'bad: {bad}/{n}')
    return bad",w != target,46,not w == target,False,23.643540225079384,N/A
"def download_progress_hook(self, count, blockSize, totalSize):
    percent = int(count * blockSize * 100 / totalSize)
<mask>:
        self.old_percent = percent
        print('>', end='')
        sys.stdout.flush()
    if percent == 100:
        print('\ndone!')",percent >= 2 + self.old_percent,26,percent != self.old_percent,False,43.79518644116555,N/A
"def load_from_tar(lang, file_name='word_count.json'):
    archive_name = os.path.join(PATH, f'data/{lang}.tar.gz')
<mask>:
        supported_langs = ', '.join(word_regexes.keys())
        raise NotImplementedError(textwrap.dedent(f""\n            language '{lang}' not supported\n            supported languages: {supported_langs}\n            you can easily add new languages by following instructions at\n            https://github.com/fsondej/autocorrect/tree/master#adding-new-languages\n            ""))
    if not os.path.isfile(archive_name):
        print('dictionary for this language not found, downloading...')
        urls = [gateway + path for gateway in ipfs_gateways for path in ipfs_paths[lang]]
        if lang in backup_urls:
            urls += backup_urls[lang]
        for url in urls:
            progress = ProgressBar()
            try:
                urlretrieve(url, archive_name, progress.download_progress_hook)
                error_message = None
                break
            except Exception as ex:
                print(f""couldn't download {url}, trying next url..."")
                error_message = str(ex)
        if error_message is not None:
            raise ConnectionError(error_message + '\nFix your network connection, or manually download \n{}\nand put it in \nPATH_TO_REPO/autocorrect/data/'.format(url))
    with closing(tarfile.open(archive_name, 'r:gz')) as tarf:
        with closing(tarf.extractfile(file_name)) as file:
            return json.load(file)",lang not in word_regexes,122,not lang in word_regexes,False,56.234132519034915,N/A
"def __init__(self, lang='en', threshold=0, nlp_data=None, fast=False, only_replacements=False):
    self.lang = lang
    self.threshold = threshold
    self.nlp_data = load_from_tar(lang) if nlp_data is None else nlp_data
    self.fast = fast
    self.only_replacements = only_replacements
<mask>:
        self.nlp_data = {k: v for k, v in self.nlp_data.items() if v >= threshold}",threshold > 0,42,threshold is not None,False,15.97357760615681,N/A
"def get_candidates(self, word):
    w = Word(word, self.lang, self.only_replacements)
<mask>:
        candidates = self.existing([word]) or self.existing(w.typos()) or [word]
    else:
        candidates = self.existing([word]) or self.existing(w.typos()) or self.existing(w.double_typos()) or [word]
    return [(self.nlp_data.get(c, 0), c) for c in candidates]",self.fast,34,self.only_replacements,False,21.3643503198117,N/A
"def autocorrect_word(self, word):
    """"""most likely correction for everything up to a double typo""""""
<mask>:
        return ''
    candidates = self.get_candidates(word)
    if word[0].isupper():
        decapitalized = word[0].lower() + word[1:]
        candidates += self.get_candidates(decapitalized)
    best_word = max(candidates)[1]
    if word[0].isupper():
        best_word = best_word[0].upper() + best_word[1:]
    return best_word",word == '',41,not word,False,18.393972058572114,N/A
"def typos(self):
    """"""letter combinations one typo away from word""""""
<mask>:
        return chain(self._replaces())
    else:
        return chain(self._deletes(), self._transposes(), self._replaces(), self._inserts())",self.only_replacements,18,self.word,False,28.254432923044853,N/A
"def _gen_entities(location_name: str, coordinator: GismeteoDataUpdateCoordinator, config: ConfigType):
    """"""Generate entities.""""""
    entities = [GismeteoSensor(coordinator, desc, location_name) for desc in SENSOR_DESCRIPTIONS]
    days = config.get(CONF_FORECAST_DAYS)
<mask>:
        for day in range(days + 1):
            entities.extend([GismeteoSensor(coordinator, desc, location_name, day) for desc in FORECAST_SENSOR_DESCRIPTIONS])
    return entities",days is not None,38,days,False,4.9787068367863965,N/A
"@property
def native_value(self) -> StateType | date | datetime | Decimal:
    """"""Return the value reported by the sensor.""""""
    try:
<mask>:
            res = getattr(self._gismeteo, self.entity_description.key)()
        else:
            forecast = self._gismeteo.forecast_data(self._day, ForecastMode.DAILY)
            if self.entity_description.key == ATTR_FORECAST_CONDITION:
                res = getattr(self._gismeteo, self.entity_description.key)(forecast, ForecastMode.DAILY)
            else:
                res = getattr(self._gismeteo, self.entity_description.key)(forecast)
        return res
    except KeyError:
        _LOGGER.warning('Condition is currently not available: %s', self.entity_description.key)
        return None",self._day is None,56,self.entity_description.key == ATTR_DATE_CONDITION,False,6.285596338261262,N/A
"def __init__(self, params: dict[str, Any] | None=None) -> None:
    """"""Initialize cache.""""""
    _LOGGER.debug('Initializing cache')
    params = params or {}
    cache_dir = params.get('cache_dir')
    self._cache_time = params.get('cache_time', 0)
    self._domain = params.get('domain')
<mask>:
        self._cache_dir = Path(cache_dir).resolve()
    if params.get('clean_dir', False):
        self._clean_dir()",cache_dir,36,cache_dir,True,100.00000000000004,N/A
"def _clean_dir(self, cache_time: int=0) -> None:
    """"""Clean cache.""""""
    now_time = time.time()
    cache_time = max(cache_time, self._cache_time)
<mask>:
        _LOGGER.debug('Cleaning cache directory %s', self._cache_dir)
        files = os.listdir(self._cache_dir)
        _LOGGER.debug(files)
        for file_name in files:
            file_path = self._cache_dir / file_name
            try:
                file_time = file_path.stat().st_mtime
                if file_time + cache_time <= now_time:
                    file_path.unlink()
            except FileNotFoundError:
                pass",self._cache_dir and self._cache_dir.exists(),48,os.path.isdir(self._cache_dir),False,31.789186946706785,N/A
"def _get_file_path(self, file_name: str) -> Path:
    """"""Get path of cache file.""""""
<mask>:
        file_name = f'{self._domain}.{file_name}'
    return self._cache_dir / file_name",self._domain,19,self._domain,True,100.00000000000004,N/A
"def cached_for(self, file_name: str) -> float | None:
    """"""Return caching time of file if exists. Otherwise, None.""""""
    file_path = self._get_file_path(file_name)
<mask>:
        return None
    file_time = file_path.stat().st_mtime
    return time.time() - file_time",not file_path.exists() or not file_path.is_file(),30,not file_path,False,2.3517745856009116,N/A
"def is_cached(self, file_name: str, cache_time: int=0) -> bool:
    """"""Return True if cache file is exists.""""""
    file_path = self._get_file_path(file_name)
<mask>:
        return False
    file_time = file_path.stat().st_mtime
    cache_time = max(cache_time, self._cache_time)
    return file_time + cache_time > time.time()",not file_path.exists() or not file_path.is_file(),34,not file_path,False,2.3517745856009116,N/A
"def _get_api_client(hass: HomeAssistant, config: ConfigType | None) -> GismeteoApiClient:
    """"""Prepare Gismeteo API client instance.""""""
<mask>:
        config = {}
    return GismeteoApiClient(async_get_clientsession(hass), latitude=config.get(CONF_LATITUDE, hass.config.latitude), longitude=config.get(CONF_LONGITUDE, hass.config.longitude), params={CONF_DOMAIN: DOMAIN, CONF_TIMEZONE: str(hass.config.time_zone), CONF_CACHE_DIR: config.get(CONF_CACHE_DIR, hass.config.path(STORAGE_DIR)), CONF_CACHE_TIME: UPDATE_INTERVAL.total_seconds(), CONF_SHOW_ON_MAP: config.get(CONF_SHOW_ON_MAP, False)})",config is None,36,config is None,True,100.00000000000004,N/A
"def __init__(self, session: ClientSession, location_key: int | None=None, latitude: float | None=None, longitude: float | None=None, params: dict | None=None) -> None:
    """"""Initialize.""""""
    params = params or {}
    self._session = session
    self._cache = Cache(params) if params.get('cache_dir') is not None else None
    self._latitude = latitude
    self._longitude = longitude
    self._show_on_map = params.get(CONF_SHOW_ON_MAP, False)
    self._attributes = {}
<mask>:
        _LOGGER.debug('Place location ID used')
        self._attributes = {ATTR_ID: location_key}
    elif self._valid_coordinates(latitude, longitude):
        _LOGGER.debug('Place coordinates used')
    else:
        raise InvalidCoordinatesError
    self._current = {}
    self._forecast_hourly = []
    self._forecast_daily = []
    self._parsed = {}",location_key is not None,84,location_key,False,36.78794411714425,N/A
"@staticmethod
def _valid_coordinates(latitude: float, longitude: float) -> bool:
    """"""Return True if coordinates are valid.""""""
    try:
<mask>:
            return False
    except TypeError:
        return False
    else:
        return True","not isinstance(latitude, int | float) or not isinstance(longitude, int | float) or abs(latitude) > MAX_LATITUDE or (abs(longitude) > MAX_LONGITUDE)",25,latitude != longitude,False,0.003010114240464669,N/A
"@property
def attributes(self) -> dict[str, Any] | None:
    """"""Return an attributes.""""""
    attrs = self._attributes.copy()
<mask>:
        attrs[ATTR_LATITUDE] = self._latitude
        attrs[ATTR_LONGITUDE] = self._longitude
    else:
        attrs[ATTR_LAT] = self._latitude
        attrs[ATTR_LON] = self._longitude
    return attrs",self._show_on_map,30,self._longitude is not None,False,21.191828141393902,N/A
"def forecast_data(self, pos: int, mode: str=ForecastMode.HOURLY) -> dict[str, Any]:
    """"""Return forecast data.""""""
    now = dt_util.now()
    forecast = []
    for data in self._forecast_hourly if mode == ForecastMode.HOURLY else self._forecast_daily:
        fc_time = data.get(ATTR_FORECAST_TIME)
<mask>:
            continue
        if fc_time < now:
            forecast = [data]
        else:
            forecast.append(data)
    try:
        return forecast[pos]
    except IndexError:
        return {}",fc_time is None,49,fc_time is None,True,100.00000000000004,N/A
"@staticmethod
def _get(var: dict, k: str, func: Callable | None=None) -> StateType:
    res = var.get(k)
<mask>:
        try:
            res = func(res)
        except (TypeError, ValueError, ArithmeticError):
            return None
    return res",func is not None,28,func,False,4.9787068367863965,N/A
"def check_voc_keys(conf: ConfigType) -> ConfigType:
    """"""Ensure CONF_TVOC, CONF_VOC_INDEX or none of them are provided.""""""
    keys: Final = [CONF_TVOC, CONF_VOC_INDEX]
    count = sum((param in conf for param in keys))
<mask>:
        raise vol.Invalid('You must provide none or only one of the following: , '.join(keys))
    return conf",count > 1,44,count != 1,False,18.99589214128981,N/A
"def async_added_to_hass(self) -> None:
    """"""Register callbacks.""""""

    @callback
    def sensor_state_listener(event: Event) -> None:
        """"""Handle device state changes.""""""
        self.update()

    @callback
    def sensor_startup(event: Event) -> None:
        """"""Update template on startup.""""""
        entity_ids = []
        for src in self._sources.values():
<mask>:
                entity_ids.extend(src)
            else:
                entity_ids.append(src)
        _LOGGER.debug('[%s] Setup states tracking for %s', self._entity_id, ', '.join(entity_ids))
        async_track_state_change_event(self.hass, entity_ids, sensor_state_listener)
        self.update()
    if not self._added:
        self._added = True
        self.hass.bus.async_listen_once(EVENT_HOMEASSISTANT_START, sensor_startup)","isinstance(src, list)",59,"isinstance(src, list)",True,100.00000000000004,N/A
"@property
def iaq_level(self) -> str | None:
    """"""Get IAQ level.""""""
<mask>:
        return None
    if self._iaq_index <= 25:
        return LEVEL_INADEQUATE
    if self._iaq_index <= 38:
        return LEVEL_POOR
    if self._iaq_index <= 51:
        return LEVEL_FAIR
    if self._iaq_index <= 60:
        return LEVEL_GOOD
    return LEVEL_EXCELLENT",self._iaq_index is None,39,self._iaq_index is None,True,100.00000000000004,N/A
"def update(self) -> None:
    """"""Update index state.""""""
    _LOGGER.debug('[%s] State update', self._entity_id)
    iaq = 0
    sources = 0
    indexes = {}
    for src in self._sources:
        try:
            idx = self.__getattribute__(f'_{src}_index')
            _LOGGER.debug('[%s] %s_index=%s', self._entity_id, src, idx)
<mask>:
                iaq += idx
                sources += 1
                indexes[src] = idx
        except Exception:
            _LOGGER.exception('Exception occurred')
    if iaq:
        self._indexes = indexes
        self._iaq_index = int(65 * iaq / (5 * sources))
        self._iaq_sources = int(sources)
        _LOGGER.debug('[%s] Update IAQ index to %d (%d sources used)', self._entity_id, self._iaq_index, self._iaq_sources)",idx is not None,76,idx is not None,True,100.00000000000004,N/A
"def _get_number_state(self, entity_id: str, entity_unit: str | None=None, source_type: str='', mweight: float | None=None) -> float | None:
    """"""Convert value to number.""""""
    target_unit = None
<mask>:
        entity_unit = {entity_unit: 1}
    entity = self.hass.states.get(entity_id)
    if entity is None:
        _LOGGER.warning('Entity %s not found', entity_id)
        return None
    if not isinstance(entity, State):
        _LOGGER.warning('State of entity %s be instance of class State', entity_id)
        return None
    value = entity.state
    unit = entity.attributes.get(ATTR_UNIT_OF_MEASUREMENT)
    _LOGGER.debug('[%s] %s=%s %s', self._entity_id, entity_id, value, unit if unit and self._has_state(value) else '')
    if not self._has_state(value):
        _LOGGER.debug('State of entity %s is unknown', entity_id)
        return None
    if entity_unit is not None:
        target_unit = next(iter(entity_unit))
        if unit not in entity_unit:
            if mweight is None:
                _LOGGER.debug('Entity %s has inappropriate ""%s"" units for %s source. Ignored.', entity_id, unit, source_type)
                return None
            entity_unit = entity_unit.copy()
            if 'ppb' in (unit, target_unit):
                mweight /= 1000
            if 'µg/m³' in (unit, target_unit):
                mweight *= 1000
            if unit in {'ppm', 'ppb'}:
                entity_unit[unit] = mweight / 24.45
            else:
                entity_unit[unit] = 24.45 / mweight
    value = float(value)
    if entity_unit is not None and unit != target_unit:
        value *= entity_unit[unit]
        _LOGGER.debug('[%s] %s=%s %s (converted)', self._entity_id, entity_id, value, target_unit)
    return value","entity_unit is not None and (not isinstance(entity_unit, dict))",183,entity_unit is not None,False,13.533528323661276,N/A
"def _test_sort(elements, _type):
    values = [e.value for e in elements]
    for wrong, right in zip(values, natsorted(values)):
<mask>:
            print(f'{_type} is not sorted, {right!r} should come before {wrong!r}')
            return True
    return False",wrong != right,30,right < _type,False,15.97357760615681,N/A
"def trove_tester(classifiers, deprecated_classifiers):
    for classifier in classifiers:
        split = classifier.split(' :: ')
<mask>:
            raise InvalidClassifier(f'Top-level classifier {classifier!r} is invalid')
        for i in range(2, len(split)):
            parent = ' :: '.join(split[:i])
            if parent not in classifiers:
                raise InvalidClassifier(f'Classifier {parent!r} is missing')
        for sub in split:
            if sub.strip().rstrip() != sub:
                raise InvalidClassifier('Classifiers starting or ending with whitespace are invalid')
            if sub.lower().startswith('private'):
                raise InvalidClassifier(""Classifiers starting with 'Private' are invalid"")
            if ':' in sub:
                raise InvalidClassifier(""Classifiers containing ':' are invalid"")
    for deprecated_classifier, deprecated_by in deprecated_classifiers.items():
        if deprecated_classifier in classifiers:
            raise InvalidClassifier(f'Classifier {deprecated_classifier!r} in both valid and deprecated classifiers')
        for new_classifier in deprecated_by:
            if new_classifier not in classifiers:
                raise InvalidClassifier(f'Classifier {new_classifier!r} does not exist')
    print('All classifiers passed :)')",len(split) == 1,112,len(split) < 2,False,43.01250851313264,N/A
"def _get_obs(self):
<mask>:
        return [np.random.random(s) for s in self.observation_shape]
    else:
        return np.random.random(self.observation_shape)",type(self.observation_shape) is list,12,self.random,False,5.33657305117355,N/A
"def test_ring_buffer():

    def assert_elements(b, ref):
        assert len(b) == len(ref)
        for idx in range(b.maxlen):
<mask>:
                with pytest.raises(KeyError):
                    b[idx]
            else:
                assert b[idx] == ref[idx]
    b = RingBuffer(5)
    assert_elements(b, [])
    b.append(1)
    assert_elements(b, [1])
    b.append(2)
    assert_elements(b, [1, 2])
    b.append(3)
    assert_elements(b, [1, 2, 3])
    b.append(4)
    assert_elements(b, [1, 2, 3, 4])
    b.append(5)
    assert_elements(b, [1, 2, 3, 4, 5])
    b.append(6)
    assert_elements(b, [2, 3, 4, 5, 6])
    b.append(7)
    assert_elements(b, [3, 4, 5, 6, 7])
    b.append(8)
    assert_elements(b, [4, 5, 6, 7, 8])",idx >= len(ref),73,idx == 0,False,8.9730240870212,N/A
"def test_training_flag():
    obs_size = (3, 4)
    obs0 = np.random.random(obs_size)
    terminal0 = False
    obs1 = np.random.random(obs_size)
    terminal1 = True
    obs2 = np.random.random(obs_size)
    terminal2 = False
    for training in (True, False):
        memory = SequentialMemory(3, window_length=2)
        state = np.array(memory.get_recent_state(obs0))
        assert state.shape == (2,) + obs_size
        assert np.allclose(state[0], 0.0)
        assert np.all(state[1] == obs0)
        assert memory.nb_entries == 0
        memory.append(obs0, 0, 0.0, terminal1, training=training)
        state = np.array(memory.get_recent_state(obs1))
        assert state.shape == (2,) + obs_size
        assert np.all(state[0] == obs0)
        assert np.all(state[1] == obs1)
<mask>:
            assert memory.nb_entries == 1
        else:
            assert memory.nb_entries == 0
        memory.append(obs1, 0, 0.0, terminal2, training=training)
        state = np.array(memory.get_recent_state(obs2))
        assert state.shape == (2,) + obs_size
        assert np.allclose(state[0], 0.0)
        assert np.all(state[1] == obs2)
        if training:
            assert memory.nb_entries == 2
        else:
            assert memory.nb_entries == 0",training,117,training,True,100.00000000000004,N/A
"def contains(self, x):
<mask>:
        as_int = x
    elif isinstance(x, (np.generic, np.ndarray)) and (x.dtype.kind in np.typecodes['AllInteger'] and x.shape == ()):
        as_int = int(x)
    else:
        return False
    return as_int >= 0 and as_int < self.n","isinstance(x, int)",33,"isinstance(x, int)",True,100.00000000000004,N/A
"def step(self, action):
    rewards = [[0, 3], [1, 2]]
    assert self.action_space.contains(action)
<mask>:
        self.firstAction = action
        reward = 0
        done = False
    else:
        reward = rewards[self.firstAction][action]
        done = True
    return (self.get_obs(), reward, done, {})",self.firstAction is None,33,self.firstAction is None,True,100.00000000000004,N/A
"def get_earliest_class_that_defined_member(member, cls):
    ancestors = get_classes_ancestors([cls])
    result = None
    for ancestor in ancestors:
<mask>:
            result = ancestor
    if not result:
        return cls
    return result",member in dir(ancestor),24,ancestor.get_name() == member,False,5.934202609760488,N/A
"def get_classes_ancestors(classes):
    ancestors = []
    for cls in classes:
        ancestors += cls.__bases__
    filtered_ancestors = []
    for ancestor in ancestors:
<mask>:
            continue
        filtered_ancestors.append(ancestor)
    if filtered_ancestors:
        return filtered_ancestors + get_classes_ancestors(filtered_ancestors)
    else:
        return filtered_ancestors",ancestor.__name__ in ['object'],31,"ancestor in filter(lambda x: isinstance(x, ancestor, ClassifierMixin))",False,2.908317710573757,N/A
"def get_function_signature(function, method=True):
    signature = getattr(function, '_legacy_support_signature', None)
<mask>:
        signature = inspect.getargspec(function)
    defaults = signature.defaults
    if method:
        args = signature.args[1:]
    else:
        args = signature.args
    if defaults:
        kwargs = zip(args[-len(defaults):], defaults)
        args = args[:-len(defaults)]
    else:
        kwargs = []
    st = '%s.%s(' % (function.__module__, function.__name__)
    for a in args:
        st += str(a) + ', '
    for a, v in kwargs:
        if isinstance(v, str):
            v = ""'"" + v + ""'""
        st += str(a) + '=' + str(v) + ', '
    if kwargs or args:
        return st[:-2] + ')'
    else:
        return st + ')'",signature is None,91,signature is None,True,100.00000000000004,N/A
"def visualize_log(filename, figsize=None, output=None):
    with open(filename, 'r') as f:
        data = json.load(f)
<mask>:
        raise ValueError('Log file ""{}"" does not contain the ""episode"" key.'.format(filename))
    episodes = data['episode']
    keys = sorted(list(set(data.keys()).difference(set(['episode']))))
    if figsize is None:
        figsize = (15.0, 5.0 * len(keys))
    f, axarr = plt.subplots(len(keys), sharex=True, figsize=figsize)
    for idx, key in enumerate(keys):
        axarr[idx].plot(episodes, data[key])
        axarr[idx].set_ylabel(key)
    plt.xlabel('episodes')
    plt.tight_layout()
    if output is None:
        plt.show()
    else:
        plt.savefig(output)",'episode' not in data,62,'episode' not in data,True,100.00000000000004,N/A
"def sample_batch_indexes(low, high, size):
    """"""Return a sample of (size) unique elements between low and high

        # Argument
            low (int): The minimum value for our samples
            high (int): The maximum value for our samples
            size (int): The number of samples to pick

        # Returns
            A list of samples of length size, with values between low and high
        """"""
<mask>:
        try:
            r = xrange(low, high)
        except NameError:
            r = range(low, high)
        batch_idxs = random.sample(r, size)
    else:
        warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')
        batch_idxs = np.random.random_integers(low, high - 1, size=size)
    assert len(batch_idxs) == size
    return batch_idxs",high - low >= size,102,low < high,False,12.753667906901528,N/A
"def __getitem__(self, idx):
    """"""Return element of buffer at specific index

        # Argument
            idx (int): Index wanted

        # Returns
            The element of buffer at given index
        """"""
<mask>:
        raise KeyError()
    return self.data[idx]",idx < 0 or idx >= self.length(),31,idx >= len(self.data),False,17.39350277271197,N/A
"def zeroed_observation(observation):
    """"""Return an array of zeros with same shape as given observation

    # Argument
        observation (list): List of observation
    
    # Return
        A np.ndarray of zeros with observation.shape
    """"""
<mask>:
        return np.zeros(observation.shape)
    elif hasattr(observation, '__iter__'):
        out = []
        for x in observation:
            out.append(zeroed_observation(x))
        return out
    else:
        return 0.0","hasattr(observation, 'shape')",48,"isinstance(observation, list)",False,32.46679154750991,N/A
"def get_recent_state(self, current_observation):
    """"""Return list of last observations

        # Argument
            current_observation (object): Last observation

        # Returns
            A list of the last observations
        """"""
    state = [current_observation]
    idx = len(self.recent_observations) - 1
    for offset in range(0, self.window_length - 1):
        current_idx = idx - offset
        current_terminal = self.recent_terminals[current_idx - 1] if current_idx - 1 >= 0 else False
<mask>:
            break
        state.insert(0, self.recent_observations[current_idx])
    while len(state) < self.window_length:
        state.insert(0, zeroed_observation(state[0]))
    return state",current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal),68,not current_terminal,False,1.1702651167821572,N/A
"def sample(self, batch_size, batch_idxs=None):
    """"""Return a randomized batch of experiences

        # Argument
            batch_size (int): Size of the all batch
            batch_idxs (int): Indexes to extract
        # Returns
            A list of experiences randomly selected
        """"""
    assert self.nb_entries >= self.window_length + 2, 'not enough entries in the memory'
<mask>:
        batch_idxs = sample_batch_indexes(self.window_length, self.nb_entries - 1, size=batch_size)
    batch_idxs = np.array(batch_idxs) + 1
    assert np.min(batch_idxs) >= self.window_length + 1
    assert np.max(batch_idxs) < self.nb_entries
    assert len(batch_idxs) == batch_size
    experiences = []
    for idx in batch_idxs:
        terminal0 = self.terminals[idx - 2]
        while terminal0:
            idx = sample_batch_indexes(self.window_length + 1, self.nb_entries, size=1)[0]
            terminal0 = self.terminals[idx - 2]
        assert self.window_length + 1 <= idx < self.nb_entries
        state0 = [self.observations[idx - 1]]
        for offset in range(0, self.window_length - 1):
            current_idx = idx - 2 - offset
            assert current_idx >= 1
            current_terminal = self.terminals[current_idx - 1]
            if current_terminal and (not self.ignore_episode_boundaries):
                break
            state0.insert(0, self.observations[current_idx])
        while len(state0) < self.window_length:
            state0.insert(0, zeroed_observation(state0[0]))
        action = self.actions[idx - 1]
        reward = self.rewards[idx - 1]
        terminal1 = self.terminals[idx - 1]
        state1 = [np.copy(x) for x in state0[1:]]
        state1.append(self.observations[idx])
        assert len(state0) == self.window_length
        assert len(state1) == len(state0)
        experiences.append(Experience(state0=state0, action=action, reward=reward, state1=state1, terminal1=terminal1))
    assert len(experiences) == batch_size
    return experiences",batch_idxs is None,190,batch_idxs is None,True,100.00000000000004,N/A
"def clone_optimizer(optimizer):
<mask>:
        return optimizers.get(optimizer)
    params = dict([(k, v) for k, v in optimizer.get_config().items()])
    config = {'class_name': optimizer.__class__.__name__, 'config': params}
    if hasattr(optimizers, 'optimizer_from_config'):
        clone = optimizers.optimizer_from_config(config)
    else:
        clone = optimizers.deserialize(config)
    return clone",type(optimizer) is str,32,"hasattr(optimizers, 'get(optimizer)')",False,15.619699684601283,N/A
"def get_object_config(o):
<mask>:
        return None
    config = {'class_name': o.__class__.__name__, 'config': o.get_config()}
    return config",o is None,13,o is None,True,100.00000000000004,N/A
"def huber_loss(y_true, y_pred, clip_value):
    assert clip_value > 0.0
    x = y_true - y_pred
<mask>:
        return 0.5 * K.square(x)
    condition = K.abs(x) < clip_value
    squared_loss = 0.5 * K.square(x)
    linear_loss = clip_value * (K.abs(x) - 0.5 * clip_value)
    if K.backend() == 'tensorflow':
        import tensorflow as tf
        if hasattr(tf, 'select'):
            return tf.select(condition, squared_loss, linear_loss)
        else:
            return tf.where(condition, squared_loss, linear_loss)
    elif K.backend() == 'theano':
        from theano import tensor as T
        return T.switch(condition, squared_loss, linear_loss)
    else:
        raise RuntimeError('Unknown backend ""{}"".'.format(K.backend()))",np.isinf(clip_value),76,clip_value == 0.0,False,21.64910073203448,N/A
"def update(self, x):
<mask>:
        x = x.reshape(-1, *self.shape)
    assert x.shape[1:] == self.shape
    self._count += x.shape[0]
    self._sum += np.sum(x, axis=0)
    self._sumsq += np.sum(np.square(x), axis=0)
    self.mean = self._sum / float(self._count)
    self.std = np.sqrt(np.maximum(np.square(self.eps), self._sumsq / float(self._count) - np.square(self.mean)))",x.ndim == len(self.shape),36,x.shape[1:] == self.shape,False,18.52797255583095,N/A
"def __init__(self, inner_policy, attr, value_max, value_min, value_test, nb_steps):
<mask>:
        raise ValueError('Policy does not have attribute ""{}"".'.format(attr))
    super(LinearAnnealedPolicy, self).__init__()
    self.inner_policy = inner_policy
    self.attr = attr
    self.value_max = value_max
    self.value_min = value_min
    self.value_test = value_test
    self.nb_steps = nb_steps","not hasattr(inner_policy, attr)",36,attr not in inner_policy.attributes,False,20.612390921238426,N/A
"def get_current_value(self):
    """"""Return current annealing value

        # Returns
            Value to use in annealing
        """"""
<mask>:
        a = -float(self.value_max - self.value_min) / float(self.nb_steps)
        b = float(self.value_max)
        value = max(self.value_min, a * float(self.agent.step) + b)
    else:
        value = self.value_test
    return value",self.agent.training,39,self.value_min and self.value_max,False,8.29519350710986,N/A
"def select_action(self, q_values):
    """"""Return the selected action

        # Arguments
            q_values (np.ndarray): List of the estimations of Q for each action

        # Returns
            Selection action
        """"""
    assert q_values.ndim == 1
    nb_actions = q_values.shape[0]
<mask>:
        action = np.random.randint(0, nb_actions)
    else:
        action = np.argmax(q_values)
    return action",np.random.uniform() < self.eps,43,nb_actions > 0,False,0.0,N/A
"def select_action(self, q_values):
    """"""Return the selected action
        The selected action follows the BoltzmannQPolicy with probability epsilon
        or return the Greedy Policy with probability (1 - epsilon)

        # Arguments
            q_values (np.ndarray): List of the estimations of Q for each action

        # Returns
            Selection action
        """"""
    assert q_values.ndim == 1
    q_values = q_values.astype('float64')
    nb_actions = q_values.shape[0]
<mask>:
        exp_values = np.exp(np.clip(q_values / self.tau, self.clip[0], self.clip[1]))
        probs = exp_values / np.sum(exp_values)
        action = np.random.choice(range(nb_actions), p=probs)
    else:
        action = np.argmax(q_values)
    return action",np.random.uniform() < self.eps,77,nb_actions > 1,False,0.0,N/A
"def select_action(self, q_values):
    """"""Return the selected action

        # Arguments
            q_values (np.ndarray): List of the estimations of Q for each action

        # Returns
            Selection action
        """"""
    assert self.agent.training, 'BoltzmannGumbelQPolicy should only be used for training, not testing'
    assert q_values.ndim == 1, q_values.ndim
    q_values = q_values.astype('float64')
<mask>:
        self.action_counts = np.ones(q_values.shape)
    assert self.action_counts is not None, self.agent.step
    assert self.action_counts.shape == q_values.shape, (self.action_counts.shape, q_values.shape)
    beta = self.C / np.sqrt(self.action_counts)
    Z = np.random.gumbel(size=q_values.shape)
    perturbation = beta * Z
    perturbed_q_values = q_values + perturbation
    action = np.argmax(perturbed_q_values)
    self.action_counts[action] += 1
    return action",self.agent.step == 0,86,self.action_counts is None,False,11.386050660556927,N/A
"def __init__(self, mu, sigma, sigma_min, n_steps_annealing):
    self.mu = mu
    self.sigma = sigma
    self.n_steps = 0
<mask>:
        self.m = -float(sigma - sigma_min) / float(n_steps_annealing)
        self.c = sigma
        self.sigma_min = sigma_min
    else:
        self.m = 0.0
        self.c = sigma
        self.sigma_min = sigma",sigma_min is not None,39,sigma_min != 0,False,30.213753973567677,N/A
"def _set_env(self, env):
    """""" Set environment for each callback in callbackList """"""
    for callback in self.callbacks:
<mask>:
            callback._set_env(env)","callable(getattr(callback, '_set_env', None))",18,"hasattr(callback, '_set_env')",False,47.46358914289605,N/A
"def on_episode_begin(self, episode, logs={}):
    """""" Called at beginning of each episode for each callback in callbackList""""""
    for callback in self.callbacks:
<mask>:
            callback.on_episode_begin(episode, logs=logs)
        else:
            callback.on_epoch_begin(episode, logs=logs)","callable(getattr(callback, 'on_episode_begin', None))",26,"isinstance(callback, LooseEpochCallback)",False,7.244320397501573,N/A
"def on_episode_end(self, episode, logs={}):
    """""" Called at end of each episode for each callback in callbackList""""""
    for callback in self.callbacks:
<mask>:
            callback.on_episode_end(episode, logs=logs)
        else:
            callback.on_epoch_end(episode, logs=logs)","callable(getattr(callback, 'on_episode_end', None))",26,"isinstance(callback, EpochCallback)",False,7.244320397501573,N/A
"def on_step_begin(self, step, logs={}):
    """""" Called at beginning of each step for each callback in callbackList""""""
    for callback in self.callbacks:
<mask>:
            callback.on_step_begin(step, logs=logs)
        else:
            callback.on_batch_begin(step, logs=logs)","callable(getattr(callback, 'on_step_begin', None))",26,"isinstance(callback, Step)",False,7.244320397501573,N/A
"def on_step_end(self, step, logs={}):
    """""" Called at end of each step for each callback in callbackList""""""
    for callback in self.callbacks:
<mask>:
            callback.on_step_end(step, logs=logs)
        else:
            callback.on_batch_end(step, logs=logs)","callable(getattr(callback, 'on_step_end', None))",26,"isinstance(callback, Step)",False,7.244320397501573,N/A
"def process_state_batch(self, batch):
<mask>:
        self.normalizer = WhiteningNormalizer(shape=batch.shape[1:], dtype=batch.dtype)
    self.normalizer.update(batch)
    return self.normalizer.normalize(batch)",self.normalizer is None,11,not self.normalizer,False,46.30777161991026,N/A
"def select_action(self, state, stochastic=False):
    batch = np.array([state])
<mask>:
        batch = self.processor.process_state_batch(batch)
    action = self.model.predict_on_batch(batch).flatten()
    if stochastic or self.training:
        return np.random.choice(np.arange(self.nb_actions), p=np.exp(action) / np.sum(np.exp(action)))
    return np.argmax(action)",self.processor is not None,25,self.processor,False,36.78794411714425,N/A
"def update_theta(self, theta):
<mask>:
        assert theta.shape == self.theta.shape, 'Invalid theta, shape is {0} but should be {1}'.format(theta.shape, self.theta.shape)
        assert not np.isnan(theta).any(), 'Invalid theta, NaN encountered'
        assert (theta[self.num_weights:] >= 0.0).all(), 'Invalid theta, standard deviations must be nonnegative'
        self.theta = theta
    else:
        means = np.ones(self.num_weights) * self.init_mean
        stdevs = np.ones(self.num_weights) * self.init_stdev
        self.theta = np.hstack((means, stdevs))",theta is not None,54,self.num_weights == 0,False,0.0,N/A
"def backward(self, reward, terminal):
<mask>:
        self.memory.append(self.recent_observation, self.recent_action, reward, terminal, training=self.training)
    metrics = [np.nan for _ in self.metrics_names]
    if not self.training:
        return metrics
    if terminal:
        params = self.get_weights_flat(self.model.get_weights())
        self.memory.finalize_episode(params)
        if self.step > self.nb_steps_warmup and self.episode % self.train_interval == 0:
            params, reward_totals = self.memory.sample(self.batch_size)
            best_idx = np.argsort(np.array(reward_totals))[-self.num_best:]
            best = np.vstack([params[i] for i in best_idx])
            if reward_totals[best_idx[-1]] > self.best_seen[0]:
                self.best_seen = (reward_totals[best_idx[-1]], params[best_idx[-1]])
            metrics = [np.mean(np.array(reward_totals)[best_idx])]
            if self.processor is not None:
                metrics += self.processor.metrics
            min_std = self.noise_ampl * np.exp(-self.step * self.noise_decay_const)
            mean = np.mean(best, axis=0)
            std = np.std(best, axis=0) + min_std
            new_theta = np.hstack((mean, std))
            self.update_theta(new_theta)
        self.choose_weights()
        self.episode += 1
    return metrics",self.step % self.memory_interval == 0,99,self.recent_observation is not None and self.recent_action is not None,False,6.839596061560946,N/A
"@property
def metrics_names(self):
    names = ['mean_best_reward']
<mask>:
        names += self.processor.metrics_names[:]
    return names",self.processor is not None,12,self.processor,False,36.78794411714425,N/A
"def __init__(self, model, nb_actions, policy=None, test_policy=None, gamma=0.99, nb_steps_warmup=10, train_interval=1, delta_clip=np.inf, *args, **kwargs):
    super(SarsaAgent, self).__init__(*args, **kwargs)
<mask>:
        policy = EpsGreedyQPolicy()
    if test_policy is None:
        test_policy = GreedyQPolicy()
    self.model = model
    self.nb_actions = nb_actions
    self.policy = policy
    self.test_policy = test_policy
    self.gamma = gamma
    self.nb_steps_warmup = nb_steps_warmup
    self.train_interval = train_interval
    self.delta_clip = delta_clip
    self.compiled = False
    self.actions = None
    self.observations = None
    self.rewards = None",policy is None,62,policy is None,True,100.00000000000004,N/A
"def process_state_batch(self, batch):
    batch = np.array(batch)
<mask>:
        return batch
    return self.processor.process_state_batch(batch)",self.processor is None,11,self.processor is None,True,100.00000000000004,N/A
"def reset_states(self):
    self.actions = collections.deque(maxlen=2)
    self.observations = collections.deque(maxlen=2)
    self.rewards = collections.deque(maxlen=2)
<mask>:
        self.model.reset_states()",self.compiled,13,"hasattr(self.model, 'reset_states')",False,8.392229812593097,N/A
"def forward(self, observation):
    q_values = self.compute_q_values([observation])
<mask>:
        action = self.policy.select_action(q_values=q_values)
    else:
        action = self.test_policy.select_action(q_values=q_values)
    self.observations.append(observation)
    self.actions.append(action)
    return action",self.training,18,self.test_policy is None,False,13.134549472120788,N/A
"def backward(self, reward, terminal):
    metrics = [np.nan for _ in self.metrics_names]
<mask>:
        return metrics
    if self.step > self.nb_steps_warmup and self.step % self.train_interval == 0:
        self.rewards.append(reward)
        if len(self.observations) < 2:
            return metrics
        state0_batch = [self.observations[0]]
        reward_batch = [self.rewards[0]]
        action_batch = [self.actions[0]]
        terminal1_batch = [0.0] if terminal else [1.0]
        state1_batch = [self.observations[1]]
        action1_batch = [self.actions[1]]
        state0_batch = self.process_state_batch(state0_batch)
        state1_batch = self.process_state_batch(state1_batch)
        terminal1_batch = np.array(terminal1_batch)
        reward_batch = np.array(reward_batch)
        assert reward_batch.shape == (1,)
        assert terminal1_batch.shape == reward_batch.shape
        assert len(action_batch) == len(reward_batch)
        batch = self.process_state_batch(state1_batch)
        q_values = self.compute_q_values(batch)
        q_values = q_values.reshape((1, self.nb_actions))
        q_batch = q_values[0, action1_batch]
        assert q_batch.shape == (1,)
        targets = np.zeros((1, self.nb_actions))
        dummy_targets = np.zeros((1,))
        masks = np.zeros((1, self.nb_actions))
        discounted_reward_batch = self.gamma * q_batch
        discounted_reward_batch *= terminal1_batch
        assert discounted_reward_batch.shape == reward_batch.shape
        Rs = reward_batch + discounted_reward_batch
        for idx, (target, mask, R, action) in enumerate(zip(targets, masks, Rs, action_batch)):
            target[action] = R
            dummy_targets[idx] = R
            mask[action] = 1.0
        targets = np.array(targets).astype('float32')
        masks = np.array(masks).astype('float32')
        state0_batch = state0_batch.reshape((1,) + state0_batch.shape)
        ins = [state0_batch] if type(self.model.input) is not list else state0_batch
        metrics = self.trainable_model.train_on_batch(ins + [targets, masks], [dummy_targets, targets])
        metrics = [metric for idx, metric in enumerate(metrics) if idx not in (1, 2)]
        metrics += self.policy.metrics
        if self.processor is not None:
            metrics += self.processor.metrics
    return metrics",not self.training,199,reward == 0.0,False,0.0,N/A
"def __init__(self, nb_actions, actor, critic, critic_action_input, memory, gamma=0.99, batch_size=32, nb_steps_warmup_critic=1000, nb_steps_warmup_actor=1000, train_interval=1, memory_interval=1, delta_range=None, delta_clip=np.inf, random_process=None, custom_model_objects={}, target_model_update=0.001, **kwargs):
<mask>:
        raise ValueError('Actor ""{}"" has more than one output. DDPG expects an actor that has a single output.'.format(actor))
    if hasattr(critic.output, '__len__') and len(critic.output) > 1:
        raise ValueError('Critic ""{}"" has more than one output. DDPG expects a critic that has a single output.'.format(critic))
    if critic_action_input not in critic.input:
        raise ValueError('Critic ""{}"" does not have designated action input ""{}"".'.format(critic, critic_action_input))
    if not hasattr(critic.input, '__len__') or len(critic.input) < 2:
        raise ValueError('Critic ""{}"" does not have enough inputs. The critic must have at exactly two inputs, one for the action and one for the observation.'.format(critic))
    super(DDPGAgent, self).__init__(**kwargs)
    if target_model_update < 0:
        raise ValueError('`target_model_update` must be >= 0.')
    elif target_model_update >= 1:
        target_model_update = int(target_model_update)
    else:
        target_model_update = float(target_model_update)
    if delta_range is not None:
        warnings.warn(""`delta_range` is deprecated. Please use `delta_clip` instead, which takes a single scalar. For now we're falling back to `delta_range[1] = {}`"".format(delta_range[1]))
        delta_clip = delta_range[1]
    self.nb_actions = nb_actions
    self.nb_steps_warmup_actor = nb_steps_warmup_actor
    self.nb_steps_warmup_critic = nb_steps_warmup_critic
    self.random_process = random_process
    self.delta_clip = delta_clip
    self.gamma = gamma
    self.target_model_update = target_model_update
    self.batch_size = batch_size
    self.train_interval = train_interval
    self.memory_interval = memory_interval
    self.custom_model_objects = custom_model_objects
    self.actor = actor
    self.critic = critic
    self.critic_action_input = critic_action_input
    self.critic_action_input_idx = self.critic.input.index(critic_action_input)
    self.memory = memory
    self.compiled = False
    self.reset_states()","hasattr(actor.output, '__len__') and len(actor.output) > 1",214,"hasattr(actor, '__len__') and len(actor.output) > 1",False,83.40614345461115,N/A
"def compile(self, optimizer, metrics=[]):
    metrics += [mean_q]
<mask>:
        if len(optimizer) != 2:
            raise ValueError('More than two optimizers provided. Please only provide a maximum of two optimizers, the first one for the actor and the second one for the critic.')
        actor_optimizer, critic_optimizer = optimizer
    else:
        actor_optimizer = optimizer
        critic_optimizer = clone_optimizer(optimizer)
    if type(actor_optimizer) is str:
        actor_optimizer = optimizers.get(actor_optimizer)
    if type(critic_optimizer) is str:
        critic_optimizer = optimizers.get(critic_optimizer)
    assert actor_optimizer != critic_optimizer
    if len(metrics) == 2 and hasattr(metrics[0], '__len__') and hasattr(metrics[1], '__len__'):
        actor_metrics, critic_metrics = metrics
    else:
        actor_metrics = critic_metrics = metrics

    def clipped_error(y_true, y_pred):
        return K.mean(huber_loss(y_true, y_pred, self.delta_clip), axis=-1)
    self.target_actor = clone_model(self.actor, self.custom_model_objects)
    self.target_actor.compile(optimizer='sgd', loss='mse')
    self.target_critic = clone_model(self.critic, self.custom_model_objects)
    self.target_critic.compile(optimizer='sgd', loss='mse')
    self.actor.compile(optimizer='sgd', loss='mse')
    if self.target_model_update < 1.0:
        critic_updates = get_soft_target_model_updates(self.target_critic, self.critic, self.target_model_update)
        critic_optimizer = AdditionalUpdatesOptimizer(critic_optimizer, critic_updates)
    self.critic.compile(optimizer=critic_optimizer, loss=clipped_error, metrics=critic_metrics)
    combined_inputs = []
    state_inputs = []
    for i in self.critic.input:
        if i == self.critic_action_input:
            combined_inputs.append([])
        else:
            combined_inputs.append(i)
            state_inputs.append(i)
    combined_inputs[self.critic_action_input_idx] = self.actor(state_inputs)
    combined_output = self.critic(combined_inputs)
    updates = actor_optimizer.get_updates(params=self.actor.trainable_weights, loss=-K.mean(combined_output))
    if self.target_model_update < 1.0:
        updates += get_soft_target_model_updates(self.target_actor, self.actor, self.target_model_update)
    updates += self.actor.updates
    if K.backend() == 'tensorflow':
        self.actor_train_fn = K.function(state_inputs + [K.learning_phase()], [self.actor(state_inputs)], updates=updates)
    else:
        if self.uses_learning_phase:
            state_inputs += [K.learning_phase()]
        self.actor_train_fn = K.function(state_inputs, [self.actor(state_inputs)], updates=updates)
    self.actor_optimizer = actor_optimizer
    self.compiled = True","type(optimizer) in (list, tuple)",194,"isinstance(optimizer, dict)",False,9.911450612811139,N/A
"def reset_states(self):
<mask>:
        self.random_process.reset_states()
    self.recent_action = None
    self.recent_observation = None
    if self.compiled:
        self.actor.reset_states()
        self.critic.reset_states()
        self.target_actor.reset_states()
        self.target_critic.reset_states()",self.random_process is not None,16,self.random_process,False,54.88116360940266,N/A
"def select_action(self, state):
    batch = self.process_state_batch([state])
    action = self.actor.predict_on_batch(batch).flatten()
    assert action.shape == (self.nb_actions,)
<mask>:
        noise = self.random_process.sample()
        assert noise.shape == action.shape
        action += noise
    return action",self.training and self.random_process is not None,26,self.random_process,False,24.659696394160658,N/A
"def __init__(self, nb_actions, memory, gamma=0.99, batch_size=32, nb_steps_warmup=1000, train_interval=1, memory_interval=1, target_model_update=10000, delta_range=None, delta_clip=np.inf, custom_model_objects={}, **kwargs):
    super(AbstractDQNAgent, self).__init__(**kwargs)
<mask>:
        raise ValueError('`target_model_update` must be >= 0.')
    elif target_model_update >= 1:
        target_model_update = int(target_model_update)
    else:
        target_model_update = float(target_model_update)
    if delta_range is not None:
        warnings.warn(""`delta_range` is deprecated. Please use `delta_clip` instead, which takes a single scalar. For now we're falling back to `delta_range[1] = {}`"".format(delta_range[1]))
        delta_clip = delta_range[1]
    self.nb_actions = nb_actions
    self.gamma = gamma
    self.batch_size = batch_size
    self.nb_steps_warmup = nb_steps_warmup
    self.train_interval = train_interval
    self.memory_interval = memory_interval
    self.target_model_update = target_model_update
    self.delta_clip = delta_clip
    self.custom_model_objects = custom_model_objects
    self.memory = memory
    self.compiled = False",target_model_update < 0,96,target_model_update <= 0,False,70.71067811865478,N/A
"def __init__(self, model, policy=None, test_policy=None, enable_double_dqn=False, enable_dueling_network=False, dueling_type='avg', *args, **kwargs):
    super(DQNAgent, self).__init__(*args, **kwargs)
<mask>:
        raise ValueError('Model ""{}"" has more than one output. DQN expects a model that has a single output.'.format(model))
    if model.output._keras_shape != (None, self.nb_actions):
        raise ValueError('Model output ""{}"" has invalid shape. DQN expects a model that has one dimension for each action, in this case {}.'.format(model.output, self.nb_actions))
    self.enable_double_dqn = enable_double_dqn
    self.enable_dueling_network = enable_dueling_network
    self.dueling_type = dueling_type
    if self.enable_dueling_network:
        layer = model.layers[-2]
        nb_action = model.output._keras_shape[-1]
        y = Dense(nb_action + 1, activation='linear')(layer.output)
        if self.dueling_type == 'avg':
            outputlayer = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:] - K.mean(a[:, 1:], axis=1, keepdims=True), output_shape=(nb_action,))(y)
        elif self.dueling_type == 'max':
            outputlayer = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:] - K.max(a[:, 1:], axis=1, keepdims=True), output_shape=(nb_action,))(y)
        elif self.dueling_type == 'naive':
            outputlayer = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:], output_shape=(nb_action,))(y)
        else:
            assert False, ""dueling_type must be one of {'avg','max','naive'}""
        model = Model(inputs=model.input, outputs=outputlayer)
    self.model = model
    if policy is None:
        policy = EpsGreedyQPolicy()
    if test_policy is None:
        test_policy = GreedyQPolicy()
    self.policy = policy
    self.test_policy = test_policy
    self.reset_states()","hasattr(model.output, '__len__') and len(model.output) > 1",174,len(model.outputs) > 1,False,7.667748342246425,N/A
"def get_config(self):
    config = super(DQNAgent, self).get_config()
    config['enable_double_dqn'] = self.enable_double_dqn
    config['dueling_type'] = self.dueling_type
    config['enable_dueling_network'] = self.enable_dueling_network
    config['model'] = get_object_config(self.model)
    config['policy'] = get_object_config(self.policy)
    config['test_policy'] = get_object_config(self.test_policy)
<mask>:
        config['target_model'] = get_object_config(self.target_model)
    return config",self.compiled,30,self.target_model is not None,False,11.044795567078939,N/A
"def compile(self, optimizer, metrics=[]):
    metrics += [mean_q]
    self.target_model = clone_model(self.model, self.custom_model_objects)
    self.target_model.compile(optimizer='sgd', loss='mse')
    self.model.compile(optimizer='sgd', loss='mse')
<mask>:
        updates = get_soft_target_model_updates(self.target_model, self.model, self.target_model_update)
        optimizer = AdditionalUpdatesOptimizer(optimizer, updates)

    def clipped_masked_error(args):
        y_true, y_pred, mask = args
        loss = huber_loss(y_true, y_pred, self.delta_clip)
        loss *= mask
        return K.sum(loss, axis=-1)
    y_pred = self.model.output
    y_true = Input(name='y_true', shape=(self.nb_actions,))
    mask = Input(name='mask', shape=(self.nb_actions,))
    loss_out = Lambda(clipped_masked_error, output_shape=(1,), name='loss')([y_true, y_pred, mask])
    ins = [self.model.input] if type(self.model.input) is not list else self.model.input
    trainable_model = Model(inputs=ins + [y_true, mask], outputs=[loss_out, y_pred])
    assert len(trainable_model.output_names) == 2
    combined_metrics = {trainable_model.output_names[1]: metrics}
    losses = [lambda y_true, y_pred: y_pred, lambda y_true, y_pred: K.zeros_like(y_pred)]
    trainable_model.compile(optimizer=optimizer, loss=losses, metrics=combined_metrics)
    self.trainable_model = trainable_model
    self.compiled = True",self.target_model_update < 1.0,106,self.target_model_update is not None,False,63.894310424627285,N/A
"def make_gym_env(env_id, num_env=2, seed=123, wrapper_kwargs=None, start_index=0):
    """"""
    Create a wrapped, SubprocVecEnv for Gym Environments.
    """"""
<mask>:
        wrapper_kwargs = {}

    def make_env(rank):

        def _thunk():
            env = gym.make(env_id)
            env.seed(seed + rank)
            return env
        return _thunk
    set_global_seeds(seed)
    return SubprocVecEnv([make_env(i + start_index) for i in range(num_env)])",wrapper_kwargs is None,42,wrapper_kwargs is None,True,100.00000000000004,N/A
"def worker(remote, parent_remote, env_fn_wrapper):
    parent_remote.close()
    env = env_fn_wrapper.x()
    while True:
        cmd, data = remote.recv()
<mask>:
            ob, reward, done, info = env.step(data)
            if done:
                ob = env.reset()
            remote.send((ob, reward, done, info))
        elif cmd == 'reset':
            ob = env.reset()
            remote.send(ob)
        elif cmd == 'render':
            remote.send(env.render(mode='rgb_array'))
        elif cmd == 'close':
            remote.close()
            break
        elif cmd == 'get_spaces':
            remote.send((env.observation_space, env.action_space))
        elif cmd == 'seed':
            val = env.seed(data)
            remote.send(val)
        else:
            raise NotImplementedError",cmd == 'step',66,cmd == 'step',True,100.00000000000004,N/A
"def close(self):
<mask>:
        return
    if self.waiting:
        for remote in self.remotes:
            remote.recv()
    for remote in self.remotes:
        remote.send(('close', None))
    for p in self.ps:
        p.join()
    self.closed = True",self.closed,25,self.closed,True,100.00000000000004,N/A
"def ignored(name):
<mask>:
        return False
    if name in FISH_READONLY:
        return True
    if name in IGNORED or name.startswith('BASH_FUNC'):
        return True
    if name.startswith('%'):
        return True
    return False",name == 'PWD',25,name in NO_EXCLUDE,False,10.682175159905848,N/A
"def gen_script():
    env_reader = ""%s -c 'import os,json; print(json.dumps({k:v for k,v in os.environ.items()}))'"" % sys.executable
    args = [BASH, '-c', env_reader]
    output = subprocess.check_output(args, universal_newlines=True)
    old_env = output.strip()
    pipe_r, pipe_w = os.pipe()
<mask>:
        os.set_inheritable(pipe_w, True)
    command = 'eval $1 && ({}; alias) >&{}'.format(env_reader, pipe_w)
    args = [BASH, '-c', command, 'bass', ' '.join(sys.argv[1:])]
    p = subprocess.Popen(args, universal_newlines=True, close_fds=False)
    os.close(pipe_w)
    with os.fdopen(pipe_r) as f:
        new_env = f.readline()
        alias_str = f.read()
    if p.wait() != 0:
        raise subprocess.CalledProcessError(returncode=p.returncode, cmd=' '.join(sys.argv[1:]), output=new_env + alias_str)
    new_env = new_env.strip()
    old_env = json.loads(old_env)
    new_env = json.loads(new_env)
    script_lines = []
    for k, v in new_env.items():
        if ignored(k):
            continue
        v1 = old_env.get(k)
        if not v1:
            script_lines.append(comment('adding %s=%s' % (k, v)))
        elif v1 != v:
            script_lines.append(comment('updating %s=%s -> %s' % (k, v1, v)))
            if k == 'PWD':
                script_lines.append('cd %s' % escape(v))
                continue
        else:
            continue
        if k == 'PATH':
            value = ' '.join([escape(directory) for directory in v.split(':')])
        else:
            value = escape(v)
        script_lines.append('set -g -x %s %s' % (k, value))
    for var in set(old_env.keys()) - set(new_env.keys()):
        script_lines.append(comment('removing %s' % var))
        script_lines.append('set -e %s' % var)
    script = '\n'.join(script_lines)
    alias_lines = []
    for line in alias_str.splitlines():
        _, rest = line.split(None, 1)
        k, v = rest.split('=', 1)
        alias_lines.append('alias ' + escape_identifier(k) + '=' + v)
    alias = '\n'.join(alias_lines)
    return script + '\n' + alias","sys.version_info >= (3, 4)",208,os.get_inheritable(pipe_w) is None,False,4.789232204309912,N/A
"def set_background(file_path):
    de = get_desktop_environment()
<mask>:
        subprocess.call(['osascript', '-e', 'tell application ""System Events""\nset theDesktops to a reference to every desktop\nrepeat with aDesktop in theDesktops\nset the picture of aDesktop to ""' + file_path + '""\nend repeat\nend tell'])
    else:
        fetch_envvar('DBUS_SESSION_BUS_ADDRESS')
        fetch_envvar('DISPLAY')
        if de in ['gnome', 'unity', 'cinnamon', 'pantheon', 'gnome-classic', 'budgie-desktop']:
            if de == 'unity':
                subprocess.call(['gsettings', 'set', 'org.gnome.desktop.background', 'draw-background', 'false'])
            subprocess.call(['gsettings', 'set', 'org.gnome.desktop.background', 'picture-uri', 'file://' + file_path])
            subprocess.call(['gsettings', 'set', 'org.gnome.desktop.background', 'picture-options', 'scaled'])
            subprocess.call(['gsettings', 'set', 'org.gnome.desktop.background', 'primary-color', '#000000'])
            if de == 'unity':
                assert os.system('bash -c ""gsettings set org.gnome.desktop.background draw-background true""') == 0
        elif de == 'mate':
            subprocess.call(['gsettings', 'set', 'org.mate.background', 'picture-filename', file_path])
        elif de == 'i3':
            subprocess.call(['feh', '--bg-max', file_path])
        elif de == 'xfce4':
            displays = subprocess.getoutput('xfconf-query --channel xfce4-desktop --list | grep last-image').split()
            for display in displays:
                subprocess.call(['xfconf-query', '--channel', 'xfce4-desktop', '--property', display, '--set', file_path])
        elif de == 'lxde':
            subprocess.call(['pcmanfm', '--set-wallpaper', file_path, '--wallpaper-mode=fit'])
        elif de == 'kde':
            if plasma_version() > LooseVersion('5.7'):
                'Command per https://github.com/boramalper/himawaripy/issues/57\n\n                Sets \'FillMode\' to 1, which is ""Scaled, Keep Proportions""\n                Forces \'Color\' to black, which sets the background colour.\n                '
                script = 'var a = desktops();for (i = 0; i < a.length; i++) {{d = a[i];d.wallpaperPlugin = ""org.kde.image"";d.currentConfigGroup = Array(""Wallpaper"", ""org.kde.image"", ""General"");d.writeConfig(""Image"", ""file://{}"");d.writeConfig(""FillMode"", 1);d.writeConfig(""Color"", ""#000"");}}'
                try:
                    subprocess.check_output(['qdbus', 'org.kde.plasmashell', '/PlasmaShell', 'org.kde.PlasmaShell.evaluateScript', script.format(file_path)])
                except subprocess.CalledProcessError as e:
                    if 'Widgets are locked' in e.output.decode('utf-8'):
                        print('Cannot change the wallpaper while widgets are locked! (unlock the widgets)')
                    else:
                        raise e
            else:
                print(""Couldn't detect plasmashell 5.7 or higher."")
        elif has_program('feh'):
            print(""Couldn't detect your desktop environment ('{}'), but you have 'feh' installed so we will use it..."".format(de))
            subprocess.call(['feh', '--bg-max', file_path])
        elif has_program('nitrogen'):
            print(""Couldn't detect your desktop environment ('{}'), but you have 'nitrogen' installed so we will use it..."".format(de))
            subprocess.call(['nitrogen', '--restore'])
        else:
            return False
    return True",de == 'mac',273,de == 'osascript',False,59.460355750136046,N/A
"def get_desktop_environment():
<mask>:
        return 'windows'
    elif sys.platform == 'darwin':
        return 'mac'
    else:
        fetch_envvar('DESKTOP_SESSION')
        desktop_session = os.environ.get('DESKTOP_SESSION')
        if desktop_session is not None:
            desktop_session = desktop_session.lower()
            if desktop_session in ['gnome', 'unity', 'cinnamon', 'mate', 'xfce4', 'lxde', 'fluxbox', 'blackbox', 'openbox', 'icewm', 'jwm', 'afterstep', 'trinity', 'kde', 'pantheon', 'gnome-classic', 'i3', 'budgie-desktop']:
                return desktop_session
            elif 'xfce' in desktop_session or desktop_session.startswith('xubuntu'):
                return 'xfce4'
            elif desktop_session.startswith('ubuntu'):
                return 'unity'
            elif desktop_session.startswith('lubuntu'):
                return 'lxde'
            elif desktop_session.startswith('kubuntu'):
                return 'kde'
            elif desktop_session.startswith('razor'):
                return 'razor-qt'
            elif desktop_session.startswith('wmaker'):
                return 'windowmaker'
            elif desktop_session.startswith('peppermint'):
                return 'gnome'
        fetch_envvar('KDE_FULL_SESSION')
        fetch_envvar('GNOME_DESKTOP_SESSION_ID')
        if os.environ.get('KDE_FULL_SESSION') == 'true':
            return 'kde'
        elif os.environ.get('GNOME_DESKTOP_SESSION_ID'):
            if 'deprecated' not in os.environ.get('GNOME_DESKTOP_SESSION_ID'):
                return 'gnome2'
        elif is_running('xfce-mcs-manage'):
            return 'xfce4'
        elif is_running('ksmserver'):
            return 'kde'
        fetch_envvar('XDG_CURRENT_DESKTOP')
        current_desktop = os.environ.get('XDG_CURRENT_DESKTOP')
        if current_desktop:
            current_desktop = current_desktop.lower()
            if current_desktop in ['gnome', 'unity', 'kde', 'gnome-classic', 'mate']:
                return current_desktop
            elif current_desktop == 'xfce':
                return 'xfce4'
            elif current_desktop == 'x-cinnamon':
                return 'cinnamon'
    return 'unknown'","sys.platform in ['win32', 'cygwin']",137,sys.platform == 'win32',False,19.692104496063735,N/A
"def fetch_envvar(varname):
    """"""
    When himawaripy is called by cron or an init, the environment variables that
    are available to us is severely limited, causing problems with some
    processes we call (such as gsetings [due to lack of
    $DBUS_SESSION_BUS_ADDRESS for instance]).

    To try fix this issue as much as we can, without resorting to additional
    shell scripts, we try fetching values of the environment variables we are
    interested in (before we access them!) from another process. pulseauido
    seemed to me a good choice, for it's available virtually on all desktop
    installations (regardless of DE), and seem to have all the environment
    variables that we are (potentially) interested in (*e.g.* gdm does NOT have
    $DISPLAY whereas pulseaudio do!)
    """"""
<mask>:
        return
    val = os.popen('bash -c ""grep -z ^{}= /proc/$(pgrep -n pulseaudio)/environ | cut -d= -f2-""'.format(varname)).read().strip('\x00\n')
    if val:
        print('Fetched env. var. {} as `{}`'.format(varname, val))
        os.environ[varname] = val
    else:
        print('Could NOT retrieve env. var. {}'.format(varname))",varname in os.environ,152,os.getenv('DISPLAY') != varname,False,10.552670315936318,N/A
"def calculate_time_offset(latest_date, auto, preferred_offset):
<mask>:
        preferred_offset = int(datetime.now(tzlocal()).strftime('%z')[0:3])
        print('Detected offset: UTC{:+03d}:00'.format(preferred_offset))
        if 11 >= preferred_offset > 10:
            preferred_offset = 10
            print('Offset is greater than +10, +10 will be used...')
        elif 12 >= preferred_offset > 11:
            preferred_offset = -12
            print('Offset is greater than +10, -12 will be used...')
    himawari_offset = 10
    offset = int(preferred_offset - himawari_offset)
    offset_tmp = datetime.fromtimestamp(mktime(latest_date)) + timedelta(hours=offset)
    offset_time = offset_tmp.timetuple()
    return offset_time",auto,65,preferred_offset is None or auto,False,6.567274736060395,N/A
"def download_chunk(args):
    global counter
    x, y, latest, level = args
    url_format = 'https://himawari8-dl.nict.go.jp/himawari8/img/D531106/{}d/{}/{}_{}_{}.png'
    url = url_format.format(level, WIDTH, strftime('%Y/%m/%d/%H%M%S', latest), x, y)
    tiledata = download(url)
<mask>:
        sys.exit('No image available for {}.'.format(strftime('%Y/%m/%d %H:%M:%S', latest)))
    with counter.get_lock():
        counter.value += 1
        if counter.value == level * level:
            print('Downloading tiles: completed.')
        else:
            print('Downloading tiles: {}/{} completed...'.format(counter.value, level * level))
    return (x, y, tiledata)",tiledata.__sizeof__() == 2867,58,tiledata is None,False,1.3699439807202476,N/A
"def parse_args():
    parser = argparse.ArgumentParser(description='set (near-realtime) picture of Earth as your desktop background', epilog='https://labs.boramalper.org/himawaripy')
    parser.add_argument('--version', action='version', version='%(prog)s {}.{}.{}'.format(*HIMAWARIPY_VERSION))
    group = parser.add_mutually_exclusive_group()
    group.add_argument('--auto-offset', action='store_true', dest='auto_offset', default=False, help='determine offset automatically')
    group.add_argument('-o', '--offset', type=int, dest='offset', default=10, help='UTC time offset in hours, must be less than or equal to +10')
    parser.add_argument('-l', '--level', type=int, choices=[4, 8, 16, 20], dest='level', default=4, help='increases the quality (and the size) of each tile. possible values are 4, 8, 16, 20')
    parser.add_argument('-d', '--deadline', type=int, dest='deadline', default=6, help='deadline in minutes to download all the tiles, set 0 to cancel')
    parser.add_argument('--save-battery', action='store_true', dest='save_battery', default=False, help='stop refreshing on battery')
    parser.add_argument('--output-dir', type=str, dest='output_dir', help='directory to save the temporary background image', default=appdirs.user_cache_dir(appname='himawaripy', appauthor=False))
    parser.add_argument('--dont-change', action='store_true', dest='dont_change', default=False, help=""don't change the wallpaper (just download it)"")
    args = parser.parse_args()
<mask>:
        sys.exit('OFFSET has to be between -12 and +10!\n')
    if not args.deadline >= 0:
        sys.exit('DEADLINE has to be greater than (or equal to if you want to disable) zero!\n')
    return args",not -12 <= args.offset <= 10,153,not args.offset >= 0,False,18.094495256969623,N/A
"def is_discharging():
<mask>:
        if len(glob('/sys/class/power_supply/BAT*')) > 1:
            print('Multiple batteries detected, using BAT0.')
        with open('/sys/class/power_supply/BAT0/status') as f:
            status = f.readline().strip()
            return status == 'Discharging'
    elif sys.platform == 'darwin':
        return b'discharging' in subprocess.check_output(['pmset', '-g', 'batt'])
    else:
        sys.exit('Battery saving feature works only on linux or mac!\n')",sys.platform.startswith('linux'),43,sys.platform == 'linux',False,23.263472697663296,N/A
"def download(url):
    exception = None
    for i in range(1, 4):
        try:
            with urllib.request.urlopen(url, context=ssl.SSLContext(ssl.PROTOCOL_TLS)) as response:
                return response.read()
        except Exception as e:
            exception = e
            print(""[{}/3] Retrying to download '{}'..."".format(i, url))
            time.sleep(1)
            pass
<mask>:
        raise exception
    else:
        sys.exit(""Could not download '{}'!\n"".format(url))",exception,41,exception,True,100.00000000000004,N/A
"def test_iterate_empty_psml_capture(simple_summary_capture):
    simple_summary_capture.display_filter = 'frame.len == 1'
    q = Queue()
    p = Process(target=_iterate_capture_object, args=(simple_summary_capture, q))
    p.start()
    p.join(2)
    try:
        no_hang = q.get_nowait()
    except Empty:
        no_hang = False
<mask>:
        p.terminate()
    assert no_hang",p.is_alive(),30,no_hang,False,7.253154775624655,N/A
"def get_config():
<mask>:
        config_path = fp_config_path
    elif pyshark_config_path.exists():
        config_path = pyshark_config_path
    else:
        return None
    config = ConfigParser()
    config.read(config_path)
    return config",fp_config_path.exists(),20,fp_config_path.exists(),True,100.00000000000004,N/A
"def load_mapping(self, tshark_version, tshark_path=None):
<mask>:
        return
    mapping_cache_file = cache.get_cache_dir(tshark_version).joinpath(_MAPPING_CACHE_NAME)
    if mapping_cache_file.exists():
        self._protocol_to_mapping = json.load(mapping_cache_file.open())
    else:
        self._protocol_to_mapping = tshark.get_ek_field_mapping(tshark_path=tshark_path)
        mapping_cache_file.open('w').write(json.dumps(self._protocol_to_mapping))",self._protocol_to_mapping,19,tshark_version is None or tshark_path is None,False,4.456882760699063,N/A
"def cast_field_value(self, protocol, field_name, field_value):
    """"""Casts the field value to its proper type according to the mapping""""""
<mask>:
        return [self.cast_field_value(protocol, field_name, item) for item in field_value]
    if not isinstance(field_value, str):
        return field_value
    field_type = self.get_field_type(protocol, field_name)
    if field_type == str:
        return field_value
    if field_type == int and field_value.startswith('0x'):
        return int(field_value, 16)
    if field_type == bytes:
        try:
            return binascii.unhexlify(field_value.replace(':', ''))
        except binascii.Error:
            return field_value
    try:
        return field_type(field_value)
    except ValueError:
        return field_value","isinstance(field_value, list)",70,"isinstance(field_value, list)",True,100.00000000000004,N/A
"def get_field_type(self, protocol, field_name):
    """"""Gets the Python type for the given field (only for EK fields).

        If we are unfamiliar with the type, str will be returned.
        """"""
<mask>:
        raise ProtocolMappingNotInitialized('Protocol mapping not initialized. Call load_mapping() first')
    if protocol not in self._protocol_to_mapping:
        raise FieldNotFound(f'Type mapping for protocol {protocol} not found')
    fields = self._protocol_to_mapping[protocol]['properties']
    if field_name not in fields:
        return str
    return self._get_python_type_for_field_type(fields[field_name]['type'])",not self._protocol_to_mapping,62,protocol not in self._protocol_mapping,False,51.33450480401705,N/A
"@classmethod
def _get_python_type_for_field_type(cls, field_type):
<mask>:
        return int
    if field_type == 'float':
        return float
    if field_type == 'date':
        return float
    if field_type == 'byte':
        return bytes
    return str","field_type in ('integer', 'long', 'short')",27,field_type == 'int',False,13.13084334918613,N/A
"def get_cache_dir(tshark_version) -> pathlib.Path:
    cache_dir = pathlib.Path(appdirs.user_cache_dir(appname='pyshark', version=tshark_version))
<mask>:
        cache_dir.mkdir(parents=True)
    return cache_dir",not cache_dir.exists(),12,not cache_dir.exists(),True,100.00000000000004,N/A
"def get_process_path(tshark_path=None, process_name='tshark'):
    """"""Finds the path of the tshark executable.

    If the user has provided a path
    or specified a location in config.ini it will be used. Otherwise default
    locations will be searched.

    :param tshark_path: Path of the tshark binary
    :raises TSharkNotFoundException in case TShark is not found in any location.
    """"""
    possible_paths = []
    config = get_config()
<mask>:
        try:
            possible_paths.append(config.get(process_name, f'{process_name}_path'))
        except NoSectionError:
            pass
    if tshark_path is not None:
        user_tshark_path = os.path.join(os.path.dirname(tshark_path), f'{process_name}.exe' if sys.platform.startswith('win') else process_name)
        possible_paths.insert(0, user_tshark_path)
    if sys.platform.startswith('win'):
        for env in ('ProgramFiles(x86)', 'ProgramFiles'):
            program_files = os.getenv(env)
            if program_files is not None:
                possible_paths.append(os.path.join(program_files, 'Wireshark', f'{process_name}.exe'))
    else:
        os_path = os.getenv('PATH', '/usr/bin:/usr/sbin:/usr/lib/tshark:/usr/local/bin')
        for path in os_path.split(':'):
            possible_paths.append(os.path.join(path, process_name))
    if sys.platform.startswith('darwin'):
        possible_paths.append(f'/Applications/Wireshark.app/Contents/MacOS/{process_name}')
    for path in possible_paths:
        if os.path.exists(path):
            if sys.platform.startswith('win'):
                path = path.replace('\\', '/')
            return path
    raise TSharkNotFoundException(f'TShark not found. Try adding its location to the configuration file. Searched these paths: {possible_paths}')",config,142,config.has_section(process_name),False,4.196114906296549,N/A
"def get_tshark_version(tshark_path=None):
    parameters = [get_process_path(tshark_path), '-v']
    with open(os.devnull, 'w') as null:
        version_output = subprocess.check_output(parameters, stderr=null).decode('ascii')
    version_line = version_output.splitlines()[0]
    pattern = '.*\\s(\\d+\\.\\d+\\.\\d+).*'
    m = re.match(pattern, version_line)
<mask>:
        raise TSharkVersionException('Unable to parse TShark version from: {}'.format(version_line))
    version_string = m.groups()[0]
    return version.parse(version_string)",not m,39,not m,True,100.00000000000004,N/A
"def get_tshark_display_filter_flag(tshark_version):
    """"""Returns '-Y' for tshark versions >= 1.10.0 and '-R' for older versions.""""""
<mask>:
        return '-Y'
    else:
        return '-R'",tshark_version >= version.parse('1.10.0'),20,"tshark_version < (1, 10)",False,16.14682615668325,N/A
"def get_all_tshark_interfaces_names(tshark_path=None):
    """"""Returns a list of all possible interface names. Some interfaces may have aliases""""""
    parameters = [get_process_path(tshark_path), '-D']
    with open(os.devnull, 'w') as null:
        tshark_interfaces = subprocess.check_output(parameters, stderr=null).decode('utf-8')
    all_interface_names = []
    for line in tshark_interfaces.splitlines():
        matches = _TSHARK_INTERFACE_ALIAS_PATTERN.findall(line)
<mask>:
            all_interface_names.extend([name for name in matches[0] if name])
    return all_interface_names",matches,48,matches,True,100.00000000000004,N/A
"def get_ek_field_mapping(tshark_path=None):
    parameters = [get_process_path(tshark_path), '-G', 'elastic-mapping']
    with open(os.devnull, 'w') as null:
        mapping = subprocess.check_output(parameters, stderr=null).decode('ascii')
    mapping = json.loads(mapping, object_pairs_hook=_duplicate_object_hook)['mappings']
<mask>:
        pass
    elif 'doc' in mapping:
        mapping = mapping['doc']
    elif 'pcap_file' in mapping:
        mapping = mapping['pcap_file']
    else:
        raise TSharkVersionException(f'Your tshark version does not support elastic-mapping. Please upgrade.')
    return mapping['properties']['layers']['properties']",'dynamic' in mapping and 'properties' in mapping,49,'pcap_file' in mapping and 'pcap_file' in mapping,False,17.542198478193427,N/A
"def _extract_packet_from_data(self, data, got_first_packet=True):
    """"""Returns a packet's data and any remaining data after reading that first packet""""""
    tag_start = 0
<mask>:
        tag_start = data.find(b'{')
        if tag_start == -1:
            return (None, data)
    packet_separator, end_separator, end_tag_strip_length = self._get_json_separators()
    found_separator = None
    tag_end = data.find(packet_separator)
    if tag_end == -1:
        tag_end = data.find(end_separator)
        if tag_end != -1:
            found_separator = end_separator
    else:
        found_separator = packet_separator
    if found_separator:
        tag_end += len(found_separator) - end_tag_strip_length
        return (data[tag_start:tag_end].strip().strip(b','), data[tag_end + 1:])
    return (None, data)",not got_first_packet,75,got_first_packet,False,81.87307530779823,N/A
"def _get_json_separators(self):
    """"""""Returns the separators between packets in a JSON output

        Returns a tuple of (packet_separator, end_of_file_separator, characters_to_disregard).
        The latter variable being the number of characters to ignore in order to pass the packet (i.e. extra newlines,
        commas, parenthesis).
        """"""
<mask>:
        return (f'{os.linesep}  }},{os.linesep}'.encode(), f'}}{os.linesep}]'.encode(), 1 + len(os.linesep))
    else:
        return (f'}}{os.linesep}{os.linesep}  ,'.encode(), f'}}{os.linesep}{os.linesep}]'.encode(), 1)",not self._tshark_version or self._tshark_version >= version.parse('3.0.0'),54,self.is_json,False,0.7890622507494028,N/A
"def duplicate_object_hook(ordered_pairs):
    """"""Make lists out of duplicate keys.""""""
    json_dict = {}
    for key, val in ordered_pairs:
        existing_val = json_dict.get(key)
<mask>:
            json_dict[key] = val
        elif isinstance(existing_val, list):
            existing_val.append(val)
        else:
            json_dict[key] = [existing_val, val]
    return json_dict",not existing_val,34,existing_val is None,False,39.76353643835252,N/A
"def packet_from_json_packet(json_pkt, deduplicate_fields=True):
    """"""Creates a Pyshark Packet from a tshark json single packet.

    Before tshark 2.6, there could be duplicate keys in a packet json, which creates the need for
    deduplication and slows it down significantly.
    """"""
<mask>:
        pkt_dict = json.loads(json_pkt.decode('utf-8'), object_pairs_hook=duplicate_object_hook)
    elif USE_UJSON:
        pkt_dict = ujson.loads(json_pkt)
    else:
        pkt_dict = json.loads(json_pkt.decode('utf-8'))
    frame_dict = pkt_dict['_source']['layers'].pop('frame')
    layers = []
    for layer in frame_dict['frame.protocols'].split(':'):
        layer_dict = pkt_dict['_source']['layers'].pop(layer, None)
        if layer_dict is not None:
            layers.append(JsonLayer(layer, layer_dict))
    for name, layer in pkt_dict['_source']['layers'].items():
        layers.append(JsonLayer(name, layer))
    return Packet(layers=layers, frame_info=JsonLayer('frame', frame_dict), number=int(frame_dict.get('frame.number', 0)), length=int(frame_dict['frame.len']), sniff_time=frame_dict['frame.time_epoch'], interface_captured=frame_dict.get('frame.interface_id'))",deduplicate_fields,88,deduplicate_fields,True,100.00000000000004,N/A
"def packet_from_xml_packet(xml_pkt, psml_structure=None):
    """"""
    Gets a TShark XML packet object or string, and returns a pyshark Packet objec.t

    :param xml_pkt: str or xml object.
    :param psml_structure: a list of the fields in each packet summary in the psml data. If given, packets will
    be returned as a PacketSummary object.
    :return: Packet object.
    """"""
<mask>:
        parser = lxml.objectify.makeparser(huge_tree=True, recover=True, encoding='utf-8')
        xml_pkt = xml_pkt.decode(errors='ignore').translate(DEL_BAD_XML_CHARS)
        xml_pkt = lxml.objectify.fromstring(xml_pkt.encode('utf-8'), parser)
    if psml_structure:
        return _packet_from_psml_packet(xml_pkt, psml_structure)
    return _packet_from_pdml_packet(xml_pkt)","not isinstance(xml_pkt, lxml.objectify.ObjectifiedElement)",73,"isinstance(xml_pkt, str)",False,37.848698581337665,N/A
"def _extract_tag_from_xml_data(data, tag_name=b'packet'):
    """"""Gets data containing a (part of) tshark xml.

    If the given tag is found in it, returns the tag data and the remaining data.
    Otherwise returns None and the same data.

    :param data: string of a partial tshark xml.
    :param tag_name: A bytes string of the tag name
    :return: a tuple of (tag, data). tag will be None if none is found.
    """"""
    opening_tag = b'<' + tag_name + b'>'
    closing_tag = opening_tag.replace(b'<', b'</')
    tag_end = data.find(closing_tag)
<mask>:
        tag_end += len(closing_tag)
        tag_start = data.find(opening_tag)
        return (data[tag_start:tag_end], data[tag_end:])
    return (None, data)",tag_end != -1,93,tag_end != -1,True,100.00000000000004,N/A
"def _extract_packet_from_data(self, data, got_first_packet=True):
    """"""Returns a packet's data and any remaining data after reading that first packet""""""
    start_index = 0
    data = data.lstrip()
<mask>:
        start_index = data.find(_ENCODED_OS_LINESEP) + 1
    linesep_location = data.find(_ENCODED_OS_LINESEP, start_index)
    if linesep_location == -1:
        return (None, data)
    return (data[start_index:linesep_location], data[linesep_location + 1:])","data.startswith(b'{""ind')",45,got_first_packet,False,0.0,N/A
"def packet_from_ek_packet(json_pkt):
<mask>:
        pkt_dict = ujson.loads(json_pkt)
    else:
        pkt_dict = json.loads(json_pkt.decode('utf-8'))
    frame_dict = pkt_dict['layers'].pop('frame')
    layers = []
    for layer in frame_dict['frame_frame_protocols'].split(':'):
        layer_dict = pkt_dict['layers'].pop(layer, None)
        if layer_dict is not None:
            layers.append(EkLayer(layer, layer_dict))
    for name, layer in pkt_dict['layers'].items():
        layers.append(EkLayer(name, layer))
    return Packet(layers=layers, frame_info=EkLayer('frame', frame_dict), number=int(frame_dict.get('frame_frame_number', 0)), length=int(frame_dict['frame_frame_len']), sniff_time=frame_dict['frame_frame_time_epoch'], interface_captured=frame_dict.get('rame_frame_interface_id'))",USE_UJSON,47,"isinstance(json_pkt, str)",False,5.522397783539471,N/A
"def __init__(self, display_filter=None, only_summaries=False, eventloop=None, decryption_key=None, encryption_type='wpa-pwd', output_file=None, decode_as=None, disable_protocol=None, tshark_path=None, override_prefs=None, capture_filter=None, use_json=False, include_raw=False, use_ek=False, custom_parameters=None, debug=False):
    self.loaded = False
    self.tshark_path = tshark_path
    self._override_prefs = override_prefs
    self.debug = debug
    self.use_json = use_json
    self._use_ek = use_ek
    self.include_raw = include_raw
    self._packets = []
    self._current_packet = 0
    self._display_filter = display_filter
    self._capture_filter = capture_filter
    self._only_summaries = only_summaries
    self._output_file = output_file
    self._running_processes = set()
    self._decode_as = decode_as
    self._disable_protocol = disable_protocol
    self._log = logging.Logger(self.__class__.__name__, level=self.DEFAULT_LOG_LEVEL)
    self._closed = False
    self._custom_parameters = custom_parameters
    self._eof_reached = False
    self._last_error_line = None
    self._stderr_handling_tasks = []
    self.__tshark_version = None
<mask>:
        raise RawMustUseJsonException('use_json/use_ek must be True if include_raw')
    if self.debug:
        self.set_debug()
    self.eventloop = eventloop
    if self.eventloop is None:
        self._setup_eventloop()
    if encryption_type and encryption_type.lower() in self.SUPPORTED_ENCRYPTION_STANDARDS:
        self.encryption = (decryption_key, encryption_type.lower())
    else:
        standards = ', '.join(self.SUPPORTED_ENCRYPTION_STANDARDS)
        raise UnknownEncryptionStandardException(f'Only the following standards are supported: {standards}.')",include_raw and (not (use_json or use_ek)),130,self.use_json and self.use_ek,False,15.477529118979081,N/A
"def next_packet(self) -> Packet:
<mask>:
        raise StopIteration()
    cur_packet = self._packets[self._current_packet]
    self._current_packet += 1
    return cur_packet",self._current_packet >= len(self._packets),15,self._current_packet >= len(self._packets),True,100.00000000000004,N/A
"def load_packets(self, packet_count=0, timeout=None):
    """"""Reads the packets from the source (cap, interface, etc.) and adds it to the internal list.

        If 0 as the packet_count is given, reads forever

        :param packet_count: The amount of packets to add to the packet list (0 to read forever)
        :param timeout: If given, automatically stops after a given amount of time.
        """"""
    initial_packet_amount = len(self._packets)

    def keep_packet(pkt):
        self._packets.append(pkt)
<mask>:
            raise StopCapture()
    try:
        self.apply_on_packets(keep_packet, timeout=timeout, packet_count=packet_count)
        self.loaded = True
    except asyncTimeoutError:
        pass",packet_count != 0 and len(self._packets) - initial_packet_amount >= packet_count,77,initial_packet_amount > packet_count,False,12.636692760130936,N/A
"def set_debug(self, set_to=True, log_level=logging.DEBUG):
    """"""Sets the capture to debug mode (or turns it off if specified).""""""
<mask>:
        handler = logging.StreamHandler(sys.stdout)
        handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        self._log.addHandler(handler)
        self._log.level = log_level
    self.debug = set_to",set_to,34,not self._log.handlers,False,6.567274736060395,N/A
"def _setup_eventloop(self):
    """"""Sets up a new eventloop as the current one according to the OS.""""""
<mask>:
        current_eventloop = asyncio.get_event_loop_policy().get_event_loop()
        if isinstance(current_eventloop, asyncio.ProactorEventLoop):
            self.eventloop = current_eventloop
        else:
            if asyncio.all_tasks():
                warnings.warn('The running eventloop has tasks but pyshark must set a new eventloop to continue. Existing tasks may not run.')
            self.eventloop = asyncio.ProactorEventLoop()
            asyncio.set_event_loop(self.eventloop)
    else:
        try:
            self.eventloop = asyncio.get_event_loop_policy().get_event_loop()
        except RuntimeError:
            if threading.current_thread() != threading.main_thread():
                self.eventloop = asyncio.new_event_loop()
                asyncio.set_event_loop(self.eventloop)
            else:
                raise
        if os.name == 'posix' and isinstance(threading.current_thread(), threading._MainThread):
            asyncio.set_child_watcher(asyncio.SafeChildWatcher())
            asyncio.get_child_watcher().attach_loop(self.eventloop)",os.name == 'nt',77,os.name == 'win32',False,75.98356856515926,N/A
"def next(self) -> Packet:
    """"""Returns the next packet in the cap.

        If the capture's keep_packets flag is True, will also keep it in the internal packet list.
        """"""
<mask>:
        return self._packet_generator.send(None)
    elif self._current_packet >= len(self._packets):
        packet = self._packet_generator.send(None)
        self._packets += [packet]
    return super(FileCapture, self).next_packet()",not self.keep_packets,44,self.keep_packets,False,81.87307530779823,N/A
"def __getitem__(self, packet_index):
<mask>:
        raise NotImplementedError('Cannot use getitem if packets are not kept')
    while packet_index >= len(self._packets):
        try:
            self.next()
        except StopIteration:
            raise KeyError(f'Packet of index {packet_index} does not exist in capture')
    return super(FileCapture, self).__getitem__(packet_index)",not self.keep_packets,34,packet_index >= len(self._packets),False,9.578464408619821,N/A
"def __repr__(self):
<mask>:
        return f'<{self.__class__.__name__} {self.input_filepath.as_posix()}>'
    else:
        return f'<{self.__class__.__name__} {self.input_filepath.as_posix()} ({len(self._packets)} packets)>'",self.keep_packets,12,len(self._packets) == 0,False,11.868405219520975,N/A
"def _get_json_separators(self):
    """"""""Returns the separators between packets in a JSON output

        Returns a tuple of (packet_separator, end_of_file_separator, characters_to_disregard).
        The latter variable being the number of characters to ignore in order to pass the packet (i.e. extra newlines,
        commas, parenthesis).
        """"""
<mask>:
        return (f'{os.linesep}  }}'.encode(), f'}}{os.linesep}]'.encode(), 0)
    else:
        return (f'}}{os.linesep}{os.linesep}'.encode(), f'}}{os.linesep}{os.linesep}]', 1)",self._get_tshark_version() >= version.parse('2.6.7'),51,self.ignore_newlines,False,1.7560903341711829,N/A
"def _write_packet(self, packet, sniff_time):
<mask>:
        now = time.time()
    elif isinstance(sniff_time, datetime.datetime):
        now = sniff_time.timestamp()
    else:
        now = float(sniff_time)
    secs = int(now)
    usecs = int(now * 1000000 % 1000000)
    self._current_tshark.stdin.write(struct.pack('IIII', secs, usecs, len(packet), len(packet)))
    self._current_tshark.stdin.write(packet)",sniff_time is None,34,sniff_time is None,True,100.00000000000004,N/A
"def parse_packet(self, binary_packet, sniff_time=None, timeout=DEFAULT_TIMEOUT):
    """"""Parses a single binary packet and returns its parsed version.

        DOES NOT CLOSE tshark. It must be closed manually by calling close() when you're done
        working with it.
        Use parse_packets when parsing multiple packets for faster parsing
        """"""
<mask>:
        sniff_time = [sniff_time]
    return self.parse_packets([binary_packet], sniff_time, timeout)[0]",sniff_time is not None,51,"isinstance(sniff_time, int)",False,20.556680845025987,N/A
"def parse_packets(self, binary_packets, sniff_times=None, timeout=DEFAULT_TIMEOUT):
    """"""Parses binary packets and return a list of parsed packets.

        DOES NOT CLOSE tshark. It must be closed manually by calling close() when you're done
        working with it.
        """"""
<mask>:
        self._setup_eventloop()
    return self.eventloop.run_until_complete(self.parse_packets_async(binary_packets, sniff_times, timeout))",self.eventloop is None,40,not self.eventloop,False,46.30777161991026,N/A
"def __init__(self, interface=None, bpf_filter=None, display_filter=None, only_summaries=False, decryption_key=None, encryption_type='wpa-pwk', output_file=None, decode_as=None, disable_protocol=None, tshark_path=None, override_prefs=None, capture_filter=None, monitor_mode=False, use_json=False, use_ek=False, include_raw=False, eventloop=None, custom_parameters=None, debug=False):
    """"""Creates a new live capturer on a given interface. Does not start the actual capture itself.

        :param interface: Name of the interface to sniff on or a list of names (str). If not given, runs on all interfaces.
        :param bpf_filter: BPF filter to use on packets.
        :param display_filter: Display (wireshark) filter to use.
        :param only_summaries: Only produce packet summaries, much faster but includes very little information
        :param decryption_key: Optional key used to encrypt and decrypt captured traffic.
        :param encryption_type: Standard of encryption used in captured traffic (must be either 'WEP', 'WPA-PWD', or
        'WPA-PWK'. Defaults to WPA-PWK).
        :param output_file: Additionally save live captured packets to this file.
        :param decode_as: A dictionary of {decode_criterion_string: decode_as_protocol} that are used to tell tshark
        to decode protocols in situations it wouldn't usually, for instance {'tcp.port==8888': 'http'} would make
        it attempt to decode any port 8888 traffic as HTTP. See tshark documentation for details.
        :param tshark_path: Path of the tshark binary
        :param override_prefs: A dictionary of tshark preferences to override, {PREFERENCE_NAME: PREFERENCE_VALUE, ...}.
        :param capture_filter: Capture (wireshark) filter to use.
        :param disable_protocol: Tells tshark to remove a dissector for a specifc protocol.
        :param use_ek: Uses tshark in EK JSON mode. It is faster than XML but has slightly less data.
        :param use_json: DEPRECATED. Use use_ek instead.
        :param custom_parameters: A dict of custom parameters to pass to tshark, i.e. {""--param"": ""value""} or
        else a list of parameters in the format [""--foo"", ""bar"", ""--baz"", ""foo""].
        """"""
    super(LiveCapture, self).__init__(display_filter=display_filter, only_summaries=only_summaries, decryption_key=decryption_key, encryption_type=encryption_type, output_file=output_file, decode_as=decode_as, disable_protocol=disable_protocol, tshark_path=tshark_path, override_prefs=override_prefs, capture_filter=capture_filter, use_json=use_json, use_ek=use_ek, include_raw=include_raw, eventloop=eventloop, custom_parameters=custom_parameters, debug=debug)
    self.bpf_filter = bpf_filter
    self.monitor_mode = monitor_mode
    all_interfaces = get_tshark_interfaces(tshark_path)
<mask>:
        self.interfaces = all_interfaces
    elif isinstance(interface, str):
        self.interfaces = [interface]
    else:
        self.interfaces = interface",interface is None,299,"not isinstance(interface, list)",False,6.567274736060395,N/A
"def _verify_capture_parameters(self):
    all_interfaces_names = tshark.get_all_tshark_interfaces_names(self.tshark_path)
    all_interfaces_lowercase = [interface.lower() for interface in all_interfaces_names]
    for each_interface in self.interfaces:
<mask>:
            continue
        if each_interface.isnumeric():
            continue
        if each_interface.lower() not in all_interfaces_lowercase:
            raise UnknownInterfaceException(f""Interface '{each_interface}' does not exist, unable to initiate capture. Perhaps permissions are missing?\nPossible interfaces: {os.linesep.join(all_interfaces_names)}"")",each_interface.startswith('rpcap://'),42,each_interface.isstring(),False,22.273858658245697,N/A
"def _get_dumpcap_parameters(self):
    params = ['-q']
<mask>:
        params += ['-P']
    if self.bpf_filter:
        params += ['-f', self.bpf_filter]
    if self.monitor_mode:
        params += ['-I']
    for interface in self.interfaces:
        params += ['-i', interface]
    params += ['-w', '-']
    return params",self._get_tshark_version() < version.parse('2.5.0'),34,self.monitor_mode,False,2.1448935777350973,N/A
"def __init__(self, name=None, showname=None, value=None, show=None, hide=None, pos=None, size=None, unmaskedvalue=None):
    self.name = name
    self.showname = showname
    self.raw_value = value
    self.show = show
    self.pos = pos
    self.size = size
    self.unmaskedvalue = unmaskedvalue
<mask>:
        self.hide = True
    else:
        self.hide = False",hide and hide == 'yes',39,hide,False,0.673794699908547,N/A
"def get_default_value(self) -> str:
    """"""Gets the best 'value' string this field has.""""""
    val = self.show
<mask>:
        val = self.raw_value
    if not val:
        val = self.showname
    return val",not val,27,self.raw_value,False,0.0,N/A
"@property
def showname_value(self) -> typing.Union[str, None]:
    """"""The ""pretty value"" (as displayed by Wireshark) of the field.""""""
<mask>:
        return self.showname.split(': ', 1)[1]
    return None",self.showname and ': ' in self.showname,23,self.showname is not None,False,13.13084334918613,N/A
"@property
def showname_key(self) -> typing.Union[str, None]:
    """"""The ""pretty name"" (as displayed by Wireshark) of the field.""""""
<mask>:
        return self.showname.split(': ', 1)[0]
    return None",self.showname and ': ' in self.showname,23,self.showname is not None,False,13.13084334918613,N/A
"@property
def binary_value(self) -> bytes:
    """"""Converts this field to binary (assuming it's a binary string)""""""
    str_raw_value = str(self.raw_value)
<mask>:
        str_raw_value = '0' + str_raw_value
    return binascii.unhexlify(str_raw_value)",len(str_raw_value) % 2 == 1,26,not str_raw_value.startswith('0'),False,31.818770336963667,N/A
"@functools.wraps(termcolor.colored)
def colored(text, *args, **kwargs):
    try:
        enable_color = sys.stdout.isatty()
    except (AttributeError, NotImplementedError, FileNotFoundError):
        enable_color = False
<mask>:
        return termcolor.colored(text, *args, **kwargs)
    return text",enable_color,23,enable_color,True,100.00000000000004,N/A
"def __init__(self, layers=None, frame_info=None, number=None, length=None, captured_length=None, sniff_time=None, interface_captured=None):
    """"""
        Creates a Packet object with the given layers and info.

        :param layers: A list of BaseLayer objects.
        :param frame_info: Layer object for the entire packet frame (information like frame length, packet number, etc.
        :param length: Length of the actual packet.
        :param captured_length: The length of the packet that was actually captured (could be less then length)
        :param sniff_time: The time the packet was captured (timestamp)
        :param interface_captured: The interface the packet was captured in.
        """"""
<mask>:
        self.layers = []
    else:
        self.layers = layers
    self.frame_info = frame_info
    self.number = number
    self.interface_captured = interface_captured
    self.captured_length = captured_length
    self.length = length
    self.sniff_timestamp = sniff_time",layers is None,111,layers is None,True,100.00000000000004,N/A
"def __getitem__(self, item):
    """"""
        Gets a layer according to its index or its name

        :param item: layer index or name
        :return: BaseLayer object.
        """"""
<mask>:
        return self.layers[item]
    for layer in self.layers:
        if layer.layer_name.lower() == item.lower():
            return layer
    raise KeyError('Layer does not exist in packet')","isinstance(item, int)",44,"isinstance(item, BaseLayer)",False,53.7284965911771,N/A
"def __repr__(self):
    transport_protocol = ''
<mask>:
        transport_protocol = self.transport_layer + '/'
    return f'<{transport_protocol}{self.highest_layer} Packet>'",self.transport_layer != self.highest_layer and self.transport_layer is not None,14,self.transport_layer,False,4.0762203978366225,N/A
"def __getattr__(self, item):
    """"""
        Allows layers to be retrieved via get attr. For instance: pkt.ip
        """"""
    for layer in self.layers:
<mask>:
            return layer
    raise AttributeError(f'No attribute named {item}')",layer.layer_name.lower() == item.lower(),28,"layer.getattr(item, None) == layer.getattr(item, None)",False,12.24299914053352,N/A
"@property
def transport_layer(self) -> BaseLayer:
    for layer in consts.TRANSPORT_LAYERS:
<mask>:
            return layer",layer in self,12,layer.is_connected(),False,6.567274736060395,N/A
"def get_field_as_list(self, name) -> list:
    """"""Helper function to get a certain field always as a list.

        Some fields may appear once or more in the packet. The field will appear as a list if it appears more
        than once. In order to avoid checking certain fields if they're lists or not, this function will
        return the field inside a list at all times.

        For example, in a DNS packet there may be one or more responses.
        A packet with with one response (www.google.com) will return:
            >>> print(pkt.dns.resp_name)
            ""www.google.com""
        While a packet with two responses will return:
            >>> print(pkt.dns.resp_name)
            [""www.google.com"", ""www.google2.com""]

        To avoid this changing behaviour, use:
            >>> print(pkt.dns.get_field_as_list(""resp_name""))
            [""www.google.com""]
        """"""
    field_value = self.get_field(name)
<mask>:
        return field_value
    return [field_value]","isinstance(field_value, list)",118,"isinstance(field_value, list)",True,100.00000000000004,N/A
"def get_field(self, name) -> typing.Union['EkMultiField', None, str, int, bool, bytes, list]:
    name = name.replace('.', '_')
<mask>:
        return self._get_field_value(name)
    for prefix in self._get_possible_layer_prefixes():
        nested_field = self._get_nested_field(prefix, name)
        if nested_field is not None:
            return nested_field
    return None",name in self._fields_dict,35,self._is_field_defined(name),False,15.851165692617148,N/A
"@property
def all_field_names(self):
    """"""Gets all field names, including subfields""""""
    names = set()
    for field_name in self._fields_dict:
        for prefix in self._get_possible_layer_prefixes():
<mask>:
                names.add(_remove_ek_prefix(prefix, field_name))
                break
    return list(names)",field_name.startswith(prefix),26,field_name.startswith(prefix),True,100.00000000000004,N/A
"def _get_nested_field(self, prefix, name):
    """"""Gets a field that is directly on the layer

        Returns either a multifield or a raw value.
        """"""
    field_ek_name = f'{prefix}_{name}'
<mask>:
        if self._field_has_subfields(field_ek_name):
            return EkMultiField(self, self._fields_dict, name, value=self._get_field_value(field_ek_name))
        return self._get_field_value(field_ek_name)
    for possible_nested_name in self._fields_dict:
        if possible_nested_name.startswith(f'{field_ek_name}_'):
            return EkMultiField(self, self._fields_dict, name, value=None)
    return None",field_ek_name in self._fields_dict,48,self._is_field_present(field_ek_name),False,33.88714363186177,N/A
"def _field_has_subfields(self, field_ek_name):
    field_ek_name_with_ext = f'{field_ek_name}_'
    for field_name in self._fields_dict:
<mask>:
            return True
    return False",field_name.startswith(field_ek_name_with_ext),15,field_name_with_ext.endswith(field_ek_name_with_ext),False,59.32180640699454,N/A
"def __init__(self, xml_obj=None, raw_mode=False):
    super().__init__(xml_obj.attrib['name'])
    self.raw_mode = raw_mode
    self._all_fields = {}
    for field in xml_obj.findall('.//field'):
        attributes = dict(field.attrib)
        field_obj = LayerField(**attributes)
<mask>:
            self._all_fields[attributes['name']].add_field(field_obj)
        else:
            self._all_fields[attributes['name']] = LayerFieldsContainer(field_obj)",attributes['name'] in self._all_fields,27,raw_mode,False,1.9119108411650758,N/A
"def get_field(self, name) -> typing.Union[LayerFieldsContainer, None]:
    """"""Gets the XML field object of the given name.""""""
    field = self._all_fields.get(name)
<mask>:
        return field
    for field_name, field in self._all_fields.items():
        if self._sanitize_field_name(name) == self._sanitize_field_name(field_name):
            return field
    return None",field is not None,34,field is not None,True,100.00000000000004,N/A
"def get_field_value(self, name, raw=False) -> typing.Union[LayerFieldsContainer, None]:
    """"""Tries getting the value of the given field.

        Tries it in the following order: show (standard nice display), value (raw value),
        showname (extended nice display).

        :param name: The name of the field
        :param raw: Only return raw value
        :return: str of value
        """"""
    field = self.get_field(name)
<mask>:
        return None
    if raw:
        return field.raw_value
    return field",field is None,62,not field,False,30.326532985631665,N/A
"def __getattr__(self, item):
    val = self.get_field(item)
<mask>:
        raise AttributeError()
    if self.raw_mode:
        return val.raw_value
    return val",val is None,15,not val,False,30.326532985631665,N/A
"def __getattr__(self, item):
    val = self.get_field(item)
<mask>:
        raise AttributeError(f'{item} does not exist in Layer')
    return val",val is None,16,val is None,True,100.00000000000004,N/A
"def pretty_print(self, writer=None):
<mask>:
        writer = sys.stdout
    if self.layer_name == DATA_LAYER_NAME:
        writer.write('DATA')
        return
    text = f'Layer {self.layer_name.upper()}{os.linesep}:'
    writer.write(common.colored(text, color='yellow', attrs=['bold']))
    self._pretty_print_layer_fields(writer)",not writer,21,not writer,True,100.00000000000004,N/A
"def __init__(self, layer_name, layer_dict, full_name=None, is_intermediate=False):
    """"""Creates a JsonLayer. All sublayers and fields are created lazily later.""""""
    super().__init__(layer_name)
    self.duplicate_layers = []
    self._showname_fields_converted_to_regular = False
<mask>:
        self._full_name = self._layer_name
    else:
        self._full_name = full_name
    self._is_intermediate = is_intermediate
    self._wrapped_fields = {}
    if isinstance(layer_dict, list):
        self.duplicate_layers = [JsonLayer(layer_name, duplicate_dict, full_name=full_name, is_intermediate=is_intermediate) for duplicate_dict in layer_dict[1:]]
        layer_dict = layer_dict[0]
    if not isinstance(layer_dict, dict):
        self.value = layer_dict
        self._all_fields = {}
        return
    self._all_fields = layer_dict",not full_name,68,full_name is None,False,39.76353643835252,N/A
"def get_field(self, name):
    """"""Gets a field by its full or partial name.""""""
    self._convert_showname_field_names_to_field_names()
    field = self._wrapped_fields.get(name)
<mask>:
        is_fake = False
        field = self._get_internal_field_by_name(name)
        if field is None:
            is_fake = self._is_fake_field(name)
            if not is_fake:
                raise AttributeError(f'No such field {name}')
        field = self._make_wrapped_field(name, field, is_fake=is_fake)
        self._wrapped_fields[name] = field
    return field",field is None,48,field is None,True,100.00000000000004,N/A
"def has_field(self, dotted_name) -> bool:
    """"""Checks whether the layer has the given field name.

        Can get a dotted name, i.e. layer.sublayer.subsublayer.field
        """"""
    parts = dotted_name.split('.')
    cur_layer = self
    for part in parts:
<mask>:
            cur_layer = cur_layer.get_field(part)
        else:
            return False
    return True",part in cur_layer.field_names,41,part in cur_layer.fields,False,60.80253214198355,N/A
"def _pretty_print_layer_fields(self, file: io.IOBase):
    for field_line in self._get_all_field_lines():
<mask>:
            field_name, field_line = field_line.split(':', 1)
            file.write(colored(field_name + ':', 'green', attrs=['bold']))
        file.write(colored(field_line, attrs=['bold']))",':' in field_line,21,':' in field_line,True,100.00000000000004,N/A
"def _get_field_or_layer_repr(self, field):
<mask>:
        yield ('\t' + field.layer_name + ':' + os.linesep)
        for line in field._get_all_field_lines():
            yield ('\t' + line)
    elif isinstance(field, list):
        for subfield_or_layer in field:
            yield from self._get_field_or_layer_repr(subfield_or_layer)
    else:
        yield f'\t{self._sanitize_field_name(field.name)}: {field.raw_value}{os.linesep}'","isinstance(field, JsonLayer)",34,"isinstance(field, Field)",False,53.7284965911771,N/A
"def find_version(*parts):
    here = pathlib.Path(__file__).parent
    version_file = here.joinpath(*parts).read_text()
    version_match = re.search('^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]$', version_file, re.M)
<mask>:
        return version_match.group(1)
    raise RuntimeError('Unable to find version string.')",version_match,24,version_match,True,100.00000000000004,N/A
"def _check_input_config(data):
    """"""
    Checks the types of the values in *data* against the expected types of
    config-values. If a value has the wrong type, raise an InputError.
    """"""
    for key, value in data.items():
<mask>:
            raise InputError(f'Unknown configuration key: {key}')
        if type(value) is not type(DEFAULTS[key]):
            expected_type = type(DEFAULTS[key]).__name__
            raise InputError(f'Data type for {key} must be {expected_type!r}')",key not in DEFAULTS,55,key not in DEFAULTS,True,100.00000000000004,N/A
"def _check_output_config(config):
    """"""
    Run sanity checks on the generated config after all parsing and
    preprocessing is done.

    Raise InputError if an error is encountered.
    """"""
<mask>:
        raise InputError('Please pass at least one file or directory')",not config['paths'],35,not os.path.isfile(config.output_dir),False,3.673526562988939,N/A
"def make_config(argv=None, tomlfile=None):
    """"""
    Returns a config object for vulture, merging both ``pyproject.toml`` and
    CLI arguments (CLI arguments have precedence).

    :param argv: The CLI arguments to be parsed. This value is transparently
        passed through to :py:meth:`argparse.ArgumentParser.parse_args`.
    :param tomlfile: An IO instance containing TOML data. By default this will
        auto-detect an existing ``pyproject.toml`` file and exists solely for
        unit-testing.
    """"""
    cli_config = _parse_args(argv)
    detected_toml_path = ''
<mask>:
        config = _parse_toml(tomlfile)
        detected_toml_path = str(tomlfile)
    else:
        toml_path = pathlib.Path(cli_config['config']).resolve()
        if toml_path.is_file():
            with open(toml_path, 'rb') as fconfig:
                config = _parse_toml(fconfig)
            detected_toml_path = str(toml_path)
        else:
            config = {}
    config.update(cli_config)
    for key, value in DEFAULTS.items():
        config.setdefault(key, value)
    if detected_toml_path and config['verbose']:
        print(f'Reading configuration from {detected_toml_path}')
    _check_output_config(config)
    return config",tomlfile,112,tomlfile,True,100.00000000000004,N/A
"def get_report(self, add_size=False):
<mask>:
        line_format = 'line' if self.size == 1 else 'lines'
        size_report = f', {self.size:d} {line_format}'
    else:
        size_report = ''
    return f'{utils.format_path(self.filename)}:{self.first_lineno:d}: {self.message} ({self.confidence}% confidence{size_report})'",add_size,27,add_size,True,100.00000000000004,N/A
"def get_whitelist_string(self):
    filename = utils.format_path(self.filename)
<mask>:
        return f'# {self.message} ({filename}:{self.first_lineno})'
    else:
        prefix = ''
        if self.typ in ['attribute', 'method', 'property']:
            prefix = '_.'
        return f'{prefix}{self.name}  # unused {self.typ} ({filename}:{self.first_lineno:d})'",self.typ == 'unreachable_code',29,self.typ == 'message',False,54.44460596606694,N/A
"def scavenge(self, paths, exclude=None):

    def prepare_pattern(pattern):
<mask>:
            pattern = f'*{pattern}*'
        return pattern
    exclude = [prepare_pattern(pattern) for pattern in exclude or []]

    def exclude_path(path):
        return _match(path, exclude, case=False)
    paths = [Path(path) for path in paths]
    for module in utils.get_modules(paths):
        if exclude_path(module):
            self._log('Excluded:', module)
            continue
        self._log('Scanning:', module)
        try:
            module_string = utils.read_file(module)
        except utils.VultureInputException as err:
            self._log(f'Error: Could not read file {module} - {err}\nTry to change the encoding to UTF-8.', file=sys.stderr, force=True)
            self.exit_code = ExitCode.InvalidInput
        else:
            self.scan(module_string, filename=module)
    unique_imports = {item.name for item in self.defined_imports}
    for import_name in unique_imports:
        path = Path('whitelists') / (import_name + '_whitelist.py')
        if exclude_path(path):
            self._log('Excluded whitelist:', path)
        else:
            try:
                module_data = pkgutil.get_data('vulture', str(path))
                self._log('Included whitelist:', path)
            except OSError:
                continue
            assert module_data is not None
            module_string = module_data.decode('utf-8')
            self.scan(module_string, filename=path)",not any((char in pattern for char in '*?[')),120,pattern,False,1.1253517471925916e-05,N/A
"def get_unused_code(self, min_confidence=0, sort_by_size=False) -> List[Item]:
    """"""
        Return ordered list of unused Item objects.
        """"""
<mask>:
        raise ValueError('min_confidence must be between 0 and 100.')

    def by_name(item):
        return (str(item.filename).lower(), item.first_lineno)

    def by_size(item):
        return (item.size,) + by_name(item)
    unused_code = self.unused_attrs + self.unused_classes + self.unused_funcs + self.unused_imports + self.unused_methods + self.unused_props + self.unused_vars + self.unreachable_code
    confidently_unused = [obj for obj in unused_code if obj.confidence >= min_confidence]
    return sorted(confidently_unused, key=by_size if sort_by_size else by_name)",not 0 <= min_confidence <= 100,70,min_confidence < 0 or min_confidence > 100,False,25.965358893403383,N/A
"def _log(self, *args, file=None, force=False):
<mask>:
        file = file or sys.stdout
        try:
            print(*args, file=file)
        except UnicodeEncodeError:
            x = ' '.join(map(str, args))
            print(x.encode(), file=file)",self.verbose or force,23,force,False,1.8315638888734187,N/A
"def get_first_line_number(node):
    """"""
    From Python 3.8 onwards, lineno for decorated objects is the line at which
    the object definition starts, which is different from what Python < 3.8
    reported -- the lineno of the first decorator. To preserve this behaviour
    of Vulture for newer Python versions, which is also more accurate for
    counting the size of the unused code chunk (if the property is unused, we
    also don't need it's decorators), we return the lineno of the first
    decorator, if there are any.
    """"""
    decorators = getattr(node, 'decorator_list', [])
<mask>:
        return decorators[0].lineno
    return node.lineno",decorators,94,decorators,True,100.00000000000004,N/A
"def visit(self, node):
    """"""When called, all children of this node have already been visited.""""""
<mask>:
        self._mark_as_no_fall_through(node)
        if isinstance(node, ast.Break):
            self._current_loop_has_break_statement = True
    elif isinstance(node, (ast.Module, ast.FunctionDef, ast.AsyncFunctionDef, ast.With, ast.AsyncWith)):
        self._can_fall_through_statements_analysis(node.body)
    elif isinstance(node, ast.While):
        self._handle_reachability_while(node)
        self._current_loop_has_break_statement = False
    elif isinstance(node, (ast.For, ast.AsyncFor)):
        self._can_fall_through_statements_analysis(node.body)
        self._current_loop_has_break_statement = False
    elif isinstance(node, ast.If):
        self._handle_reachability_if(node)
    elif isinstance(node, ast.IfExp):
        self._handle_reachability_if_expr(node)
    elif isinstance(node, ast.Try):
        self._handle_reachability_try(node)","isinstance(node, (ast.Break, ast.Continue, ast.Return, ast.Raise))",57,self._current_loop_has_break_statement is None,False,1.6037083902133962,N/A
"def _can_fall_through_statements_analysis(self, statements):
    """"""Report unreachable statements.
        Return True if we can execute the full list of statements.
        """"""
    for idx, statement in enumerate(statements):
<mask>:
            try:
                next_sibling = statements[idx + 1]
            except IndexError:
                next_sibling = None
            if next_sibling is not None:
                class_name = statement.__class__.__name__.lower()
                self._report(name=class_name, first_node=next_sibling, last_node=statements[-1], message=f""unreachable code after '{class_name}'"")
            return False
    return True",not self._can_fall_through(statement),54,idx < len(statements),False,3.550932348642477,N/A
"def _handle_reachability_if(self, node):
    has_else = bool(node.orelse)
<mask>:
        self._report(name='if', first_node=node, last_node=node.body if isinstance(node, ast.IfExp) else node.body[-1], message=""unsatisfiable 'if' condition"")
        if_can_fall_through = True
        else_can_fall_through = self._can_else_fall_through(node.orelse, condition_always_true=False)
    elif utils.condition_is_always_true(node.test):
        if_can_fall_through = self._can_fall_through_statements_analysis(node.body)
        else_can_fall_through = self._can_else_fall_through(node.orelse, condition_always_true=True)
        if has_else:
            self._report(name='else', first_node=node.orelse[0], last_node=node.orelse[-1], message=""unreachable 'else' block"")
        else:
            self._report(name='if', first_node=node, message='redundant if-condition')
    else:
        if_can_fall_through = self._can_fall_through_statements_analysis(node.body)
        else_can_fall_through = self._can_else_fall_through(node.orelse, condition_always_true=False)
    statement_can_fall_through = if_can_fall_through or else_can_fall_through
    if not statement_can_fall_through:
        self._mark_as_no_fall_through(node)",utils.condition_is_always_false(node.test),64,has_else,False,0.70335269181743,N/A
"def _can_else_fall_through(self, orelse, condition_always_true):
<mask>:
        return not condition_always_true
    return self._can_fall_through_statements_analysis(orelse)",not orelse,10,orelse.condition.is_true(),False,4.767707020457095,N/A
"def _handle_reachability_if_expr(self, node):
<mask>:
        self._report(name='ternary', first_node=node, last_node=node.body if isinstance(node, ast.IfExp) else node.body[-1], message=""unsatisfiable 'ternary' condition"")
    elif utils.condition_is_always_true(node.test):
        else_body = node.orelse
        self._report(name='ternary', first_node=else_body, message=""unreachable 'else' expression"")",utils.condition_is_always_false(node.test),25,utils.condition_is_always_true(node.test),False,78.25422900366432,N/A
"def _safe_eval(node, default):
    """"""
    Safely evaluate the Boolean expression under the given AST node.

    Substitute `default` for all sub-expressions that cannot be
    evaluated (because variables or functions are undefined).

    We could use eval() to evaluate more sub-expressions. However, this
    function is not safe for arbitrary Python code. Even after
    overwriting the ""__builtins__"" dictionary, the original dictionary
    can be restored
    (https://nedbatchelder.com/blog/201206/eval_really_is_dangerous.html).

    """"""
<mask>:
        results = [_safe_eval(value, default) for value in node.values]
        if isinstance(node.op, ast.And):
            return all(results)
        else:
            return any(results)
    elif isinstance(node, ast.UnaryOp) and isinstance(node.op, ast.Not):
        return not _safe_eval(node.operand, not default)
    else:
        try:
            return ast.literal_eval(node)
        except ValueError:
            return default","isinstance(node, ast.BoolOp)",97,"isinstance(node, ast.List)",False,70.71067811865478,N/A
"def get_decorator_name(decorator):
<mask>:
        decorator = decorator.func
    try:
        parts = []
        while isinstance(decorator, ast.Attribute):
            parts.append(decorator.attr)
            decorator = decorator.value
        parts.append(decorator.id)
    except AttributeError:
        parts = []
    return '@' + '.'.join(reversed(parts))","isinstance(decorator, ast.Call)",27,"isinstance(decorator, ast.Function)",False,70.71067811865478,N/A
"def get_modules(paths):
    """"""Retrieve Python files to check.

    Loop over all given paths, abort if any ends with .pyc, add the other given
    files (even those not ending with .py) and collect all .py files under the
    given directories.

    """"""
    modules = []
    for path in paths:
        path = path.resolve()
<mask>:
            if path.suffix == '.pyc':
                sys.exit(f'Error: *.pyc files are not supported: {path}')
            else:
                modules.append(path)
        elif path.is_dir():
            modules.extend(path.rglob('*.py'))
        else:
            sys.exit(f'Error: {path} could not be found.')
    return modules",path.is_file(),75,path.endswith('.py'),False,13.134549472120788,N/A
"def parse_noqa(code):
    noqa_lines = defaultdict(set)
    for lineno, line in enumerate(code, start=1):
        match = NOQA_REGEXP.search(line)
<mask>:
            for error_code in _parse_error_codes(match):
                error_code = NOQA_CODE_MAP.get(error_code, error_code)
                noqa_lines[error_code].add(lineno)
    return noqa_lines",match,26,match,True,100.00000000000004,N/A
"def check_size(example, size):
    tree = ast.parse(example)
    for node in tree.body:
<mask>:
            assert count_lines(node) == size
            break
    else:
        raise AssertionError('Failed to find top-level class ""Foo"" in code')","isinstance(node, ast.ClassDef) and node.name == 'Foo'",26,node.class_name == 'Foo',False,17.13859898988808,N/A
"def check_condition(code, result):
    condition = ast.parse(code, mode='eval').body
<mask>:
        assert utils.condition_is_always_true(condition)
    else:
        assert utils.condition_is_always_false(condition)",result,13,result,True,100.00000000000004,N/A
"def check(name, text):
    print('*' * 60)
    print(text)
    print('*' * 60)
    response = input(f'Accept this {name} (Y/n)? ').strip().lower()
<mask>:
        sys.exit(1)",response and response != 'y',19,response == 'Y',False,11.521590992286539,N/A
"@pytest.mark.parametrize('extension,color,mesh,comments', [('.ply', True, True, False), ('_ascii.ply', True, True, True), ('_ascii_vertex_index.ply', True, True, True), ('.npz', True, True, False), ('.obj', False, True, False), ('.off', False, False, False), ('_color.off', True, False, False), ('.bin', False, False, False), ('.las', True, False, False), ('.laz', True, False, False)])
def test_from_file(data_path, extension, color, mesh, comments):
<mask>:
        pytest.xfail('TODO: Review laz decompression error')
    cloud = PyntCloud.from_file(str(data_path / 'diamond{}'.format(extension)))
    assert_points_xyz(cloud)
    if color:
        assert_points_color(cloud)
    if mesh:
        assert_mesh(cloud)
    if comments:
        assert cloud.comments == ['PyntCloud is cool']",extension == '.laz',73,'laz' not in str(data_path / 'diamond{}'.format(extension)),False,4.480836160121357,N/A
"@pytest.mark.parametrize('extension,color,mesh,comments', [('.ply', True, True, False), ('_ascii.ply', True, True, True), ('.npz', True, True, False), ('.obj', False, True, False), ('.bin', False, False, False)])
def test_to_file(tmpdir, diamond, extension, color, mesh, comments):
    extra_write_args = {}
<mask>:
        extra_write_args['also_save'] = ['mesh']
    if comments:
        extra_write_args['comments'] = ['PyntCloud is cool']
    if extension == '.ply':
        extra_write_args['as_text'] = False
    if extension == '_ascii.ply':
        extra_write_args['as_text'] = True
    diamond.to_file(str(tmpdir.join('written{}'.format(extension))), **extra_write_args)
    written_file = PyntCloud.from_file(str(tmpdir.join('written{}'.format(extension))))
    assert_points_xyz(written_file)
    if color:
        assert_points_color(written_file)
    if mesh:
        assert_mesh(written_file)
    if comments:
        assert written_file.comments == ['PyntCloud is cool']",mesh,76,mesh,True,100.00000000000004,N/A
"def __repr__(self):
    default = ['_PyntCloud__points', '_PyntCloud__mesh', 'structures', 'xyz', 'centroid']
    others = ['\n\t {}: {}'.format(x, str(type(getattr(self, x)))) for x in self.__dict__ if x not in default]
<mask>:
        n_faces = 0
    else:
        n_faces = len(self.mesh)
    return DESCRIPTION.format(len(self.points), len(self.points.columns) - 3, n_faces, self.structures.n_kdtrees, self.structures.n_voxelgrids, self.centroid[0], self.centroid[1], self.centroid[2], ''.join(others))",self.mesh is None,45,self.mesh is None,True,100.00000000000004,N/A
"@points.setter
def points(self, df):
<mask>:
        raise TypeError('Points argument must be a DataFrame')
    elif not set(['x', 'y', 'z']).issubset(df.columns):
        raise ValueError('Points must have x, y and z coordinates')
    self._update_points(df)","not isinstance(df, pd.DataFrame)",27,"not isinstance(df, DataFrame)",False,53.137468984124546,N/A
"@mesh.setter
def mesh(self, df):
<mask>:
        if not isinstance(df, pd.DataFrame):
            raise TypeError('Mesh argument must be a DataFrame')
        elif not set(['v1', 'v2', 'v3']).issubset(df.columns):
            print(df.columns)
            raise ValueError('Mesh must have v1, v2 and v3 columns, at least')
        self.__mesh = df
    else:
        self.__mesh = None",df is not None,40,df is not None,True,100.00000000000004,N/A
"@classmethod
def from_file(cls, filename, **kwargs):
    """"""Extract data from file and construct a PyntCloud with it.

        Parameters
        ----------
        filename: str
            Path to the file from which the data will be read

        kwargs: only usable in some formats

        Returns
        -------
        PyntCloud: object
            PyntCloud instance, containing all valid elements in the file.
        """"""
    ext = filename.split('.')[-1].upper()
<mask>:
        raise ValueError('Unsupported file format; supported formats are: {}'.format(list(FROM_FILE)))
    else:
        return cls(**FROM_FILE[ext](filename, **kwargs))",ext not in FROM_FILE,66,ext not in FROM_FILE,True,100.00000000000004,N/A
"@classmethod
def from_instance(cls, library, instance, **kwargs):
    """"""Convert library's instance to PyntCloud intstance.

        Parameters
        ----------
        library: str
            Name of the library of the instance to be converted from.
        instance:
            `library's` instance
        kwargs: only usable in some formats

        Returns
        -------
        PyntCloud: object
            PyntCloud instance, containing all valid elements in the file.
        """"""
    library = library.upper()
<mask>:
        raise ValueError('Unsupported library; supported libraries are: {}'.format(list(FROM_INSTANCE)))
    else:
        return cls(**FROM_INSTANCE[library](instance, **kwargs))",library not in FROM_INSTANCE,65,library not in FROM_INSTANCE,True,100.00000000000004,N/A
"def plot_voxelgrid_with_matplotlib(voxelgrid, feature_vector, cmap='Oranges'):
<mask>:
        raise ImportError('matplotlib is required for 2d plotting')
    z_dim = voxelgrid.x_y_z[2]
    fig, axes = plt.subplots(int(np.ceil(z_dim / 4)), np.min((z_dim, 4)), figsize=(20, 20))
    plt.tight_layout()
    for i, ax in enumerate(axes.flat if z_dim > 1 else [plt.gca()]):
        if i < z_dim:
            ax.imshow(feature_vector[:, :, i], cmap=cmap, interpolation='nearest')
            ax.set_title('Level ' + str(i))
        else:
            ax.axis('off')",plt is None,52,voxelgrid.x_y_z[2] is None,False,6.772997136689072,N/A
"def plot_voxelgrid_with_pythreejs(voxel_centers, voxel_colors, width, height, **kwargs):
<mask>:
        raise ImportError('pythreejs is needed for plotting with pythreejs backend.')
    if display is None:
        raise ImportError('IPython is needed for plotting with pythreejs backend.')
    centroid, camera_position = get_centroid_and_camera_position(voxel_centers)
    camera = pythreejs.PerspectiveCamera(fov=90, aspect=width / height, position=camera_position, up=[0, 0, 1])
    mesh = get_voxelgrid_pythreejs(voxel_centers, voxel_colors)
    scene = pythreejs.Scene(children=[camera, mesh], background=None)
    controls = pythreejs.OrbitControls(controlling=camera, target=tuple(centroid))
    camera.lookAt(tuple(centroid))
    renderer = pythreejs.Renderer(scene=scene, camera=camera, controls=[controls], width=width, height=height)
    display(renderer)",pythreejs is None,65,pythreejs is None,True,100.00000000000004,N/A
"def plot_voxelgrid(voxelgrid, d=3, mode='binary', backend='pythreejs', cmap='Oranges', **kwargs):
    feature_vector = voxelgrid.get_feature_vector(mode)
<mask>:
        return plot_voxelgrid_with_matplotlib(voxelgrid, feature_vector, cmap)
    elif d != 3:
        raise ValueError('d must be 2 or 3')
    if mode != 'binary' and plt is None:
        raise ImportError('matplotlib is required for non-binary plotting')
    elif mode != 'binary':
        s_m = plt.cm.ScalarMappable(cmap=cmap)
        flattened = feature_vector.ravel()
        rgba = s_m.to_rgba(flattened[np.nonzero(flattened)])
        voxel_colors = rgba[:, :3].astype(np.float32)
    elif voxelgrid.colors is not None:
        voxel_colors = (voxelgrid.voxel_colors / 255).astype(np.float32)
    else:
        voxel_colors = np.full((voxelgrid.n_voxels, 3), 200, dtype=np.int)
    feature_vector = voxelgrid.get_feature_vector(mode)
    scaled_shape = np.asarray(voxelgrid.shape) / min(voxelgrid.shape)
    voxel_centers = (np.argwhere(feature_vector) * scaled_shape).astype(np.float32)
    if backend == 'pythreejs':
        plot_voxelgrid_with_pythreejs(voxel_centers, voxel_colors, **kwargs)
    else:
        raise NotImplementedError('{} backend is not supported'.format(backend))",d == 2,102,d == 2,True,100.00000000000004,N/A
"def plot_with_pythreejs(cloud, **kwargs):
<mask>:
        raise ImportError('ipywidgets is needed for plotting with pythreejs backend.')
    if pythreejs is None:
        raise ImportError('pythreejs is needed for plotting with pythreejs backend.')
    if display is None:
        raise ImportError('IPython is needed for plotting with pythreejs backend.')
    colors = get_colors(cloud, kwargs['use_as_color'], kwargs['cmap'])
    ptp = np.ptp(cloud.xyz)
    children = []
    widgets = []
    if kwargs['mesh']:
        raise NotImplementedError('Plotting mesh geometry with pythreejs backend is not supported yet.')
    if kwargs['polylines']:
        lines = get_polylines_pythreejs(kwargs['polylines'])
        children.extend(lines)
    points = get_pointcloud_pythreejs(cloud.xyz, colors)
    children.append(points)
    initial_point_size = kwargs['initial_point_size'] or ptp / 10
    size = ipywidgets.FloatSlider(value=initial_point_size, min=0.0, max=initial_point_size * 10, step=initial_point_size / 100)
    ipywidgets.jslink((size, 'value'), (points.material, 'size'))
    widgets.append(ipywidgets.Label('Point size:'))
    widgets.append(size)
    if kwargs['scene']:
        kwargs['scene'].children = [points] + list(kwargs['scene'].children)
    else:
        camera = get_camera_pythreejs(cloud.centroid, cloud.xyz, kwargs['width'], kwargs['height'])
        children.append(camera)
        controls = [get_orbit_controls(camera, cloud.centroid)]
        scene = pythreejs.Scene(children=children)
        renderer = pythreejs.Renderer(scene=scene, camera=camera, controls=controls, width=kwargs['width'], height=kwargs['height'])
        display(renderer)
        color = ipywidgets.ColorPicker(value=kwargs['background'])
        ipywidgets.jslink((color, 'value'), (scene, 'background'))
        widgets.append(ipywidgets.Label('Background color:'))
        widgets.append(color)
    display(ipywidgets.HBox(children=widgets))
    return scene if kwargs['return_scene'] else None",ipywidgets is None,148,ipywidgets is None,True,100.00000000000004,N/A
"def get_colors(cloud, use_as_color, cmap):
    try:
        colors = cloud.points[use_as_color].values
    except KeyError:
        colors = None
<mask>:
        import matplotlib.pyplot as plt
        s_m = plt.cm.ScalarMappable(cmap=cmap)
        colors = s_m.to_rgba(colors)[:, :-1] * 255
    elif colors is None:
        colors = np.repeat([[255, 125, 0]], cloud.xyz.shape[0], axis=0)
    return colors.astype(np.uint8)","use_as_color != ['red', 'green', 'blue'] and colors is not None",40,cmap is not None,False,1.3983735350395892,N/A
"def plot_with_pyvista(cloud, **kwargs):
    """"""Plot using PyVista. Additional kwargs for controoling PyVista scene are
    listed here.


    Parameters
    ----------
    off_screen : bool, optional
        Renders off screen when False.  Useful for automated screenshots.

    notebook : bool, optional
        When True, the resulting plot is placed inline a jupyter notebook.
        Assumes a jupyter console is active.  Automatically enables off_screen.

    render_points_as_spheres : bool, optional
        Render the points as spheres

    eye_dome_lighting : bool, optional
        Leverage PyVista's Eyd Dome Lighting (EDL) shading for improved
        depth perception.

    use_panel : bool, optional
        If False, the interactive rendering from panel will not be used in
        notebooks

    cpos : list(tuple(floats))
        The camera position to use

    title : string, optional
        Title of plotting window.

    screenshot : string, optional
        The path to the PNG file to save a screenshot

    point_size : float, optional
        Alias for ``initial_point_size``
    """"""
<mask>:
        raise ImportError('PyVista must be installed to use it for plotting.')
    point_size = kwargs['initial_point_size']
    if point_size is None:
        point_size = kwargs.pop('point_size', 5.0)
    colors = get_colors(cloud, kwargs['use_as_color'], kwargs['cmap'])
    poly_data = cloud.to_instance('pyvista', mesh=kwargs.pop('mesh', False))
    plotter = pv.Plotter(window_size=[kwargs.pop('width'), kwargs.pop('height')], off_screen=kwargs.pop('off_screen', None), notebook=kwargs.pop('notebook', None))
    plotter.add_mesh(poly_data, point_size=point_size, scalars=colors, rgb=True, render_points_as_spheres=kwargs.pop('render_points_as_spheres', False))
    if kwargs.pop('eye_dome_lighting', None):
        plotter.enable_eye_dome_lighting()
    return plotter.show(use_panel=kwargs.pop('use_panel', None), title=kwargs.pop('title', None), screenshot=kwargs.pop('screenshot', False), cpos=kwargs.pop('cpos', None))",pv is None,192,not pv,False,30.326532985631665,N/A
"def compute(self):
    """"""ABC API.""""""
    xyzmin = self._points.min(0)
    xyzmax = self._points.max(0)
    xyz_range = np.ptp(self._points, 0)
<mask>:
        margin = max(xyz_range) - xyz_range
        xyzmin = xyzmin - margin / 2
        xyzmax = xyzmax + margin / 2
    for n, size in enumerate(self.sizes):
        if size is None:
            continue
        margin = (np.ptp(self._points, 0)[n] // size + 1) * size - np.ptp(self._points, 0)[n]
        xyzmin[n] -= margin / 2
        xyzmax[n] += margin / 2
        self.x_y_z[n] = ((xyzmax[n] - xyzmin[n]) / size).astype(int)
    self.xyzmin = xyzmin
    self.xyzmax = xyzmax
    segments = []
    shape = []
    for i in range(3):
        s, step = np.linspace(xyzmin[i], xyzmax[i], num=self.x_y_z[i] + 1, retstep=True)
        segments.append(s)
        shape.append(step)
    self.segments = segments
    self.shape = shape
    self.n_voxels = np.prod(self.x_y_z)
    self.id = 'V({},{},{})'.format(self.x_y_z, self.sizes, self.regular_bounding_box)
    self.voxel_x = np.clip(np.searchsorted(self.segments[0], self._points[:, 0]) - 1, 0, self.x_y_z[0])
    self.voxel_y = np.clip(np.searchsorted(self.segments[1], self._points[:, 1]) - 1, 0, self.x_y_z[1])
    self.voxel_z = np.clip(np.searchsorted(self.segments[2], self._points[:, 2]) - 1, 0, self.x_y_z[2])
    self.voxel_n = np.ravel_multi_index([self.voxel_x, self.voxel_y, self.voxel_z], self.x_y_z)
    midsegments = [(self.segments[i][1:] + self.segments[i][:-1]) / 2 for i in range(3)]
    self.voxel_centers = cartesian(midsegments).astype(np.float32)
    if self.colors is not None:
        order = np.argsort(self.voxel_n)
        _, breaks, counts = np.unique(self.voxel_n[order], return_index=True, return_counts=True)
        repeated_counts = np.repeat(counts[:, None], 3, axis=1)
        squared_colors = np.square(self.colors[order].astype(np.int64))
        summed_colors = np.add.reduceat(squared_colors, breaks, axis=0)
        averaged_colors = np.sqrt(summed_colors / repeated_counts)
        self.voxel_colors = np.rint(averaged_colors).astype(np.uint8)",self.regular_bounding_box,199,xyz_range != 0,False,6.870636427700047,N/A
"def get_feature_vector(self, mode='binary'):
    """"""Return a vector of size self.n_voxels. See mode options below.

        Parameters
        ----------
        mode: str in available modes. See Notes
            Default ""binary""

        Returns
        -------
        feature_vector: [n_x, n_y, n_z] ndarray
            See Notes.

        Notes
        -----
        Available modes are:

        binary
            0 for empty voxels, 1 for occupied.

        density
            number of points inside voxel / total number of points.

        TDF
            Truncated Distance Function. Value between 0 and 1 indicating the distance
            between the voxel's center and the closest point. 1 on the surface,
            0 on voxels further than 2 * voxel side.

        x_max, y_max, z_max
            Maximum coordinate value of points inside each voxel.

        x_mean, y_mean, z_mean
            Mean coordinate value of points inside each voxel.
        """"""
    vector = np.zeros(self.n_voxels)
<mask>:
        vector[np.unique(self.voxel_n)] = 1
    elif mode == 'density':
        count = np.bincount(self.voxel_n)
        vector[:len(count)] = count
        vector /= len(self.voxel_n)
    elif mode == 'TDF':
        kdt = KDTree(self._points)
        vector, i = kdt.query(self.voxel_centers, workers=-1)
    elif mode.endswith('_max'):
        if not is_numba_avaliable:
            raise ImportError('numba is required to compute {}'.format(mode))
        axis = {'x_max': 0, 'y_max': 1, 'z_max': 2}
        vector = groupby_max(self._points, self.voxel_n, axis[mode], vector)
    elif mode.endswith('_mean'):
        if not is_numba_avaliable:
            raise ImportError('numba is required to compute {}'.format(mode))
        axis = {'x_mean': 0, 'y_mean': 1, 'z_mean': 2}
        voxel_sum = groupby_sum(self._points, self.voxel_n, axis[mode], np.zeros(self.n_voxels))
        voxel_count = groupby_count(self._points, self.voxel_n, np.zeros(self.n_voxels))
        vector = np.nan_to_num(voxel_sum / voxel_count)
    else:
        raise NotImplementedError('{} is not a supported feature vector mode'.format(mode))
    return vector.reshape(self.x_y_z)",mode == 'binary',219,mode == 'binary',True,100.00000000000004,N/A
"def get_voxel_neighbors(self, voxel):
    """"""Get valid, non-empty 26 neighbors of voxel.

        Parameters
        ----------
        voxel: int in self.set_voxel_n

        Returns
        -------
        neighbors: list of int
            Indices of the valid, non-empty 26 neighborhood around voxel.
        """"""
    x, y, z = np.unravel_index(voxel, self.x_y_z)
    valid_x = []
    valid_y = []
    valid_z = []
<mask>:
        valid_x.append(x - 1)
    if y - 1 >= 0:
        valid_y.append(y - 1)
    if z - 1 >= 0:
        valid_z.append(z - 1)
    valid_x.append(x)
    valid_y.append(y)
    valid_z.append(z)
    if x + 1 < self.x_y_z[0]:
        valid_x.append(x + 1)
    if y + 1 < self.x_y_z[1]:
        valid_y.append(y + 1)
    if z + 1 < self.x_y_z[2]:
        valid_z.append(z + 1)
    valid_neighbor_indices = cartesian((valid_x, valid_y, valid_z))
    ravel_indices = np.ravel_multi_index((valid_neighbor_indices[:, 0], valid_neighbor_indices[:, 1], valid_neighbor_indices[:, 2]), self.x_y_z)
    return [x for x in ravel_indices if x in np.unique(self.voxel_n)]",x - 1 >= 0,123,x - 1 >= 0,True,100.00000000000004,N/A
"def __setitem__(self, key, val):
<mask>:
        raise TypeError('{} must be base.Structure subclass'.format(key))
    if key.startswith('V'):
        self.n_voxelgrids += 1
    elif key.startswith('K'):
        self.n_kdtrees += 1
    elif key.startswith('D'):
        self.n_delaunays += 1
    elif key.startswith('CH'):
        self.n_convex_hulls += 1
    else:
        raise ValueError('{} is not a valid structure.id'.format(key))
    super().__setitem__(key, val)","not issubclass(val.__class__, Structure)",41,"not isinstance(key, base.Structure)",False,8.046371859135371,N/A
"def are_valid(self, k_points):
    x = np.ones((4, 4))
    x[:-1, :] = k_points.T
<mask>:
        return False
    else:
        return True",np.linalg.det(x) == 0,17,x.sum() < self.rate,False,5.708765135015525,N/A
"def single_fit(points, model, sampler=RandomRansacSampler, model_kwargs={}, sampler_kwargs={}, max_iterations=100, return_model=False, n_inliers_to_stop=None):
    """"""RANdom SAmple Consensus for fitting model a single model to points.

    points: ndarray
        (N, M) ndarray where N is the number of points and M is the number
        scalar fields associated to each of those points.
        M is usually 3 for representing the x, y, and z coordinates of each point.

    model: Ransac_Model
        Class (NOT INSTANCE!) representing the model that will be fitted to points.
        Check ransac/models for reference.

    sampler: Ransac_Sampler
        Class (NOT INSTANCE!) used to sample points on each iteration.
        Check ransac/samplers for reference.

    model_kwargs: dict, optional
        Default: {}
        Arguments that will be used on model's instantiation.
        Variable according to passed model.

    sampler_kwargs: dict, optional
        Default: {}
        Arguments that will be used on sampler's instantiation.
        Variable according to passed sampler.

    max_iterations: int, optional
        Default: 100
        Maximum number of iterations.

    return_model: bool, optional (default False)
        Whether the best fitted model will be returned or not.

    n_inliers_to_stop: int, optional
        Default None
        If the model fits a number of inliers > n_inliers_to_stop the loop will end.

    """"""
    model = model(**model_kwargs)
    sampler = sampler(points, model.k, **sampler_kwargs)
    n_best_inliers = 0
<mask>:
        n_inliers_to_stop = len(points)
    for i in range(max_iterations):
        k_points = sampler.get_sample()
        if not model.are_valid(k_points):
            print(k_points)
            continue
        model.fit(k_points)
        all_distances = model.get_distances(points)
        inliers = all_distances <= model.max_dist
        n_inliers = np.sum(inliers)
        if n_inliers > n_best_inliers:
            n_best_inliers = n_inliers
            best_inliers = inliers
            if n_best_inliers > n_inliers_to_stop:
                break
    if return_model:
        model.least_squares_fit(points[best_inliers])
        return (best_inliers, model)
    else:
        return best_inliers",n_inliers_to_stop is None,237,n_inliers_to_stop is None,True,100.00000000000004,N/A
"def read_npz(filename, points_name='points', mesh_name='mesh'):
    """"""Read a .npz file and store all possible elements in pandas DataFrame
    Parameters
    ----------
    filename: str
        Path to the filename
    Returns
    -------
    data: dict
        If possible, elements as pandas DataFrames else input format
    """"""
    data = {}
    with np.load(filename) as npz:
        data['points'] = pd.DataFrame(npz[points_name])
<mask>:
            data['mesh'] = pd.DataFrame(npz[mesh_name])
    return data",mesh_name in npz,54,mesh_name,False,51.341711903259224,N/A
"def write_npz(filename, **kwargs):
    """"""
    Parameters
    ----------
    filename: str
        The created file will be named with this

    kwargs: Elements of the pyntcloud to be saved

    Returns
    -------
    boolean
        True if no problems
    """"""
    for k in kwargs:
<mask>:
            kwargs[k] = kwargs[k].to_records(index=False)
    np.savez_compressed(filename, **kwargs)
    return True","isinstance(kwargs[k], pd.DataFrame)",44,k.is_index,False,3.8261660656802645,N/A
"def from_pyvista(poly_data, **kwargs):
    """"""Load a PyntCloud mesh from PyVista's PolyData instance""""""
    try:
        import pyvista as pv
    except ImportError:
        raise ImportError('PyVista must be installed. Try `pip install pyvista`')
<mask>:
        raise TypeError('Type {} not yet supported for conversion.'.format(type(poly_data)))
    mesh = None
    if poly_data.faces is not None:
        mesh = poly_data.faces.reshape(-1, 4)
        if not np.all(3 == mesh[:, 0]):
            raise ValueError('This mesh is not triangulated. Try triangulating the mesh before passing to PyntCloud.')
        mesh = pd.DataFrame(data=mesh[:, 1:], columns=['v1', 'v2', 'v3'])
    points = pd.DataFrame(data=poly_data.points, columns=['x', 'y', 'z'])
    scalars = poly_data.point_data
    for name, array in scalars.items():
        if array.ndim == 1:
            points[name] = array
        elif array.ndim == 2:
            if name == 'RGB':
                points['red'] = array[:, 0]
                points['green'] = array[:, 1]
                points['blue'] = array[:, 2]
            elif name == 'Normals':
                points['nx'] = array[:, 0]
                points['ny'] = array[:, 1]
                points['nz'] = array[:, 2]
            else:
                for n in range(array.shape[1]):
                    points['{}_{}'.format(name, n)] = array[:, n]
        else:
            warnings.warn('Ignoring scalar field {} with ndim > 2 ({})'.format(name, array.ndim))
    return {'points': points, 'mesh': mesh}","not isinstance(poly_data, pv.PolyData)",158,"not isinstance(poly_data, pv.PolyData)",True,100.00000000000004,N/A
"def to_pyvista(cloud, mesh=False, use_as_color=('red', 'green', 'blue'), **kwargs):
    """"""Convert PyntCloud's instance `cloud` to PyVista's PolyData instance""""""
    try:
        import pyvista as pv
    except ImportError:
        raise ImportError('PyVista must be installed. Try `pip install pyvista`')
<mask>:
        mesh = cloud.mesh[['v1', 'v2', 'v3']].values
    else:
        mesh = None
    if mesh is not None:
        types = np.full(len(mesh), 3, dtype=int)
        faces = np.insert(mesh, 0, types, axis=1)
        poly = pv.PolyData(cloud.xyz, faces)
    else:
        poly = pv.PolyData(cloud.xyz)
    avoid = ['x', 'y', 'z']
    if all((c in cloud.points.columns for c in use_as_color)):
        colors = cloud.points[list(use_as_color)].values
        poly.point_data['RGB'] = colors
        avoid += list(use_as_color)
    for name in cloud.points.columns:
        if name not in avoid:
            poly.point_data[name] = cloud.points[name]
    return poly",mesh and cloud.mesh is not None,101,mesh,False,0.09118819655545167,N/A
"def from_open3d(o3d_data, **kwargs):
    """"""Create a PyntCloud instance from Open3D's PointCloud/TriangleMesh instance""""""
    try:
        import open3d as o3d
    except ImportError:
        raise ImportError('Open3D must be installed. Try `pip install open3d`')
<mask>:
        raise TypeError(f'Type {type(o3d_data)} not supported for conversion.Expected {o3d.geometry.PointCloud} or {o3d.geometry.TriangleMesh}')
    mesh = None
    if isinstance(o3d_data, o3d.geometry.TriangleMesh):
        mesh = pd.DataFrame(data=np.asarray(o3d_data.triangles), columns=['v1', 'v2', 'v3'])
        points = pd.DataFrame(data=np.asarray(o3d_data.vertices), columns=['x', 'y', 'z'])
        if o3d_data.vertex_colors:
            colors = (np.asarray(o3d_data.vertex_colors) * 255).astype(np.uint8)
            points['red'] = colors[:, 0]
            points['green'] = colors[:, 1]
            points['blue'] = colors[:, 2]
        if o3d_data.vertex_normals:
            normals = np.asarray(o3d_data.vertex_normals)
            points['nx'] = normals[:, 0]
            points['ny'] = normals[:, 1]
            points['nz'] = normals[:, 2]
    elif isinstance(o3d_data, o3d.geometry.PointCloud):
        points = pd.DataFrame(data=np.asarray(o3d_data.points), columns=['x', 'y', 'z'])
        if o3d_data.colors:
            colors = (np.asarray(o3d_data.colors) * 255).astype(np.uint8)
            points['red'] = colors[:, 0]
            points['green'] = colors[:, 1]
            points['blue'] = colors[:, 2]
        if o3d_data.normals:
            normals = np.asarray(o3d_data.normals)
            points['nx'] = normals[:, 0]
            points['ny'] = normals[:, 1]
            points['nz'] = normals[:, 2]
    return {'points': points, 'mesh': mesh}","not isinstance(o3d_data, (o3d.geometry.PointCloud, o3d.geometry.TriangleMesh))",142,"not isinstance(o3d_data, (o3d.geometry.PointCloud, o3d.geometry.TriangleMesh))",True,100.00000000000004,N/A
"def to_open3d(cloud, mesh=True, colors=True, normals=True, **kwargs):
    """"""Convert PyntCloud's instance `cloud` to Open3D's PointCloud/TriangleMesh instance""""""
    try:
        import open3d as o3d
    except ImportError:
        raise ImportError('Open3D must be installed. Try `pip install open3d`')
<mask>:
        triangle_mesh = o3d.geometry.TriangleMesh()
        triangle_mesh.triangles = o3d.utility.Vector3iVector(cloud.mesh[['v1', 'v2', 'v3']].values)
        triangle_mesh.vertices = o3d.utility.Vector3dVector(cloud.xyz)
        if colors and {'red', 'green', 'blue'}.issubset(cloud.points.columns):
            triangle_mesh.vertex_colors = o3d.utility.Vector3dVector(cloud.points[['red', 'green', 'blue']].values)
        if normals and {'nx', 'ny', 'nz'}.issubset(cloud.points.columns):
            triangle_mesh.vertex_normals = o3d.utility.Vector3dVector(cloud.points[['nx', 'ny', 'nz']].values)
        return triangle_mesh
    else:
        point_cloud = o3d.geometry.PointCloud()
        point_cloud.points = o3d.utility.Vector3dVector(cloud.xyz)
        if colors and {'red', 'green', 'blue'}.issubset(cloud.points.columns):
            point_cloud.colors = o3d.utility.Vector3dVector(cloud.points[['red', 'green', 'blue']].values)
        if normals and {'nx', 'ny', 'nz'}.issubset(cloud.points.columns):
            point_cloud.normals = o3d.utility.Vector3dVector(cloud.points[['nx', 'ny', 'nz']].values)
        return point_cloud",mesh and cloud.mesh is not None,97,mesh,False,0.09118819655545167,N/A
"def read_off(filename):
    with open(filename) as off:
        first_line = off.readline()
<mask>:
            raise ValueError('The file does not start with the word OFF')
        color = True if 'C' in first_line else False
        n_points = 0
        n_faces = 0
        count = 1
        for line in off:
            count += 1
            if line.startswith('#'):
                continue
            line = line.strip().split()
            if len(line) > 1:
                n_points = int(line[0])
                n_faces = int(line[1])
                break
        if n_points == 0:
            raise ValueError('The file has no points')
        data = {}
        point_names = ['x', 'y', 'z']
        point_types = {'x': np.float32, 'y': np.float32, 'z': np.float32}
        if color:
            point_names.extend(['red', 'green', 'blue'])
            point_types = dict(point_types, **{'red': np.uint8, 'green': np.uint8, 'blue': np.uint8})
        data['points'] = pd.read_csv(off, sep=' ', header=None, engine='c', nrows=n_points, names=point_names, dtype=point_types, index_col=False, comment='#')
        data['mesh'] = pd.read_csv(filename, sep=' ', header=None, engine='c', skiprows=count + n_points, nrows=n_faces, usecols=[1, 2, 3], names=['v1', 'v2', 'v3'], comment='#')
        return data",'OFF' not in first_line,134,first_line.startswith('word OFF'),False,17.747405280050266,N/A
"def get_color_dtype(data, column_names):
    has_color = all((column in data['points'] for column in column_names))
<mask>:
        color_data_types = [data['points'][column_name].dtype for column_name in column_names]
        if len(set(color_data_types)) > 1:
            raise TypeError(f'Data types of color values are inconsistent: got {color_data_types}')
        color_data_type = color_data_types[0]
    else:
        color_data_type = None
    return color_data_type",has_color,43,has_color,True,100.00000000000004,N/A
"def convert_color_to_dtype(data, output_dtype):
    assert output_dtype in ['uint8', 'uint16']
    column_names = ['red', 'green', 'blue']
    input_dtype = get_color_dtype(data, column_names)
<mask>:
        if input_dtype not in ['uint8', 'uint16']:
            raise ValueError(f""Invalid color dtype. Expected one of ['uint8', 'uint16'], but got {input_dtype}"")
        if input_dtype == 'uint8' and output_dtype == 'uint16':
            data['points'].loc[:, column_names] *= 256
        elif input_dtype == 'uint16' and output_dtype == 'uint8':
            column_max_values = data['points'].loc[:, column_names].max()
            if column_max_values.to_numpy().max() >= 256:
                data['points'].loc[:, column_names] /= 256
        data['points'] = data['points'].astype({'red': output_dtype, 'green': output_dtype, 'blue': output_dtype})
    return data",input_dtype is not None,78,input_dtype is not None,True,100.00000000000004,N/A
"def read_las_with_laspy(filename):
<mask>:
        raise ImportError('laspy (>=2.0) is needed for reading .las files.')
    data = {}
    with laspy.open(filename) as las_file:
        las = las_file.read()
        data['points'] = pd.DataFrame(las.points.array)
        data['points'].columns = (name.lower() for name in data['points'].columns)
        data['points']['x'] = pd.Series(np.array(las.x))
        data['points']['y'] = pd.Series(np.array(las.y))
        data['points']['z'] = pd.Series(np.array(las.z))
        data['las_header'] = las.header
    return data",laspy is None,46,not LossPy_2_0,False,0.0,N/A
"def read_las_with_pylas(filename):
    data = {}
<mask>:
        raise ImportError('pylas is needed for reading .las files.')
    with pylas.open(filename) as las_file:
        las = las_file.read()
        data['points'] = pd.DataFrame(las.points)
        data['points'].columns = (name.lower() for name in data['points'].columns)
        data['points']['x'] = pd.Series(np.array(las.x))
        data['points']['y'] = pd.Series(np.array(las.y))
        data['points']['z'] = pd.Series(np.array(las.z))
        data['las_header'] = las.header
    return data",pylas is None,45,not pylas,False,30.326532985631665,N/A
"def read_las(filename, xyz_dtype='float32', rgb_dtype='uint8', backend='laspy'):
    """"""Read a .las/laz file and store elements in pandas DataFrame.

    Parameters
    ----------
    filename: str
        Path to the filename
    xyz_dtype: str
        Defines the data type of the xyz coordinate
    rgb_dtype: str
        Defines the data type of the color
    Returns
    -------
    data: dict
        Elements as pandas DataFrames.
    """"""
<mask>:
        data = read_las_with_pylas(filename)
    elif backend == 'laspy':
        data = read_las_with_laspy(filename)
    else:
        raise ValueError(f""Unsupported backend. Expected one of ['pylas', 'laspy'] but got {backend}"")
    data = convert_location_to_dtype(data, xyz_dtype)
    data = convert_color_to_dtype(data, rgb_dtype)
    return data",backend == 'pylas',84,backend == 'pylas',True,100.00000000000004,N/A
"def write_ply(filename, points=None, mesh=None, as_text=False, comments=None):
    """"""Write a PLY file populated with the given fields.

    Parameters
    ----------
    filename: str
        The created file will be named with this
    points: ndarray
    mesh: ndarray
    as_text: boolean
        Set the write mode of the file. Default: binary
    comments: list of string

    Returns
    -------
    boolean
        True if no problems

    """"""
<mask>:
        filename += '.ply'
    with open(filename, 'w') as ply:
        header = ['ply']
        if as_text:
            header.append('format ascii 1.0')
        else:
            header.append('format binary_' + sys.byteorder + '_endian 1.0')
        if comments:
            for comment in comments:
                header.append('comment ' + comment)
        if points is not None:
            header.extend(describe_element('vertex', points))
        if mesh is not None:
            mesh = mesh.copy()
            mesh.insert(loc=0, column='n_points', value=3)
            mesh['n_points'] = mesh['n_points'].astype('u1')
            header.extend(describe_element('face', mesh))
        header.append('end_header')
        for line in header:
            ply.write('%s\n' % line)
    if as_text:
        if points is not None:
            points.to_csv(filename, sep=' ', index=False, header=False, mode='a', encoding='ascii')
        if mesh is not None:
            mesh.to_csv(filename, sep=' ', index=False, header=False, mode='a', encoding='ascii')
    else:
        with open(filename, 'ab') as ply:
            if points is not None:
                points.to_records(index=False).tofile(ply)
            if mesh is not None:
                mesh.to_records(index=False).tofile(ply)
    return True",not filename.endswith('ply'),166,not filename.endswith('.ply'),False,46.713797772819994,N/A
"def describe_element(name, df):
    """"""Takes the columns of the dataframe and builds a ply-like description

    Parameters
    ----------
    name: str
    df: pandas DataFrame

    Returns
    -------
    element: list[str]
    """"""
    property_formats = {'f': 'float', 'u': 'uchar', 'i': 'int', 'b': 'bool'}
    element = ['element ' + name + ' ' + str(len(df))]
<mask>:
        element.append('property list uchar int vertex_indices')
    else:
        for i in range(len(df.columns)):
            f = property_formats[str(df.dtypes[i])[0]]
            element.append('property ' + f + ' ' + df.columns.values[i])
    return element",name == 'face',72,df.dtypes == [],False,13.134549472120788,N/A
"def read_bin(filename, shape=None, **kwargs):
    """"""Read a _raw binary_ file and store all possible elements in pandas DataFrame.

    If the shape of the array is known, it can be specified using `shape`. The
    first three columns are used for x, y and z.  Otherwise the binary file is
    assumed have row-major format, three columns are formed and used as x, y and
    z , respectively.

    NOTE: binary files that are not `raw` will not behave as expected. If they
    contain a header/footer with meta data, or were generated e.g. via Protobuf,
    then bahviour is also undefined.

    Parameters
    ----------
    filename: str
        Path to the filename
    shape: (n_rows, n_cols) - shape to be formed from the loaded binary array, optional.
    **kwargs:
    kwargs: numpy.fromfile supported kwargs
        Check NumPy documentation for all possibilities.

    Returns
    -------
    data: dict
        If possible, elements as pandas DataFrames else a NumPy ndarray
    """"""
    data = {}
    kwargs['dtype'] = kwargs.get('dtype', np.float32)
    arr = np.fromfile(filename, **kwargs)
<mask>:
        try:
            arr = arr.reshape(shape)
        except ValueError:
            raise ValueError('The array cannot be reshaped to {0} as it has {1} elements, which is not divisible by three'.format(shape, arr.size))
    else:
        arr = arr.reshape((-1, 3))
    data['points'] = pd.DataFrame(arr[:, 0:3], columns=['x', 'y', 'z'])
    return data",shape is not None,195,shape,False,4.9787068367863965,N/A
"def write_bin(filename, **kwargs):
    """"""Write the raw point data in `PyntCloud.xyz` to a binary file.

    Parameters
    ----------
    filename: str
        The created file will be named with this
    kwargs: numpy.ndarray.tofile supported kwargs
        Check NumPy documentation on raw binary files for all possibilities.

    Returns
    -------
    boolean
        True if no problems
    """"""
    point_array = kwargs['points'][['x', 'y', 'z']].values
    del kwargs['points']
    remaining_kwargs = set(kwargs.keys()) - set(['sep', 'format'])
<mask>:
        raise ValueError('Only keyword arguments meant for numpy.ndarray.tofile are accepted. Please see the numpy documentation')
    point_array.tofile(filename, **kwargs)
    return True",not len(remaining_kwargs) == 0,80,remaining_kwargs != set(),False,19.03868163669696,N/A
"def read_obj(filename):
    """"""Reads and obj file and return the elements as pandas Dataframes.

    Parameters
    ----------
    filename: str
        Path to the obj file.

    Returns
    -------
    Each obj element found as pandas Dataframe.

    """"""
    v = []
    vn = []
    vt = []
    f = []
    with open(filename) as obj:
        for line in obj:
<mask>:
                v.append(line.strip()[1:].split())
            elif line.startswith('vn'):
                vn.append(line.strip()[2:].split())
            elif line.startswith('vt'):
                vt.append(line.strip()[2:].split())
            elif line.startswith('f'):
                f.append(line.strip()[1:].lstrip())
    points = pd.DataFrame(v, dtype='f4', columns=['x', 'y', 'z', 'w'][:len(v[0])])
    if len(vn) > 0:
        points = points.join(pd.DataFrame(vn, dtype='f4', columns=['nx', 'ny', 'nz']))
    if len(vt) > 0:
        points = points.join(pd.DataFrame(vt, dtype='f4', columns=['u', 'v']))
    data = {'points': points}
    if len(f) < 1:
        return data
    mesh_columns = []
    if f[0].count('//') > 0:
        for i in range(f[0].count('//')):
            mesh_columns.append('v{}'.format(i + 1))
            mesh_columns.append('vn{}'.format(i + 1))
    elif f[0].count('/') > 0:
        if len(vn) > 0:
            for i in range(f[0].count('/') // 2):
                mesh_columns.append('v{}'.format(i + 1))
                mesh_columns.append('vt{}'.format(i + 1))
                mesh_columns.append('vn{}'.format(i + 1))
        else:
            for i in range(f[0].count('/')):
                mesh_columns.append('v{}'.format(i + 1))
                mesh_columns.append('vt{}'.format(i + 1))
    else:
        for i in range(sum((c.isdigit() for c in f[0].split(' ')))):
            mesh_columns.append('v{}'.format(i + 1))
    mesh = pd.DataFrame([re.split('\\D+', x) for x in f], columns=mesh_columns).astype('i4')
    mesh -= 1
    data['mesh'] = mesh
    return data",line.startswith('v '),183,line.startswith('v'),False,45.48019047027906,N/A
"def write_obj(filename, points=None, mesh=None):
    """"""
    Parameters
    ----------
    filename:   str
        The created file will be named with this
    points:     pd.DataFrame
    mesh:       pd.DataFrame

    Returns
    -------
    boolean
        True if no problems

    """"""
<mask>:
        filename += '.obj'
    if points is not None:
        points = points.copy()
        points = points[['x', 'y', 'z']]
        points.insert(loc=0, column='obj_v', value='v')
        points.to_csv(filename, sep=' ', index=False, header=False, mode='a', encoding='ascii')
    if mesh is not None:
        mesh = mesh.copy()
        mesh = mesh[['v1', 'v2', 'v3']]
        mesh += 1
        mesh.insert(loc=0, column='obj_f', value='f')
        mesh.to_csv(filename, sep=' ', index=False, header=False, mode='a', encoding='ascii')
    return True",not filename.endswith('obj'),84,not filename.endswith('.obj'),False,46.713797772819994,N/A
"def parse_header(lines):
    metadata = {}
    for ln in lines:
<mask>:
            continue
        match = re.match('(\\w+)\\s+([\\w\\s\\.]+)', ln)
        if not match:
            warnings.warn(""warning: can't understand line: %s"" % ln)
            continue
        key, value = (match.group(1).lower(), match.group(2))
        if key == 'version':
            metadata[key] = value
        elif key in ('fields', 'type'):
            metadata[key] = value.split()
        elif key in ('size', 'count'):
            metadata[key] = map(int, value.split())
        elif key in ('width', 'height', 'points'):
            metadata[key] = int(value)
        elif key == 'viewpoint':
            metadata[key] = map(float, value.split())
        elif key == 'data':
            metadata[key] = value.strip().lower()
    if 'count' not in metadata:
        metadata['count'] = [1] * len(metadata['fields'])
    if 'viewpoint' not in metadata:
        metadata['viewpoint'] = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]
    if 'version' not in metadata:
        metadata['version'] = '.7'
    return metadata",ln.startswith('#') or len(ln) < 2,113,ln.startswith('#') or ln.startswith('#'),False,51.54458901398172,N/A
"def build_dtype(metadata):
    """"""build numpy structured array dtype from pcl metadata.
    note that fields with count > 1 are 'flattened' by creating multiple
    single-count fields.
    TODO: allow 'proper' multi-count fields.
    """"""
    fieldnames = []
    typenames = []
    for f, c, t, s in zip(metadata['fields'], metadata['count'], metadata['type'], metadata['size']):
        np_type = pcd_type_to_numpy_type[t, s]
<mask>:
            fieldnames.append(f)
            typenames.append(np_type)
        else:
            fieldnames.extend(['%s_%04d' % (f, i) for i in range(c)])
            typenames.extend([np_type] * c)
    dtype = np.dtype(list(zip(fieldnames, typenames)))
    return dtype",c == 1,71,c == 1,True,100.00000000000004,N/A
"def read_pcd(filename):
    """"""Reads and pcd file and return the elements as pandas Dataframes.

    Parameters
    ----------
    filename: str
        Path to the pcd file.

    Returns
    -------
    pandas Dataframe.

    """"""
    data = {}
    with open(filename, 'rb') as f:
        header = []
        while True:
            ln = f.readline().strip().decode()
            header.append(ln)
<mask>:
                metadata = parse_header(header)
                dtype = build_dtype(metadata)
                break
        if metadata['data'] == 'ascii':
            pc_data = np.loadtxt(f, dtype=dtype, delimiter=' ')
        elif metadata['data'] == 'binary':
            rowstep = metadata['points'] * dtype.itemsize
            buf = f.read(rowstep)
            pc_data = np.fromstring(buf, dtype=dtype)
        elif metadata['data'] == 'binary_compressed':
            raise NotImplementedError('lzf compression not supported.')
    df = pd.DataFrame(pc_data)
    col = 'rgb'
    if col in df.columns:
        packed_rgb = df.rgb.values
        packed_rgb = packed_rgb.astype(np.float32).tostring()
        packed_rgb = np.frombuffer(packed_rgb, dtype=np.int32)
        df['red'] = np.asarray(packed_rgb >> 16 & 255, dtype=np.uint8)
        df['green'] = np.asarray(packed_rgb >> 8 & 255, dtype=np.uint8)
        df['blue'] = np.asarray(packed_rgb & 255, dtype=np.uint8)
        df.drop(col, axis=1, inplace=True)
    data['points'] = df
    return data",ln.startswith('DATA'),137,len(header) == 0,False,7.809849842300637,N/A
"def compute(self):
    name = '{}({})'.format('clusters', self.voxelgrid_id)
    to_be_processed = np.zeros(self.voxelgrid.n_voxels, dtype=bool)
    to_be_processed[np.unique(self.voxelgrid.voxel_n)] = True
    clusters = np.zeros(self.voxelgrid.voxel_n.shape[0])
    C = 0
    while np.any(to_be_processed):
        Q = []
        Q.append(np.random.choice(np.where(to_be_processed)[0]))
        for voxel in Q:
            clusters[np.where(self.voxelgrid.voxel_n == voxel)[0]] = C
            to_be_processed[voxel] = False
            neighbors = self.voxelgrid.get_voxel_neighbors(voxel)
            for neighbor in neighbors:
<mask>:
                    Q.append(neighbor)
                    to_be_processed[neighbor] = False
        C += 1
    self.to_be_added[name] = clusters",to_be_processed[neighbor],55,self.voxelgrid.voxel_n[neighbor] == True,False,12.35622127262679,N/A
"def get_and_set(self):
    sf_added = []
    for k, v in self.to_be_added.items():
        sf_added.append(k)
        self.pyntcloud.points[k] = v
<mask>:
        return sf_added[0]
    else:
        return sf_added",len(sf_added) == 1,20,len(sf_added) == 1,True,100.00000000000004,N/A
"def convert_columns_dtype(df, old_dtype, new_dtype):
    """"""
    Parameters
    ----------
    df: pandas.DataFrame

    old_dtype: numpy dtype

    new_dtype: numpy dtype
    """"""
    changed = []
    for column in df.columns:
<mask>:
            df[column] = df[column].astype(new_dtype)
            changed.append(column)
    return changed",df[column].dtype == old_dtype,30,"not isinstance(df[column], old_dtype)",False,30.66148710292676,N/A
"@njit
def groupby_max(xyz, indices, N, out):
    for i in range(xyz.shape[0]):
<mask>:
            out[indices[i]] = xyz[i][N]
    return out",xyz[i][N] > out[indices[i]],16,i in indices,False,0.6349677360219386,N/A
"def quadrilateral_to_triangular(mesh):
    new_mesh = pd.DataFrame()
    quadrilateral_vertex = mesh[['v1', 'v2', 'v3', 'v4']].values
    triangular_vertex = np.vstack((quadrilateral_vertex[:, [0, 1, 2]], quadrilateral_vertex[:, [2, 3, 0]]))
    new_mesh['v1'] = triangular_vertex[:, 0]
    new_mesh['v2'] = triangular_vertex[:, 1]
    new_mesh['v3'] = triangular_vertex[:, 2]
<mask>:
        quadrilateral_vertex_normals = mesh[['vn1', 'vn2', 'vn3', 'vn4']].values
        triangular_vertex_normals = np.vstack((quadrilateral_vertex_normals[:, [0, 1, 2]], quadrilateral_vertex_normals[:, [2, 3, 0]]))
        new_mesh['vn1'] = triangular_vertex_normals[:, 0]
        new_mesh['vn2'] = triangular_vertex_normals[:, 1]
        new_mesh['vn3'] = triangular_vertex_normals[:, 2]
    if 'vt1' in mesh.columns:
        quadrilateral_vertex_texture = mesh[['vt1', 'vt2', 'vt3', 'vt4']].values
        triangular_vertex_texture = np.vstack((quadrilateral_vertex_texture[:, [0, 1, 2]], quadrilateral_vertex_texture[:, [2, 3, 0]]))
        new_mesh['vt1'] = triangular_vertex_texture[:, 0]
        new_mesh['vt2'] = triangular_vertex_texture[:, 1]
        new_mesh['vt3'] = triangular_vertex_texture[:, 2]
    return new_mesh",'vn1' in mesh.columns,96,'vn1' in mesh.columns,True,100.00000000000004,N/A
"def cartesian(arrays, out=None):
    """"""Generate a cartesian product of input arrays.

    Parameters
    ----------
    arrays : list of array-like
        1-D arrays to form the cartesian product of.
    out : ndarray
        Array to place the cartesian product in.

    Returns
    -------
    out : ndarray
        2-D array of shape (M, len(arrays)) containing cartesian products
        formed of input arrays.

    Examples
    --------
    >>> cartesian(([1, 2, 3], [4, 5], [6, 7]))
    array([[1, 4, 6],
           [1, 4, 7],
           [1, 5, 6],
           [1, 5, 7],
           [2, 4, 6],
           [2, 4, 7],
           [2, 5, 6],
           [2, 5, 7],
           [3, 4, 6],
           [3, 4, 7],
           [3, 5, 6],
           [3, 5, 7]])

    """"""
    arrays = [np.asarray(x) for x in arrays]
    shape = (len(x) for x in arrays)
    dtype = arrays[0].dtype
    ix = np.indices(shape)
    ix = ix.reshape(len(arrays), -1).T
<mask>:
        out = np.empty_like(ix, dtype=dtype)
    for n, arr in enumerate(arrays):
        out[:, n] = arrays[n][ix[:, n]]
    return out",out is None,141,out is None,True,100.00000000000004,N/A
"def PCA(data, correlation=False, sort=True):
    """"""Applies Principal Component Analysis to the data

    Parameters
    ----------
    data: array
        The array containing the data. The array must have NxM dimensions, where each
        of the N rows represents a different individual record and each of the M columns
        represents a different variable recorded for that individual record.
            array([
            [V11, ... , V1m],
            ...,
            [Vn1, ... , Vnm]])

    correlation(Optional) : bool
            Set the type of matrix to be computed (see Notes):
                If True compute the correlation matrix.
                If False(Default) compute the covariance matrix.

    sort(Optional) : bool
            Set the order that the eigenvalues/vectors will have
                If True(Default) they will be sorted (from higher value to less).
                If False they won't.
    Returns
    -------
    eigenvalues: (1,M) array
        The eigenvalues of the corresponding matrix.

    eigenvector: (M,M) array
        The eigenvectors of the corresponding matrix.

    Notes
    -----
    The correlation matrix is a better choice when there are different magnitudes
    representing the M variables. Use covariance matrix in other cases.

    """"""
    mean = np.mean(data, axis=0)
    data_adjust = data - mean
<mask>:
        matrix = np.corrcoef(data_adjust.T)
    else:
        matrix = np.cov(data_adjust.T)
    eigenvalues, eigenvectors = np.linalg.eig(matrix)
    if sort:
        sort = eigenvalues.argsort()[::-1]
        eigenvalues = eigenvalues[sort]
        eigenvectors = eigenvectors[:, sort]
    return (eigenvalues, eigenvectors)",correlation,194,correlation,True,100.00000000000004,N/A
"def point_in_array_2D(point, array_2D):
    point = np.array(point, dtype=array_2D.dtype)
    for other_point in array_2D:
<mask>:
            return True",np.all(point == other_point),14,np.allclose(point[other_point] - point[0]),False,13.834368456410951,N/A
"def compute(self):
<mask>:
        raise ValueError(""n can't be higher than the number of points in the PyntCloud."")
    return self.points.sample(self.n).reset_index(drop=True)",self.n > len(self.points),18,self.n < self.n,False,20.024850746991504,N/A
"def __init__(self, *, pyntcloud, n, d_metric=np.eye(3)):
    """"""d_metric -> Euclidean distance space by default, can be modified to other Mahalanobis distance as well""""""
    super().__init__(pyntcloud=pyntcloud)
    self.n = n
<mask>:
        raise ValueError('the distance metric must be positive semi-definite')
    self.d_metric = d_metric",not np.all(np.linalg.eigvals(d_metric) >= 0),38,self.n < 0,False,0.7724896059301557,N/A
"def compute(self):
    """"""incremental farthest search""""""
<mask>:
        raise ValueError(""sampled points can't be more than the original input"")
    remaining_points = self.points.values
    select_idx = np.random.randint(low=0, high=len(self.points))
    solution_set = remaining_points[select_idx:select_idx + 1]
    remaining_points = np.delete(remaining_points, select_idx, 0)
    for _ in range(self.n - 1):
        distance_sum = self.cal_distance(remaining_points, solution_set)
        select_idx = np.argmax(distance_sum)
        solution_set = np.concatenate([solution_set, remaining_points[select_idx:select_idx + 1]], axis=0)
        remaining_points = np.delete(remaining_points, select_idx, 0)
    return pd.DataFrame(solution_set, columns=self.points.columns)",self.n > len(self.points),61,len(self.points) > self.n,False,67.56000774035174,N/A
"def extract_info(self):
    v1, v2, v3 = self.pyntcloud.get_mesh_vertices(rgb=self.rgb, normals=self.normals)
    self.v1_xyz = v1[:, :3]
    self.v2_xyz = v2[:, :3]
    self.v3_xyz = v3[:, :3]
<mask>:
        self.v1_rgb = v1[:, 3:6]
        self.v2_rgb = v2[:, 3:6]
        self.v3_rgb = v3[:, 3:6]
        if self.normals:
            self.v1_normals = v1[:, 6:]
            self.v2_normals = v2[:, 6:]
            self.v3_normals = v3[:, 6:]
    elif self.normals:
        self.v1_normals = v1[:, 3:6]
        self.v2_normals = v2[:, 3:6]
        self.v3_normals = v3[:, 3:6]",self.rgb,61,self.rgb,True,100.00000000000004,N/A
"def compute(self):
    areas = triangle_area_multi(self.v1_xyz, self.v2_xyz, self.v3_xyz)
    probabilities = areas / np.sum(areas)
    random_idx = np.random.choice(np.arange(len(areas)), size=self.n, p=probabilities)
    v1_xyz = self.v1_xyz[random_idx]
    v2_xyz = self.v2_xyz[random_idx]
    v3_xyz = self.v3_xyz[random_idx]
    u = np.random.uniform(low=0.0, high=1.0, size=(self.n, 1))
    v = np.random.uniform(low=0.0, high=1 - u, size=(self.n, 1))
    result = pd.DataFrame()
    result_xyz = v1_xyz * u + v2_xyz * v + (1 - (u + v)) * v3_xyz
    result_xyz = result_xyz.astype(np.float32)
    result['x'] = result_xyz[:, 0]
    result['y'] = result_xyz[:, 1]
    result['z'] = result_xyz[:, 2]
<mask>:
        v1_rgb = self.v1_rgb[random_idx]
        v2_rgb = self.v2_rgb[random_idx]
        v3_rgb = self.v3_rgb[random_idx]
        result_rgb = v1_rgb * u + v2_rgb * v + (1 - (u + v)) * v3_rgb
        result_rgb = result_rgb.astype(np.uint8)
        result['red'] = result_rgb[:, 0]
        result['green'] = result_rgb[:, 1]
        result['blue'] = result_rgb[:, 2]
    if self.normals:
        v1_normals = self.v1_normals[random_idx]
        v2_normals = self.v2_normals[random_idx]
        v3_normals = self.v3_normals[random_idx]
        sum_normals = v1_normals + v2_normals + v3_normals
        result_normals = sum_normals / np.linalg.norm(sum_normals, axis=1)[..., None]
        result_normals = result_normals.astype(np.float32)
        result['nx'] = result_normals[:, 0]
        result['ny'] = result_normals[:, 1]
        result['nz'] = result_normals[:, 2]
    return result",self.rgb,159,self.rgb,True,100.00000000000004,N/A
"def compute(self):
    voxel_n_id = 'voxel_n({})'.format(self.voxelgrid_id)
<mask>:
        self.pyntcloud.points[voxel_n_id] = self.voxelgrid.voxel_n
    nearests = []
    for voxel_n, x in self.pyntcloud.points.groupby(voxel_n_id, sort=False):
        xyz = x.loc[:, ['x', 'y', 'z']].values
        center = self.voxelgrid.voxel_centers[voxel_n]
        voxel_nearest = cdist([center], xyz)[0].argsort()[:self.n]
        nearests.extend(x.index.values[voxel_nearest])
    return self.pyntcloud.points.iloc[nearests].reset_index(drop=True)",voxel_n_id not in self.pyntcloud.points,34,voxel_n_id not in self.pyntcloud.points,True,100.00000000000004,N/A
"def compute(self):
    voxel_n_id = 'voxel_n({})'.format(self.voxelgrid_id)
<mask>:
        self.pyntcloud.points[voxel_n_id] = self.voxelgrid.voxel_n
    return self.pyntcloud.points.iloc[self.pyntcloud.points.groupby(voxel_n_id)['z'].idxmax()].reset_index(drop=True)",voxel_n_id not in self.pyntcloud.points,11,voxel_n_id not in self.pyntcloud.points,True,100.00000000000004,N/A
"def spherical_to_cartesian(r, theta, phi, degrees=True):
    """"""
    Convert spherical coordinates (r, theta, phi) to cartesian (x, y, z).

    Parameters
    ----------
    r: (N,) ndarray
        Radial distance.
    theta: (N,) ndarray
        Azimuthal angle.
    phi: (N,) ndarray
        Polar angle.
    degrees: bool, optional
        If True, theta and phi will be assumed to be in degrees.

    Returns
    -------
    xyz: (N, 3) ndarray
        Corresponding cartesian coordinates.

    Notes
    -----
    Use notation of mathematical systems, NOT physics.
    """"""
<mask>:
        theta = np.deg2rad(theta)
        phi = np.deg2rad(phi)
    sin_theta = np.sin(theta)
    cos_theta = np.cos(theta)
    sin_phi = np.sin(phi)
    cos_phi = np.cos(phi)
    xyz = np.empty((r.shape[0], 3), dtype=np.float32)
    xyz[:, 0] = r * sin_phi * cos_theta
    xyz[:, 1] = r * sin_phi * sin_theta
    xyz[:, 2] = r * cos_phi
    return xyz",degrees,116,degrees,True,100.00000000000004,N/A
"def cartesian_to_spherical(xyz, degrees=True):
    """"""
    Convert cartesian coordinates (x, y, z) to spherical (r, theta, phi).

    Parameters
    ----------
    xyz: (N, 3) ndarray
        Corresponding cartesian coordinates.
    degrees: bool, optional
        If True, azimuthal and polar will be returned in degrees.

    Returns
    -------
    radius: (N,) ndarray
        Radial distance.
    inclination: (N,) ndarray
        Polar angle.
    azimuth: (N,) ndarray
        Azimuthal angle.
    """"""
    x = xyz[:, 0]
    y = xyz[:, 1]
    z = xyz[:, 2]
    radius = np.nan_to_num(np.sqrt(x * x + y * y + z * z))
    inclination = np.nan_to_num(np.arccos(z / radius))
    azimuth = np.nan_to_num(np.arctan2(y, x))
<mask>:
        inclination = np.rad2deg(inclination)
        azimuth = np.rad2deg(azimuth)
    return (radius, inclination, azimuth)",degrees,100,degrees,True,100.00000000000004,N/A
"def cylindrical_to_cartesian(ro, phi, z, degrees=True):
    """"""
    Convert cylindrical coordinates (ro, phi, zeta) to cartesian (x, y, z).

    Parameters
    ----------
    ro: (N,) ndarray
        Radial distance.
    phi: (N,) ndarray
        Angular position.
    z: (N,) ndarray
        Altitude.
    degrees: bool, optional
        If True, angular will be assumed to be in degrees.

    Returns
    -------
    xyz: (N, 3) ndarray
        Corresponding cartesian coordinates.

    Notes
    -----
    The polar axis in the cylindrical system corresponds to the 'x' axis in the
    cartesian system.

    The longitudinal axis corresponds to the 'z' axis.
    """"""
<mask>:
        phi = np.deg2rad(phi)
    sin_phi = np.sin(phi)
    cos_phi = np.cos(phi)
    xyz = np.empty((ro.shape[0], 3), dtype=np.float32)
    xyz[:, 0] = ro * cos_phi
    xyz[:, 1] = ro * sin_phi
    xyz[:, 2] = z
    return xyz",degrees,115,degrees,True,100.00000000000004,N/A
"def cartesian_to_cylindrical(xyz, degrees=True):
    """"""
    Convert cartesian coordinates (x, y, z) to cylindrical (ro, phi, zeta).

    Parameters
    ----------
    xyz: (N, 3) ndarray
        Corresponding cartesian coordinates.
    degrees: bool, optional
        If True, azimuthal and polar will be returned in degrees.

    Returns
    -------
    radial_cylindrical: (N,) ndarray
        Radial distance.
    angular_cylindrical: (N,) ndarray
        Angular position.
    z: (N,) ndarray
        Altitude.

    Notes
    -----
    The polar axis in the cylindrical system corresponds to the 'x' axis in the
    cartesian system.

    The longitudinal axis corresponds to the 'z' axis.
    """"""
    x = xyz[:, 0]
    y = xyz[:, 1]
    z = xyz[:, 2]
    radial_cylindrical = np.nan_to_num(np.sqrt(x * x + y * y))
    angular_cylindrical = np.nan_to_num(np.arctan2(y, x))
<mask>:
        angular_cylindrical = np.rad2deg(angular_cylindrical)
    return (radial_cylindrical, angular_cylindrical, z)",degrees,113,degrees,True,100.00000000000004,N/A
"def cylindrical_to_spherical(ro, phi, zeta, degrees=True, phi_is_inclination=True):
    """"""
    Convert cylindrical coordinates (ro, phi, zeta) to spherical (r, theta, phi).

    Parameters
    ----------
    ro: (N,) ndarray
        Radial distance.
    phi: (N,) ndarray
        Angular position.
    zeta: (N,) ndarray
        Altitude.
    degrees: bool, optional
        If True, azimuthal and polar will be returned in degrees.
    phi_is_inclination: bool, optional
        See https://en.wikipedia.org/wiki/Cylindrical_coordinate_system#Spherical_coordinates.

    Returns
    -------
    r: (N,) ndarray
        Radial distance.
    theta: (N,) ndarray
        Azimuthal angle.
    phi: (N,) ndarray
        Polar angle.
    """"""
    r = np.sqrt(ro * ro + zeta * zeta)
    theta = phi
<mask>:
        phi = np.arctan2(ro, zeta)
    else:
        phi = np.arctan2(zeta, ro)
    if degrees:
        phi = np.rad2deg(phi)
    return (r, theta, phi)",phi_is_inclination,101,phi_is_inclination,True,100.00000000000004,N/A
"def coplanar_area(points, plane_normal=None):
    """"""Area of the coplanar polygon formed by the given points.

    Parameters
    ----------
    points: array
        The vertices of the selected area, the points are expected to be coplanar.
        Expected format:
            array([
            [x1,y1,z1],
            ...,
            [xn,yn,zn]])

    Returns
    -------
    area : float
        The area of the polygon formed by the given coplanar points.

    """"""
<mask>:
        pass
    else:
        pass
    points_rolled = np.roll(points, len(points) - 1, axis=0)
    cross_product = np.cross(points, points_rolled)
    summed = np.sum(cross_product, axis=0)
    total = np.dot(summed, plane_normal)
    area = 0.5 * abs(total)
    return area",not plane_normal,83,plane_normal is None,False,39.76353643835252,N/A
"def Rx(angle, degrees=True):
<mask>:
        cx = np.cos(np.deg2rad(angle))
        sx = np.sin(np.deg2rad(angle))
    else:
        cx = np.cos(angle)
        sx = np.sin(angle)
    return np.array([[1, 0, 0], [0, cx, sx], [0, -sx, cx]])",degrees,27,degrees,True,100.00000000000004,N/A
"def Ry(angle, degrees=True):
<mask>:
        cy = np.cos(np.deg2rad(angle))
        sy = np.sin(np.deg2rad(angle))
    else:
        cy = np.cos(angle)
        sy = np.sin(angle)
    return np.array([[cy, 0, -sy], [0, 1, 0], [sy, 0, cy]])",degrees,27,degrees,True,100.00000000000004,N/A
"def Rz(angle, degrees=True):
<mask>:
        cz = np.cos(np.deg2rad(angle))
        sz = np.sin(np.deg2rad(angle))
    else:
        cz = np.cos(angle)
        sz = np.sin(angle)
    return np.array([[cz, sz, 0], [-sz, cz, 0], [0, 0, 1]])",degrees,27,degrees,True,100.00000000000004,N/A
"def __init__(self, point=None, normal=None):
    self.point = point
    self.normal = normal
<mask>:
        self.normal /= np.linalg.norm(normal)",normal is not None,14,normal is not None,True,100.00000000000004,N/A
"def get_projections(self, points, only_distances=False):
    vectors = points - self.point
    distances = np.abs(np.dot(vectors, self.normal))
<mask>:
        return distances
    projections = points - distances[:, None] * self.normal
    return (distances, projections)",only_distances,27,only_distances,True,100.00000000000004,N/A
"def get_projections(self, points, only_distances=False):
    vectors = points - self.center
    lengths = np.linalg.norm(vectors, axis=1)
    distances = np.abs(lengths - self.radius)
<mask>:
        return distances
    scales = self.radius / lengths
    projections = scales[:, None] * vectors + self.center
    return (distances, projections)",only_distances,37,only_distances,True,100.00000000000004,N/A
"def check_address(address):
    """"""
    Check if the format of the address is correct

    Arguments:
        address (tuple):
            (``str``, ``int``) representing an IP address and port,
            respectively

            .. note::
                alternatively a local ``address`` can be a ``str`` when working
                with UNIX domain sockets, if supported by the platform
    Raises:
        ValueError:
            raised when address has an incorrect format

    Example:
        >>> check_address(('127.0.0.1', 22))
    """"""
<mask>:
        check_host(address[0])
        check_port(address[1])
    elif isinstance(address, string_types):
        if os.name != 'posix':
            raise ValueError('Platform does not support UNIX domain sockets')
        if not (os.path.exists(address) or os.access(os.path.dirname(address), os.W_OK)):
            raise ValueError('ADDRESS not a valid socket domain socket ({0})'.format(address))
    else:
        raise ValueError('ADDRESS is not a tuple, string, or character buffer ({0})'.format(type(address).__name__))","isinstance(address, tuple)",104,"isinstance(address, tuple)",True,100.00000000000004,N/A
"def check_addresses(address_list, is_remote=False):
    """"""
    Check if the format of the addresses is correct

    Arguments:
        address_list (list[tuple]):
            Sequence of (``str``, ``int``) pairs, each representing an IP
            address and port respectively

            .. note::
                when supported by the platform, one or more of the elements in
                the list can be of type ``str``, representing a valid UNIX
                domain socket

        is_remote (boolean):
            Whether or not the address list
    Raises:
        AssertionError:
            raised when ``address_list`` contains an invalid element
        ValueError:
            raised when any address in the list has an incorrect format

    Example:

        >>> check_addresses([('127.0.0.1', 22), ('127.0.0.1', 2222)])
    """"""
    assert all((isinstance(x, (tuple, string_types)) for x in address_list))
<mask>:
        raise AssertionError('UNIX domain sockets not allowed for remoteaddresses')
    for address in address_list:
        check_address(address)","is_remote and any((isinstance(x, string_types) for x in address_list))",114,not is_remote,False,0.5144328746192984,N/A
"def create_logger(logger=None, loglevel=None, capture_warnings=True, add_paramiko_handler=True):
    """"""
    Attach or create a new logger and add a console handler if not present

    Arguments:

        logger (Optional[logging.Logger]):
            :class:`logging.Logger` instance; a new one is created if this
            argument is empty

        loglevel (Optional[str or int]):
            :class:`logging.Logger`'s level, either as a string (i.e.
            ``ERROR``) or in numeric format (10 == ``DEBUG``)

            .. note:: a value of 1 == ``TRACE`` enables Tracing mode

        capture_warnings (boolean):
            Enable/disable capturing the events logged by the warnings module
            into ``logger``'s handlers

            Default: True

            .. note:: ignored in python 2.6

        add_paramiko_handler (boolean):
            Whether or not add a console handler for ``paramiko.transport``'s
            logger if no handler present

            Default: True
    Return:
        :class:`logging.Logger`
    """"""
    logger = logger or logging.getLogger('sshtunnel.SSHTunnelForwarder')
<mask>:
        logger.setLevel(loglevel or DEFAULT_LOGLEVEL)
        console_handler = logging.StreamHandler()
        _add_handler(logger, handler=console_handler, loglevel=loglevel or DEFAULT_LOGLEVEL)
    if loglevel:
        logger.setLevel(loglevel)
        for handler in logger.handlers:
            handler.setLevel(loglevel)
    if add_paramiko_handler:
        _check_paramiko_handlers(logger=logger)
    if capture_warnings and sys.version_info >= (2, 7):
        logging.captureWarnings(True)
        pywarnings = logging.getLogger('py.warnings')
        pywarnings.handlers.extend(logger.handlers)
    return logger","not any((isinstance(x, logging.Handler) for x in logger.handlers))",150,not logger.handlers,False,1.1702651167821572,N/A
"def _add_handler(logger, handler=None, loglevel=None):
    """"""
    Add a handler to an existing logging.Logger object
    """"""
    handler.setLevel(loglevel or DEFAULT_LOGLEVEL)
<mask>:
        _fmt = '%(asctime)s| %(levelname)-4.3s|%(threadName)10.9s/%(lineno)04d@%(module)-10.9s| %(message)s'
        handler.setFormatter(logging.Formatter(_fmt))
    else:
        handler.setFormatter(logging.Formatter('%(asctime)s| %(levelname)-8s| %(message)s'))
    logger.addHandler(handler)",handler.level <= logging.DEBUG,29,not handler,False,2.489353418393197,N/A
"def _check_paramiko_handlers(logger=None):
    """"""
    Add a console handler for paramiko.transport's logger if not present
    """"""
    paramiko_logger = logging.getLogger('paramiko.transport')
<mask>:
        if logger:
            paramiko_logger.handlers = logger.handlers
        else:
            console_handler = logging.StreamHandler()
            console_handler.setFormatter(logging.Formatter('%(asctime)s | %(levelname)-8s| PARAMIKO: %(lineno)03d@%(module)-10s| %(message)s'))
            paramiko_logger.addHandler(console_handler)",not paramiko_logger.handlers,34,not paramiko_logger.handlers,True,100.00000000000004,N/A
"def tearDown(self):
    self.log.info('tearDown for: {0}()'.format(self._testMethodName.upper()))
    self.stop_echo_and_ssh_server()
    for thread in self.running_threads:
        x = self.threads[thread]
        self.log.info('thread {0} ({1})'.format(thread, 'alive' if x.is_alive() else 'defunct'))
    while self.running_threads:
        for thread in self.running_threads:
            x = self.threads[thread]
            self.wait_for_thread(self.threads[thread], who='tearDown')
<mask>:
                self.log.info('thread {0} now stopped'.format(thread))
    for attr in ['server', 'tc', 'ts', 'socks', 'ssockl', 'esockl']:
        if hasattr(self, attr):
            self.log.info('tearDown() {0}'.format(attr))
            getattr(self, attr).close()",not x.is_alive(),53,x.is_alive(),False,86.6877899750182,N/A
"def wait_for_thread(self, thread, timeout=THREADS_TIMEOUT, who=None):
<mask>:
        self.log.debug('{0}waiting for {1} to end...'.format('{0} '.format(who) if who else '', thread.name))
        thread.join(timeout)",thread.is_alive(),18,thread.is_alive(),True,100.00000000000004,N/A
"def _run_echo_server(self, timeout=sshtunnel.SSH_TIMEOUT):
    self.log.info('echo-server Started')
    self.ssh_event.wait(timeout)
    socks = [self.esockl]
    try:
        while self.is_server_working:
            inputready, _, _ = select.select(socks, [], [], timeout)
            for s in inputready:
<mask>:
                    try:
                        client, address = self.esockl.accept()
                        self.log.info('echo-server accept() {0}'.format(address))
                    except OSError:
                        self.log.info('echo-server accept() OSError')
                        break
                    socks.append(client)
                else:
                    try:
                        data = s.recv(1000)
                        self.log.info('echo-server echoing {0}'.format(data))
                        s.send(data)
                    except OSError:
                        self.log.warning('echo-server OSError')
                        continue
                    finally:
                        s.close()
                        socks.remove(s)
        self.log.info('<<< echo-server received STOP signal')
    except Exception as e:
        self.log.info('echo-server got Exception: {0}'.format(repr(e)))
    finally:
        self.is_server_working = False
        if 'forward-server' in self.threads:
            t = self.threads['forward-server']
            self.wait_for_thread(t, timeout=None, who='echo-server')
            self.running_threads.remove('forward-server')
        for s in socks:
            s.close()
        self.log.info('echo-server shutting down')
        self.running_threads.remove('echo-server')",s == self.esockl,94,s == self.esockl,True,100.00000000000004,N/A
"def _do_forwarding(self, timeout=sshtunnel.SSH_TIMEOUT):
    self.log.debug('forward-server Start')
    self.ssh_event.wait(THREADS_TIMEOUT)
    try:
        schan = self.ts.accept(timeout=timeout)
        info = 'forward-server schan <> echo'
        self.log.info(info + ' accept()')
        echo = socket.create_connection((self.eaddr, self.eport))
        while self.is_server_working:
            rqst, _, _ = select.select([schan, echo], [], [], timeout)
<mask>:
                data = schan.recv(1024)
                self.log.debug('{0} -->: {1}'.format(info, repr(data)))
                echo.send(data)
                if len(data) == 0:
                    break
            if echo in rqst:
                data = echo.recv(1024)
                self.log.debug('{0} <--: {1}'.format(info, repr(data)))
                schan.send(data)
                if len(data) == 0:
                    break
        self.log.info('<<< forward-server received STOP signal')
    except socket.error:
        self.log.critical('{0} sending RST'.format(info))
    finally:
        if schan:
            self.log.debug('{0} closing connection...'.format(info))
            schan.close()
            echo.close()
            self.log.debug('{0} connection closed.'.format(info))",schan in rqst,87,echo in self.sending_rst,False,6.567274736060395,N/A
"def test_show_running_version(self):
    """""" Test that _cli_main() function quits when Enter is pressed """"""
    with capture_stdout_stderr() as (out, err):
        with self.assertRaises(SystemExit):
            sshtunnel._cli_main(args=['-V'])
<mask>:
        version = err.getvalue().split()[-1]
    else:
        version = out.getvalue().split()[-1]
    self.assertEqual(version, sshtunnel.__version__)","sys.version_info < (3, 4)",31,err.getvalue() != 'OK',False,4.995138898472386,N/A
"def run_postgres_query(port, query=PG_QUERY):
    import psycopg2
    ASYNC_OK = 1
    ASYNC_READ_TIMEOUT = 2
    ASYNC_WRITE_TIMEOUT = 3
    ASYNC_TIMEOUT = 0.2

    def wait(conn):
        while 1:
            state = conn.poll()
<mask>:
                break
            elif state == psycopg2.extensions.POLL_WRITE:
                select.select([], [conn.fileno()], [])
            elif state == psycopg2.extensions.POLL_READ:
                select.select([conn.fileno()], [], [])
            else:
                raise psycopg2.OperationalError('poll() returned %s from _wait function' % state)

    def wait_timeout(conn):
        while 1:
            state = conn.poll()
            if state == psycopg2.extensions.POLL_OK:
                return ASYNC_OK
            elif state == psycopg2.extensions.POLL_WRITE:
                timeout_status = select.select([], [conn.fileno()], [], ASYNC_TIMEOUT)
                if timeout_status == ([], [], []):
                    return ASYNC_WRITE_TIMEOUT
            elif state == psycopg2.extensions.POLL_READ:
                timeout_status = select.select([conn.fileno()], [], [], ASYNC_TIMEOUT)
                if timeout_status == ([], [], []):
                    return ASYNC_READ_TIMEOUT
            else:
                raise psycopg2.OperationalError('poll() returned %s from _wait_timeout function' % state)
    pg_conn = psycopg2.connect(host='127.0.0.1', hostaddr='127.0.0.1', port=port, database=PG_DATABASE_NAME, user=PG_USERNAME, password=PG_PASSWORD, sslmode='disable', async_=1)
    wait(pg_conn)
    cur = pg_conn.cursor()
    cur.execute(query)
    res = wait_timeout(cur.connection)
    while res != ASYNC_OK:
        res = wait_timeout(cur.connection)
    return cur.fetchone()",state == psycopg2.extensions.POLL_OK,136,state == psycopg2.extensions.POLL_OK,True,100.00000000000004,N/A
"def wait_and_check_or_restart_if_required(tunnel, i=1):
    logging.warning('Sleeping for %s second...', i)
    while i:
        time.sleep(1)
<mask>:
            logging.info('Running tunnel.check_tunnels... (i=%s)', i)
            tunnel.check_tunnels()
            logging.info('Check result: %r (i=%s)', tunnel.tunnel_is_up, i)
            if not tunnel.is_active:
                logging.warning('Tunnel is DOWN! restarting ...')
                tunnel.restart()
        i -= 1",i % 10 == 0,35,tunnel.tunnel_is_up,False,0.0,N/A
"def show_threading_state_if_required():
    current_threads = list(threading.enumerate())
<mask>:
        logging.warning('[1] THREAD INFO')
        logging.info('Threads: %r', current_threads)
        logging.info('Threads.daemon: %r', [x.daemon for x in current_threads])
    if len(current_threads) > 1:
        logging.warning('[2] STACK INFO')
        code = ['\n\n*** STACKTRACE - START ***\n']
        for threadId, stack in sys._current_frames().items():
            code.append('\n# ThreadID: %s' % threadId)
            for filename, lineno, name, line in traceback.extract_stack(stack):
                code.append('File: ""%s"", line %d, in %s' % (filename, lineno, name))
                if line:
                    code.append('  %s' % line.strip())
        code.append('\n*** STACKTRACE - END ***\n\n')
        logging.info('\n'.join(code))",len(current_threads) > 1,72,len(current_threads) > 1,True,100.00000000000004,N/A
"def build_extensions(self):
    c = self.compiler
    _compile = c._compile

    def c_compile(obj, src, ext, cc_args, extra_postargs, pp_opts):
        cc_args = cc_args + ['-D_POSIX_C_SOURCE=200112L'] if src.startswith('crfsuite/') else cc_args
        cc_args = cc_args + ['-std=c99'] if src.endswith('.c') else cc_args
        return _compile(obj, src, ext, cc_args, extra_postargs, pp_opts)
<mask>:
        c._compile = c_compile
    elif self.compiler.compiler_type == 'msvc':
        if sys.version_info[:2] < (3, 5):
            c.include_dirs.extend(['crfsuite/win32'])
    build_ext.build_extensions(self)",c.compiler_type == 'unix' and any((item == 'gcc' or item.endswith('-gcc') for item in c.compiler)),55,self.compiler.compiler_type == 'pyc',False,7.1199931325627155,N/A
"def _apply_parser(parser, log):
    for line in log:
        event = parser.feed(line)
<mask>:
            print(parser.last_log, end='')
            print('============== ' + event)",event and event != 'featgen_progress',17,parser.last_log,False,5.862502026550896,N/A
"def feed(self, line):
    line = line.lstrip().rstrip('\r\n')
<mask>:
        return
    m = re.match('(FILEHEADER|LABELS|ATTRIBUTES|TRANSITIONS|STATE_FEATURES) = {', line)
    if m:
        self.state = m.group(1)
    elif line == '}':
        self.state = None
    else:
        getattr(self, 'parse_%s' % self.state)(line)",not line,31,not line,True,100.00000000000004,N/A
"def feed(self, line):
    self.log.append(line)
<mask>:
        self.state = 'STARTING'
        self.handle_STARTING(line)
        self.events.append(('start', 0, len(self.log)))
        return 'start'
    event = getattr(self, 'handle_' + self.state)(line)
    if event is not None:
        start, end = (self.events[-1][2], len(self.log))
        if event in ('prepared', 'optimization_end'):
            end -= 1
        self.events.append((event, start, end))
    return event",self.state is None,43,self.state == 'STARTED',False,30.213753973567677,N/A
"def handle_FEATGEN(self, line):
<mask>:
        self.featgen_percent += 2
        return 'featgen_progress'
    m = re.match('Number of features: (\\d+)', line)
    if m:
        self.featgen_num_features = int(m.group(1))
        return None
    if self._seconds(line) is not None:
        self.featgen_seconds = self._seconds(line)
        self.state = 'AFTER_FEATGEN'
        return 'featgen_end'",line in '0123456789.10',36,self.state == 'BEFORE_FEATGEN',False,0.0,N/A
"def handle_AFTER_FEATGEN(self, line):
<mask>:
        self.state = 'ITERATION'
        self.handle_ITERATION(line)
        return 'prepared'
    if 'terminated with error' in line:
        self.state = 'AFTER_ITERATION'
        return 'prepare_error'",self._iteration_head(line) is not None,21,'started with iteration' in line,False,2.634191962725227,N/A
"def handle_ITERATION(self, line):
<mask>:
        self.last_iteration = {'num': self._iteration_head(line), 'scores': {}}
        self.iterations.append(self.last_iteration)
    elif line == '\n':
        self.state = 'AFTER_ITERATION'
        return 'iteration'

    def add_re(key, pattern, typ):
        m = re.match(pattern, line)
        if m:
            self.last_iteration[key] = typ(m.group(1))
    add_re('loss', 'Loss: (\\d+\\.\\d+)', float)
    add_re('feature_norm', 'Feature norm: (\\d+\\.\\d+)', float)
    add_re('error_norm', 'Error norm: (\\d+\\.\\d+)', float)
    add_re('active_features', 'Active features: (\\d+)', int)
    add_re('linesearch_trials', 'Line search trials: (\\d+)', int)
    add_re('linesearch_step', 'Line search step: (\\d+\\.\\d+)', float)
    add_re('time', 'Seconds required for this iteration: (\\d+\\.\\d+)', float)
    m = re.match('Macro-average precision, recall, F1: \\((\\d\\.\\d+), (\\d\\.\\d+), (\\d\\.\\d+)\\)', line)
    if m:
        self.last_iteration['avg_precision'] = float(m.group(1))
        self.last_iteration['avg_recall'] = float(m.group(2))
        self.last_iteration['avg_f1'] = float(m.group(3))
    m = re.match('Item accuracy: (\\d+) / (\\d+)', line)
    if m:
        acc = fractions.Fraction(int(m.group(1)), int(m.group(2)))
        self.last_iteration['item_accuracy'] = acc
        self.last_iteration['item_accuracy_float'] = float(acc)
    m = re.match('Instance accuracy: (\\d+) / (\\d+)', line)
    if m:
        acc = fractions.Fraction(int(m.group(1)), int(m.group(2)))
        self.last_iteration['instance_accuracy'] = acc
        self.last_iteration['instance_accuracy_float'] = float(acc)
    m = re.match('\\s{4}(.+): \\((\\d+), (\\d+), (\\d+)\\) \\((\\d\\.\\d+), (\\d\\.\\d+), (\\d\\.\\d+)\\)', line)
    if m:
        self.last_iteration['scores'][m.group(1)] = LabelScore(**{'match': int(m.group(2)), 'model': int(m.group(3)), 'ref': int(m.group(4)), 'precision': float(m.group(5)), 'recall': float(m.group(6)), 'f1': float(m.group(7))})
    m = re.match('\\s{4}(.+): \\(0, 0, 0\\) \\(\\*{6}, \\*{6}, \\*{6}\\)', line)
    if m:
        self.last_iteration['scores'][m.group(1)] = LabelScore(**{'match': 0, 'model': 0, 'ref': 0, 'precision': None, 'recall': None, 'f1': None})",self._iteration_head(line) is not None,185,self.last_iteration is None,False,10.063351655856652,N/A
"def _qInstallMessageHandler(handler):
    """"""Install a message handler that works in all bindings

    Args:
        handler: A function that takes 3 arguments, or None
    """"""

    def messageOutputHandler(*args):
<mask>:
            msgType, logContext, msg = args
        elif len(args) == 2:
            msgType, msg = args
            logContext = None
        else:
            raise TypeError('handler expected 2 or 3 arguments, got {0}'.format(len(args)))
        if isinstance(msg, bytes):
            msg = msg.decode()
        handler(msgType, logContext, msg)
    passObject = messageOutputHandler if handler else handler
    if Qt.IsPySide or Qt.IsPyQt4:
        return Qt._QtCore.qInstallMsgHandler(passObject)
    elif Qt.IsPySide2 or Qt.IsPyQt5 or Qt.IsPySide6:
        return Qt._QtCore.qInstallMessageHandler(passObject)",len(args) == 3,81,len(args) == 3,True,100.00000000000004,N/A
"def _getcpppointer(object):
<mask>:
        return getattr(Qt, '_shiboken6').getCppPointer(object)[0]
    elif hasattr(Qt, '_shiboken2'):
        return getattr(Qt, '_shiboken2').getCppPointer(object)[0]
    elif hasattr(Qt, '_shiboken'):
        return getattr(Qt, '_shiboken').getCppPointer(object)[0]
    elif hasattr(Qt, '_sip'):
        return getattr(Qt, '_sip').unwrapinstance(object)
    raise AttributeError(""'module' has no attribute 'getCppPointer'"")","hasattr(Qt, '_shiboken6')",30,"hasattr(Qt, '_shiboken6')",True,100.00000000000004,N/A
"def _wrapinstance(ptr, base=None):
    """"""Enable implicit cast of pointer to most suitable class

    This behaviour is available in sip per default.

    Based on http://nathanhorne.com/pyqtpyside-wrap-instance

    Usage:
        This mechanism kicks in under these circumstances.
        1. Qt.py is using PySide 1 or 2.
        2. A `base` argument is not provided.

        See :func:`QtCompat.wrapInstance()`

    Arguments:
        ptr (long): Pointer to QObject in memory
        base (QObject, optional): Base class to wrap with. Defaults to QObject,
            which should handle anything.

    """"""
    assert isinstance(ptr, long), ""Argument 'ptr' must be of type <long>""
    assert base is None or issubclass(base, Qt.QtCore.QObject), ""Argument 'base' must be of type <QObject>""
<mask>:
        func = getattr(Qt, '_sip').wrapinstance
    elif Qt.IsPySide2:
        func = getattr(Qt, '_shiboken2').wrapInstance
    elif Qt.IsPySide6:
        func = getattr(Qt, '_shiboken6').wrapInstance
    elif Qt.IsPySide:
        func = getattr(Qt, '_shiboken').wrapInstance
    else:
        raise AttributeError(""'module' has no attribute 'wrapInstance'"")
    if base is None:
        if Qt.IsPyQt4 or Qt.IsPyQt5:
            base = Qt.QtCore.QObject
        else:
            q_object = func(long(ptr), Qt.QtCore.QObject)
            meta_object = q_object.metaObject()
            while True:
                class_name = meta_object.className()
                try:
                    base = getattr(Qt.QtWidgets, class_name)
                except AttributeError:
                    try:
                        base = getattr(Qt.QtCore, class_name)
                    except AttributeError:
                        meta_object = meta_object.superClass()
                        continue
                break
    return func(long(ptr), base)",Qt.IsPyQt4 or Qt.IsPyQt5,172,Qt.IsPySide1,False,14.506309551249304,N/A
"def _isvalid(object):
    """"""Check if the object is valid to use in Python runtime.

    Usage:
        See :func:`QtCompat.isValid()`

    Arguments:
        object (QObject): QObject to check the validity of.

    """"""
<mask>:
        return getattr(Qt, '_shiboken6').isValid(object)
    elif hasattr(Qt, '_shiboken2'):
        return getattr(Qt, '_shiboken2').isValid(object)
    elif hasattr(Qt, '_shiboken'):
        return getattr(Qt, '_shiboken').isValid(object)
    elif hasattr(Qt, '_sip'):
        return not getattr(Qt, '_sip').isdeleted(object)
    else:
        raise AttributeError(""'module' has no attribute isValid"")","hasattr(Qt, '_shiboken6')",56,"hasattr(Qt, '_shiboken6')",True,100.00000000000004,N/A
"def _translate(context, sourceText, *args):
    try:
        app = Qt.QtCore.QCoreApplication
    except AttributeError:
        raise NotImplementedError('Missing QCoreApplication implementation for {}'.format(Qt.__binding__))

    def get_arg(index):
        try:
            return args[index]
        except IndexError:
            pass
    n = -1
    encoding = None
<mask>:
        disambiguation, encoding, n = args
    else:
        disambiguation = get_arg(0)
        n_or_encoding = get_arg(1)
        if isinstance(n_or_encoding, int):
            n = n_or_encoding
        else:
            encoding = n_or_encoding
    if Qt.__binding__ in ('PySide2', 'PySide6', 'PyQt5'):
        sanitized_args = [context, sourceText, disambiguation, n]
    else:
        sanitized_args = [context, sourceText, disambiguation, encoding or app.CodecForTr, n]
    return app.translate(*sanitized_args)",len(args) == 3,77,len(args) == 2,False,80.91067115702207,N/A
"def parse(fname):
    """"""Return blocks of code as list of dicts

    Arguments:
        fname (str): Relative name of caveats file

    """"""
    blocks = list()
    with io.open(fname, 'r', encoding='utf-8') as f:
        in_block = False
        current_block = None
        current_header = ''
        for line in f:
<mask>:
                current_header = line.rstrip()
            if line.startswith('```'):
                in_block = False
            if in_block:
                current_block.append(line)
            if line.startswith('```python'):
                in_block = True
                current_block = list()
                current_block.append(current_header)
                blocks.append(current_block)
    tests = list()
    for block in blocks:
        header = block[0].strip('# ').rstrip().lower()
        header = re.sub('\\W', '_', header)
        if 'untested' in block[1].lower():
            continue
        data = re.sub(' ', '', block[1])
        data = data.strip('#').rstrip().split(',')
        binding, doctest_version = (data + [None])[:2]
        if doctest_version is not None:
            if doctest_version not in ('Python2', 'Python3'):
                raise SyntaxError('Invalid Python version:\n%s\nPython version must follow binding, e.g.\n# PyQt5, Python3' % doctest_version)
            active_version = 'Python%i' % sys.version_info[0]
            if doctest_version != active_version:
                continue
        tests.append({'header': header, 'binding': binding, 'body': block[2:]})
    return tests",line.startswith('#### '),141,current_header == '',False,0.0,N/A
"def format_(blocks):
    """"""Produce Python module from blocks of tests

    Arguments:
        blocks (list): Blocks of tests from func:`parse()`

    """"""
    tests = list()
    function_count = 0
    for block in blocks:
<mask>:
            block['body'].insert(0, "">>> assert False, 'Body must be in docstring format'\n"")
        if not block['binding'] in ('PySide', 'PySide2', 'PyQt5', 'PyQt4'):
            block['body'].insert(0, "">>> assert False, 'Invalid binding'\n"")
        if sys.version_info > (3, 4) and block['binding'] in 'PySide':
            continue
        else:
            function_count += 1
            block['header'] = block['header']
            block['count'] = str(function_count)
            block['body'] = '    '.join(block['body'])
            tests.append('\ndef test_{count}_{header}():\n    \'\'\'Test {header}\n\n    >>> import os, sys\n    >>> PYTHON = sys.version_info[0]\n    >>> long = int if PYTHON == 3 else long\n    >>> _ = os.environ.pop(""QT_VERBOSE"", None)  # Disable debug output\n    >>> os.environ[""QT_PREFERRED_BINDING""] = ""{binding}""\n    {body}\n    \'\'\'\n\n    '.format(**block))
    return tests",not any((line[:3] == '>>>' for line in block['body'])),117,block['header'] == 'docstring',False,2.349298790798877,N/A
"def assert_raises(expected_exception, callable_obj=None, *args, **kwargs):
    """"""
        Custom implementation of assert_raises using unittest.

        Parameters:
        - expected_exception: The exception type that is expected to be raised.
        - callable_obj: The callable object that is expected to raise the exception.
        - *args, **kwargs: Arguments and keyword arguments to pass to the callable object.

        Usage example:
        with assert_raises(SomeException):
            function_that_raises_some_exception()
        """"""
    context = unittest.TestCase().assertRaises(expected_exception)
<mask>:
        with context:
            callable_obj(*args, **kwargs)
    else:
        return context",callable_obj,66,callable_obj,True,100.00000000000004,N/A
"def _pyside2_commit_date():
    """"""Return the commit date of PySide2""""""
    import PySide2
<mask>:
        commit_date = PySide2.__build_commit_date__
        datetime_object = datetime.datetime.strptime(commit_date[:commit_date.rfind('+')], '%Y-%m-%dT%H:%M:%S')
        return datetime_object
    else:
        return None","hasattr(PySide2, '__build_commit_date__')",23,'+' in sys.argv,False,2.1590567826234346,N/A
"@contextlib.contextmanager
def ignoreQtMessageHandler(msgs):
    """"""A context that ignores specific qMessages for all bindings

    Args:
        msgs: list of message strings to ignore
    """"""
    from Qt import QtCompat

    def messageOutputHandler(msgType, logContext, msg):
<mask>:
            return
        sys.stderr.write('{0}\n'.format(msg))
    QtCompat.qInstallMessageHandler(messageOutputHandler)
    try:
        yield
    finally:
        QtCompat.qInstallMessageHandler(None)",msg in msgs,37,msgType != 'QMessages',False,0.0,N/A
"def test_environment():
    """"""Tests require all bindings to be installed (except PySide on py3.5+)""""""
<mask>:
        imp.find_module('PySide')
    elif os.environ.get('QT_PREFERRED_BINDING') == 'PySide6':
        imp.find_module('PySide6')
    else:
        imp.find_module('PySide2')
        imp.find_module('PyQt4')
        imp.find_module('PyQt5')","sys.version_info < (3, 5)",24,os.environ.get('QT_PREFERRED_BINDING') == 'PySide',False,3.716499092256817,N/A
"def test_load_ui_returntype():
    """"""load_ui returns an instance of QObject""""""
    import sys
    from Qt import QtWidgets, QtCore, QtCompat
<mask>:
        app = QtWidgets.QApplication(sys.argv)
    else:
        app = QtWidgets.QApplication.instance()
    obj = QtCompat.loadUi(self.ui_qwidget)
    assert isinstance(obj, QtCore.QObject)
    app.exit()",not QtWidgets.QApplication.instance(),31,sys.argv,False,5.197112497172873,N/A
"def sort_common_members():
    """"""Sorts the keys and members""""""
    filename = PREFIX + '/common_members.json'
    sorted_json_data = {}
    json_data = read_json(filename)
    all_keys = []
    for key, value in json_data.items():
        all_keys.append(key)
    sorted_keys = sorted(all_keys)
    for key in sorted_keys:
<mask>:
            sorted_json_data[key] = sorted(json_data[key])
    print('--> Sorted/cleaned ' + os.path.basename(filename))
    write_json(sorted_json_data, filename)",len(json_data[key]) > 0,45,key not in json_data,False,14.110009442520557,N/A
"def pyside_load_ui(uifile, base_instance=None):
    """"""Provide PyQt4.uic.loadUi functionality to PySide

    Args:
        uifile (str): Absolute path to .ui file
        base_instance (QWidget): The widget into which UI widgets are loaded


    Note:
        pysideuic is required for this to work with PySide.

        This seems to work correctly in Maya as well as outside of it as
        opposed to other implementations which involve overriding QUiLoader.

    Returns:
        QWidget: the base instance

    """"""
    form_class, base_class = load_ui_type(uifile)
<mask>:
        typeName = form_class.__name__
        finalType = type(typeName, (form_class, base_class), {})
        base_instance = finalType()
    else:
        if not isinstance(base_instance, base_class):
            raise RuntimeError('The base_instance passed to loadUi does not inherit from needed base type (%s)' % type(base_class))
        typeName = type(base_instance).__name__
        base_instance.__class__ = type(typeName, (form_class, type(base_instance)), {})
    base_instance.setupUi(base_instance)
    return base_instance",not base_instance,114,base_instance is None,False,39.76353643835252,N/A
"def load_ui_wrapper(uifile, base_instance=None):
    """"""Load a Qt Designer .ui file and returns an instance of the user interface

    Args:
        uifile (str): Absolute path to .ui file
        base_instance (QWidget): The widget into which UI widgets are loaded

    Returns:
        function: pyside_load_ui or uic.loadUi

    """"""
<mask>:
        return pyside_load_ui(uifile, base_instance)
    elif 'PyQt' in __binding__:
        uic = __import__(__binding__ + '.uic').uic
        return uic.loadUi(uifile, base_instance)",'PySide' in __binding__,57,'PyPI' in __binding__,False,80.91067115702207,N/A
"def setup_ui(uifile, base_instance=None):
    """"""Load a Qt Designer .ui file and returns an instance of the user interface

    Args:
        uifile (str): Absolute path to .ui file
        base_instance (QWidget): The widget into which UI widgets are loaded

    Returns:
        QWidget: the base instance

    """"""
    ui = QtCompat.loadUi(uifile)
<mask>:
        return ui
    else:
        for member in dir(ui):
            if not member.startswith('__') and member is not 'staticMetaObject':
                setattr(base_instance, member, getattr(ui, member))
        return ui",not base_instance,66,base_instance is None,False,39.76353643835252,N/A
"def delete_api_files():
    """""" Deletes unused API files """"""
<mask>:
        files = ['.graphqlrc', 'backend/config/schema.py', 'backend/apps/users/schema.py', 'frontend/src/apollo.js']
        shutil.rmtree(os.path.join(PROJECT_DIRECTORY, 'frontend/src/graphql'))
    else:
        files = ['backend/config/api.py', 'backend/apps/users/views.py', 'backend/apps/users/serializers.py']
        shutil.rmtree(os.path.join(PROJECT_DIRECTORY, 'frontend/src/store'))
    for filename in files:
        os.remove(os.path.join(PROJECT_DIRECTORY, filename))",'{{ cookiecutter.api }}' == 'REST',31,os.path.exists(PROJECT_DIRECTORY),False,3.435488317233919,N/A
"@action(methods=['GET'], detail=False)
def profile(self, request):
<mask>:
        serializer = self.serializer_class(request.user)
        return Response(status=status.HTTP_200_OK, data=serializer.data)
    return Response(status=status.HTTP_401_UNAUTHORIZED)",request.user.is_authenticated,14,request.method == 'GET',False,13.741272855400096,N/A
"@action(methods=['POST'], detail=False)
def login(self, request, format=None):
    email = request.data.get('email', None)
    password = request.data.get('password', None)
    user = authenticate(username=email, password=password)
<mask>:
        login(request, user)
        return Response(status=status.HTTP_200_OK)
    return Response(status=status.HTTP_404_NOT_FOUND)",user,25,user,True,100.00000000000004,N/A
"@action(methods=['POST'], detail=False)
def register(self, request):
    last_name = request.data.get('last_name', None)
    first_name = request.data.get('first_name', None)
    email = request.data.get('email', None)
    password = request.data.get('password', None)
<mask>:
        return Response({'status': 210})
    user = User.objects.create(email=email, password=password, last_name=last_name, first_name=first_name, is_admin=False)
    return Response(UserSerializer(user).data, status=status.HTTP_201_CREATED)",User.objects.filter(email__iexact=email).exists(),35,not email or not password,False,0.9690650671856613,N/A
"def mutate(self, info, email, password, first_name, last_name):
<mask>:
        errors = ['emailAlreadyExists']
        return Register(success=False, errors=errors)
    user = User.objects.create(email=email, last_name=last_name, first_name=first_name)
    user.set_password(password)
    user.save()
    return Register(success=True)",User.objects.filter(email__iexact=email).exists(),23,Email.objects.filter(email=email).exists(),False,63.160377007081344,N/A
"def branch(cond1, cond2):
<mask>:
        print('condition tested both ways')
    if cond2:
        print('condition not tested both ways')",cond1,15,cond1,True,100.00000000000004,N/A
"@staticmethod
def make_test_results(with_branches=False, name_prefix=''):
    results = ({'source': 'def hello():\n    print(\'world\')\n\n\nclass Foo:\n    """""" Bar """"""\n\n\ndef baz():\n    print(\'this is not tested\')\n\ndef branch(cond1, cond2):\n    if cond1:\n        print(\'condition tested both ways\')\n    if cond2:\n        print(\'condition not tested both ways\')\n', 'name': f'{name_prefix}project.py', 'coverage': [1, 1, None, None, 1, None, None, None, 1, 0, None, 1, 1, 1, 1, 1]}, {'source': ""from project import branch\nfrom project import hello\n\nif __name__ == '__main__':\n    hello()\n    branch(False, True)\n    branch(True, True)\n"", 'name': f'{name_prefix}runtests.py', 'coverage': [1, 1, None, 1, 1, 1, 1]})
<mask>:
        results[0]['branches'] = [13, 0, 14, 1, 13, 0, 15, 1, 15, 0, 16, 1, 15, 0, 12, 0]
        results[1]['branches'] = [4, 0, 5, 1, 4, 0, 1, 0]
    return results",with_branches,110,with_branches,True,100.00000000000004,N/A
"def test_func(max_val):
    for idx in range(0, max_val):
<mask>:
            print('Miss 1', idx)
        elif idx == 4:
            print('Hit 1', idx)
        elif idx == 6:
            print('Hit 2', idx)
        elif idx == 12:
            print('Miss 2', idx)
        else:
            print('Other', idx)",idx == -1,35,idx == 3,False,59.460355750136046,N/A
"def git_branch() -> Optional[str]:
    branch = None
<mask>:
        github_ref = os.environ.get('GITHUB_REF')
        if github_ref.startswith('refs/heads/') or github_ref.startswith('refs/tags/'):
            branch = github_ref.split('/', 2)[-1]
        else:
            branch = os.environ.get('GITHUB_HEAD_REF')
    else:
        branch = os.environ.get('APPVEYOR_REPO_BRANCH') or os.environ.get('BUILDKITE_BRANCH') or os.environ.get('CI_BRANCH') or os.environ.get('CIRCLE_BRANCH') or os.environ.get('GIT_BRANCH') or os.environ.get('TRAVIS_BRANCH') or os.environ.get('BRANCH_NAME') or run_command('git', 'rev-parse', '--abbrev-ref', 'HEAD')
    return branch",os.environ.get('GITHUB_ACTIONS'),46,os.environ.get('GITHUB_REF'),False,78.25422900366438,N/A
"def git_info() -> Dict[str, Dict[str, Any]]:
    """"""
    A hash of Git data that can be used to display more information to users.

    Example:
    -------
        ""git"": {
            ""head"": {
                ""id"": ""5e837ce92220be64821128a70f6093f836dd2c05"",
                ""author_name"": ""Wil Gieseler"",
                ""author_email"": ""wil@example.com"",
                ""committer_name"": ""Wil Gieseler"",
                ""committer_email"": ""wil@example.com"",
                ""message"": ""depend on simplecov >= 0.7""
            },
            ""branch"": ""master"",
            ""remotes"": [{
                ""name"": ""origin"",
                ""url"": ""https://github.com/lemurheavy/coveralls-ruby.git""
            }]
        }
    """"""
    try:
        branch = git_branch()
        head = {'id': gitlog('%H'), 'author_name': gitlog('%aN'), 'author_email': gitlog('%ae'), 'committer_name': gitlog('%cN'), 'committer_email': gitlog('%ce'), 'message': gitlog('%s')}
        remotes = [{'name': line.split()[0], 'url': line.split()[1]} for line in run_command('git', 'remote', '-v').splitlines() if '(fetch)' in line]
    except (CoverallsException, OSError) as ex:
        branch = os.environ.get('GIT_BRANCH')
        head = {'id': os.environ.get('GIT_ID'), 'author_name': os.environ.get('GIT_AUTHOR_NAME'), 'author_email': os.environ.get('GIT_AUTHOR_EMAIL'), 'committer_name': os.environ.get('GIT_COMMITTER_NAME'), 'committer_email': os.environ.get('GIT_COMMITTER_EMAIL'), 'message': os.environ.get('GIT_MESSAGE')}
        remotes = [{'name': os.environ.get('GIT_REMOTE'), 'url': os.environ.get('GIT_URL')}]
<mask>:
            log.warning('Failed collecting git data. Are you running coveralls inside a git repository? Is git installed?', exc_info=ex)
            return {}
    return {'git': {'branch': branch, 'head': head, 'remotes': remotes}}",not all(head.values()),147,len(remotes) == 0,False,5.868924818816531,N/A
"def __eq__(self, other):
<mask>:
        return str(self) == str(other)
    return False","isinstance(other, self.__class__)",10,"isinstance(other, self.__class__)",True,100.00000000000004,N/A
"@staticmethod
def sanitize_dir(directory: str) -> str:
<mask>:
        directory = directory.replace(os.path.sep, '/')
        if directory[-1] != '/':
            directory += '/'
    return directory",directory,20,os.path.sep in directory,False,6.567274736060395,N/A
"def report(self, cov: coverage.Coverage) -> None:
    try:
        for fr, analysis in get_analysis_to_report(cov, None):
            self.parse_file(fr, analysis)
    except Exception as e:
<mask>:
            return
        raise CoverallsException(f'Got coverage library error: {e}') from e",str(e) == 'No data to report.',29,'No such file' in str(e),False,22.17204504793461,N/A
"@staticmethod
def get_hits(line_num: int, analysis: Analysis) -> Optional[int]:
    """"""
        Source file stats for each line.

        * A positive integer if the line is covered, representing the number
          of times the line is hit during the test suite.
        * 0 if the line is not covered by the test suite.
        * null to indicate the line is not relevant to code coverage (it may
          be whitespace or a comment).
        """"""
<mask>:
        return 0
    if line_num not in analysis.statements:
        return None
    return 1",line_num in analysis.missing,81,analysis is None,False,7.253154775624655,N/A
"@staticmethod
def get_arcs(analysis: Analysis) -> List[int]:
    """"""
        Hit stats for each branch.

        Returns a flat list where every four values represent a branch:
        1. line-number
        2. block-number (not used)
        3. branch-number
        4. hits (we only get 1/0 from coverage.py)
        """"""
    has_arcs: bool
    try:
        has_arcs = analysis.has_arcs()
    except TypeError:
        has_arcs = analysis.has_arcs
<mask>:
        return []
    missing_arcs: Dict[int, List[int]] = analysis.missing_branch_arcs()
    try:
        executed_arcs = analysis.executed_branch_arcs()
    except AttributeError:
        executed = analysis.arcs_executed()
        lines = analysis._branch_lines()
        branch_lines = set(lines)
        eba = collections.defaultdict(list)
        for l1, l2 in executed:
            if l1 in branch_lines:
                eba[l1].append(l2)
        executed_arcs = eba
    branches: List[int] = []
    for l1, l2s in executed_arcs.items():
        for l2 in l2s:
            branches.extend((l1, 0, abs(l2), 1))
    for l1, l2s in missing_arcs.items():
        for l2 in l2s:
            branches.extend((l1, 0, abs(l2), 0))
    return branches",not has_arcs,122,not has_arcs,True,100.00000000000004,N/A
"def parse_file(self, cu: FileReporter, analysis: Analysis) -> None:
    """"""Generate data for single file.""""""
    filename = cu.relative_filename()
    posix_filename = filename.replace(os.path.sep, '/')
<mask>:
        posix_filename = posix_filename[len(self.base_dir):]
    posix_filename = self.src_dir + posix_filename
    token_lines = cu.source_token_lines()
    coverage_lines = [self.get_hits(i, analysis) for i, _ in enumerate(token_lines, 1)]
    results = {'name': posix_filename, 'source': cu.source(), 'coverage': coverage_lines}
    branches = self.get_arcs(analysis)
    if branches:
        results['branches'] = branches
    self.coverage.append(results)",self.base_dir and posix_filename.startswith(self.base_dir),59,self.base_dir and self.base_dir.endswith(self.base_dir),False,54.68017145144114,N/A
"def ensure_token(self):
<mask>:
        return
    if os.environ.get('GITHUB_ACTIONS'):
        raise CoverallsException('Running on Github Actions but GITHUB_TOKEN is not set. Add ""env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}"" to your step config.')
    raise CoverallsException(f'Not on TravisCI. You have to provide either repo_token in {self.config_filename} or set the COVERALLS_REPO_TOKEN env var.')",self.config.get('repo_token') or not self._token_required,44,self.repo_token,False,2.24408361890442,N/A
"def load_config(self, kwargs, service_name):
    """"""
        Loads all coveralls configuration in the following precedence order.

            1. automatic CI configuration
            2. COVERALLS_* env vars
            3. .coveralls.yml config file
            4. CLI flags
        """"""
    self.load_config_from_ci_environment()
    self.load_config_from_environment()
    self.load_config_from_file()
    self.config.update(kwargs)
<mask>:
        self._coveralls_host = self.config.pop('coveralls_host')
    if service_name:
        self.config['service_name'] = service_name",self.config.get('coveralls_host'),43,'coveralls_host' in self.config,False,26.782849591300856,N/A
"@staticmethod
def load_config_from_buildkite():
    pr = os.environ.get('BUILDKITE_PULL_REQUEST')
<mask>:
        pr = None
    return ('buildkite', os.environ.get('BUILDKITE_JOB_ID'), None, pr)",pr == 'false',15,pr is None,False,19.716118825581447,N/A
"def load_config_from_github(self):
    self.config['repo_token'] = os.environ.get('GITHUB_TOKEN')
    pr = None
<mask>:
        pr = os.environ.get('GITHUB_REF', '//').split('/')[2]
    job = os.environ.get('GITHUB_RUN_ID')
    number = os.environ.get('GITHUB_RUN_ID')
    return ('github', job, number, pr)","os.environ.get('GITHUB_REF', '').startswith('refs/pull/')",24,"os.environ.get('GITHUB_REF', '//').split('/')[2]",False,45.359393336810825,N/A
"def load_config_from_generic_ci_environment(self):
    config = {'service_name': os.environ.get('CI_NAME'), 'service_number': os.environ.get('CI_BUILD_NUMBER'), 'service_build_url': os.environ.get('CI_BUILD_URL'), 'service_job_id': os.environ.get('CI_JOB_ID'), 'service_branch': os.environ.get('CI_BRANCH')}
    pr_match = NUMBER_REGEX.findall(os.environ.get('CI_PULL_REQUEST', ''))
<mask>:
        config['service_pull_request'] = pr_match[-1]
    non_empty = {key: value for key, value in config.items() if value}
    self.config.update(non_empty)",pr_match,34,pr_match,True,100.00000000000004,N/A
"def main(argv=None):
    version = importlib.metadata.version('coveralls')
    options = docopt.docopt(__doc__, argv=argv, version=version)
<mask>:
        options['--verbose'] = True
    level = logging.DEBUG if options['--verbose'] else logging.INFO
    log.addHandler(logging.StreamHandler())
    log.setLevel(level)
    token_required = not options['debug'] and (not options['--output'])
    try:
        coverallz = Coveralls(token_required, config_file=options['--rcfile'], service_name=options['--service'], base_dir=options.get('--basedir') or '', src_dir=options.get('--srcdir') or '')
        if options['--merge']:
            coverallz.merge(options['--merge'])
        if options['debug']:
            log.info('Testing coveralls-python...')
            coverallz.wear(dry_run=True)
            return
        if options['--output']:
            log.info('Write coverage report to file...')
            coverallz.save_report(options['--output'])
            return
        if options['--submit']:
            with open(options['--submit']) as report_file:
                coverallz.submit_report(report_file.read())
            return
        if options['--finish']:
            log.info('Finishing parallel jobs...')
            coverallz.parallel_finish()
            log.info('Done')
            return
        log.info('Submitting coverage to coveralls.io...')
        result = coverallz.wear()
        log.info('Coverage submitted!')
        log.debug(result)
        if result:
            log.info(result.get('message'))
            log.info(result.get('url'))
    except KeyboardInterrupt:
        log.info('Aborted')
    except Exception as e:
        log.exception('Error running coveralls: %s', e)
        sys.exit(1)",options['debug'],103,options['--verbose'] is None,False,17.965205598154213,N/A
"def __exit__(self, *args):
    super(PrintingBlockTimer, self).__exit__(*args)
    msg = 'Execution took {0:f}s'.format(self.exec_duration)
<mask>:
        msg = self.prefix + ': ' + msg
    log.debug(msg)",self.prefix,20,self.prefix,True,100.00000000000004,N/A
"def docker_client():
    global _docker_client
<mask>:
        _docker_client = docker.Client(base_url=app.config['DOCKER_URL'])
    return _docker_client",_docker_client is None,10,_docker_client is None,True,100.00000000000004,N/A
"def iam_client():
    global _iam_client
<mask>:
        _iam_client = boto3.client('iam')
    return _iam_client",_iam_client is None,10,_iam_client is None,True,100.00000000000004,N/A
"def sts_client():
    global _sts_client
<mask>:
        aws_region = app.config.get('AWS_REGION')
        _sts_client = boto3.client(service_name='sts', region_name=aws_region, endpoint_url=f'https://sts.{aws_region}.amazonaws.com') if aws_region else boto3.client(service_name='sts')
    return _sts_client",_sts_client is None,19,_sts_client is None,True,100.00000000000004,N/A
"@log_exec_time
def find_container(ip):
    pattern = re.compile(app.config['HOSTNAME_MATCH_REGEX'])
    client = docker_client()
    container_id = CONTAINER_MAPPING.get(ip)
<mask>:
        log.info('Container id for IP {0} in cache'.format(ip))
        try:
            with PrintingBlockTimer('Container inspect'):
                container = client.inspect_container(container_id)
            if container['State']['Running']:
                return container
            else:
                log.error('Container id {0} is no longer running'.format(ip))
                if ip in CONTAINER_MAPPING:
                    del CONTAINER_MAPPING[ip]
        except docker.errors.NotFound:
            msg = 'Container id {0} no longer mapped to {1}'
            log.error(msg.format(container_id, ip))
            if ip in CONTAINER_MAPPING:
                del CONTAINER_MAPPING[ip]
    _fqdn = None
    with PrintingBlockTimer('Reverse DNS'):
        if app.config['ROLE_REVERSE_LOOKUP']:
            try:
                _fqdn = socket.gethostbyaddr(ip)[0]
            except socket.error as e:
                log.error('gethostbyaddr failed: {0}'.format(e.args))
                pass
    with PrintingBlockTimer('Container fetch'):
        _ids = [c['Id'] for c in client.containers()]
    for _id in _ids:
        try:
            with PrintingBlockTimer('Container inspect'):
                c = client.inspect_container(_id)
        except docker.errors.NotFound:
            log.error('Container id {0} not found'.format(_id))
            continue
        _ip = c['NetworkSettings']['IPAddress']
        if ip == _ip:
            msg = 'Container id {0} mapped to {1} by IP match'
            log.debug(msg.format(_id, ip))
            CONTAINER_MAPPING[ip] = _id
            return c
        _networks = c['NetworkSettings']['Networks']
        if _networks:
            for _network in _networks:
                if _networks[_network]['IPAddress'] == ip:
                    msg = 'Container id {0} mapped to {1} by sub-network IP match'
                    log.debug(msg.format(_id, ip))
                    CONTAINER_MAPPING[ip] = _id
                    return c
        try:
            _labels = c.get('Config', {}).get('Labels', {})
        except (KeyError, ValueError):
            _labels = {}
        try:
            if _labels.get('io.rancher.container.ip'):
                _ip = _labels.get('io.rancher.container.ip').split('/')[0]
        except docker.errors.NotFound:
            log.error('Container: {0} Label container.ip not found'.format(_id))
        if ip == _ip:
            msg = 'Container id {0} mapped to {1} by Rancher IP match'
            log.debug(msg.format(_id, ip))
            CONTAINER_MAPPING[ip] = _id
            return c
        if app.config['ROLE_REVERSE_LOOKUP']:
            hostname = c['Config']['Hostname']
            domain = c['Config']['Domainname']
            fqdn = '{0}.{1}'.format(hostname, domain)
            _groups = re.match(pattern, _fqdn).groups()
            groups = re.match(pattern, fqdn).groups()
            if _groups and groups:
                if groups[0] == _groups[0]:
                    msg = 'Container id {0} mapped to {1} by FQDN match'
                    log.debug(msg.format(_id, ip))
                    CONTAINER_MAPPING[ip] = _id
                    return c
        if app.config['MESOS_STATE_LOOKUP']:
            mesos_container = find_mesos_container(ip)
            if mesos_container is not None:
                return mesos_container
    log.error('No container found for ip {0}'.format(ip))
    return None",container_id,286,container_id,True,100.00000000000004,N/A
"def bool_env(var_name, default=False):
    """"""
    Get an environment variable coerced to a boolean value.
    Example:
        Bash:
            $ export SOME_VAL=True
        settings.py:
            SOME_VAL = bool_env('SOME_VAL', False)
    Arguments:
        var_name: The name of the environment variable.
        default: The default to use if `var_name` is not specified in the
                 environment.
    Returns: `var_name` or `default` coerced to a boolean using the following
        rules:
            ""False"", ""false"" or """" => False
            Any other non-empty string => True
    """"""
    test_val = getenv(var_name, default)
<mask>:
        return False
    return bool(test_val)","test_val in ('False', 'false', '0')",78,test_val is None,False,11.976547020391715,N/A
"@app.route('/<api_version>/meta-data/iam/info', strict_slashes=False)
@app.route('/<api_version>/meta-data/iam/info/<path:junk>')
def iam_role_info(api_version, junk=None):
<mask>:
        return passthrough(request.path)
    role_params_from_ip = roles.get_role_params_from_ip(request.remote_addr)
    if role_params_from_ip['name']:
        log.debug('Providing IAM role info for {0}'.format(role_params_from_ip['name']))
        return jsonify(roles.get_role_info_from_params(role_params_from_ip))
    else:
        log.error('Role name not found; returning 404.')
        return ('', 404)",not _supports_iam(api_version),32,request.path.startswith('/'),False,4.990049701936832,N/A
"@app.route('/<api_version>/meta-data/iam/security-credentials/')
def iam_role_name(api_version):
<mask>:
        return passthrough(request.path)
    role_params_from_ip = roles.get_role_params_from_ip(request.remote_addr)
    if role_params_from_ip['name']:
        return role_params_from_ip['name']
    else:
        log.error('Role name not found; returning 404.')
        return ('', 404)",not _supports_iam(api_version),23,request.path.startswith('/api/'),False,4.456882760699063,N/A
"@app.route('/<api_version>/meta-data/iam/security-credentials/<path:requested_role>', strict_slashes=False)
def iam_sts_credentials(api_version, requested_role):
<mask>:
        return passthrough(request.path)
    try:
        role_params = roles.get_role_params_from_ip(request.remote_addr, requested_role=requested_role.rstrip('/'))
    except roles.UnexpectedRoleError:
        msg = ""Role name {0} doesn't match expected role for container""
        log.error(msg.format(requested_role))
        return ('', 404)
    log.debug('Providing assumed role credentials for {0}'.format(role_params['name']))
    assumed_role = roles.get_assumed_role_credentials(role_params=role_params, api_version=api_version)
    return jsonify(assumed_role)",not _supports_iam(api_version),42,request.path.startswith('/'),False,4.990049701936832,N/A
"@app.route('/<api_version>/meta-data/iam/info', strict_slashes=False)
@app.route('/<api_version>/meta-data/iam/info/<path:junk>')
def get_iam_info(api_version, junk=None):
    role_params_from_ip = roles.get_role_params_from_ip(request.remote_addr)
<mask>:
        log.debug('Providing IAM role info for {0}'.format(role_params_from_ip['name']))
        return jsonify(roles.get_role_info_from_params(role_params_from_ip))
    else:
        log.error('Role name not found; returning 404.')
        return ('', 404)",role_params_from_ip['name'],28,role_params_from_ip,False,65.14390575310559,N/A
"@app.route('/<api_version>/meta-data/iam/security-credentials/')
def get_security_credentials_slash(api_version):
    role_params = roles.get_role_params_from_ip(request.remote_addr)
<mask>:
        return ('', 404)
    return (role_params['name'], 200)",not role_params['name'],13,not role_params,False,47.23665527410149,N/A
"def parse_sml(response: Dict[str, Any]) -> Dict[str, Any]:
    """"""Parse the json for a SML Hue motion sensor and return the data.""""""
    data = {}
<mask>:
        lightlevel = response['state']['lightlevel']
        tholddark = response['config']['tholddark']
        tholdoffset = response['config']['tholdoffset']
        if lightlevel is not None:
            lx = round(float(10 ** ((lightlevel - 1) / 10000)), 2)
            dark = response['state']['dark']
            daylight = response['state']['daylight']
            data = {'light_level': lightlevel, 'lx': lx, 'dark': dark, 'daylight': daylight, 'threshold_dark': tholddark, 'threshold_offset': tholdoffset}
        else:
            data = {'light_level': 'No light level data', 'lx': None, 'dark': None, 'daylight': None, 'threshold_dark': None, 'threshold_offset': None}
    elif response['type'] == 'ZLLTemperature':
        temp = response['state']['temperature']
        temp = temp / 100.0 if temp is not None else 'No temperature data'
        data = {'temperature': temp}
    elif response['type'] == 'ZLLPresence':
        name_raw = response['name']
        arr = name_raw.split()
        arr.insert(-1, 'motion')
        name = ' '.join(arr)
        hue_state = response['state']['presence']
        state = STATE_ON if hue_state is True else STATE_OFF
        data = {'model': 'SML', 'name': name, 'state': state, 'battery': response['config']['battery'], 'on': response['config']['on'], 'reachable': response['config']['reachable'], 'sensitivity': response['config']['sensitivity'], 'last_updated': response['state']['lastupdated'].split('T')}
    return data",response['type'] == 'ZLLLightLevel',159,response['type'] == 'ZLLLight',False,80.91067115702207,N/A
"def parse_hue_api_response(sensors: Iterable[Dict[str, Any]]):
    """"""Take in the Hue API json response.""""""
    data_dict = {}
    for sensor in filter(lambda x: x['modelid'].startswith('SML'), sensors):
        model_id = sensor['modelid'][0:3]
        unique_sensor_id = sensor['uniqueid']
        _key = model_id + '_' + unique_sensor_id[:-5]
        parsed_sensor = parse_sml(sensor)
<mask>:
            data_dict[_key] = parsed_sensor
        else:
            data_dict[_key].update(parsed_sensor)
    return data_dict",_key not in data_dict,45,"isinstance(parsed_sensor, dict)",False,6.567274736060395,N/A
"@property
def is_on(self):
    """"""Return the state of the sensor.""""""
    data = self.sensor_data
<mask>:
        return data['state'] == STATE_ON
    return False",data and data['model'] == 'SML' and data['changed'],19,'state' in data,False,0.70335269181743,N/A
"def _register_new_entity(self, dev_id, model, new_entities_to_add):
    """"""Register a new Entity and add it in platform queue for HA setup.""""""
    entity_cls, async_add_entities = self._registered_platforms[model]
    platform_entity = entity_cls(dev_id, self)
    self.registered_entities[dev_id] = platform_entity
<mask>:
        new_entities_to_add[async_add_entities] = [entity_cls, []]
    new_entities_to_add[async_add_entities][1].append(platform_entity)",async_add_entities not in new_entities_to_add,35,async_add_entities not in new_entities_to_add,True,100.00000000000004,N/A
"def make_annot_files(args, bed_for_annot):
    print('making annot file')
    df_bim = pd.read_csv(args.bimfile, delim_whitespace=True, usecols=[0, 1, 2, 3], names=['CHR', 'SNP', 'CM', 'BP'])
    iter_bim = [['chr' + str(x1), x2 - 1, x2] for x1, x2 in np.array(df_bim[['CHR', 'BP']])]
    bimbed = BedTool(iter_bim)
    annotbed = bimbed.intersect(bed_for_annot)
    bp = [x.start + 1 for x in annotbed]
    df_int = pd.DataFrame({'BP': bp, 'ANNOT': 1})
    df_annot = pd.merge(df_bim, df_int, how='left', on='BP')
    df_annot.fillna(0, inplace=True)
    df_annot = df_annot[['ANNOT']].astype(int)
<mask>:
        with gzip.open(args.annot_file, 'wb') as f:
            df_annot.to_csv(f, sep='\t', index=False)
    else:
        df_annot.to_csv(args.annot_file, sep='\t', index=False)",args.annot_file.endswith('.gz'),78,args.gzip,False,2.739887961440494,N/A
"def gencov_obs_to_liab(gencov_obs, P1, P2, K1, K2):
    """"""
    Converts genetic covariance on the observed scale in an ascertained sample to genetic
    covariance on the liability scale in the population

    Parameters
    ----------
    gencov_obs : float
        Genetic covariance on the observed scale in an ascertained sample.
    P1, P2 : float in (0,1)
        Prevalences of phenotypes 1,2 in the sample.
    K1, K2 : float in (0,1)
        Prevalences of phenotypes 1,2 in the population.

    Returns
    -------
    gencov_liab : float
        Genetic covariance between liabilities in the population.

    Note: if a trait is a QT, set P = K = None.

    """"""
    c1 = 1
    c2 = 1
<mask>:
        c1 = np.sqrt(h2_obs_to_liab(1, P1, K1))
    if P2 is not None and K2 is not None:
        c2 = np.sqrt(h2_obs_to_liab(1, P2, K2))
    return gencov_obs * c1 * c2",P1 is not None and K1 is not None,128,P1 is not None and K1 is not None,True,100.00000000000004,N/A
"def h2_obs_to_liab(h2_obs, P, K):
    """"""
    Converts heritability on the observed scale in an ascertained sample to heritability
    on the liability scale in the population.

    Parameters
    ----------
    h2_obs : float
        Heritability on the observed scale in an ascertained sample.
    P : float in (0,1)
        Prevalence of the phenotype in the sample.
    K : float in (0,1)
        Prevalence of the phenotype in the population.

    Returns
    -------
    h2_liab : float
        Heritability of liability in the population.

    """"""
<mask>:
        return h2_obs
    if K <= 0 or K >= 1:
        raise ValueError('K must be in the range (0,1)')
    if P <= 0 or P >= 1:
        raise ValueError('P must be in the range (0,1)')
    thresh = norm.isf(K)
    conversion_factor = K ** 2 * (1 - K) ** 2 / (P * (1 - P) * norm.pdf(thresh) ** 2)
    return h2_obs * conversion_factor",np.isnan(P) and np.isnan(K),137,"not isinstance(h2_obs, float)",False,3.6353588668522963,N/A
"@classmethod
def aggregate(cls, y, x, N, M, intercept=None):
<mask>:
        intercept = cls.__null_intercept__
    num = M * (np.mean(y) - intercept)
    denom = np.mean(np.multiply(x, N))
    return num / denom",intercept is None,27,intercept is None,True,100.00000000000004,N/A
"def _combine_twostep_jknives(self, step1_jknife, step2_jknife, M_tot, c, Nbar=1):
    """"""Combine free intercept and constrained intercept jackknives for --two-step.""""""
    n_blocks, n_annot = step1_jknife.delete_values.shape
    n_annot -= 1
<mask>:
        raise ValueError('twostep not yet implemented for partitioned LD Score.')
    step1_int, _ = self._intercept(step1_jknife)
    est = np.hstack((step2_jknife.est, np.array(step1_int).reshape((1, 1))))
    delete_values = np.zeros((n_blocks, n_annot + 1))
    delete_values[:, n_annot] = step1_jknife.delete_values[:, n_annot]
    delete_values[:, 0:n_annot] = step2_jknife.delete_values - c * (step1_jknife.delete_values[:, n_annot] - step1_int).reshape((n_blocks, n_annot))
    pseudovalues = jk.Jackknife.delete_values_to_pseudovalues(delete_values, est)
    jknife_est, jknife_var, jknife_se, jknife_cov = jk.Jackknife.jknife(pseudovalues)
    jknife = namedtuple('jknife', ['est', 'jknife_se', 'jknife_est', 'jknife_var', 'jknife_cov', 'delete_values'])
    return jknife(est, jknife_se, jknife_est, jknife_var, jknife_cov, delete_values)",n_annot > 2,91,n_blocks == 1,False,16.233395773754953,N/A
"def _select_and_log(x, ii, log, msg):
    """"""Fiter down to rows that are True in ii. Log # of SNPs removed.""""""
    new_len = ii.sum()
<mask>:
        raise ValueError(msg.format(N=0))
    else:
        x = x[ii]
        log.log(msg.format(N=new_len))
    return x",new_len == 0,32,new_len == 0,True,100.00000000000004,N/A
"def smart_merge(x, y):
    """"""Check if SNP columns are equal. If so, save time by using concat instead of merge.""""""
<mask>:
        x = x.reset_index(drop=True)
        y = y.reset_index(drop=True).drop('SNP', 1)
        out = pd.concat([x, y], axis=1)
    else:
        out = pd.merge(x, y, how='inner', on='SNP')
    return out",len(x) == len(y) and (x.index == y.index).all() and (x.SNP == y.SNP).all(),41,len(x.columns) == len(y.columns),False,8.326564304711624,N/A
"def _read_annot(args, log):
    """"""Read annot matrix.""""""
    try:
<mask>:
            overlap_matrix, M_tot = _read_chr_split_files(args.ref_ld_chr, args.ref_ld, log, 'annot matrix', ps.annot, frqfile=args.frqfile)
        elif args.ref_ld_chr is not None:
            overlap_matrix, M_tot = _read_chr_split_files(args.ref_ld_chr, args.ref_ld, log, 'annot matrix', ps.annot, frqfile=args.frqfile_chr)
    except Exception:
        log.log('Error parsing .annot file.')
        raise
    return (overlap_matrix, M_tot)",args.ref_ld is not None,43,args.ref_ld_chr is not None,False,52.53819788848316,N/A
"def _read_M(args, log, n_annot):
    """"""Read M (--M, --M-file, etc).""""""
<mask>:
        try:
            M_annot = [float(x) for x in _splitp(args.M)]
        except ValueError as e:
            raise ValueError('Could not cast --M to float: ' + str(e.args))
    elif args.ref_ld:
        M_annot = ps.M_fromlist(_splitp(args.ref_ld), common=not args.not_M_5_50)
    elif args.ref_ld_chr:
        M_annot = ps.M_fromlist(_splitp(args.ref_ld_chr), _N_CHR, common=not args.not_M_5_50)
    try:
        M_annot = np.array(M_annot).reshape((1, n_annot))
    except ValueError as e:
        raise ValueError('# terms in --M must match # of LD Scores in --ref-ld.\n' + str(e.args))
    return M_annot",args.M,73,args.M,True,100.00000000000004,N/A
"def _read_w_ld(args, log):
    """"""Read regression SNP LD.""""""
<mask>:
        raise ValueError('--w-ld must point to a single fileset (no commas allowed).')
    w_ld = _read_chr_split_files(args.w_ld_chr, args.w_ld, log, 'regression weight LD Score', ps.ldscore_fromlist)
    if len(w_ld.columns) != 2:
        raise ValueError('--w-ld may only have one LD Score column.')
    w_ld.columns = ['SNP', 'LD_weights']
    log.log('Read regression weight LD Scores for {N} SNPs.'.format(N=len(w_ld)))
    return w_ld","args.w_ld and ',' in args.w_ld or (args.w_ld_chr and ',' in args.w_ld_chr)",56,len(args.w_ld_chr) == 1,False,10.226845564003401,N/A
"def sub_chr(s, chrom):
    """"""Substitute chr for @, else append chr to the end of str.""""""
<mask>:
        s += '@'
    return s.replace('@', str(chrom))",'@' not in s,22,s[-1] != '@',False,19.070828081828378,N/A
"def get_present_chrs(fh, num):
    """"""Checks which chromosomes exist, assuming that the file base will be appended by a dot in any suffix.""""""
    chrs = []
    for chrom in xrange(1, num):
<mask>:
            chrs.append(chrom)
    return chrs","glob.glob(sub_chr(fh, chrom) + '.*')",33,not fh.endswith(chrom),False,3.431131994523984,N/A
"def which_compression(fh):
    """"""Given a file prefix, figure out what sort of compression to use.""""""
<mask>:
        suffix = '.bz2'
        compression = 'bz2'
    elif os.access(fh + '.gz', 4):
        suffix = '.gz'
        compression = 'gzip'
    elif os.access(fh, 4):
        suffix = ''
        compression = None
    else:
        raise IOError('Could not open {F}[./gz/bz2]'.format(F=fh))
    return (suffix, compression)","os.access(fh + '.bz2', 4)",50,"os.access(fh + '.bz2', 4)",True,100.00000000000004,N/A
"def get_compression(fh):
    """"""Which sort of compression should we use with read_csv?""""""
<mask>:
        compression = 'gzip'
    elif fh.endswith('bz2'):
        compression = 'bz2'
    else:
        compression = None
    return compression",fh.endswith('gz'),26,fh.endswith('gzip'),False,53.7284965911771,N/A
"def read_cts(fh, match_snps):
    """"""Reads files for --cts-bin.""""""
    compression = get_compression(fh)
    cts = read_csv(fh, compression=compression, header=None, names=['SNP', 'ANNOT'])
<mask>:
        raise ValueError('--cts-bin and the .bim file must have identical SNP columns.')
    return cts.ANNOT.values","not series_eq(cts.SNP, match_snps)",31,cts.SNP != match_snps,False,18.505100105615163,N/A
"def _check_shape(x, y):
    """"""Check that x and y have the correct shapes (for regression jackknives).""""""
<mask>:
        raise ValueError('x and y must be 2D arrays.')
    if x.shape[0] != y.shape[0]:
        raise ValueError('Number of datapoints in x != number of datapoints in y.')
    if y.shape[1] != 1:
        raise ValueError('y must have shape (n_snp, 1)')
    n, p = x.shape
    if p > n:
        raise ValueError('More dimensions than datapoints.')
    return (n, p)",len(x.shape) != 2 or len(y.shape) != 2,67,"not isinstance(x.shape, np.ndarray)",False,12.547112659387805,N/A
"def _check_shape_block(xty_block_values, xtx_block_values):
    """"""Check that xty_block_values and xtx_block_values have correct shapes.""""""
<mask>:
        raise ValueError('Shape of xty_block_values must equal shape of first two dimensions of xty_block_values.')
    if len(xtx_block_values.shape) < 3:
        raise ValueError('xtx_block_values must be a 3D array.')
    if xtx_block_values.shape[1] != xtx_block_values.shape[2]:
        raise ValueError('Last two axes of xtx_block_values must have same dimension.')
    return xtx_block_values.shape[0:2]",xtx_block_values.shape[0:2] != xty_block_values.shape,52,len(xty_block_values.shape) != 2,False,28.883149310991286,N/A
"def __init__(self, x, y, n_blocks=None, separators=None):
    self.N, self.p = _check_shape(x, y)
<mask>:
        if max(separators) != self.N:
            raise ValueError('Max(separators) must be equal to number of data points.')
        if min(separators) != 0:
            raise ValueError('Max(separators) must be equal to 0.')
        self.separators = sorted(separators)
        self.n_blocks = len(separators) - 1
    elif n_blocks is not None:
        self.n_blocks = n_blocks
        self.separators = self.get_separators(self.N, self.n_blocks)
    else:
        raise ValueError('Must specify either n_blocks are separators.')
    if self.n_blocks > self.N:
        raise ValueError('More blocks than data points.')",separators is not None,75,separators is not None,True,100.00000000000004,N/A
"@classmethod
def delete_values_to_pseudovalues(cls, delete_values, est):
    """"""
        Converts whole-data estimate and delete values to pseudovalues.

        Parameters
        ----------
        delete_values : np.matrix with shape (n_blocks, p)
            Delete values.
        est : np.matrix with shape (1, p):
            Whole-data estimate.

        Returns
        -------
        pseudovalues : np.matrix with shape (n_blocks, p)
            Psuedovalues.

        Raises
        ------
        ValueError :
            If est.shape != (1, delete_values.shape[1])

        """"""
    n_blocks, p = delete_values.shape
<mask>:
        raise ValueError('Different number of parameters in delete_values than in est.')
    return n_blocks * est - (n_blocks - 1) * delete_values","est.shape != (1, p)",79,n_blocks != len(delete_values),False,8.913765521398126,N/A
"def __init__(self, x, y, n_blocks=None, nn=False, separators=None):
    Jackknife.__init__(self, x, y, n_blocks, separators)
<mask>:
        func = lambda x, y: np.atleast_2d(nnls(x, np.array(y).T[0])[0])
    else:
        func = lambda x, y: np.atleast_2d(np.linalg.lstsq(x, np.array(y).T[0])[0])
    self.est = func(x, y)
    self.delete_values = self.delete_values(x, y, func, self.separators)
    self.pseudovalues = self.delete_values_to_pseudovalues(self.delete_values, self.est)
    self.jknife_est, self.jknife_var, self.jknife_se, self.jknife_cov = self.jknife(self.pseudovalues)",nn,48,nn,True,100.00000000000004,N/A
"@pytest.fixture
def markdown_examples():
<mask>:
        filehandle = importlib.resources.as_file(importlib.resources.files('python_jsonschema_objects.examples') / 'README.md')
    else:
        filehandle = importlib.resources.path('python_jsonschema_objects.examples', 'README.md')
    with filehandle as md:
        examples = pjs.markdown_support.extract_code_blocks(md)
    return {json.loads(v)['title']: json.loads(v) for v in examples['schema']}","hasattr(importlib.resources, 'as_file')",28,"sys.version_info < (3, 0)",False,5.604233375480572,N/A
"def markdown_to_rst(src):
    pandoc.core.PANDOC_PATH = '/usr/local/bin/pandoc'
<mask>:
        raise Exception('Pandoc not available')
    doc = pandoc.Document()
    doc.markdown = open('README.md').read()
    return doc.rst",not os.path.exists(pandoc.core.PANDOC_PATH),18,not pandoc,False,0.10630920484560723,N/A
"def default(self, obj):
    from python_jsonschema_objects import classbuilder, wrapper_types
<mask>:
        return obj.for_json()
    else:
        return json.JSONEncoder.default(self, obj)","isinstance(obj, (wrapper_types.ArrayWrapper, classbuilder.ProtocolBase, classbuilder.LiteralValue))",15,"wrapper_types and issubclass(obj, classbuilder)",False,10.980266522628492,N/A
"def propmerge(into, data_from):
    """"""Merge JSON schema requirements into a dictionary""""""
    newprops = copy.deepcopy(into)
    for prop, propval in data_from.items():
<mask>:
            newprops[prop] = propval
            continue
        new_sp = newprops[prop]
        for subprop, spval in propval.items():
            if subprop not in new_sp:
                new_sp[subprop] = spval
            elif subprop == 'enum':
                new_sp[subprop] = set(spval) & set(new_sp[subprop])
            elif subprop == 'type':
                if spval != new_sp[subprop]:
                    raise TypeError(""Type cannot conflict in allOf'"")
            elif subprop in ('minLength', 'minimum'):
                new_sp[subprop] = new_sp[subprop] if new_sp[subprop] > spval else spval
            elif subprop in ('maxLength', 'maximum'):
                new_sp[subprop] = new_sp[subprop] if new_sp[subprop] < spval else spval
            elif subprop == 'multipleOf':
                if new_sp[subprop] % spval == 0:
                    new_sp[subprop] = spval
                else:
                    raise AttributeError('Cannot set conflicting multipleOf values')
            else:
                new_sp[subprop] = spval
        newprops[prop] = new_sp
    return newprops",prop not in newprops,119,prop not in newprops,True,100.00000000000004,N/A
"def resolve_ref_uri(base, ref):
<mask>:
        uri = base.rsplit('#', 1)[0] + ref
    else:
        uri = ref
    return uri",ref[0] == '#',16,'#' in base,False,17.86690863748233,N/A
"@classmethod
def from_object(cls, obj, names=None):
<mask>:
        names = dir(obj)
    ns = {name: getattr(obj, name) for name in names}
    return cls(ns)",names is None,20,names is None,True,100.00000000000004,N/A
"@classmethod
def from_mapping(cls, ns, names=None):
<mask>:
        ns = {name: ns[name] for name in names}
    return cls(ns)",names,16,names,True,100.00000000000004,N/A
"@registry.register()
def multipleOf(param, value, _):
    value = decimal.Decimal(str(value))
    divisor = decimal.Decimal(str(param))
<mask>:
        raise ValidationError('{0} is not a multiple of {1}'.format(value, param))",value % divisor != 0,21,value % divisor != 0,True,100.00000000000004,N/A
"@registry.register()
def enum(param, value, _):
<mask>:
        raise ValidationError('{0} is not one of {1}'.format(value, param))",value not in param,14,value not in SUPPORTED_VALUES,False,30.213753973567677,N/A
"@registry.register()
def const(param, value, _):
<mask>:
        raise ValidationError('{0} is not constant {1}'.format(value, param))",value != param,13,not value,False,18.393972058572114,N/A
"@registry.register()
def minimum(param, value, type_data):
    exclusive = type_data.get('exclusiveMinimum')
<mask>:
        if value <= param:
            raise ValidationError('{0} is less than or equal to {1}'.format(value, param))
    elif value < param:
        raise ValidationError('{0} is less than {1}'.format(value, param))",exclusive,34,exclusive,True,100.00000000000004,N/A
"@registry.register()
def maximum(param, value, type_data):
    exclusive = type_data.get('exclusiveMaximum')
<mask>:
        if value >= param:
            raise ValidationError('{0} is greater than or equal to {1}'.format(value, param))
    elif value > param:
        raise ValidationError('{0} is greater than {1}'.format(value, param))",exclusive,34,exclusive,True,100.00000000000004,N/A
"def __init__(self, schema_uri: typing.Union[typing.AnyStr, typing.Mapping], resolved: typing.Dict[typing.AnyStr, typing.Mapping]={}, registry: Optional[referencing.Registry]=None, resolver: Optional[referencing.typing.Retrieve]=None, specification_uri: Optional[str]=None):
<mask>:
        uri = os.path.normpath(schema_uri)
        self.basedir = os.path.dirname(uri)
        with codecs.open(uri, 'r', 'utf-8') as fin:
            self.schema = json.loads(fin.read())
    else:
        self.schema = schema_uri
        uri = os.path.normpath(FILE)
        self.basedir = os.path.dirname(uri)
    if '$schema' in self.schema and self.schema['$schema'].rstrip('#') not in SUPPORTED_VERSIONS:
        warnings.warn('Schema version {} not recognized. Some keywords and features may not be supported.'.format(self.schema['$schema']))
    if registry is not None:
        if not isinstance(registry, referencing.Registry):
            raise TypeError('registry must be a Registry instance')
        if resolver is not None:
            raise AttributeError('Cannot specify both registry and resolver. If you provide your own registry, pass the resolver directly to that')
        self.registry = registry
    elif resolver is not None:

        def file_and_memory_handler(uri):
            if uri.startswith('file:'):
                return Resource.from_contents(self.relative_file_resolver(uri))
            return resolver(uri)
        self.registry = Registry(retrieve=file_and_memory_handler)
    else:

        def file_and_memory_handler(uri):
            if uri.startswith('file:'):
                return Resource.from_contents(self.relative_file_resolver(uri))
            raise RuntimeError('No remote resource resolver provided. Cannot resolve {}'.format(uri))
        self.registry = Registry(retrieve=file_and_memory_handler)
    if '$schema' not in self.schema:
        warnings.warn('Schema version not specified. Defaulting to {}'.format(specification_uri or 'http://json-schema.org/draft-04/schema'))
        updated = {'$schema': specification_uri or 'http://json-schema.org/draft-04/schema'}
        updated.update(self.schema)
        self.schema = updated
    schema = Resource.from_contents(self.schema)
    if schema.id() is None:
        warnings.warn(""Schema id not specified. Defaulting to 'self'"")
        updated = {'$id': 'self', 'id': 'self'}
        updated.update(self.schema)
        self.schema = updated
        schema = Resource.from_contents(self.schema)
    self.registry = self.registry.with_resource('', schema)
    if len(resolved) > 0:
        warnings.warn(""Use of 'memory:' URIs is deprecated. Provide a registry with properly resolved references if you want to resolve items externally."", DeprecationWarning)
    for uri, contents in resolved.items():
        from referencing.jsonschema import specification_with
        specification = specification_with(specification_uri or self.schema['$schema'])
        self.registry = self.registry.with_resource('memory:' + uri, referencing.Resource.from_contents(contents, specification))
    validatorClass = jsonschema.validators.validator_for({'$schema': specification_uri or self.schema['$schema']})
    meta_validator = validatorClass(validatorClass.META_SCHEMA, registry=self.registry)
    meta_validator.validate(self.schema)
    self.validator = validatorClass(self.schema, registry=self.registry)
    self._classes = None
    self._resolved = None","isinstance(schema_uri, str)",262,"isinstance(schema_uri, str)",True,100.00000000000004,N/A
"def get_class(self, uri):
<mask>:
        self._classes = self.build_classes()
    return self._resolved.get(uri, None)",self._resolved is None,10,self._classes is None,False,37.99178428257963,N/A
"def build_classes(self, strict=False, named_only=False, standardize_names=True, any_of: typing.Optional[typing.Literal['use-first']]=None):
    """"""
        Build all of the classes named in the JSONSchema.

        Class names will be transformed using inflection by default, so names
        with spaces in the schema will be camelcased, while names without
        spaces will have internal capitalization dropped. Thus ""Home Address""
        becomes ""HomeAddress"", while ""HomeAddress"" becomes ""Homeaddress"". To
        disable this behavior, pass standardize_names=False, but be aware that
        accessing names with spaces from the namespace can be problematic.

        Args:
            strict: (bool) use this to validate required fields while creating the class
            named_only: (bool) If true, only properties with an actual title attribute
                will be included in the resulting namespace (although all will be
                generated).
            standardize_names: (bool) If true (the default), class names will be
                transformed by camel casing
            any_of: (literal) If not set to None, defines the way anyOf clauses are resolved:
                - 'use-first': Generate to the first matching schema in the list under the anyOf
                - None: default behavior, anyOf is not supported in the schema

        Returns:
            A namespace containing all the generated classes

        """"""
    opts = {'strict': strict, 'any_of': any_of}
    builder = classbuilder.ClassBuilder(self.resolver, opts)
    for nm, defn in self.schema.get('definitions', {}).items():
        resolved = self.resolver.lookup('#/definitions/' + nm)
        uri = python_jsonschema_objects.util.resolve_ref_uri(self.resolver._base_uri, '#/definitions/' + nm)
        builder.construct(uri, resolved.contents)
<mask>:
        name_transform = lambda t: inflection.camelize(inflection.parameterize(str(t), '_'))
    else:
        name_transform = lambda t: t
    nm = self.schema['title'] if 'title' in self.schema else self.schema['$id']
    nm = inflection.parameterize(str(nm), '_')
    builder.construct(nm, self.schema)
    self._resolved = builder.resolved
    classes = {}
    for uri, klass in builder.resolved.items():
        title = getattr(klass, '__title__', None)
        if title is not None:
            classes[name_transform(title)] = klass
        elif not named_only:
            classes[name_transform(uri.split('/')[-1])] = klass
    return python_jsonschema_objects.util.Namespace.from_mapping(classes)",standardize_names,261,not standardize_names,False,59.460355750136046,N/A
"def extract_code_blocks(filename):
    with open(filename) as fin:
        doc = fin.read().split('\n')
    M = markdown.Markdown(extensions=[SpecialFencedCodeExtension()])
    preprocessors = M.preprocessors
    tree_processors = M.treeprocessors
<mask>:
        preprocessors = preprocessors.values()
        tree_processors = tree_processors.values()
    for prep in preprocessors:
        doc = prep.run(doc)
    root = M.parser.parseDocument(doc).getroot()
    for treeproc in tree_processors:
        newRoot = treeproc.run(root)
        if newRoot is not None:
            root = newRoot
    return SpecialFencePreprocessor.EXAMPLES",markdown_version_info[0] == 2,52,len(preprocessors) > 1,False,0.0,N/A
"def extendMarkdown(self, md, md_globals=None):
    """"""Add FencedBlockPreprocessor to the Markdown instance.""""""
    md.registerExtension(self)
<mask>:
        md.preprocessors.register(SpecialFencePreprocessor(md), 'fenced_code_block', 10)
    else:
        md.preprocessors.add('fenced_code_block', SpecialFencePreprocessor(md), '>normalize_whitespace')",markdown_version_info[0] >= 3,19,self.is_fenced,False,3.2174093287959424,N/A
"def run(self, lines):
    text = '\n'.join(lines)
    while True:
        m = self.FENCED_BLOCK_RE.search(text)
<mask>:
            if m.group('lang'):
                lang = m.group('lang')
                example = m.group('code')
                try:
                    self.EXAMPLES[lang].append(example)
                except KeyError:
                    self.EXAMPLES[lang] = [example]
            text = '%s\n%s' % (text[:m.start()], text[m.end():])
        else:
            break
    return text.split('\n')",m,37,m,True,100.00000000000004,N/A
"def __init__(self, value, typ=None):
    """"""@todo: to be defined

        :value: @todo

        """"""
<mask>:
        self._value = value._value
    else:
        self._value = value
    self.validate()
    constval = self.const()
    if constval is not None:
        self._value = constval","isinstance(value, LiteralValue)",31,"hasattr(value, '_value')",False,22.089591134157878,N/A
"def validate(self):
    info = self.propinfo('__literal__')
    for param, paramval in sorted(info.items(), key=lambda x: x[0].lower() != 'type'):
        validator = validators.registry(param)
<mask>:
            validator(paramval, self._value, info)",validator is not None,22,validator,False,4.9787068367863965,N/A
"def __eq__(self, other):
<mask>:
        return self._value == other._value
    return self._value == other","isinstance(other, LiteralValue)",12,"isinstance(other, self.__class__)",False,22.416933501922287,N/A
"def __get__(self, obj, owner=None):
<mask>:
        return self
    try:
        return obj._properties[self.prop]
    except KeyError:
        raise AttributeError('No such attribute')",obj is None and owner is not None,16,obj is None,False,18.887560283756194,N/A
"def __set__(self, obj, val):
    info = self.info
<mask>:
        ok = False
        errors = []
        type_checks = []
        for typ in info['type']:
            if not isinstance(typ, dict):
                type_checks.append(typ)
                continue
            typ = next((t for n, t in validators.SCHEMA_TYPE_MAPPING if typ['type'] == n))
            if typ is None:
                typ = type(None)
            if isinstance(typ, (list, tuple)):
                type_checks.extend(typ)
            else:
                type_checks.append(typ)
        for typ in type_checks:
            if not isinstance(typ, TypeProxy) and isinstance(val, typ):
                ok = True
                break
            elif hasattr(typ, 'isLiteralClass'):
                try:
                    validator = typ(val)
                    validator.validate()
                except Exception as e:
                    errors.append(""Failed to coerce to '{0}': {1}"".format(typ, e))
                    pass
                else:
                    ok = True
                    break
            elif util.safe_issubclass(typ, ProtocolBase):
                try:
                    val = typ(**val)
                    val.validate()
                except Exception as e:
                    errors.append(""Failed to coerce to '{0}': {1}"".format(typ, e))
                    pass
                else:
                    ok = True
                    break
            elif util.safe_issubclass(typ, wrapper_types.ArrayWrapper):
                try:
                    val = typ(val)
                    val.validate()
                except Exception as e:
                    errors.append(""Failed to coerce to '{0}': {1}"".format(typ, e))
                    pass
                else:
                    ok = True
                    break
            elif isinstance(typ, TypeProxy):
                try:
                    if isinstance(val, dict):
                        val = typ(**val)
                    else:
                        val = typ(val)
                    val.validate()
                except Exception as e:
                    errors.append(""Failed to coerce to '{0}': {1}"".format(typ, e))
                    pass
                else:
                    ok = True
                    break
        if not ok:
            errstr = '\n'.join(errors)
            raise validators.ValidationError('Object must be one of {0}: \n{1}'.format(info['type'], errstr))
    elif info['type'] == 'array':
        val = info['validator'](val)
        val.validate()
    elif util.safe_issubclass(info['type'], wrapper_types.ArrayWrapper):
        val = info['type'](val)
        val.validate()
    elif getattr(info['type'], 'isLiteralClass', False) is True:
        if not isinstance(val, info['type']):
            validator = info['type'](val)
            validator.validate()
            if validator._value is not None:
                val = validator
    elif util.safe_issubclass(info['type'], ProtocolBase):
        if not isinstance(val, info['type']):
            val = info['type'](**val)
        val.validate()
    elif isinstance(info['type'], TypeProxy):
        if isinstance(val, dict):
            val = info['type'](**val)
        else:
            val = info['type'](val)
    elif isinstance(info['type'], TypeRef):
        if not isinstance(val, info['type'].ref_class):
            val = info['type'](**val)
        val.validate()
    elif info['type'] is None:
        if val is not None:
            raise validators.ValidationError('None is only valid value for null')
    else:
        raise TypeError(""Unknown object type: '{0}'"".format(info['type']))
    obj._properties[self.prop] = val","isinstance(info['type'], (list, tuple))",288,type(obj) is type,False,3.005799339448764,N/A
"def __delete__(self, obj):
    prop = self.prop
<mask>:
        raise AttributeError(""'%s' is required"" % prop)
    else:
        obj._properties[prop] = None",prop in obj.__required__,17,obj is None,False,3.7238938287986976,N/A
"def __init__(self, name, schemadef, builder):
    import python_jsonschema_objects.classbuilder as cb
    self._pattern_types = []
    self._additional_type = True
    addlProp = schemadef.get('additionalProperties', True)
<mask>:
        self._additional_type = False
    elif addlProp is True:
        self._additional_type = True
    else:
        if '$ref' in addlProp:
            typ = builder.resolve_type(addlProp['$ref'], name)
        else:
            uri = '{0}/{1}_{2}'.format(name, '<additionalProperties>', '<anonymous>')
            builder.resolved[uri] = builder.construct(uri, addlProp, (cb.ProtocolBase,))
            typ = builder.resolved[uri]
        self._additional_type = typ
    for pattern, typedef in schemadef.get('patternProperties', {}).items():
        if '$ref' in typedef:
            typ = builder.resolve_type(typedef['$ref'], name)
        else:
            uri = '{0}/{1}_{2}'.format(name, '<patternProperties>', pattern)
            builder.resolved[uri] = builder.construct(uri, typedef, (cb.ProtocolBase,))
            typ = builder.resolved[uri]
        self._pattern_types.append(PatternDef(pattern=re.compile(pattern), schema_type=typ))",addlProp is False,86,addlProp is False,True,100.00000000000004,N/A
"def _make_type(self, typ, val):
    import python_jsonschema_objects.classbuilder as cb
<mask>:
        return typ(val)
    if util.safe_issubclass(typ, cb.ProtocolBase):
        return typ(**val)
    if util.safe_issubclass(typ, wrapper_types.ArrayWrapper):
        return typ(val)
    if isinstance(typ, cb.TypeProxy):
        if isinstance(val, dict):
            val = typ(**val)
        else:
            val = typ(val)
        return val
    raise validators.ValidationError('additionalProperty type {0} was neither a literal nor a schema wrapper: {1}'.format(typ, val))","getattr(typ, 'isLiteralClass', None) is True",50,"util.safe_issubclass(typ, cb.Literal)",False,13.545994273378144,N/A
"def instantiate(self, name, val):
    import python_jsonschema_objects.classbuilder as cb
    for p in self._pattern_types:
<mask>:
            logger.debug('Found patternProperties match: %s %s' % (p.pattern.pattern, name))
            return self._make_type(p.schema_type, val)
    if self._additional_type is True:
        valtype = [k for k, t in validators.SCHEMA_TYPE_MAPPING if t is not None and isinstance(val, t)]
        valtype = valtype[0]
        return MakeLiteral(name, valtype, val)
    elif isinstance(self._additional_type, (type, cb.TypeProxy)):
        return self._make_type(self._additional_type, val)
    raise validators.ValidationError('additionalProperties not permitted and no patternProperties specified')",p.pattern.search(name),66,"isinstance(p, cb.Pattern)",False,7.809849842300637,N/A
"def __eq__(self, other):
<mask>:
        return self.for_json() == other.for_json()
    else:
        return self.for_json() == other","isinstance(other, ArrayWrapper)",13,"isinstance(other, JSONObject)",False,53.7284965911771,N/A
"def __init__(self, ary):
    """"""Initialize a wrapper for the array

        Args:
            ary: (list-like, or ArrayWrapper)
        """"""
    ' Marks whether or not the underlying array has been modified '
    self._dirty = True
    ' Holds a typed copy of the array '
    self._typed = None
<mask>:
        self.data = ary
    else:
        raise TypeError('Invalid value given to array validator: {0}'.format(ary))
    logger.debug(fmt('Initializing ArrayWrapper {} with {}', self, ary))","isinstance(ary, (list, tuple, collections.abc.Sequence))",62,"isinstance(ary, list)",False,10.621255679580061,N/A
"@property
def typed_elems(self):
    logger.debug(fmt('Accessing typed_elems of ArrayWrapper {} ', self))
<mask>:
        self.validate()
    return self._typed",self._typed is None or self._dirty is True,14,not self._typed,False,13.501633901742348,N/A
"def for_json(self):
    from python_jsonschema_objects import classbuilder
    out = []
    for item in self.typed_elems:
<mask>:
            out.append(item.for_json())
        else:
            out.append(item)
    return out","isinstance(item, (classbuilder.ProtocolBase, classbuilder.LiteralValue, ArrayWrapper))",19,"isinstance(item, classbuilder)",False,11.41328199148675,N/A
"def as_dict(self):
    """"""Return a dictionary containing the current values
        of the object.

        Returns:
            (dict): The object represented as a dictionary
        """"""
    out = {}
    for prop in self:
        propval = getattr(self, prop)
<mask>:
            out[prop] = propval.for_json()
        elif isinstance(propval, list):
            out[prop] = [getattr(x, 'for_json', lambda: x)() for x in propval]
        elif isinstance(propval, (ProtocolBase, LiteralValue)):
            out[prop] = propval.as_dict()
        elif propval is not None or self.__propinfo__[prop].get('type', None) == 'null':
            out[prop] = propval
    return out","hasattr(propval, 'for_json')",71,"isinstance(propval, (ProtocolBase, LiteralValue))",False,16.784459625186194,N/A
"def __eq__(self, other):
<mask>:
        return False
    return self.as_dict() == other.as_dict()","not isinstance(other, ProtocolBase)",10,"not isinstance(other, self.__class__)",False,30.26643726685862,N/A
"def __new__(cls, **props):
    """"""Overridden to support oneOf, where we need to
        instantiate a different class depending on what
        value we've seen""""""
<mask>:
        new = super(ProtocolBase, cls).__new__
        if new is object.__new__:
            return new(cls)
        return new(cls, **props)
    valid_types = cls.__validation__.get('type', None)
    if valid_types is None or not isinstance(valid_types, list):
        new = super(ProtocolBase, cls).__new__
        if new is object.__new__:
            return new(cls)
        return new(cls, **props)
    obj = None
    validation_errors = []
    for klass in valid_types:
        try:
            obj = klass(**props)
            obj.validate()
        except validators.ValidationError as e:
            validation_errors.append((klass, e))
        else:
            break
    else:
        raise validators.ValidationError('Unable to instantiate any valid types: \n'.join(('{0}: {1}\n'.format(k, e) for k, e in validation_errors)))
    return obj","getattr(cls, '__validation__', None) is None",101,"cls.__validation__.get('type', None) is None",False,47.92365811426397,N/A
"def __init__(self, **props):
    logger.debug(util.lazy_format(""Creating '{0}'"", self.__class__))
    self._extended_properties = dict()
    self._properties = dict(zip(self.__prop_names__.values(), [None for x in range(len(self.__prop_names__))]))
    for name in self.__has_default__:
<mask>:
            try:
                default_value = self.__propinfo__[name]['default']
            except KeyError:
                try:
                    default_value = self.__propinfo__[name]['const']
                except KeyError:
                    raise jsonschema.exceptions.SchemaError('Schema parsing error. Expected {0} to have default or const value'.format(name))
            logger.debug(util.lazy_format(""Initializing '{0}' to '{1}'"", name, default_value))
            setattr(self, name, copy.deepcopy(default_value))
    for prop in props:
        try:
            logger.debug(util.lazy_format(""Setting value for '{0}' to {1}"", prop, props[prop]))
            if props[prop] is not None:
                setattr(self, prop, props[prop])
        except validators.ValidationError as e:
            import sys
            e = type(e)(str(e) + "" \nwhile setting '{0}' in {1}"".format(prop, self.__class__.__name__))
            raise e.with_traceback(sys.exc_info()[2])
    if getattr(self, '__strict__', None):
        self.validate()",name not in props,100,name not in self.__prop_names__,False,12.605968092174914,N/A
"def __setattr__(self, name, val):
    name = str(name)
<mask>:
        object.__setattr__(self, name, val)
    elif name in self.__propinfo__:
        prop = getattr(self.__class__, self.__prop_names__[name])
        prop.__set__(self, val)
    else:
        try:
            val = self.__extensible__.instantiate(name, val)
        except Exception as e:
            raise validators.ValidationError(""Attempted to set unknown property '{0}': {1} "".format(name, e))
        self._extended_properties[name] = val",name in self.__object_attr_list__,44,name in self.__class__.__dict__,False,38.50322886878713,N/A
"@pytest.mark.parametrize('version, warn, error', [('http://json-schema.org/schema#', True, True), ('http://json-schema.org/draft-03/schema#', False, False), ('http://json-schema.org/draft-04/schema#', False, False), ('http://json-schema.org/draft-06/schema#', True, False), ('http://json-schema.org/draft-07/schema#', True, False)])
def test_warnings_on_schema_version(version, warn, error):
    schema = {'$schema': version, '$id': 'test', 'type': 'object', 'properties': {}}
    with warnings.catch_warnings(record=True) as w:
        try:
            pjs.ObjectBuilder(schema)
        except Exception:
            assert error == True
        else:
            warn_msgs = [str(m.message) for m in w]
            present = ['Schema version %s not recognized' % version in msg for msg in warn_msgs]
<mask>:
                assert any(present)
            else:
                assert not any(present)",warn,74,warn,True,100.00000000000004,N/A
"@pytest.mark.parametrize('permit_addl,property,value,is_error', [(False, 'foo', 'hello', False), (False, 'foobarro', 'hello', False), (False, 'foo', 24, True), (False, 'barkeep', 24, False), (False, 'barkeep', 'John', True), (False, 'extraprop', 'John', True), (True, 'extraprop', 'John', False), (True, 'foobar', True, False), (True, 'foobar', 'John', True), (True, 'foobar', 24, True)])
def test_pattern_properties_work(base_schema, permit_addl, property, value, is_error):
    base_schema['additionalProperties'] = permit_addl
    builder = pjo.ObjectBuilder(base_schema)
    ns = builder.build_classes()
    props = dict([(property, value)])
<mask>:
        with pytest.raises(pjo.ValidationError):
            t = ns.Example(**props)
            t.validate()
    else:
        t = ns.Example(**props)
        t.validate()",is_error,72,is_error,True,100.00000000000004,N/A
"def pad_size(n, k):
    """"""
    The smallest number that has to be added to n to equal a multiple of k.
    """"""
<mask>:
        return k - n % k
    else:
        return 0",n % k,31,n % k == 0,False,30.213753973567677,N/A
"def ab(x):
<mask>:
        return '%s:%s' % (len(x), b32encode(x[-3:]))
    elif len(x) == 2:
        return '%s:%s' % (len(x), b32encode(x[-2:]))
    elif len(x) == 1:
        return '%s:%s' % (len(x), b32encode(x[-1:]))
    elif len(x) == 0:
        return '%s:%s' % (len(x), '--empty--')",len(x) >= 3,35,len(x) == 3,False,48.892302243490086,N/A
"def _build_header(m, k, pad, sh):
    """"""
    @param m: the total number of shares; 1 <= m <= 256
    @param k: the number of shares required to reconstruct; 1 <= k <= m
    @param pad: the number of bytes of padding added to the file before encoding; 0 <= pad < k
    @param sh: the shnum of this share; 0 <= k < m

    @return: a compressed string encoding m, k, pad, and sh
    """"""
    assert m >= 1
    assert m <= 2 ** 8
    assert k >= 1
    assert k <= m
    assert pad >= 0
    assert pad < k
    assert sh >= 0
    assert sh < m
    bitsused = 0
    val = 0
    val |= m - 1
    bitsused += 8
    kbits = log_ceil(m, 2)
    val <<= kbits
    bitsused += kbits
    val |= k - 1
    padbits = log_ceil(k, 2)
    val <<= padbits
    bitsused += padbits
    val |= pad
    shnumbits = log_ceil(m, 2)
    val <<= shnumbits
    bitsused += shnumbits
    val |= sh
    assert bitsused >= 8, bitsused
    assert bitsused <= 32, bitsused
<mask>:
        val <<= 16 - bitsused
        cs = struct.pack('>H', val)
        assert cs[:-2] == b'\x00' * (len(cs) - 2)
        return cs[-2:]
    if bitsused <= 24:
        val <<= 24 - bitsused
        cs = struct.pack('>I', val)
        assert cs[:-3] == b'\x00' * (len(cs) - 3)
        return cs[-3:]
    else:
        val <<= 32 - bitsused
        cs = struct.pack('>I', val)
        assert cs[:-4] == b'\x00' * (len(cs) - 4)
        return cs[-4:]",bitsused <= 16,236,bitsused <= 16,True,100.00000000000004,N/A
"def _parse_header(inf):
    """"""
    @param inf: an object which I can call read(1) on to get another byte

    @return: tuple of (m, k, pad, sh,); side-effect: the first one to four
        bytes of inf will be read
    """"""
    ch = inf.read(1)
<mask>:
        raise CorruptedShareFilesError(""Share files were corrupted -- share file %r didn't have a complete metadata header at the front.  Perhaps the file was truncated."" % (inf.name,))
    byte = ord(ch)
    m = byte + 1
    kbits = log_ceil(m, 2)
    b2_bits_left = 8 - kbits
    kbitmask = MASK(kbits) << b2_bits_left
    ch = inf.read(1)
    if not ch:
        raise CorruptedShareFilesError(""Share files were corrupted -- share file %r didn't have a complete metadata header at the front.  Perhaps the file was truncated."" % (inf.name,))
    byte = ord(ch)
    k = ((byte & kbitmask) >> b2_bits_left) + 1
    shbits = log_ceil(m, 2)
    padbits = log_ceil(k, 2)
    val = byte & ~kbitmask
    needed_padbits = padbits - b2_bits_left
    if needed_padbits > 0:
        ch = inf.read(1)
        if not ch:
            raise CorruptedShareFilesError(""Share files were corrupted -- share file %r didn't have a complete metadata header at the front.  Perhaps the file was truncated."" % (inf.name,))
        byte = struct.unpack('>B', ch)[0]
        val <<= 8
        val |= byte
        needed_padbits -= 8
    assert needed_padbits <= 0
    extrabits = -needed_padbits
    pad = val >> extrabits
    val &= MASK(extrabits)
    needed_shbits = shbits - extrabits
    if needed_shbits > 0:
        ch = inf.read(1)
        if not ch:
            raise CorruptedShareFilesError(""Share files were corrupted -- share file %r didn't have a complete metadata header at the front.  Perhaps the file was truncated."" % (inf.name,))
        byte = struct.unpack('>B', ch)[0]
        val <<= 8
        val |= byte
        needed_shbits -= 8
    assert needed_shbits <= 0
    gotshbits = -needed_shbits
    sh = val >> gotshbits
    return (m, k, pad, sh)",not ch,282,not ch,True,100.00000000000004,N/A
"def encode_to_files(inf, fsize, dirname, prefix, k, m, suffix='.fec', overwrite=False, verbose=False):
    """"""
    Encode inf, writing the shares to specially named, newly created files.

    @param fsize: calling read() on inf must yield fsize bytes of data and
        then raise an EOFError
    @param dirname: the name of the directory into which the sharefiles will
        be written
    """"""
    mlen = len(str(m))
    format = FORMAT_FORMAT % (mlen, mlen)
    padbytes = pad_size(fsize, k)
    fns = []
    fs = []
    got_error = False
    try:
        for shnum in range(m):
            hdr = _build_header(m, k, padbytes, shnum)
            fn = os.path.join(dirname, format % (prefix, shnum, m, suffix))
<mask>:
                print('Creating share file %r...' % (fn,))
            if overwrite:
                f = open(fn, 'wb')
            else:
                flags = os.O_WRONLY | os.O_CREAT | os.O_EXCL | (hasattr(os, 'O_BINARY') and os.O_BINARY)
                fd = os.open(fn, flags)
                f = os.fdopen(fd, 'wb')
            fs.append(f)
            fns.append(fn)
            f.write(hdr)
        sumlen = [0]

        def cb(blocks, length):
            assert len(blocks) == len(fs)
            oldsumlen = sumlen[0]
            sumlen[0] += length
            if verbose:
                if int(float(oldsumlen) / fsize * 10) != int(float(sumlen[0]) / fsize * 10):
                    print(str(int(float(sumlen[0]) / fsize * 10) * 10) + '% ...', end=' ')
            if sumlen[0] > fsize:
                raise IOError('Wrong file size -- possibly the size of the file changed during encoding.  Original size: %d, observed size at least: %s' % (fsize, sumlen[0]))
            for i in range(len(blocks)):
                data = blocks[i]
                fs[i].write(data)
                length -= len(data)
        encode_file_stringy_easyfec(inf, cb, k, m, chunksize=4096)
    except EnvironmentError as le:
        print('Cannot complete because of exception: ')
        print(le)
        got_error = True
    finally:
        for f in fs:
            f.close()
        if got_error:
            print('Cleaning up...')
            for fn in fns:
                if verbose:
                    print('Cleaning up: trying to remove %r...' % (fn,))
                try:
                    os.remove(fn)
                except EnvironmentError:
                    pass
            return 1
    if verbose:
        print()
        print('Done!')
    return 0",verbose,271,verbose,True,100.00000000000004,N/A
"def decode(self, blocks, sharenums, padlen):
    """"""
        @param padlen: the number of bytes of padding to strip off;  Note that
            the padlen is always equal to (blocksize times k) minus the length
            of data.  (Therefore, padlen can be 0.)
        """"""
    data = b''.join(self.fec.decode(blocks, sharenums))
<mask>:
        return data[:-padlen]
    else:
        return data",padlen,49,padlen > 0,False,27.516060407455225,N/A
"def main():
<mask>:
        print('zfec library version: ', libversion)
        print('zunfec command-line tool version: ', __version__)
        return 0
    parser = argparse.ArgumentParser(description='Decode data from share files.')
    parser.add_argument('-o', '--outputfile', required=True, help='file to write the resulting data to, or ""-"" for stdout', type=str, metavar='OUTF')
    parser.add_argument('sharefiles', nargs='*', help='shares file to read the encoded data from', type=str, metavar='SHAREFILE')
    parser.add_argument('-v', '--verbose', help='print out messages about progress', action='store_true')
    parser.add_argument('-f', '--force', help='overwrite any file which already in place of the output file', action='store_true')
    parser.add_argument('-V', '--version', help='print out version number and exit', action='store_true')
    args = parser.parse_args()
    if len(args.sharefiles) < 2:
        print('At least two sharefiles are required.')
        return 1
    if args.force:
        outf = open(args.outputfile, 'wb')
    else:
        try:
            flags = os.O_WRONLY | os.O_CREAT | os.O_EXCL | (hasattr(os, 'O_BINARY') and os.O_BINARY)
            outfd = os.open(args.outputfile, flags)
        except OSError:
            print('There is already a file named %r -- aborting.  Use --force to overwrite.' % (args.outputfile,))
            return 2
        outf = os.fdopen(outfd, 'wb')
    sharefs = []
    args.sharefiles.sort()
    for fn in args.sharefiles:
        sharefs.append(open(fn, 'rb'))
    try:
        filefec.decode_from_files(outf, sharefs, args.verbose)
    except filefec.InsufficientShareFilesError as e:
        print(str(e))
        return 3
    finally:
        outf.close()
        for f in sharefs:
            f.close()
    return 0",'-V' in sys.argv or '--version' in sys.argv,174,len(sys.argv) == 1,False,14.211011212459495,N/A
"def test_random(self):
    for i in range(3):
        _help_test_random()
<mask>:
        print('%d randomized tests pass.' % (i + 1))",VERBOSE,16,i % 2 == 0,False,0.0,N/A
"def test_small(self):
    for i in range(16):
        _help_test_random_with_l_easy(i)
<mask>:
        print('%d randomized tests pass.' % (i + 1))",VERBOSE,16,i % 2 == 0,False,0.0,N/A
"def test_random(self):
    for i in range(3):
        _help_test_random_easy()
<mask>:
        print('%d randomized tests pass.' % (i + 1))",VERBOSE,16,i % 2 == 0,False,0.0,N/A
"def test_filefec_header(self):
    for m in [1, 2, 3, 5, 7, 9, 11, 17, 19, 33, 35, 65, 66, 67, 129, 130, 131, 254, 255, 256]:
        for k in [1, 2, 3, 5, 9, 17, 33, 65, 129, 255, 256]:
<mask>:
                continue
            for pad in [0, 1, k - 1]:
                if pad >= k:
                    continue
                for sh in [0, 1, m - 1]:
                    if sh >= m:
                        continue
                    h = zfec.filefec._build_header(m, k, pad, sh)
                    hio = BytesIO(h)
                    rm, rk, rpad, rsh = zfec.filefec._parse_header(hio)
                    assert (rm, rk, rpad, rsh) == (m, k, pad, sh), h",k >= m,93,m >= k,False,37.99178428257963,N/A
"def call(self, context):
    """"""Internal routing for this plugin type.

        Do not override this method.
        """"""
<mask>:
        if context['action'] == 'genConfig':
            return ExtensionResponse(status=ExtensionStatus(code=0, message='OK'), response=self.content())
    message = 'Not a valid config plugin action'
    return ExtensionResponse(status=ExtensionStatus(code=1, message=message), response=[])",'action' in context,36,'action' in context,True,100.00000000000004,N/A
"def __init__(self, path=DEFAULT_SOCKET_PATH, uuid=None):
    """"""
        Keyword arguments:
        path -- the path of the extension socket to connect to
        uuid -- the additional UUID to use when constructing the socket path
        """"""
    self.path = path
    sock = None
<mask>:
        sock = TPipe(pipe_name=self.path)
    else:
        self.path += '.{}'.format(uuid) if uuid else ''
        sock = TSocket.TSocket(unix_socket=self.path)
    self._transport = TTransport.TBufferedTransport(sock)
    self._protocol = TBinaryProtocol.TBinaryProtocol(self._transport)",sys.platform == WINDOWS_PLATFORM,58,uuid is None,False,0.0,N/A
"def add_plugin(self, plugin):
    """"""Register a plugin with the extension manager. In order for the
        extension manager to broadcast a plugin, it must be added using this
        interface.

        Keyword arguments:
        plugin -- the plugin class to register
        """"""
    obj = plugin()
<mask>:
        self._registry[obj.registry_name()] = {}
    if obj.name() not in self._registry[obj.registry_name()]:
        self._registry[obj.registry_name()][obj.name()] = obj.routes()
    if obj.registry_name() not in self._plugins:
        self._plugins[obj.registry_name()] = {}
    if obj.name() not in self._plugins[obj.registry_name()]:
        self._plugins[obj.registry_name()][obj.name()] = obj",obj.registry_name() not in self._registry,68,obj.registry_name() not in self._registry,True,100.00000000000004,N/A
"def call(self, registry, item, request):
    """"""The entry-point for plugin requests

        When a plugin is accessed from another process, osquery core's
        extension manager will send a thrift request to the implementing
        extension manager's call method.

        Arguments:
        registry -- a string representing what registry is being accessed.
            for config plugins this is ""config"", for table plugins this is
            ""table"", etc.
        item -- the registry item that is being requested. this is the ""name""
            of your plugin. for example, this would be the exact name of the
            SQL table, if the plugin was a table plugin.
        """"""
<mask>:
        message = 'A registry of an unknown type was called: %s' % registry
        return ExtensionResponse(status=ExtensionStatus(code=1, message=message), response=[])
    try:
        return self._plugins[registry][item].call(request)
    except KeyError:
        message = 'Extension registry does not contain requested plugin'
        return ExtensionResponse(status=ExtensionStatus(code=1, message=message), response=[])","registry not in ['table', 'config', 'logger']",130,registry not in self._plugins,False,15.925177647011354,N/A
"def call(self, context):
    """"""Internal routing for this plugin type.

        Do not override this method.
        """"""
<mask>:
        return ExtensionResponse(status=self.log_string(context['string']), response=[])
    elif 'snapshot' in context:
        return ExtensionResponse(status=self.log_snapshot(context['snapshot']), response=[])
    elif 'health' in context:
        return ExtensionResponse(status=self.log_health(context['health']), response=[])
    elif 'init' in context:
        return ExtensionResponse(status=ExtensionStatus(code=1, message=self._use_glog_message), response=[])
    elif 'status' in context:
        return ExtensionResponse(status=ExtensionStatus(code=1, message=self._use_glog_message), response=[])
    else:
        return ExtensionResponse(status=ExtensionStatus(code=1, message=self._invalid_action_message), response=[])",'string' in context,54,'string' in context,True,100.00000000000004,N/A
"def __init__(self, path=None):
    """"""
        Keyword arguments:
        path -- the path to and osqueryd binary to spawn
        """"""
<mask>:
        if sys.platform == 'darwin':
            self.path = DARWIN_BINARY_PATH
        elif sys.platform == WINDOWS_PLATFORM:
            self.path = WINDOWS_BINARY_PATH
        else:
            self.path = LINUX_BINARY_PATH
    else:
        self.path = path
    logging.getLogger('thrift').addHandler(logging.NullHandler())
    if sys.platform == WINDOWS_PLATFORM:
        self._pidfile = (None, tempfile.gettempdir() + '\\pyosqpid-' + str(random.randint(10000, 20000)))
        pipeName = '\\\\.\\pipe\\pyosqsock-' + str(random.randint(10000, 20000))
        self._socket = (None, pipeName)
    else:
        self._socket = tempfile.mkstemp(prefix='pyosqsock')",path is None,68,path is None,True,100.00000000000004,N/A
"def __del__(self):
<mask>:
        self.connection.close()
        self.connection = None
    if self.instance is not None:
        self.instance.kill()
        self.instance.wait()
        self.instance = None
    if self._socket is not None and self._socket[0] is not None:
        os.close(self._socket[0])
        if os.path.exists(self._socket[1]):
            try:
                os.unlink(self._socket[1])
            except OSError:
                logging.warning('Failed to remove socket descriptor: %s', self._socket[1])
        self._socket = None",self.connection is not None,44,self.connection is not None,True,100.00000000000004,N/A
"def open(self, timeout=2, interval=0.01):
    """"""
        Start the instance process and open an extension client

        Keyword arguments:
        timeout -- maximum number of seconds to wait for client
        interval -- seconds between client open attempts
        """"""
    proc = [self.path, '--extensions_socket', self._socket[1], '--disable_database', '--disable_watchdog', '--disable_logging', '--ephemeral', '--config_path', '/dev/null']
    self.instance = subprocess.Popen(proc, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    self.connection = ExtensionClient(path=self._socket[1])
<mask>:
        raise Exception('Cannot start process from path: %s' % self.path)
    delay = 0
    while delay < timeout:
        try:
            self.connection.open()
            return
        except Exception:
            time.sleep(interval)
            delay += interval
    self.instance.kill()
    self.instance = None
    raise Exception('Cannot open connection: %s' % self._socket[1])",not self.is_running(),91,self.instance.poll() is not None,False,13.134549472120794,N/A
"def is_running(self):
    """"""Check if the instance has spawned.""""""
<mask>:
        return False
    return self.instance.poll() is None",self.instance is None,15,not self.instance,False,46.30777161991026,N/A
"def start_watcher(client, interval):
    """"""Ping the osquery extension manager to detect dirty shutdowns.""""""
    try:
        while True:
            status = client.extension_manager_client().ping()
<mask>:
                logging.error('Ping received nonzero status: %d', status)
                break
            time.sleep(interval)
    except socket.error as e:
        logging.error('Ping received socket.error: %s', e)
    except TTransport.TTransportException as e:
        logging.error('Ping received thrift.transport.TTransport.TTransportException: %s', e)
    except Exception as e:
        logging.error('Ping received unknown exception: %s', e)
    finally:
        os._exit(1)",status.code != 0,57,status != 0,False,38.75385825373298,N/A
"def __new__(cls, *args, **kwargs):
    """"""Override __new__ to implement custom instantiation""""""
<mask>:
        cls._instance = super(Singleton, cls).__new__(cls, *args, **kwargs)
    return cls._instance",not cls._instance,19,not cls._instance,True,100.00000000000004,N/A
"def close(self):
    """"""
        Generic close method, as both server and client rely on closing pipes
        in the same way
        """"""
<mask>:
        win32pipe.DisconnectNamedPipe(self._handle)
        self._handle = None",self._handle is not None,25,self._handle is not None,True,100.00000000000004,N/A
"def open(self):
<mask>:
        raise TTransportException(TTransportException.ALREADY_OPEN)
    h = None
    conns = 0
    while conns < self._max_conn_attempts:
        try:
            h = win32file.CreateFile(self._pipe_name, win32file.GENERIC_READ | win32file.GENERIC_WRITE, 0, None, win32file.OPEN_EXISTING, win32file.FILE_FLAG_OVERLAPPED, None)
        except pywintypes.error as e:
            if e.winerror != winerror.ERROR_PIPE_BUSY:
                raise TTransportException(TTransportException.NOT_OPEN, 'Failed to open connection to pipe: {}'.format(e))
        if h is not None and h.handle != winerror.ERROR_INVALID_HANDLE:
            self._handle = h
            return
        try:
            win32pipe.WaitNamedPipe(self._pipe_name, self._timeout)
        except Exception as e:
            if e.args[0] not in (winerror.ERROR_SEM_TIMEOUT, winerror.ERROR_PIPE_BUSY):
                raise TTransportException(type=TTransportException.UNKNOWN, message='Client failed to connect to server with {}'.format(e.args[0]))
        conns += 1
    raise TTransportException(type=TTransportException.UNKNOWN, message='Client exceeded max connection attempts')",self.is_open(),90,self._handle is not None and self._handle.is_open(),False,28.782231838054418,N/A
"def read(self, sz):
<mask>:
        raise TTransportException(type=TTransportException.NOT_OPEN, message='Called read on non-open pipe')
    buff = None
    err = None
    try:
        err, buff = win32file.ReadFile(self._handle, sz, None)
    except Exception as e:
        raise TTransportException(type=TTransportException.UNKNOWN, message='TPipe read failed')
    if err:
        raise TTransportException(type=TTransportException.UNKNOWN, message='TPipe read failed with GLE={}'.format(err))
    if len(buff) == 0:
        raise TTransportException(type=TTransportException.END_OF_FILE, message='TPipe read 0 bytes')
    return buff",not self.is_open(),54,self._handle is None,False,13.83254362586636,N/A
"def write(self, buff):
<mask>:
        raise TTransportException(type=TTransportException.NOT_OPEN, message='Called read on non-open pipe')
    bytesWritten = None
    try:
        _, bytesWritten = win32file.WriteFile(self._handle, buff, None)
    except Exception as e:
        raise TTransportException(type=TTransportException.UNKNOWN, message='Failed to write to named pipe: ' + str(e))
    if bytesWritten != len(buff):
        raise TTransportException(type=TTransportException.UNKNOWN, message='Failed to write complete buffer to named pipe')",not self.is_open(),50,self._handle is None,False,13.83254362586636,N/A
"def call(self, context):
    """"""Internal routing for this plugin type.

        Do not override this method.
        """"""
<mask>:
        return ExtensionResponse(status=ExtensionStatus(code=1, message=self._no_action_message), response=[])
    if context['action'] == 'generate':
        ctx = {}
        if 'context' in context:
            ctx = json.loads(context['context'])
        rows = self.generate(ctx)
        for i, row in enumerate(rows):
            for key, value in row.items():
                if not isinstance(value, str):
                    try:
                        rows[i][key] = str(value)
                    except ValueError as e:
                        rows[i][key] = ''
                        logging.error('Cannot convert key %s: %s' % (i, key, str(e)))
        return ExtensionResponse(status=ExtensionStatus(code=0, message='OK'), response=rows)
    elif context['action'] == 'columns':
        return ExtensionResponse(status=ExtensionStatus(code=0, message='OK'), response=self.routes())
    return ExtensionResponse(code=1, message='Unknown action')",'action' not in context,87,not context['action'],False,14.058533129758727,N/A
"def recv_extensions(self):
    iprot = self._iprot
    fname, mtype, rseqid = iprot.readMessageBegin()
<mask>:
        x = TApplicationException()
        x.read(iprot)
        iprot.readMessageEnd()
        raise x
    result = extensions_result()
    result.read(iprot)
    iprot.readMessageEnd()
    if result.success is not None:
        return result.success
    raise TApplicationException(TApplicationException.MISSING_RESULT, 'extensions failed: unknown result')",mtype == TMessageType.EXCEPTION,36,mtype != TMessageType.EXCEPTION,False,53.7284965911771,N/A
"def recv_options(self):
    iprot = self._iprot
    fname, mtype, rseqid = iprot.readMessageBegin()
<mask>:
        x = TApplicationException()
        x.read(iprot)
        iprot.readMessageEnd()
        raise x
    result = options_result()
    result.read(iprot)
    iprot.readMessageEnd()
    if result.success is not None:
        return result.success
    raise TApplicationException(TApplicationException.MISSING_RESULT, 'options failed: unknown result')",mtype == TMessageType.EXCEPTION,36,mtype != TMessageType.EXCEPTION,False,53.7284965911771,N/A
"def recv_registerExtension(self):
    iprot = self._iprot
    fname, mtype, rseqid = iprot.readMessageBegin()
<mask>:
        x = TApplicationException()
        x.read(iprot)
        iprot.readMessageEnd()
        raise x
    result = registerExtension_result()
    result.read(iprot)
    iprot.readMessageEnd()
    if result.success is not None:
        return result.success
    raise TApplicationException(TApplicationException.MISSING_RESULT, 'registerExtension failed: unknown result')",mtype == TMessageType.EXCEPTION,36,mtype != TMessageType.EXCEPTION,False,53.7284965911771,N/A
"def recv_deregisterExtension(self):
    iprot = self._iprot
    fname, mtype, rseqid = iprot.readMessageBegin()
<mask>:
        x = TApplicationException()
        x.read(iprot)
        iprot.readMessageEnd()
        raise x
    result = deregisterExtension_result()
    result.read(iprot)
    iprot.readMessageEnd()
    if result.success is not None:
        return result.success
    raise TApplicationException(TApplicationException.MISSING_RESULT, 'deregisterExtension failed: unknown result')",mtype == TMessageType.EXCEPTION,36,mtype != TMessageType.EXCEPTION,False,53.7284965911771,N/A
"def recv_query(self):
    iprot = self._iprot
    fname, mtype, rseqid = iprot.readMessageBegin()
<mask>:
        x = TApplicationException()
        x.read(iprot)
        iprot.readMessageEnd()
        raise x
    result = query_result()
    result.read(iprot)
    iprot.readMessageEnd()
    if result.success is not None:
        return result.success
    raise TApplicationException(TApplicationException.MISSING_RESULT, 'query failed: unknown result')",mtype == TMessageType.EXCEPTION,36,mtype != TMessageType.EXCEPTION,False,53.7284965911771,N/A
"def read(self, iprot):
<mask>:
        iprot._fast_decode(self, iprot, (self.__class__, self.thrift_spec))
        return
    iprot.readStructBegin()
    while True:
        fname, ftype, fid = iprot.readFieldBegin()
        if ftype == TType.STOP:
            break
        if fid == 1:
            if ftype == TType.STRING:
                self.value = iprot.readString().decode('utf-8') if sys.version_info[0] == 2 else iprot.readString()
            else:
                iprot.skip(ftype)
        elif fid == 2:
            if ftype == TType.STRING:
                self.default_value = iprot.readString().decode('utf-8') if sys.version_info[0] == 2 else iprot.readString()
            else:
                iprot.skip(ftype)
        elif fid == 3:
            if ftype == TType.STRING:
                self.type = iprot.readString().decode('utf-8') if sys.version_info[0] == 2 else iprot.readString()
            else:
                iprot.skip(ftype)
        else:
            iprot.skip(ftype)
        iprot.readFieldEnd()
    iprot.readStructEnd()","iprot._fast_decode is not None and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None)",83,self.thrift_spec is not None,False,5.641613950377738,N/A
"def write(self, oprot):
<mask>:
        oprot.trans.write(oprot._fast_encode(self, (self.__class__, self.thrift_spec)))
        return
    oprot.writeStructBegin('InternalOptionInfo')
    if self.value is not None:
        oprot.writeFieldBegin('value', TType.STRING, 1)
        oprot.writeString(self.value.encode('utf-8') if sys.version_info[0] == 2 else self.value)
        oprot.writeFieldEnd()
    if self.default_value is not None:
        oprot.writeFieldBegin('default_value', TType.STRING, 2)
        oprot.writeString(self.default_value.encode('utf-8') if sys.version_info[0] == 2 else self.default_value)
        oprot.writeFieldEnd()
    if self.type is not None:
        oprot.writeFieldBegin('type', TType.STRING, 3)
        oprot.writeString(self.type.encode('utf-8') if sys.version_info[0] == 2 else self.type)
        oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()",oprot._fast_encode is not None and self.thrift_spec is not None,59,self.thrift_spec is not None,False,28.650479686019022,N/A
"def read(self, iprot):
<mask>:
        iprot._fast_decode(self, iprot, (self.__class__, self.thrift_spec))
        return
    iprot.readStructBegin()
    while True:
        fname, ftype, fid = iprot.readFieldBegin()
        if ftype == TType.STOP:
            break
        if fid == 1:
            if ftype == TType.STRING:
                self.name = iprot.readString().decode('utf-8') if sys.version_info[0] == 2 else iprot.readString()
            else:
                iprot.skip(ftype)
        elif fid == 2:
            if ftype == TType.STRING:
                self.version = iprot.readString().decode('utf-8') if sys.version_info[0] == 2 else iprot.readString()
            else:
                iprot.skip(ftype)
        elif fid == 3:
            if ftype == TType.STRING:
                self.sdk_version = iprot.readString().decode('utf-8') if sys.version_info[0] == 2 else iprot.readString()
            else:
                iprot.skip(ftype)
        elif fid == 4:
            if ftype == TType.STRING:
                self.min_sdk_version = iprot.readString().decode('utf-8') if sys.version_info[0] == 2 else iprot.readString()
            else:
                iprot.skip(ftype)
        else:
            iprot.skip(ftype)
        iprot.readFieldEnd()
    iprot.readStructEnd()","iprot._fast_decode is not None and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None)",102,self.thrift_spec is not None,False,5.641613950377738,N/A
"def write(self, oprot):
<mask>:
        oprot.trans.write(oprot._fast_encode(self, (self.__class__, self.thrift_spec)))
        return
    oprot.writeStructBegin('InternalExtensionInfo')
    if self.name is not None:
        oprot.writeFieldBegin('name', TType.STRING, 1)
        oprot.writeString(self.name.encode('utf-8') if sys.version_info[0] == 2 else self.name)
        oprot.writeFieldEnd()
    if self.version is not None:
        oprot.writeFieldBegin('version', TType.STRING, 2)
        oprot.writeString(self.version.encode('utf-8') if sys.version_info[0] == 2 else self.version)
        oprot.writeFieldEnd()
    if self.sdk_version is not None:
        oprot.writeFieldBegin('sdk_version', TType.STRING, 3)
        oprot.writeString(self.sdk_version.encode('utf-8') if sys.version_info[0] == 2 else self.sdk_version)
        oprot.writeFieldEnd()
    if self.min_sdk_version is not None:
        oprot.writeFieldBegin('min_sdk_version', TType.STRING, 4)
        oprot.writeString(self.min_sdk_version.encode('utf-8') if sys.version_info[0] == 2 else self.min_sdk_version)
        oprot.writeFieldEnd()
    oprot.writeFieldStop()
    oprot.writeStructEnd()",oprot._fast_encode is not None and self.thrift_spec is not None,75,oprot.trans is not None,False,5.141628886547205,N/A
"def read(self, iprot):
<mask>:
        iprot._fast_decode(self, iprot, (self.__class__, self.thrift_spec))
        return
    iprot.readStructBegin()
    while True:
        fname, ftype, fid = iprot.readFieldBegin()
        if ftype == TType.STOP:
            break
        if fid == 1:
            if ftype == TType.I32:
                self.code = iprot.readI32()
            else:
                iprot.skip(ftype)
        elif fid == 2:
            if ftype == TType.STRING:
                self.message = iprot.readString().decode('utf-8') if sys.version_info[0] == 2 else iprot.readString()
            else:
                iprot.skip(ftype)
        elif fid == 3:
            if ftype == TType.I64:
                self.uuid = iprot.readI64()
            else:
                iprot.skip(ftype)
        else:
            iprot.skip(ftype)
        iprot.readFieldEnd()
    iprot.readStructEnd()","iprot._fast_decode is not None and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None)",71,self.thrift_spec is not None,False,5.641613950377738,N/A
"def __init__(self, iprot, oprot=None):
    self._iprot = self._oprot = iprot
<mask>:
        self._oprot = oprot
    self._seqid = 0",oprot is not None,16,oprot is not None,True,100.00000000000004,N/A
"def recv_ping(self):
    iprot = self._iprot
    fname, mtype, rseqid = iprot.readMessageBegin()
<mask>:
        x = TApplicationException()
        x.read(iprot)
        iprot.readMessageEnd()
        raise x
    result = ping_result()
    result.read(iprot)
    iprot.readMessageEnd()
    if result.success is not None:
        return result.success
    raise TApplicationException(TApplicationException.MISSING_RESULT, 'ping failed: unknown result')",mtype == TMessageType.EXCEPTION,36,mtype != TMessageType.EXCEPTION,False,53.7284965911771,N/A
"def recv_call(self):
    iprot = self._iprot
    fname, mtype, rseqid = iprot.readMessageBegin()
<mask>:
        x = TApplicationException()
        x.read(iprot)
        iprot.readMessageEnd()
        raise x
    result = call_result()
    result.read(iprot)
    iprot.readMessageEnd()
    if result.success is not None:
        return result.success
    raise TApplicationException(TApplicationException.MISSING_RESULT, 'call failed: unknown result')",mtype == TMessageType.EXCEPTION,36,mtype != TMessageType.EXCEPTION,False,53.7284965911771,N/A
"def recv_shutdown(self):
    iprot = self._iprot
    fname, mtype, rseqid = iprot.readMessageBegin()
<mask>:
        x = TApplicationException()
        x.read(iprot)
        iprot.readMessageEnd()
        raise x
    result = shutdown_result()
    result.read(iprot)
    iprot.readMessageEnd()
    return",mtype == TMessageType.EXCEPTION,24,mtype != TMessageType.EXCEPTION,False,53.7284965911771,N/A
"def process(self, iprot, oprot):
    name, type, seqid = iprot.readMessageBegin()
<mask>:
        iprot.skip(TType.STRUCT)
        iprot.readMessageEnd()
        x = TApplicationException(TApplicationException.UNKNOWN_METHOD, 'Unknown function %s' % name)
        oprot.writeMessageBegin(name, TMessageType.EXCEPTION, seqid)
        x.write(oprot)
        oprot.writeMessageEnd()
        oprot.trans.flush()
        return
    else:
        self._processMap[name](self, seqid, iprot, oprot)
    return True",name not in self._processMap,34,type != TType.FUNCTION,False,6.870636427700047,N/A
"def search(self, board: chess.Board, time_limit: Limit, ponder: bool, draw_offered: bool, root_moves: MOVE) -> PlayResult:
    """"""
        Choose a move using multiple different methods.

        :param board: The current position.
        :param time_limit: Conditions for how long the engine can search (e.g. we have 10 seconds and search up to depth 10).
        :param ponder: Whether the engine can ponder after playing a move.
        :param draw_offered: Whether the bot was offered a draw.
        :param root_moves: If it is a list, the engine should only play a move that is in `root_moves`.
        :return: The move to play.
        """"""
<mask>:
        my_time = time_limit.time
        my_inc = 0
    elif board.turn == chess.WHITE:
        my_time = time_limit.white_clock if isinstance(time_limit.white_clock, int) else 0
        my_inc = time_limit.white_inc if isinstance(time_limit.white_inc, int) else 0
    else:
        my_time = time_limit.black_clock if isinstance(time_limit.black_clock, int) else 0
        my_inc = time_limit.black_inc if isinstance(time_limit.black_inc, int) else 0
    possible_moves = root_moves if isinstance(root_moves, list) else list(board.legal_moves)
    if my_time / 60 + my_inc > 10:
        move = random.choice(possible_moves)
    else:
        possible_moves.sort(key=str)
        move = possible_moves[0]
    return PlayResult(move, None, draw_offered=draw_offered)","isinstance(time_limit.time, int)",164,board.turn == chess.TIME,False,4.300847718252331,N/A
"def test_lichess() -> None:
    """"""Test the lichess communication.""""""
    token = os.environ.get('LICHESS_BOT_TEST_TOKEN')
<mask>:
        pytest.skip('Lichess-bot test token must be set.')
    li = lichess.Lichess(token, 'https://lichess.org/', '0.0.0', logging.DEBUG, 3)
    assert len(li.get_online_bots()) > 20
    profile = li.get_profile()
    profile['seenAt'] = 1700000000000
    assert profile == {'blocking': False, 'count': {'ai': 3, 'all': 12, 'bookmark': 0, 'draw': 1, 'drawH': 1, 'import': 0, 'loss': 8, 'lossH': 5, 'me': 0, 'playing': 0, 'rated': 0, 'win': 3, 'winH': 3}, 'createdAt': 1627834995597, 'followable': True, 'following': False, 'id': 'badsunfish', 'perfs': {'blitz': {'games': 0, 'prog': 0, 'prov': True, 'rating': 1500, 'rd': 500}, 'bullet': {'games': 0, 'prog': 0, 'prov': True, 'rating': 1500, 'rd': 500}, 'classical': {'games': 0, 'prog': 0, 'prov': True, 'rating': 1500, 'rd': 500}, 'correspondence': {'games': 0, 'prog': 0, 'prov': True, 'rating': 1500, 'rd': 500}, 'rapid': {'games': 0, 'prog': 0, 'prov': True, 'rating': 1500, 'rd': 500}}, 'playTime': {'total': 1873, 'tv': 0}, 'seenAt': 1700000000000, 'title': 'BOT', 'url': 'https://lichess.org/@/BadSunfish', 'username': 'BadSunfish'}
    assert li.get_ongoing_games() == []
    assert li.is_online('NNWithSF') is False
    public_data = li.get_public_data('lichapibot')
    for key in public_data['perfs']:
        public_data['perfs'][key]['rd'] = 0
    assert public_data == {'blocking': False, 'count': {'ai': 1, 'all': 15774, 'bookmark': 0, 'draw': 3009, 'drawH': 3009, 'import': 0, 'loss': 6423, 'lossH': 6423, 'me': 0, 'playing': 0, 'rated': 15121, 'win': 6342, 'winH': 6341}, 'createdAt': 1524037267522, 'followable': True, 'following': False, 'id': 'lichapibot', 'perfs': {'blitz': {'games': 2430, 'prog': 3, 'prov': True, 'rating': 2388, 'rd': 0}, 'bullet': {'games': 7293, 'prog': 9, 'prov': True, 'rating': 2298, 'rd': 0}, 'classical': {'games': 0, 'prog': 0, 'prov': True, 'rating': 1500, 'rd': 0}, 'correspondence': {'games': 0, 'prog': 0, 'prov': True, 'rating': 1500, 'rd': 0}, 'rapid': {'games': 993, 'prog': -80, 'prov': True, 'rating': 2363, 'rd': 0}}, 'playTime': {'total': 4111502, 'tv': 1582068}, 'profile': {}, 'seenAt': 1669272254317, 'title': 'BOT', 'tosViolation': True, 'url': 'https://lichess.org/@/lichapibot', 'username': 'lichapibot'}",not token,275,not token,True,100.00000000000004,N/A
"def pytest_sessionfinish(session: Session, exitstatus: Union[int, ExitCode]) -> None:
    """"""
    Remove files created when testing lichess-bot.

    The only exception is if running in a GitHub action, in which case we save the engines to the cache.
    """"""
<mask>:
        shutil.rmtree('TEMP')",os.path.exists('TEMP') and (not os.getenv('GITHUB_ACTIONS')),38,os.path.exists('TEMP'),False,22.31301601484299,N/A
"def iter_lines(self, chunk_size: Optional[int]=512, decode_unicode: bool=False, delimiter: Union[str, bytes, None]=None) -> Generator[bytes, None, None]:
    """"""Send the game events to lichess-bot.""""""
    yield json.dumps({'id': 'zzzzzzzz', 'variant': {'key': 'standard', 'name': 'Standard', 'short': 'Std'}, 'clock': {'initial': 60000, 'increment': 2000}, 'speed': 'bullet', 'perf': {'name': 'Bullet'}, 'rated': True, 'createdAt': 1600000000000, 'white': {'id': 'bo', 'name': 'bo', 'title': 'BOT', 'rating': 3000}, 'black': {'id': 'b', 'name': 'b', 'title': 'BOT', 'rating': 3000, 'provisional': True}, 'initialFen': 'startpos', 'type': 'gameFull', 'state': {'type': 'gameState', 'moves': '', 'wtime': 10000, 'btime': 10000, 'winc': 100, 'binc': 100, 'status': 'started'}}).encode('utf-8')
    while True:
        board = self.board_queue.get()
        self.board_queue.task_done()
        wtime, btime, increment = self.clock_queue.get()
        self.clock_queue.task_done()
        new_game_state = {'type': 'gameState', 'moves': ' '.join((move.uci() for move in board.move_stack)), 'wtime': int(to_msec(wtime)), 'btime': int(to_msec(btime)), 'winc': int(to_msec(increment)), 'binc': int(to_msec(increment))}
<mask>:
            new_game_state['status'] = 'outoftime'
            new_game_state['winner'] = 'black'
            yield json.dumps(new_game_state).encode('utf-8')
            break
        if board.move_stack:
            new_game_state['status'] = 'started'
            yield json.dumps(new_game_state).encode('utf-8')",board.is_game_over(),131,board.move_stack,False,10.62372743739878,N/A
"def iter_lines(self, chunk_size: Optional[int]=512, decode_unicode: bool=False, delimiter: Union[str, bytes, None]=None) -> Generator[bytes, None, None]:
    """"""Send the events to lichess-bot.""""""
<mask>:
        yield b''
        time.sleep(1)
    else:
        yield json.dumps({'type': 'gameStart', 'game': {'id': 'zzzzzzzz', 'source': 'friend', 'compat': {'bot': True, 'board': True}}}).encode('utf-8')",self.sent_game,37,decode_unicode,False,14.127216461522432,N/A
"def get_game_stream(self, game_id: str) -> GameStream:
    """"""Send the `GameStream`.""""""
<mask>:
        self.move_queue.put(None)
    self.started_game_stream = True
    return GameStream(self.board_queue, self.clock_queue)",self.started_game_stream,17,self.started_game_stream,True,100.00000000000004,N/A
"def download_opening_book() -> None:
    """"""Download gm2001.bin.""""""
<mask>:
        return
    response = requests.get('https://github.com/gmcheems-org/free-opening-books/raw/main/books/bin/gm2001.bin', allow_redirects=True)
    with open('./TEMP/gm2001.bin', 'wb') as file:
        file.write(response.content)",os.path.exists('./TEMP/gm2001.bin'),18,not os.path.exists('./TEMP/gm2001.bin'),False,93.06048591020995,N/A
"def test_external_moves() -> None:
    """"""Test that the code for external moves works properly.""""""
    li = MockLichess()
    game = get_game()
    download_opening_book()
    online_cfg, online_cfg_2, draw_or_resign_cfg, polyglot_cfg = get_configs()
    starting_fen = 'rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1'
    opening_fen = 'rn1q1rk1/pbp1bpp1/1p2pn1p/3p4/2PP3B/2N1PN2/PP2BPPP/R2QK2R w KQ - 2 9'
    middlegame_fen = '8/5p2/1n1p1nk1/1p1Pp1p1/1Pp1P1Pp/r1P2B1P/2RNKP2/8 w - - 0 31'
    endgame_wdl2_fen = '2k5/4n2Q/5N2/8/8/8/1r6/2K5 b - - 0 123'
    endgame_wdl1_fen = '6N1/3n4/3k1b2/8/8/7Q/1r6/5K2 b - - 6 9'
    endgame_wdl0_fen = '6N1/3n4/3k1b2/8/8/7Q/5K2/1r6 b - - 8 10'
    is_lichess_org_up = li.is_website_up('https://lichess.org/api/cloud-eval')
    is_lichess_ovh_up = li.is_website_up('https://tablebase.lichess.ovh/standard')
    is_chessdb_cn_up = li.is_website_up('https://www.chessdb.cn/cdb.php')
<mask>:
        assert get_online_move_wrapper(li, chess.Board(starting_fen), game, online_cfg, draw_or_resign_cfg).move is not None
        assert get_online_move_wrapper(li, chess.Board(opening_fen), game, online_cfg, draw_or_resign_cfg).move is not None
        assert get_online_move_wrapper(li, chess.Board(middlegame_fen), game, online_cfg, draw_or_resign_cfg).move is None
    if is_chessdb_cn_up:
        assert get_online_move_wrapper(li, chess.Board(starting_fen), game, online_cfg_2, draw_or_resign_cfg).move is not None
        assert get_online_move_wrapper(li, chess.Board(opening_fen), game, online_cfg_2, draw_or_resign_cfg).move is not None
        assert get_online_move_wrapper(li, chess.Board(middlegame_fen), game, online_cfg_2, draw_or_resign_cfg).move is None
    if is_lichess_ovh_up:
        assert get_online_move_wrapper(li, chess.Board(endgame_wdl2_fen), game, online_cfg, draw_or_resign_cfg).resigned
        assert get_online_move_wrapper(li, chess.Board(endgame_wdl0_fen), game, online_cfg, draw_or_resign_cfg).draw_offered
        wdl1_move = get_online_move_wrapper(li, chess.Board(endgame_wdl1_fen), game, online_cfg, draw_or_resign_cfg)
        assert not wdl1_move.resigned and (not wdl1_move.draw_offered)
        assert get_online_move_wrapper(li, chess.Board(endgame_wdl2_fen).mirror(), game, online_cfg, draw_or_resign_cfg).resigned
        assert get_online_move_wrapper(li, chess.Board(endgame_wdl0_fen).mirror(), game, online_cfg, draw_or_resign_cfg).draw_offered
        wdl1_move = get_online_move_wrapper(li, chess.Board(endgame_wdl1_fen).mirror(), game, online_cfg, draw_or_resign_cfg)
        assert not wdl1_move.resigned and (not wdl1_move.draw_offered)
    if is_chessdb_cn_up:
        assert get_online_move_wrapper(li, chess.Board(endgame_wdl2_fen), game, online_cfg_2, draw_or_resign_cfg).resigned
        assert get_online_move_wrapper(li, chess.Board(endgame_wdl0_fen), game, online_cfg_2, draw_or_resign_cfg).draw_offered
        wdl1_move = get_online_move_wrapper(li, chess.Board(endgame_wdl1_fen), game, online_cfg_2, draw_or_resign_cfg)
        assert not wdl1_move.resigned and (not wdl1_move.draw_offered)
        assert get_online_move_wrapper(li, chess.Board(endgame_wdl2_fen).mirror(), game, online_cfg_2, draw_or_resign_cfg).resigned
        assert get_online_move_wrapper(li, chess.Board(endgame_wdl0_fen).mirror(), game, online_cfg_2, draw_or_resign_cfg).draw_offered
        wdl1_move = get_online_move_wrapper(li, chess.Board(endgame_wdl1_fen).mirror(), game, online_cfg_2, draw_or_resign_cfg)
        assert not wdl1_move.resigned and (not wdl1_move.draw_offered)
    assert get_book_move(chess.Board(opening_fen), game, polyglot_cfg).move == chess.Move.from_uci('h4f6')",is_lichess_org_up,248,game.name == 'lichess',False,0.0,N/A
"def download_sf() -> None:
    """"""Download Stockfish 16.""""""
    stockfish_path = f'./TEMP/sf{file_extension}'
<mask>:
        return
    windows_linux_mac = 'windows' if platform == 'win32' else 'macos' if platform == 'darwin' else 'ubuntu'
    sf_base = f'stockfish-{windows_linux_mac}-x86-64-modern'
    archive_link = f'https://github.com/official-stockfish/Stockfish/releases/download/sf_16/{sf_base}.{archive_ext}'
    response = requests.get(archive_link, allow_redirects=True)
    response.raise_for_status()
    archive_name = f'./TEMP/sf_zip.{archive_ext}'
    with open(archive_name, 'wb') as file:
        file.write(response.content)
    if archive_ext == 'zip':
        with zipfile.ZipFile(archive_name, 'r') as archive_ref:
            archive_ref.extractall('./TEMP/')
    else:
        with tarfile.TarFile(archive_name, 'r') as archive_ref:
            archive_ref.extractall('./TEMP/', filter='data')
    exe_ext = '.exe' if platform == 'win32' else ''
    shutil.copyfile(f'./TEMP/stockfish/{sf_base}{exe_ext}', stockfish_path)
    if platform != 'win32':
        st = os.stat(stockfish_path)
        os.chmod(stockfish_path, st.st_mode | stat.S_IEXEC)",os.path.exists(stockfish_path),87,not os.path.exists(stockfish_path),False,89.31539818068698,N/A
"def download_lc0() -> None:
    """"""Download Leela Chess Zero 0.29.0.""""""
<mask>:
        return
    response = requests.get('https://github.com/LeelaChessZero/lc0/releases/download/v0.29.0/lc0-v0.29.0-windows-cpu-dnnl.zip', allow_redirects=True)
    response.raise_for_status()
    with open('./TEMP/lc0_zip.zip', 'wb') as file:
        file.write(response.content)
    with zipfile.ZipFile('./TEMP/lc0_zip.zip', 'r') as zip_ref:
        zip_ref.extractall('./TEMP/')",os.path.exists('./TEMP/lc0.exe'),28,os.path.exists('./TEMP/lc0_zip.zip'),False,70.18491170272205,N/A
"def download_arasan() -> None:
    """"""Download Arasan.""""""
<mask>:
        return
    if platform == 'win32':
        response = requests.get('https://arasanchess.org/arasan24.1.zip', allow_redirects=True)
    else:
        response = requests.get('https://arasanchess.org/arasan-linux-binaries-24.2.2.tar.gz', allow_redirects=True)
    response.raise_for_status()
    with open(f'./TEMP/arasan.{archive_ext}', 'wb') as file:
        file.write(response.content)
    if archive_ext == 'zip':
        with zipfile.ZipFile(f'./TEMP/arasan.{archive_ext}', 'r') as archive_ref:
            archive_ref.extractall('./TEMP/')
    else:
        with tarfile.TarFile(f'./TEMP/arasan.{archive_ext}', 'r') as archive_ref:
            archive_ref.extractall('./TEMP/', filter='data')
    shutil.copyfile(f'./TEMP/arasanx-64{file_extension}', f'./TEMP/arasan{file_extension}')
    if platform != 'win32':
        st = os.stat(f'./TEMP/arasan{file_extension}')
        os.chmod(f'./TEMP/arasan{file_extension}', st.st_mode | stat.S_IEXEC)",os.path.exists(f'./TEMP/arasan{file_extension}'),59,not os.path.exists(f'./TEMP/arasan),False,59.85881772594013,N/A
"def lichess_org_simulator(opponent_path: Optional[str], move_queue: Queue[Optional[chess.Move]], board_queue: Queue[chess.Board], clock_queue: Queue[tuple[datetime.timedelta, datetime.timedelta, datetime.timedelta]], results: Queue[bool]) -> None:
    """"""
    Run a mocked version of the lichess.org server to provide an opponent for a test. This opponent always plays white.

    :param opponent_path: The path to the executable of the opponent. Usually Stockfish.
    :param move_queue: An interprocess queue that supplies the moves chosen by the bot being tested.
    :param board_queue: An interprocess queue where this function sends the updated board after choosing a move.
    :param clock_queue: An interprocess queue where this function sends the updated game clock after choosing a move.
    :param results: An interprocess queue where this function sends the result of the game to the testing function.
    """"""
    start_time = seconds(10)
    increment = seconds(0.1)
    board = chess.Board()
    wtime = start_time
    btime = start_time
    engine = chess.engine.SimpleEngine.popen_uci(opponent_path) if opponent_path else TrivialEngine()
    while not board.is_game_over():
<mask>:
            if not board.move_stack:
                move = engine.play(board, chess.engine.Limit(time=1))
            else:
                move_timer = Timer()
                move = engine.play(board, chess.engine.Limit(white_clock=to_seconds(wtime - seconds(2.0)), white_inc=to_seconds(increment), black_clock=to_seconds(btime), black_inc=to_seconds(increment)))
                wtime -= move_timer.time_since_reset()
                wtime += increment
            engine_move = move.move
            if engine_move is None:
                raise RuntimeError('Engine attempted to make null move.')
            board.push(engine_move)
            board_queue.put(board)
            clock_queue.put((wtime, btime, increment))
        else:
            move_timer = Timer()
            while (bot_move := move_queue.get()) is None:
                board_queue.put(board)
                clock_queue.put((wtime, btime, increment))
                move_queue.task_done()
            board.push(bot_move)
            move_queue.task_done()
            if len(board.move_stack) > 2:
                btime -= move_timer.time_since_reset()
                btime += increment
    board_queue.put(board)
    clock_queue.put((wtime, btime, increment))
    engine.quit()
    outcome = board.outcome()
    results.put(outcome is not None and outcome.winner == chess.BLACK)",board.turn == chess.WHITE,229,not engine.is_game_over(),False,4.196114906296549,N/A
"def run_bot(raw_config: CONFIG_DICT_TYPE, logging_level: int, opponent_path: Optional[str]=None) -> bool:
    """"""
    Start lichess-bot test with a mocked version of the lichess.org site.

    :param raw_config: A dictionary of values to specify the engine to test. This engine will play as white.
    :param logging_level: The level of logging to use during the test. Usually logging.DEBUG.
    :param opponent_path: The path to the executable that will play the opponent. The opponent plays as black.
    """"""
    config.insert_default_values(raw_config)
    CONFIG = config.Configuration(raw_config)
    logger.info(lichess_bot.intro())
    manager = Manager()
    board_queue: Queue[chess.Board] = manager.Queue()
    clock_queue: Queue[tuple[datetime.timedelta, datetime.timedelta, datetime.timedelta]] = manager.Queue()
    move_queue: Queue[Optional[chess.Move]] = manager.Queue()
    li = test_bot.lichess.Lichess(move_queue, board_queue, clock_queue)
    user_profile = li.get_profile()
    username = user_profile['username']
<mask>:
        return False
    logger.info(f'Welcome {username}!')
    lichess_bot.disable_restart()
    results: Queue[bool] = manager.Queue()
    thr = threading.Thread(target=lichess_org_simulator, args=[opponent_path, move_queue, board_queue, clock_queue, results])
    thr.start()
    lichess_bot.start(li, user_profile, CONFIG, logging_level, testing_log_file_name, True, one_game=True)
    result = results.get()
    results.task_done()
    results.join()
    board_queue.join()
    clock_queue.join()
    move_queue.join()
    thr.join()
    return result",user_profile.get('title') != 'BOT',140,not check_username(username),False,4.880869806051147,N/A
"def config_assert(assertion: bool, error_message: str) -> None:
    """"""Raise an exception if an assertion is false.""""""
<mask>:
        raise Exception(error_message)",not assertion,18,not assertion,True,100.00000000000004,N/A
"def config_warn(assertion: bool, warning_message: str) -> None:
    """"""Print a warning message if an assertion is false.""""""
<mask>:
        logger.warning(warning_message)",not assertion,18,assertion,False,36.78794411714425,N/A
"def set_config_default(config: CONFIG_DICT_TYPE, *sections: str, key: str, default: Any, force_empty_values: bool=False) -> CONFIG_DICT_TYPE:
    """"""
    Fill a specific config key with the default value if it is missing.

    :param config: The bot's config.
    :param sections: The sections that the key is in.
    :param key: The key to set.
    :param default: The default value.
    :param force_empty_values: Whether an empty value should be replaced with the default value.
    :return: The new config with the default value inserted if needed.
    """"""
    subconfig = config
    for section in sections:
        subconfig = subconfig.setdefault(section, {})
<mask>:
            raise Exception(f'The {section} section in {sections} should hold a set of key-value pairs, not a value.')
    if force_empty_values:
        if subconfig.get(key) in [None, '']:
            subconfig[key] = default
    else:
        subconfig.setdefault(key, default)
    return subconfig","not isinstance(subconfig, dict)",120,"not isinstance(subconfig, dict)",True,100.00000000000004,N/A
"def change_value_to_list(config: CONFIG_DICT_TYPE, *sections: str, key: str) -> None:
    """"""
    Change a single value to a list. e.g. 60 becomes [60]. Used to maintain backwards compatibility.

    :param config: The bot's config.
    :param sections: The sections that the key is in.
    :param key: The key to set.
    """"""
    subconfig = set_config_default(config, *sections, key=key, default=[])
<mask>:
        subconfig[key] = []
    if not isinstance(subconfig[key], list):
        subconfig[key] = [subconfig[key]]",subconfig[key] is None,64,key not in subconfig,False,11.521590992286539,N/A
"def should_create_challenge(self) -> bool:
    """"""Whether we should create a challenge.""""""
    matchmaking_enabled = self.matchmaking_cfg.allow_matchmaking
    time_has_passed = self.last_game_ended_delay.is_expired()
    challenge_expired = self.last_challenge_created_delay.is_expired() and self.challenge_id
    min_wait_time_passed = self.last_challenge_created_delay.time_since_reset() > self.min_wait_time
<mask>:
        self.li.cancel(self.challenge_id)
        logger.info(f'Challenge id {self.challenge_id} cancelled.')
        self.discard_challenge(self.challenge_id)
        self.show_earliest_challenge_time()
    return bool(matchmaking_enabled and (time_has_passed or challenge_expired) and min_wait_time_passed)",challenge_expired,42,self.li,False,0.0,N/A
"def create_challenge(self, username: str, base_time: int, increment: int, days: int, variant: str, mode: str) -> str:
    """"""Create a challenge.""""""
    params: dict[str, Union[str, int, bool]] = {'rated': mode == 'rated', 'variant': variant}
<mask>:
        params['days'] = days
    elif base_time or increment:
        params['clock.limit'] = base_time
        params['clock.increment'] = increment
    else:
        logger.error('At least one of challenge_days, challenge_initial_time, or challenge_increment must be greater than zero in the matchmaking section of your config file.')
        return ''
    try:
        self.update_daily_challenge_record()
        self.last_challenge_created_delay.reset()
        response = self.li.challenge(username, params)
        challenge_id = response.get('id', '')
        if not challenge_id:
            logger.error(response)
            self.add_to_block_list(username)
            self.show_earliest_challenge_time()
        return challenge_id
    except Exception as e:
        logger.warning('Could not create challenge')
        logger.debug(e, exc_info=e)
        self.show_earliest_challenge_time()
        return ''",days,101,days,True,100.00000000000004,N/A
"def update_user_profile(self) -> None:
    """"""Update our user profile data, to get our latest rating.""""""
<mask>:
        self.last_user_profile_update_time.reset()
        with contextlib.suppress(Exception):
            self.user_profile = self.li.get_profile()",self.last_user_profile_update_time.is_expired(),21,self.last_user_profile_update_time.is_set(),False,84.82198619370465,N/A
"def get_weights(self, online_bots: list[UserProfileType], rating_preference: str, min_rating: int, max_rating: int, game_type: str) -> list[int]:
    """"""Get the weight for each bot. A higher weights means the bot is more likely to get challenged.""""""

    def rating(bot: UserProfileType) -> int:
        perfs: dict[str, PerfType] = bot.get('perfs', {})
        perf: PerfType = perfs.get(game_type, {})
        return perf.get('rating', 0)
<mask>:
        reduce_ratings_by = min(min_rating - (max_rating - min_rating), min_rating - 1)
        weights = [rating(bot) - reduce_ratings_by for bot in online_bots]
    elif rating_preference == 'low':
        reduce_ratings_by = max(max_rating - (min_rating - max_rating), max_rating + 1)
        weights = [reduce_ratings_by - rating(bot) for bot in online_bots]
    else:
        weights = [1] * len(online_bots)
    return weights",rating_preference == 'high',102,rating_preference == 'high',True,100.00000000000004,N/A
"def choose_opponent(self) -> tuple[Optional[str], int, int, int, str, str]:
    """"""Choose an opponent.""""""
    override_choice = random.choice(self.matchmaking_cfg.overrides.keys() + [None])
    logger.info(f""Using the {override_choice or 'default'} matchmaking configuration."")
    override = {} if override_choice is None else self.matchmaking_cfg.overrides.lookup(override_choice)
    match_config = self.matchmaking_cfg | override
    variant = self.get_random_config_value(match_config, 'challenge_variant', self.variants)
    mode = self.get_random_config_value(match_config, 'challenge_mode', ['casual', 'rated'])
    rating_preference = match_config.rating_preference
    base_time = random.choice(match_config.challenge_initial_time)
    increment = random.choice(match_config.challenge_increment)
    days = random.choice(match_config.challenge_days)
    play_correspondence = [bool(days), not bool(base_time or increment)]
<mask>:
        base_time = 0
        increment = 0
    else:
        days = 0
    game_type = game_category(variant, base_time, increment, days)
    min_rating = match_config.opponent_min_rating
    max_rating = match_config.opponent_max_rating
    rating_diff = match_config.opponent_rating_difference
    bot_rating = self.perf().get(game_type, {}).get('rating', 0)
    if rating_diff is not None and bot_rating > 0:
        min_rating = bot_rating - rating_diff
        max_rating = bot_rating + rating_diff
    logger.info(f'Seeking {game_type} game with opponent rating in [{min_rating}, {max_rating}] ...')
    allow_tos_violation = match_config.opponent_allow_tos_violation

    def is_suitable_opponent(bot: UserProfileType) -> bool:
        perf = bot.get('perfs', {}).get(game_type, {})
        return bot['username'] != self.username() and (not self.in_block_list(bot['username'])) and (not bot.get('disabled')) and (allow_tos_violation or not bot.get('tosViolation')) and (perf.get('games', 0) > 0) and (min_rating <= perf.get('rating', 0) <= max_rating)
    online_bots = self.li.get_online_bots()
    online_bots = list(filter(is_suitable_opponent, online_bots))

    def ready_for_challenge(bot: UserProfileType) -> bool:
        aspects = [variant, game_type, mode] if self.challenge_filter == FilterType.FINE else []
        return all((self.should_accept_challenge(bot['username'], aspect) for aspect in aspects))
    ready_bots = list(filter(ready_for_challenge, online_bots))
    online_bots = ready_bots or online_bots
    bot_username = None
    weights = self.get_weights(online_bots, rating_preference, min_rating, max_rating, game_type)
    try:
        bot = random.choices(online_bots, weights=weights)[0]
        bot_profile = self.li.get_public_data(bot['username'])
        if bot_profile.get('blocking'):
            self.add_to_block_list(bot['username'])
        else:
            bot_username = bot['username']
    except Exception:
        if online_bots:
            logger.exception('Error:')
        else:
            logger.error('No suitable bots found to challenge.')
    return (bot_username, base_time, increment, days, variant, mode)",random.choice(play_correspondence),251,not play_correspondence,False,21.874242445215206,N/A
"def __init__(self, token: str, url: str, version: str, logging_level: int, max_retries: int) -> None:
    """"""
        Communication with lichess.org (and chessdb.cn for getting moves).

        :param token: The bot's token.
        :param url: The base url (lichess.org).
        :param version: The lichess-bot version running.
        :param logging_level: The logging level (logging.INFO or logging.DEBUG).
        :param max_retries: The maximum amount of retries for online moves (e.g. chessdb's opening book).
        """"""
    self.version = version
    self.header = {'Authorization': f'Bearer {token}'}
    self.baseUrl = url
    self.session = requests.Session()
    self.session.headers.update(self.header)
    self.other_session = requests.Session()
    self.set_user_agent('?')
    self.logging_level = logging_level
    self.max_retries = max_retries
    self.rate_limit_timers: defaultdict[str, Timer] = defaultdict(Timer)
    token_response = cast(TOKEN_TESTS_TYPE, self.api_post('token_test', data=token))
    token_info = token_response[token]
<mask>:
        raise RuntimeError('Token in config file is not recognized by lichess. Please check that it was copied correctly into your configuration file.')
    scopes = token_info['scopes']
    if 'bot:play' not in scopes.split(','):
        raise RuntimeError(f'Please use an API access token for your bot that has the scope ""Play games with the bot API (bot:play)"". The current token has: {scopes}.')",not token_info,157,token_info['token'] is None,False,20.556680845025987,N/A
"@backoff.on_exception(backoff.constant, (RemoteDisconnected, RequestsConnectionError, HTTPError, ReadTimeout), max_time=60, interval=0.1, giveup=is_final, on_backoff=backoff_handler, backoff_log_level=logging.DEBUG, giveup_log_level=logging.DEBUG)
def api_get(self, endpoint_name: str, *template_args: str, params: Optional[dict[str, str]]=None, stream: bool=False, timeout: int=2) -> requests.Response:
    """"""
        Send a GET to lichess.org.

        :param endpoint_name: The name of the endpoint.
        :param template_args: The values that go in the url (e.g. the challenge id if `endpoint_name` is `accept`).
        :param params: Parameters sent to lichess.org.
        :param stream: Whether the data returned from lichess.org should be streamed.
        :param timeout: The amount of time in seconds to wait for a response.
        :return: lichess.org's response.
        """"""
    logging.getLogger('backoff').setLevel(self.logging_level)
    path_template = self.get_path_template(endpoint_name)
    url = urljoin(self.baseUrl, path_template.format(*template_args))
    response = self.session.get(url, params=params, timeout=timeout, stream=stream)
<mask>:
        delay = seconds(1 if endpoint_name == 'move' else 60)
        self.set_rate_limit_delay(path_template, delay)
    response.raise_for_status()
    response.encoding = 'utf-8'
    return response",is_new_rate_limit(response),122,self.auto_reconnect,False,3.929752628321626,N/A
"@backoff.on_exception(backoff.constant, (RemoteDisconnected, RequestsConnectionError, HTTPError, ReadTimeout), max_time=60, interval=0.1, giveup=is_final, on_backoff=backoff_handler, backoff_log_level=logging.DEBUG, giveup_log_level=logging.DEBUG)
def api_post(self, endpoint_name: str, *template_args: str, data: Union[str, dict[str, str], None]=None, headers: Optional[dict[str, str]]=None, params: Optional[dict[str, str]]=None, payload: Optional[REQUESTS_PAYLOAD_TYPE]=None, raise_for_status: bool=True) -> Union[ChallengeType, Optional[TOKEN_TESTS_TYPE]]:
    """"""
        Send a POST to lichess.org.

        :param endpoint_name: The name of the endpoint.
        :param template_args: The values that go in the url (e.g. the challenge id if `endpoint_name` is `accept`).
        :param data: Data sent to lichess.org.
        :param headers: The headers for the request.
        :param params: Parameters sent to lichess.org.
        :param payload: Payload sent to lichess.org.
        :param raise_for_status: Whether to raise an exception if the response contains an error code.
        :return: lichess.org's response in a dict.
        """"""
    logging.getLogger('backoff').setLevel(self.logging_level)
    path_template = self.get_path_template(endpoint_name)
    url = urljoin(self.baseUrl, path_template.format(*template_args))
    response = self.session.post(url, data=data, headers=headers, params=params, json=payload, timeout=2)
<mask>:
        self.set_rate_limit_delay(path_template, seconds(60))
    if raise_for_status:
        response.raise_for_status()
    json_response: Union[ChallengeType, Optional[TOKEN_TESTS_TYPE]] = response.json()
    return json_response",is_new_rate_limit(response),140,self.rate_limit,False,14.628187563941408,N/A
"def get_path_template(self, endpoint_name: str) -> str:
    """"""
        Get the path template given the endpoint name. Will raise an exception if the path template is rate limited.

        :param endpoint_name: The name of the endpoint.
        :return: The path template.
        """"""
    path_template = ENDPOINTS[endpoint_name]
<mask>:
        raise RateLimitedError(f'{path_template} is rate-limited. Will retry in {sec_str(self.rate_limit_time_left(path_template))} seconds.')
    return path_template",self.is_rate_limited(path_template),53,self.rate_limit_time_left(path_template) < self.rate_limit_time_left(),False,17.797644045771207,N/A
"def accept_takeback(self, game_id: str, accept: bool) -> bool:
    """"""Answer an opponent's move takeback request.""""""
    try:
        self.api_post('takeback', game_id, 'yes' if accept else 'no')
<mask>:
            logger.info('Opponent took back previous move.')
        else:
            logger.info(""Refused opponent's take back request."")
        return accept
    except Exception:
        return False",accept,40,self.api_response['move_time'] > self.last_move_time,False,0.0,N/A
"def react(self, line: ChatLine) -> None:
    """"""
        React to a received message.

        :param line: Information about the message.
        """"""
    self.messages.append(line)
    logger.info(f'*** {self.game.url()} [{line.room}] {line.username}: {line.text}')
<mask>:
        self.command(line, line.text[1:].lower())",line.text[0] == self.command_prefix,28,line.text.startswith('#'),False,12.43423351463457,N/A
"def command(self, line: ChatLine, cmd: str) -> None:
    """"""
        Reacts to the specific commands in the chat.

        :param line: Information about the message.
        :param cmd: The command to react to.
        """"""
    from_self = line.username == self.game.username
    is_eval = cmd.startswith('eval')
<mask>:
        self.send_reply(line, 'Supported commands: !wait (wait a minute for my first move), !name, !eval (or any text starting with !eval), !queue')
    elif cmd == 'wait' and self.game.is_abortable():
        self.game.ping(seconds(60), seconds(120), seconds(120))
        self.send_reply(line, 'Waiting 60 seconds...')
    elif cmd == 'name':
        name = self.game.me.name
        self.send_reply(line, f'{name} running {self.engine.name()} (lichess-bot v{self.version})')
    elif is_eval and (from_self or line.room == 'spectator'):
        stats = self.engine.get_stats(for_chat=True)
        self.send_reply(line, ', '.join(stats))
    elif is_eval:
        self.send_reply(line, ""I don't tell that to my opponent, sorry."")
    elif cmd == 'queue':
        if self.challengers:
            challengers = ', '.join([f'@{challenger.challenger.name}' for challenger in reversed(self.challengers)])
            self.send_reply(line, f'Challenge queue: {challengers}')
        else:
            self.send_reply(line, 'No challenges queued.')","cmd in ('commands', 'help')",134,cmd == 'eval',False,7.545383788761362,N/A
"def send_message(self, room: str, message: str) -> None:
    """"""Send the message to the chat.""""""
<mask>:
        self.send_reply(ChatLine({'room': room, 'username': '', 'text': ''}), message)",message,22,self.is_reply_sent(),False,0.0,N/A
"def __init__(self, duration: timedelta=zero_seconds, backdated_timestamp: Optional[datetime]=None) -> None:
    """"""
        Start the timer.

        :param duration: The duration of time before Timer.is_expired() returns True.
        :param backdated_timestamp: When the timer should have started. Used to keep the timers between sessions.
        """"""
    self.duration = duration
    self.starting_time = perf_counter()
<mask>:
        self.starting_time -= to_seconds(datetime.now() - backdated_timestamp)",backdated_timestamp,50,backdated_timestamp is not None,False,30.213753973567677,N/A
"def is_supported_time_control(self, challenge_cfg: Configuration) -> bool:
    """"""Check whether the time control is supported.""""""
    speeds = challenge_cfg.time_controls
    increment_max: int = challenge_cfg.max_increment
    increment_min: int = challenge_cfg.min_increment
    base_max: int = challenge_cfg.max_base
    base_min: int = challenge_cfg.min_base
    days_max: float = challenge_cfg.max_days
    days_min: float = challenge_cfg.min_days
<mask>:
        return False
    require_non_zero_increment = self.challenger.is_bot and self.speed == 'bullet' and challenge_cfg.bullet_requires_increment
    increment_min = max(increment_min, 1 if require_non_zero_increment else 0)
    if self.base is not None and self.increment is not None:
        return increment_min <= self.increment <= increment_max and base_min <= self.base <= base_max
    elif self.days is not None:
        return days_min <= self.days <= days_max
    else:
        return days_max == math.inf",self.speed not in speeds,98,self.speed not in speeds,True,100.00000000000004,N/A
"def is_supported(self, config: Configuration, recent_bot_challenges: defaultdict[str, list[Timer]], opponent_engagements: Counter[str]) -> tuple[bool, str]:
    """"""Whether the challenge is supported.""""""
    try:
<mask>:
            return (True, '')
        from extra_game_handlers import is_supported_extra
        allowed_opponents: list[str] = list(filter(None, config.allow_list)) or [self.challenger.name]
        decline_reason = self.decline_due_to(config.accept_bot or not self.challenger.is_bot, 'noBot') or self.decline_due_to(not config.only_bot or self.challenger.is_bot, 'onlyBot') or self.decline_due_to(self.is_supported_time_control(config), 'timeControl') or self.decline_due_to(self.is_supported_variant(config), 'variant') or self.decline_due_to(self.is_supported_mode(config), 'casual' if self.rated else 'rated') or self.decline_due_to(self.challenger.name not in config.block_list, 'generic') or self.decline_due_to(self.challenger.name in allowed_opponents, 'generic') or self.decline_due_to(self.is_supported_recent(config, recent_bot_challenges), 'later') or self.decline_due_to(opponent_engagements[self.challenger.name] < config.max_simultaneous_games_per_user, 'later') or self.decline_due_to(is_supported_extra(self), 'generic')
        return (not decline_reason, decline_reason)
    except Exception:
        logger.exception(f'Error while checking challenge {self.id}:')
        return (False, 'generic')",self.from_self,96,self.id in self.challenge_list,False,11.339582221952005,N/A
"def pgn_event(self) -> str:
    """"""Get the event to write in the PGN file.""""""
<mask>:
        return f'{self.mode.title()} {self.perf_name.title()} game'
    else:
        return f'{self.mode.title()} {self.variant_name} game'","self.variant_name in ['Standard', 'From Position']",23,self.perf_name,False,7.4506199991604385,N/A
"def ping(self, abort_in: datetime.timedelta, terminate_in: datetime.timedelta, disconnect_in: datetime.timedelta) -> None:
    """"""
        Tell the bot when to abort, terminate, and disconnect from a game.

        :param abort_in: How many seconds to wait before aborting.
        :param terminate_in: How many seconds to wait before terminating.
        :param disconnect_in: How many seconds to wait before disconnecting.
        """"""
<mask>:
        self.abort_time = Timer(abort_in)
    self.terminate_time = Timer(terminate_in)
    self.disconnect_time = Timer(disconnect_in)",self.is_abortable(),61,self.game_state == 'stopped',False,12.22307556087252,N/A
"def result(self) -> str:
    """"""Get the result of the game.""""""

    class GameEnding(str, Enum):
        WHITE_WINS = '1-0'
        BLACK_WINS = '0-1'
        DRAW = '1/2-1/2'
        INCOMPLETE = '*'
    winner = self.state.get('winner')
    termination = self.state.get('status')
<mask>:
        result = GameEnding.WHITE_WINS
    elif winner == 'black':
        result = GameEnding.BLACK_WINS
    elif termination in [Termination.DRAW, Termination.TIMEOUT]:
        result = GameEnding.DRAW
    else:
        result = GameEnding.INCOMPLETE
    return result.value",winner == 'white',56,winner == 'white',True,100.00000000000004,N/A
"def create_engine(engine_config: Configuration, game: Optional[model.Game]=None) -> EngineWrapper:
    """"""
    Create the engine.

    Use in a with-block to automatically close the engine when exiting the game.

    :param engine_config: The options for the engine.
    :return: An engine. Either UCI, XBoard, or Homemade.
    """"""
    cfg = engine_config.engine
    engine_path = os.path.abspath(os.path.join(cfg.dir, cfg.name))
    engine_type = cfg.protocol
    commands = []
<mask>:
        commands.append(cfg.interpreter)
        commands.extend(cfg.interpreter_options)
    commands.append(engine_path)
    if cfg.engine_options:
        for k, v in cfg.engine_options.items():
            commands.append(f'--{k}={v}' if v is not None else f'--{k}')
    stderr = None if cfg.silence_stderr else subprocess.DEVNULL
    Engine: type[Union[UCIEngine, XBoardEngine, MinimalEngine]]
    if engine_type == 'xboard':
        Engine = XBoardEngine
    elif engine_type == 'uci':
        Engine = UCIEngine
    elif engine_type == 'homemade':
        Engine = get_homemade_engine(cfg.name)
    else:
        raise ValueError(f'    Invalid engine type: {engine_type}. Expected xboard, uci, or homemade.')
    options = remove_managed_options(cfg.lookup(f'{engine_type}_options') or Configuration({}))
    logger.debug(f'Starting engine: {commands}')
    return Engine(commands, options, stderr, cfg.draw_or_resign, game, cwd=cfg.working_dir)",cfg.interpreter,131,cfg.interpreter,True,100.00000000000004,N/A
"def __exit__(self, exc_type: Optional[type[BaseException]], exc_value: Optional[BaseException], traceback: Optional[TracebackType]) -> None:
    """"""Exit context and allow engine to shutdown nicely if there was no exception.""""""
<mask>:
        self.ping()
        self.quit()
    self.engine.__exit__(exc_type, exc_value, traceback)",exc_type is None,29,self.is_alive(),False,7.809849842300637,N/A
"def play_move(self, board: chess.Board, game: model.Game, li: lichess.Lichess, setup_timer: Timer, move_overhead: datetime.timedelta, can_ponder: bool, is_correspondence: bool, correspondence_move_time: datetime.timedelta, engine_cfg: Configuration, min_time: datetime.timedelta) -> None:
    """"""
        Play a move.

        :param board: The current position.
        :param game: The game that the bot is playing.
        :param li: Provides communication with lichess.org.
        :param start_time: The time that the bot received the move.
        :param move_overhead: The time it takes to communicate between the engine and lichess.org.
        :param can_ponder: Whether the engine is allowed to ponder.
        :param is_correspondence: Whether this is a correspondence or unlimited game.
        :param correspondence_move_time: The time the engine will think if `is_correspondence` is true.
        :param engine_cfg: Options for external moves (e.g. from an opening book), and for engine resignation and draw offers.
        :param min_time: Minimum time to spend, in seconds.
        :return: The move to play.
        """"""
    polyglot_cfg = engine_cfg.polyglot
    online_moves_cfg = engine_cfg.online_moves
    draw_or_resign_cfg = engine_cfg.draw_or_resign
    lichess_bot_tbs = engine_cfg.lichess_bot_tbs
    best_move: MOVE
    best_move = get_book_move(board, game, polyglot_cfg)
<mask>:
        best_move = get_egtb_move(board, game, lichess_bot_tbs, draw_or_resign_cfg)
    if not isinstance(best_move, list) and best_move.move is None:
        best_move = get_online_move(li, board, game, online_moves_cfg, draw_or_resign_cfg)
    if isinstance(best_move, list) or best_move.move is None:
        draw_offered = check_for_draw_offer(game)
        time_limit, can_ponder = move_time(board, game, can_ponder, setup_timer, move_overhead, is_correspondence, correspondence_move_time)
        try:
            best_move = self.search(board, time_limit, can_ponder, draw_offered, best_move)
        except chess.engine.EngineError as error:
            BadMove = (chess.IllegalMoveError, chess.InvalidMoveError)
            if not any((isinstance(e, BadMove) for e in error.args)):
                raise
            logger.error('Ending game due to bot attempting an illegal move.')
            logger.error(error)
            game_ender = li.abort if game.is_abortable() else li.resign
            game_ender(game.id)
            return
    elapsed = setup_timer.time_since_reset()
    if elapsed < min_time:
        time.sleep(to_seconds(min_time - elapsed))
    self.add_comment(best_move, board)
    self.print_stats()
    if best_move.resigned and len(board.move_stack) >= 2:
        li.resign(game.id)
    else:
        li.make_move(game.id, best_move)",best_move.move is None,262,"not isinstance(best_move, list) and best_move.move is None",False,34.82352832757854,N/A
"def add_go_commands(self, time_limit: chess.engine.Limit) -> chess.engine.Limit:
    """"""Add extra commands to send to the engine. For example, to search for 1000 nodes or up to depth 10.""""""
    movetime_cfg = self.go_commands.movetime
<mask>:
        movetime = msec(movetime_cfg)
        if time_limit.time is None or seconds(time_limit.time) > movetime:
            time_limit.time = to_seconds(movetime)
    time_limit.depth = self.go_commands.depth
    time_limit.nodes = self.go_commands.nodes
    return time_limit",movetime_cfg is not None,52,movetime_cfg,False,36.78794411714425,N/A
"def offer_draw_or_resign(self, result: chess.engine.PlayResult, board: chess.Board) -> chess.engine.PlayResult:
    """"""Offer draw or resign depending on the score of the engine.""""""

    def actual(score: chess.engine.PovScore) -> int:
        return score.relative.score(mate_score=40000)
    can_offer_draw = self.draw_or_resign.offer_draw_enabled
    draw_offer_moves = self.draw_or_resign.offer_draw_moves
    draw_score_range: int = self.draw_or_resign.offer_draw_score
    draw_max_piece_count = self.draw_or_resign.offer_draw_pieces
    pieces_on_board = chess.popcount(board.occupied)
    enough_pieces_captured = pieces_on_board <= draw_max_piece_count
<mask>:
        scores = self.scores[-draw_offer_moves:]

        def score_near_draw(score: chess.engine.PovScore) -> bool:
            return abs(actual(score)) <= draw_score_range
        if len(scores) == len(list(filter(score_near_draw, scores))):
            result.draw_offered = True
    resign_enabled = self.draw_or_resign.resign_enabled
    min_moves_for_resign = self.draw_or_resign.resign_moves
    resign_score: int = self.draw_or_resign.resign_score
    if resign_enabled and len(self.scores) >= min_moves_for_resign:
        scores = self.scores[-min_moves_for_resign:]

        def score_near_loss(score: chess.engine.PovScore) -> bool:
            return actual(score) <= resign_score
        if len(scores) == len(list(filter(score_near_loss, scores))):
            result.resigned = True
    return result",can_offer_draw and len(self.scores) >= draw_offer_moves and enough_pieces_captured,106,can_offer_draw and len(self.scores) >= draw_offer_moves,False,72.92129525252354,N/A
"def signal_handler(signal: int, frame: Optional[FrameType]) -> None:
    """"""Terminate lichess-bot.""""""
    in_starting_thread = __name__ == '__main__'
<mask>:
        if in_starting_thread:
            logger.debug('Received SIGINT. Terminating client.')
        stop.terminated = True
    else:
        if in_starting_thread:
            logger.debug('Received second SIGINT. Quitting now.')
        stop.force_quit = True",not stop.terminated,35,signal == signal.SIGTERM,False,8.116697886877475,N/A
"def watch_control_stream(control_queue: CONTROL_QUEUE_TYPE, li: lichess.Lichess) -> None:
    """"""Put the events in a queue.""""""
    error = None
    while not stop.terminated:
        try:
            response = li.get_event_stream()
            lines = response.iter_lines()
            for line in lines:
<mask>:
                    event = json.loads(line.decode('utf-8'))
                    control_queue.put_nowait(event)
                else:
                    control_queue.put_nowait({'type': 'ping'})
        except Exception:
            error = traceback.format_exc()
            break
    control_queue.put_nowait({'type': 'terminated', 'error': error})",line,48,line.startswith('Event'),False,8.116697886877475,N/A
"def write_pgn_records(pgn_queue: PGN_QUEUE_TYPE, config: Configuration, username: str) -> None:
    """"""Write PGN records to files as games finish.""""""
    while True:
        mark_task_done = False
        try:
            event = pgn_queue.get()
            mark_task_done = True
<mask>:
                save_pgn_record(event, config, username)
        except InterruptedError:
            pass
        except Exception:
            logger.exception('Could not write PGN to file')
        if mark_task_done:
            pgn_queue.task_done()",event,47,event,True,100.00000000000004,N/A
"def logging_configurer(level: int, filename: Optional[str], disable_auto_logs: bool) -> None:
    """"""
    Configure the logger.

    :param level: The logging level. Either `logging.INFO` or `logging.DEBUG`.
    :param filename: The filename to write the logs to. If it is `None` then the logs aren't written to a file.
    :param auto_log_filename: The filename for the automatic logger. If it is `None` then the logs aren't written to a file.
    """"""
    console_handler = RichHandler()
    console_formatter = logging.Formatter('%(message)s')
    console_handler.setFormatter(console_formatter)
    console_handler.setLevel(level)
    all_handlers: list[logging.Handler] = [console_handler]
<mask>:
        file_handler = logging.FileHandler(filename, delay=True, encoding='utf-8')
        FORMAT = '%(asctime)s %(name)s (%(filename)s:%(lineno)d) %(levelname)s %(message)s'
        file_formatter = logging.Formatter(FORMAT)
        file_handler.setFormatter(file_formatter)
        file_handler.setLevel(level)
        all_handlers.append(file_handler)
    if not disable_auto_logs:
        os.makedirs(auto_log_directory, exist_ok=True)
        auto_log_filename = os.path.join(auto_log_directory, 'lichess-bot.log')
        auto_file_handler = logging.handlers.TimedRotatingFileHandler(auto_log_filename, delay=True, encoding='utf-8', when='midnight', backupCount=7)
        auto_file_handler.setLevel(logging.DEBUG)
        FORMAT = '%(asctime)s %(name)s (%(filename)s:%(lineno)d) %(levelname)s %(message)s'
        file_formatter = logging.Formatter(FORMAT)
        auto_file_handler.setFormatter(file_formatter)
        all_handlers.append(auto_file_handler)
    logging.basicConfig(level=logging.DEBUG, handlers=all_handlers, force=True)",filename,127,filename is not None,False,15.97357760615681,N/A
"def logging_listener_proc(queue: LOGGING_QUEUE_TYPE, level: int, log_filename: Optional[str], disable_auto_logging: bool) -> None:
    """"""
    Handle events from the logging queue.

    This allows the logs from inside a thread to be printed.
    They are added to the queue, so they are printed outside the thread.
    """"""
    logging_configurer(level, log_filename, disable_auto_logging)
    logger = logging.getLogger()
    while True:
        task: Optional[logging.LogRecord] = None
        try:
            task = queue.get(block=False)
        except Empty:
            time.sleep(0.1)
        except InterruptedError:
            pass
        except Exception:
            pass
<mask>:
            continue
        logger.handle(task)
        queue.task_done()",task is None,72,task is None,True,100.00000000000004,N/A
"def build_efb_chat_as_group(self, group: EFBGroupChat, members: Optional[List[EFBGroupMember]]=None) -> GroupChat:
    """"""
        Build EFB GroupChat object from EFBGroupChat Dict

        :return: GroupChat from group_id
        :param group: EFBGroupChat object, see CustomTypes.py
        :param members: Optional, the member list for the specific group, None by default
                        Each object in members (if not None) must follow the syntax of GroupChat.add_members
        """"""
    efb_chat: GroupChat = GroupChat(channel=self.channel, **group)
<mask>:
        for member in members:
            efb_chat.add_member(**member)
    return efb_chat",members,66,members is not None,False,15.97357760615681,N/A
"def __init__(self, instance_id: InstanceID=None):
    super().__init__(instance_id)
    self.load_config()
    self.init_client_manager()
<mask>:
        self.supported_message_types = self.QQClient.supported_message_types()","hasattr(self.QQClient, 'supported_message_types')",11,self.QQClient.supported_message_types(),False,23.021036988792517,N/A
"def check_updates(self):
    try:
        data = requests.get('https://pypi.org/pypi/efb-qq-slave/json')
        data_json = json.loads(data.text)
        latest_version = data_json['info']['version']
        self.logger.debug('The latest version is {version}'.format(version=latest_version))
<mask>:
            return latest_version
        else:
            return None
    except Exception:
        self.logger.warning('Failed to check updates')
        return None",parse_version(self.__version__) < parse_version(latest_version),31,latest_version != self.latest_version,False,7.641884064868073,N/A
"def load_config(self):
    """"""
        Load configuration from path specified by the framework.

        Configuration file is in YAML format.
        """"""
    config_path = efb_utils.get_config_path(self.channel_id)
<mask>:
        return
    with config_path.open() as f:
        self.config: Dict[str, Any] = yaml.safe_load(f)",not config_path.exists(),32,not config_path,False,36.78794411714425,N/A
"def get_extra_functions(self):
    methods = {}
    for mName in dir(self.QQClient):
<mask>:
            m = getattr(self.QQClient, mName)
            if callable(m) and getattr(m, 'extra_fn', False):
                methods[mName] = m
    return methods","hasattr(self.QQClient, mName)",25,mName.startswith('_'),False,7.809849842300637,N/A
"def __init__(self, name: str, config: Dict, channel):
    """"""
        This class initializes the QQ Client specified in config.yaml
        :param name:
        :param config:
        :param channel:
        """"""
    try:
        for entry_point in pkg_resources.iter_entry_points('ehforwarderbot.qq.plugin'):
<mask>:
                c = entry_point.load()
                cls = getattr(c, name)
                self.client = cls(name, config, channel)
                return
    except:
        raise Exception('Specified client not found!')
    raise Exception('Specified client not found!')",entry_point.name == name,54,entry_point.name.endswith('Client'),False,39.281465090051306,N/A
"def __init__(self, string, filter_comments=True, filter_strings=True, filter_fypp=True):
    self._content = string
    self._it = enumerate(self._content)
    self._instring = ''
    self._infypp = False
    self._incomment = ''
    self._instring = ''
    self._filter_comments = filter_comments
    self._filter_strings = filter_strings
<mask>:
        self._notfortran_re = NOTFORTRAN_LINE_RE
    else:
        self._notfortran_re = NOTFORTRAN_FYPP_LINE_RE",filter_fypp,38,filter_fypp,True,100.00000000000004,N/A
"def update(self, string, filter_comments=True, filter_strings=True, filter_fypp=True):
    self._content = string
    self._it = enumerate(self._content)
    self._filter_comments = filter_comments
    self._filter_strings = filter_strings
<mask>:
        self._notfortran_re = NOTFORTRAN_LINE_RE
    else:
        self._notfortran_re = NOTFORTRAN_FYPP_LINE_RE",filter_fypp,26,filter_fypp,True,100.00000000000004,N/A
"def __next__(self):
    pos, char = next(self._it)
    char2 = self._content[pos:pos + 2]
<mask>:
        if not self._incomment:
            if FYPP_OPEN_RE.search(char2):
                self._instring = char2
                self._infypp = True
            elif self._notfortran_re.search(char2):
                self._incomment = char
            elif char in ['""', ""'""]:
                self._instring = char
    elif self._infypp:
        if FYPP_CLOSE_RE.search(char2):
            self._instring = ''
            self._infypp = False
            if self._filter_strings:
                self.__next__()
                return self.__next__()
    elif char in ['""', ""'""]:
        if self._instring == char:
            self._instring = ''
            if self._filter_strings:
                return self.__next__()
    if self._filter_comments:
        if self._incomment:
            raise StopIteration
    if self._filter_strings:
        if self._instring:
            return self.__next__()
    return (pos, char)",not self._instring,82,char2,False,0.0,N/A
"def __init__(self, infile, filter_fypp=True, orig_filename=None):
<mask>:
        orig_filename = infile.name
    self.line_buffer = deque([])
    self.infile = infile
    self.line_nr = 0
    self.filename = orig_filename
    self.endpos = deque([])
    self.what_omp = deque([])
    if filter_fypp:
        self.notfortran_re = NOTFORTRAN_LINE_RE
    else:
        self.notfortran_re = NOTFORTRAN_FYPP_LINE_RE",not orig_filename,36,orig_filename is None,False,39.76353643835252,N/A
"def split(self, line):
    partsplit = self._re.split(line)
    partsplit_out = []
    for n, part in enumerate(partsplit):
<mask>:
            if self._re_excl.search(partsplit[n - 1]):
                if n == 1:
                    partsplit_out = [partsplit[n - 1]]
                if n + 1 >= len(partsplit) or not partsplit_out:
                    raise FprettifyParseException('non-standard expression involving + or -', '', 0)
                partsplit_out[-1] += part + partsplit[n + 1]
            else:
                if n == 1:
                    partsplit_out = [partsplit[n - 1]]
                if n + 1 >= len(partsplit):
                    raise FprettifyParseException('non-standard expression involving + or -', '', 0)
                partsplit_out += [part, partsplit[n + 1]]
    if not partsplit_out:
        partsplit_out = partsplit
    return partsplit_out","re.search('^(\\+|-)$', part)",92,n + 1 < len(partsplit),False,2.0822836897918786,N/A
"def search(self, line):
    match = self._re.search(line)
<mask>:
        level = 0
        for pos, char in CharFilter(line):
            [what_del_open, what_del_close] = get_curr_delim(line, pos)
            if what_del_open:
                if what_del_open.group() == '(':
                    level += 1
            if what_del_close and what_del_close.group() == ')':
                if level == 1:
                    if EMPTY_RE.search(line[pos + 1:]):
                        return True
                    else:
                        return False
                else:
                    level += -1
    return False",match,54,match,True,100.00000000000004,N/A
"def build_scope_parser(fypp=True, mod=True):
    parser = {}
    parser['new'] = [parser_re(IF_RE), parser_re(DO_RE), parser_re(SELCASE_RE), parser_re(SUBR_RE), parser_re(FCT_RE), parser_re(INTERFACE_RE), parser_re(TYPE_RE), parser_re(ENUM_RE), parser_re(ASSOCIATE_RE), None, parser_re(BLK_RE), where_parser(WHERE_RE), forall_parser(FORALL_RE)]
    parser['continue'] = [parser_re(ELSE_RE), None, parser_re(CASE_RE), parser_re(CONTAINS_RE), parser_re(CONTAINS_RE), None, parser_re(CONTAINS_RE), None, None, None, None, parser_re(ELSEWHERE_RE), None]
    parser['end'] = [parser_re(ENDIF_RE), parser_re(ENDDO_RE), parser_re(ENDSEL_RE), parser_re(ENDSUBR_RE), parser_re(ENDFCT_RE), parser_re(ENDINTERFACE_RE), parser_re(ENDTYPE_RE), parser_re(ENDENUM_RE), parser_re(ENDASSOCIATE_RE), parser_re(ENDANY_RE, spec=False), parser_re(ENDBLK_RE), parser_re(ENDWHERE_RE), parser_re(ENDFORALL_RE)]
<mask>:
        parser['new'].extend([parser_re(MOD_RE), parser_re(SMOD_RE), parser_re(PROG_RE)])
        parser['continue'].extend([parser_re(CONTAINS_RE), parser_re(CONTAINS_RE), parser_re(CONTAINS_RE)])
        parser['end'].extend([parser_re(ENDMOD_RE), parser_re(ENDSMOD_RE), parser_re(ENDPROG_RE)])
    if fypp:
        parser['new'].extend(PREPRO_NEW_SCOPE)
        parser['continue'].extend(PREPRO_CONTINUE_SCOPE)
        parser['end'].extend(PREPRO_END_SCOPE)
    return parser",mod,69,mod,True,100.00000000000004,N/A
"def __init__(self, scope_parser, first_indent, rel_indent, filename):
    self._scope_storage = []
    self._indent_storage = []
    self._line_indents = []
    self._parser = scope_parser
    self._filename = filename
    self._aligner = F90Aligner(filename)
    self._initial = True
<mask>:
        for n_impl in range(first_indent % rel_indent, first_indent + 1, rel_indent):
            self._indent_storage += [n_impl]
    if not self._indent_storage:
        self._indent_storage = [0]",rel_indent > 0,47,first_indent % rel_indent == 0,False,16.784459625186194,N/A
"@classmethod
def tearDownClass(cls):
    """"""
        tearDownClass to be recognized by unittest. Used for test summary
        output.
        """"""
<mask>:
        format = '{:<20}{:<6}'
        eprint('\n' + '=' * 70)
        eprint('IGNORED errors: invalid or old Fortran')
        eprint('-' * 70)
        eprint(format.format('parse errors: ', cls.n_parsefail))
        eprint(format.format('internal errors: ', cls.n_internalfail))",cls.n_parsefail + cls.n_internalfail > 0,42,cls.n_parsefail and cls.n_internalfail,False,58.50343668259105,N/A
"def test_io(self):
    """"""simple test for io (file inplace, stdin & stdout)""""""
    outstring = []
    instring = 'CALL  alien_invasion( 👽 )'
    outstring_exp = 'CALL alien_invasion(👽)'
    alien_file = 'alien_invasion.f90'
<mask>:
        raise AlienInvasion('remove file alien_invasion.f90')
    try:
        with io.open(alien_file, 'w', encoding='utf-8') as infile:
            infile.write(instring)
        p1 = subprocess.Popen(RUNSCRIPT, stdout=subprocess.PIPE, stdin=subprocess.PIPE)
        outstring.append(p1.communicate(instring.encode('UTF-8'))[0].decode('UTF-8'))
        p1 = subprocess.Popen([RUNSCRIPT, alien_file, '--stdout'], stdout=subprocess.PIPE)
        outstring.append(p1.communicate(instring.encode('UTF-8')[0])[0].decode('UTF-8'))
        p1 = subprocess.Popen([RUNSCRIPT, alien_file])
        p1.wait()
        with io.open(alien_file, 'r', encoding='utf-8') as infile:
            outstring.append(infile.read())
        for outstr in outstring:
            self.assertEqual(outstring_exp, outstr.strip())
    except:
        if os.path.isfile(alien_file):
            os.remove(alien_file)
        raise
    else:
        os.remove(alien_file)",os.path.isfile(alien_file),78,os.path.isfile(alien_file),True,100.00000000000004,N/A
"def addtestmethod(testcase, fpath, ffile):
    """"""add a test method for each example.""""""

    def testmethod(testcase):
        """"""this is the test method invoked for each example.""""""
        dirpath_before = joinpath(BEFORE_DIR, fpath)
        dirpath_after = joinpath(AFTER_DIR, fpath)
<mask>:
            os.makedirs(dirpath_after)
        example_before = joinpath(dirpath_before, ffile)
        example_after = joinpath(dirpath_after, ffile)
        if os.path.isfile(example_after):
            os.remove(example_after)

        def test_result(path, info):
            return [os.path.relpath(path, BEFORE_DIR), info]
        with io.open(example_before, 'r', encoding='utf-8') as infile:
            outstring = io.StringIO()
            try:
                fprettify.reformat_ffile(infile, outstring)
                m = hashlib.sha256()
                m.update(outstring.getvalue().encode('utf-8'))
                test_info = 'checksum'
                test_content = test_result(example_before, m.hexdigest())
                with io.open(example_after, 'w', encoding='utf-8') as outfile:
                    outfile.write(outstring.getvalue())
                FPrettifyTestCase.n_success += 1
            except FprettifyParseException as e:
                test_info = 'parse error'
                fprettify.log_exception(e, test_info)
                test_content = test_result(example_before, test_info)
                FPrettifyTestCase.n_parsefail += 1
            except FprettifyInternalException as e:
                test_info = 'internal error'
                fprettify.log_exception(e, test_info)
                test_content = test_result(example_before, test_info)
                FPrettifyTestCase.n_internalfail += 1
            except:
                FPrettifyTestCase.n_unexpectedfail += 1
                raise
        after_exists = os.path.isfile(example_after)
        if after_exists:
            with io.open(example_before, 'r', encoding='utf-8') as infile:
                before_content = infile.read()
                before_nosp = re.sub('\\n{3,}', '\\n\\n', before_content.lower().replace(' ', '').replace('\t', ''))
            with io.open(example_after, 'r', encoding='utf-8') as outfile:
                after_content = outfile.read()
                after_nosp = after_content.lower().replace(' ', '')
            testcase.assertMultiLineEqual(before_nosp, after_nosp)
        sep_str = ' : '
        with io.open(RESULT_FILE, 'r', encoding='utf-8') as infile:
            found = False
            for line in infile:
                line_content = line.strip().split(sep_str)
                if line_content[0] == test_content[0]:
                    found = True
                    eprint(test_info, end=' ')
                    msg = '{} (old) != {} (new)'.format(line_content[1], test_content[1])
                    if test_info == 'checksum' and after_exists and (after_content.count('\n') < 10000):
                        result = list(difflib.unified_diff(before_content.splitlines(True), after_content.splitlines(True), fromfile=test_content[0], tofile=line_content[0]))
                        msg += '\n' + ''.join(result)
                    try:
                        testcase.assertEqual(line_content[1], test_content[1], msg)
                    except AssertionError:
                        FPrettifyTestCase.write_result(FAILED_FILE, test_content, sep_str)
                        raise
                    break
        if not found:
            eprint(test_info + ' new', end=' ')
            FPrettifyTestCase.write_result(RESULT_FILE, test_content, sep_str)
    testmethod.__name__ = 'test ' + joinpath(fpath, ffile)
    setattr(testcase, testmethod.__name__, testmethod)",not os.path.exists(dirpath_after),253,not os.path.isdir(dirpath_after),False,70.16879391277372,N/A
"def get_version():
    version = '%s.%s' % (VERSION[0], VERSION[1])
<mask>:
        version = '%s.%s' % (version, VERSION[2])
    if VERSION[3:] == ('alpha', 0):
        version = '%s pre-alpha' % version
    elif VERSION[3] != 'final':
        try:
            rev = VERSION[4]
        except IndexError:
            rev = 0
        version = '%s%s%s' % (version, VERSION[3][0:1], rev)
    return version",VERSION[2],48,VERSION[2],True,100.00000000000004,N/A
"@register('geocoding', deploy=True)
def test_geocoding(app_configs=None, **kwargs):
    warnings = []
<mask>:
        location = geocode('Red Square')
        if not location:
            warnings.append(Warning('Geocoding service is experiencing issues or is not properly configured'))
    return warnings","not settings.DEBUG or not getattr(settings, 'TESTING', False)",28,app_configs.get('geocoding_service'),False,3.701773936489291,N/A
"def get_context(self, name, value, attrs):
    c = super().get_context(name, value, attrs)
<mask>:
        c.update({'filename': value.name, 'url': value.url, 'thumbnail': self.thumbnail})
        try:
            c.update({'width': value.width, 'height': value.height})
        except IOError:
            msg = 'floorplan image not found while showing floorplan:\n{0}'
            logger.error(msg.format(value.name))
    return c","value and hasattr(value, 'url')",36,value.name,False,5.197112497172873,N/A
"def get_available_name(self, name, max_length=None):
    """"""
        removes file if it already exists
        """"""
<mask>:
        self.delete(name)
    return name",self.exists(name),16,os.path.exists(name),False,51.697315395717055,N/A
"def tearDown(self):
<mask>:
        return
    for fl in self.floorplan_model.objects.all():
        fl.objectlocation_set.all().delete()
        fl.delete()","not hasattr(self, 'floorplan_model')",10,not self.floorplan_model.objects.all(),False,5.063996506781411,N/A
"def _create_floorplan(self, **kwargs):
    options = dict(floor=1)
    options.update(kwargs)
<mask>:
        options['image'] = self._get_simpleuploadedfile()
    if 'location' not in options:
        options['location'] = self._create_location(type='indoor')
    fl = self.floorplan_model(**options)
    fl.full_clean()
    fl.save()
    return fl",'image' not in options,26,'image' not in options,True,100.00000000000004,N/A
"def _create_object_location(self, **kwargs):
    options = {}
    options.update(**kwargs)
<mask>:
        options['content_object'] = self._create_object()
    if 'location' not in options:
        options['location'] = self._create_location()
    elif options['location'].type == 'indoor':
        options['indoor'] = '-140.38620,40.369227'
    ol = self.object_location_model(**options)
    ol.full_clean()
    ol.save()
    return ol",'content_object' not in options,33,'content_object' not in options,True,100.00000000000004,N/A
"def _get_communicator(self, request_vars, user=None):
    communicator = WebsocketCommunicator(LocationBroadcast.as_asgi(), request_vars['path'])
<mask>:
        communicator.scope.update({'user': user, 'session': request_vars['session'], 'url_route': {'kwargs': {'pk': request_vars['pk']}}})
    return communicator",user,19,user,True,100.00000000000004,N/A
"def geocode_view(request):
    address = request.GET.get('address')
<mask>:
        return JsonResponse({'error': 'Address parameter not defined'}, status=400)
    location = geocode(address)
    if location is None:
        return JsonResponse({'error': 'Not found location with given name'}, status=404)
    return JsonResponse({'lat': location.latitude, 'lng': location.longitude})",address is None,34,not address,False,30.326532985631665,N/A
"def reverse_geocode_view(request):
    lat = request.GET.get('lat')
    lng = request.GET.get('lng')
<mask>:
        return JsonResponse({'error': 'lat or lng parameter not defined'}, status=400)
    location = reverse_geocode((lat, lng))
    if location is None:
        return JsonResponse({'address': ''}, status=404)
    location = location[0] if isinstance(location, list) else location
    address = str(location.address)
    return JsonResponse({'address': address})",not lat or not lng,44,lat is None or lng is None,False,8.643019616048525,N/A
"def _validate_geometry_if_not_mobile(self):
    """"""
        geometry can be NULL, but only if_mobile is True
        otherwise raise a ValidationError
        """"""
<mask>:
        raise ValidationError(_('geometry cannot be null'))",not self.is_mobile and (not self.geometry),23,self.geometry is None and self.geometry.if_mobile is False,False,15.310245441182436,N/A
"def save(self, *args, **kwargs):
<mask>:
        self.objectlocation_set.update(floorplan=None, indoor=None)
        self.floorplan_set.all().delete()
    return super().save(*args, **kwargs)",self.type != self._initial_type and (not self._state.adding) and (self.type == 'outdoor') and self.floorplan_set.exists(),11,self.floorplan_set.count(),False,2.108904719422291,N/A
"def __str__(self):
<mask>:
        suffix = _('{0} floor').format(ordinal(self.floor))
    else:
        suffix = _('ground floor')
    return '{0} {1}'.format(self.location.name, suffix)",self.floor != 0,16,self.floor,False,36.78794411714425,N/A
"def _validate_location_type(self):
<mask>:
        return
    if self.location.type and self.location.type != 'indoor':
        msg = 'floorplans can only be associated to locations of type ""indoor""'
        raise ValidationError(msg)","not hasattr(self, 'location') or not hasattr(self.location, 'type')",24,not self.location,False,2.4774512716690604,N/A
"def _remove_image(self):
    path = self.image.name
<mask>:
        self.image.delete(save=False)
    else:
        msg = 'floorplan image not found while deleting {0}:\n{1}'
        logger.error(msg.format(self, path))",self.image.storage.exists(path),19,os.path.exists(path),False,42.13952948452608,N/A
"def get_formset_kwargs(self, request, obj, inline, prefix):
    formset_kwargs = super().get_formset_kwargs(request, obj, inline, prefix)
<mask>:
        formset_kwargs['data']['floorplan_set-TOTAL_FORMS'] = '0'
    return formset_kwargs",request.method == 'POST' and formset_kwargs['data']['type'] == 'outdoor',18,inline,False,0.0,N/A
"def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    initial = {}
    location = self._get_initial_location()
    floorplan = self._get_initial_floorplan()
<mask>:
        initial.update({'location_selection': 'existing', 'type': location.type, 'is_mobile': location.is_mobile, 'name': location.name, 'address': location.address, 'geometry': location.geometry})
    if floorplan:
        initial.update({'floorplan_selection': 'existing', 'floorplan': floorplan.pk, 'floor': floorplan.floor, 'image': floorplan.image})
        floorplan_choices = self.fields['floorplan'].choices
        self.fields['floorplan'].choices = floorplan_choices + [(floorplan.pk, floorplan)]
    self.initial.update(initial)",location,48,location,True,100.00000000000004,N/A
"def clean_floorplan(self):
    floorplan_model = self.floorplan_model
    type_ = self.cleaned_data.get('type')
    floorplan_selection = self.cleaned_data.get('floorplan_selection')
<mask>:
        return None
    pk = self.cleaned_data['floorplan']
    if not pk:
        raise ValidationError(_('No floorplan selected'))
    try:
        fl = floorplan_model.objects.get(pk=pk)
    except floorplan_model.DoesNotExist:
        raise ValidationError(_('Selected floorplan does not exist'))
    if fl.location != self.cleaned_data['location']:
        raise ValidationError(_('This floorplan is associated to a different location'))
    return fl",type_ != 'indoor' or floorplan_selection == 'new' or (not floorplan_selection),51,not type_ or not floorplan_selection,False,10.803507398984392,N/A
"def clean(self):
    data = self.cleaned_data
    type_ = data.get('type')
    is_mobile = data['is_mobile']
    msg = _('this field is required for locations of type %(type)s')
    fields = []
<mask>:
        fields += ['location_selection', 'name', 'address', 'geometry']
    if (location := data.get('location')):
        location.type = type_
        data['indoor'] = None if type_ != 'indoor' else data.get('indoor')
    if type_ == 'indoor':
        if data.get('floorplan_selection') == 'existing':
            fields.append('floorplan')
        if data.get('image'):
            fields += ['floor', 'indoor']
    elif is_mobile and (not data.get('location')):
        data['name'] = ''
        data['address'] = ''
        data['geometry'] = ''
        data['location_selection'] = 'new'
    for field in fields:
        if field in data and data[field] in [None, '']:
            params = {'type': type_}
            err = ValidationError(msg, params=params)
            self.add_error(field, err)","not is_mobile and type_ in ['outdoor', 'indoor']",103,is_mobile and (not data.get('location')),False,20.448007360218387,N/A
"def _get_floorplan_instance(self):
    data = self.cleaned_data
    instance = self.instance
    floorplan = data.get('floorplan') or self.floorplan_model()
    floorplan.location = instance.location
    floorplan.floor = data.get('floor')
<mask>:
        floorplan.image = data['image']
    return floorplan",data.get('image') and self.initial.get('image') != data.get('image'),25,data.get('image'),False,5.881647164242991,N/A
"def update_mobile_location(sender, instance, **kwargs):
<mask>:
        group_name = 'loci.mobile-location.{0}'.format(str(instance.pk))
        channel_layer = channels.layers.get_channel_layer()
        message = json.loads(instance.geometry.geojson)
        async_to_sync(channel_layer.group_send)(group_name, {'type': 'send_message', 'message': message})",not kwargs.get('created') and instance.geometry,19,instance.geometry,False,6.948345122280157,N/A
"def connect(self):
    self.pk = None
    try:
        user = self.scope['user']
        self.pk = self.scope['url_route']['kwargs']['pk']
    except KeyError:
        self.close()
    location = _get_object_or_none(self.model, pk=self.pk)
<mask>:
        self.close()
        return
    self.accept()
    async_to_sync(self.channel_layer.group_add)('loci.mobile-location.{0}'.format(self.pk), self.channel_name)","not location or not self.is_authorized(user, location)",25,user.is_superuser and location is not None,False,12.451233733093902,N/A
"def make_layers(shapes, geom_dicts=False):
    print('Creating layers with 10 shapes each')
    layers = []
    i = 0
    features = []
    for shape in shapes:
        try:
            geom = loads_wkt(shape.strip())
<mask>:
                feature = {'geometry': mapping(geom), 'properties': {}}
            else:
                feature = {'geometry': geom, 'properties': {}}
            features.append(feature)
            if i >= 10:
                layers.append(features)
                features = []
                i = 0
            i += 1
        except Exception:
            pass
    layers.append(features)
    return layers",geom_dicts,61,geom_dicts,True,100.00000000000004,N/A
"def run_test(layers):
    print('Running perf test')
    profiler = cProfile.Profile()
    for i, layer in enumerate(layers):
        layer_description = {'features': layer, 'name': 'bar'}
        profiler.enable()
        encode(layer_description, default_options={'on_invalid_geometry': on_invalid_geometry_ignore})
        profiler.disable()
<mask>:
            print(f'{i} tiles produced')
    print('Perf result :')
    profiler.print_stats()",i % 100 == 0,32,profiler.is_enabled(),False,0.0,N/A
"def assertRoundTrip(self, input_geometry, expected_geometry, name=None, properties=None, id=None, expected_len=1, expected_properties=None):
<mask>:
        input_geometry = self.feature_geometry
    if name is None:
        name = self.layer_name
    if properties is None:
        properties = self.feature_properties
    if expected_properties is None:
        expected_properties = properties
    source = [{'name': name, 'features': [{'geometry': input_geometry, 'properties': properties}]}]
    if id:
        source[0]['features'][0]['id'] = id
    encoded = encode(source)
    decoded = decode(encoded)
    self.assertIn(name, decoded)
    layer = decoded[name]
    features = layer['features']
    self.assertEqual(expected_len, len(features))
    self.assertEqual(features[0]['properties'], expected_properties)
    self.assertEqual(features[0]['geometry'], expected_geometry)
    if id:
        self.assertEqual(features[0]['id'], id)",input_geometry is None,72,input_geometry is None,True,100.00000000000004,N/A
"def get_message(self):
    tile = {}
    for layer in self.tile.layers:
        layer_name = layer.name
        layer_options = self.per_layer_options.get(layer_name, None)
        layer_options = get_decode_options(layer_options=layer_options, default_options=self.default_options)
        keys = layer.keys
        vals = layer.values
        features = []
        for feature in layer.features:
            tags = feature.tags
            props = {}
            assert len(tags) % 2 == 0, 'Unexpected number of tags'
            for key_idx, val_idx in zip(tags[::2], tags[1::2]):
                key = keys[key_idx]
                val = vals[val_idx]
                value = self.parse_value(val)
                props[key] = value
            geometry = self.parse_geometry(geom=feature.geometry, ftype=feature.type, extent=layer.extent, y_coord_down=layer_options['y_coord_down'], transformer=layer_options['transformer'])
<mask>:
                new_feature = {'geometry': geometry, 'properties': props, 'id': feature.id, 'type': 'Feature'}
            else:
                new_feature = {'geometry': geometry, 'properties': props, 'id': feature.id, 'type': feature.type}
            features.append(new_feature)
        tile_data = {'extent': layer.extent, 'version': layer.version, 'features': features}
        if layer_options['geojson']:
            tile_data['type'] = 'FeatureCollection'
        tile[layer_name] = tile_data
    return tile",layer_options['geojson'],115,feature.type == 'FeatureCollection',False,0.0,N/A
"@staticmethod
def parse_value(val):
    for candidate in ('bool_value', 'double_value', 'float_value', 'int_value', 'sint_value', 'string_value', 'uint_value'):
<mask>:
            return getattr(val, candidate)
    raise ValueError(f'{val} is an unknown value')",val.HasField(candidate),23,candidate in val,False,12.753667906901528,N/A
"def parse_geometry(self, geom, ftype, extent, y_coord_down, transformer):
    i = 0
    coords = []
    dx = 0
    dy = 0
    parts = []
    while i != len(geom):
        item = bin(geom[i])
        ilen = len(item)
        cmd = int(self.zero_pad(item[ilen - CMD_BITS:ilen]), 2)
        cmd_len = int(self.zero_pad(item[:ilen - CMD_BITS]), 2)
        i = i + 1
<mask>:
            if ftype == POLYGON:
                self._ensure_polygon_closed(coords)
            parts.append(coords)
            coords = []
        elif cmd in (CMD_MOVE_TO, CMD_LINE_TO):
            if coords and cmd == CMD_MOVE_TO and (ftype in (LINESTRING, POLYGON)):
                if ftype == POLYGON:
                    self._ensure_polygon_closed(coords)
                parts.append(coords)
                coords = []
            for _ in range(cmd_len):
                x = geom[i]
                i = i + 1
                y = geom[i]
                i = i + 1
                x = zig_zag_decode(x)
                y = zig_zag_decode(y)
                x = x + dx
                y = y + dy
                dx = x
                dy = y
                if not y_coord_down:
                    y = extent - y
                if transformer is None:
                    coords.append([x, y])
                else:
                    coords.append([*transformer(x, y)])
    if ftype == POINT:
        if len(coords) == 1:
            return {'type': 'Point', 'coordinates': coords[0]}
        else:
            return {'type': 'MultiPoint', 'coordinates': coords}
    elif ftype == LINESTRING:
        if parts:
            if coords:
                parts.append(coords)
            if len(parts) == 1:
                return {'type': 'LineString', 'coordinates': parts[0]}
            else:
                return {'type': 'MultiLineString', 'coordinates': parts}
        else:
            return {'type': 'LineString', 'coordinates': coords}
    elif ftype == POLYGON:
        if coords:
            parts.append(coords)
        polygon = []
        polygons = []
        winding = 0
        for ring in parts:
            a = self._area_sign(ring)
            if a == 0:
                continue
            if winding == 0:
                winding = a
            if winding == a:
                if polygon:
                    polygons.append(polygon)
                polygon = [ring]
            else:
                polygon.append(ring)
        if polygon:
            polygons.append(polygon)
        if len(polygons) == 1:
            return {'type': 'Polygon', 'coordinates': polygons[0]}
        else:
            return {'type': 'MultiPolygon', 'coordinates': polygons}
    else:
        raise ValueError(f'Unknown geometry type: {ftype}')",cmd == CMD_SEG_END,263,"cmd in (CMD_LINE_TO, CMD_LINE_TO)",False,6.250381527944883,N/A
"def _drop_degenerate_inners(shape):
    """"""
    Drop degenerate (zero-size) inners from the polygon.

    This is implemented as dropping anything with a size less than 0.5, as the polygon is in integer coordinates and
    the smallest valid inner would be a triangle with height and width 1.
    """"""
    assert shape.geom_type == 'Polygon'
    new_inners = []
    for inner in shape.interiors:
<mask>:
            new_inners.append(inner)
    return Polygon(shape.exterior, new_inners)",abs(Polygon(inner).area) >= 0.5,60,inner.width < 0.5,False,3.466791587270993,N/A
"def _contour_to_poly(contour):
    poly = Polygon(contour)
<mask>:
        poly = poly.buffer(0)
    assert poly.is_valid, f'Contour {contour!r} did not make valid polygon {poly.wkt} because {explain_validity(poly)}'
    return poly",not poly.is_valid,23,len(poly.buffer) == 1,False,9.535414040914192,N/A
"def _union_in_blocks(contours, block_size):
    """"""
    Generator which yields a valid shape for each block_size multiple of input contours. This merges together the
    contours for each block before yielding them.
    """"""
    n_contours = len(contours)
    for i in range(0, n_contours, block_size):
        j = min(i + block_size, n_contours)
        inners = []
        for c in contours[i:j]:
            p = _contour_to_poly(c)
<mask>:
                inners.append(p)
            elif p.geom_type == 'MultiPolygon':
                inners.extend(p.geoms)
        holes = unary_union(inners)
        assert holes.is_valid
        yield holes",p.geom_type == 'Polygon',68,p.geom_type == 'Point',False,84.08964152537145,N/A
"def _polytree_node_to_shapely(node):
    """"""
    Recurses down a Clipper PolyTree, extracting the results as Shapely objects.

    Returns a tuple of (list of polygons, list of children)
    """"""
    polygons = []
    children = []
    for ch in node.Childs:
        p, c = _polytree_node_to_shapely(ch)
        polygons.extend(p)
        children.extend(c)
<mask>:
        assert len(children) == 0
        children = [node.Contour] if node.Contour else []
    elif node.Contour:
        poly = _contour_to_poly(node.Contour)
        block_size = 200
        inners = _union_in_blocks(children, block_size) if len(children) > block_size else _generate_polys(children)
        for inner in inners:
            try:
                diff = poly.difference(inner)
            except Exception:
                continue
            if not diff.is_valid:
                diff = diff.buffer(0)
            if diff.is_valid:
                poly = diff
        assert poly.is_valid
        if poly.geom_type == 'MultiPolygon':
            polygons.extend(poly.geoms)
        else:
            polygons.append(poly)
        children = []
    else:
        assert len(children) == 0
    return (polygons, children)",node.IsHole,113,node.Children,False,55.03212081491043,N/A
"def make_valid_multipolygon(shape):
    new_g = []
    for g in shape.geoms:
<mask>:
            continue
        valid_g = make_valid_polygon(g)
        if valid_g.geom_type == 'MultiPolygon':
            new_g.extend(valid_g.geoms)
        else:
            new_g.append(valid_g)
    return MultiPolygon(new_g)",g.is_empty,23,g.num_elements == 0,False,12.22307556087252,N/A
"def decode(tile, per_layer_options=None, default_options=None, **kwargs):
    """"""Decode the provided `tile`

    Args:
        tile:
            The tile to decode.

        per_layer_options:
            An optional dictionary containing per layer options. The keys are the layer names and the values are
            options as described further. If an option is missing for a layer or if a layer is missing, the values of
            `default_options` are taken first. Finally, the global default options are used.

        default_options:
            These options are taken for layers without entry in `per_layer_options`. For all missing options values,
            the global default values are taken.

    Returns:
        The decoded layers data.

    Notes:
        The possible options are:
            * `y_coord_down`: it suppresses flipping the y coordinate values during encoding when set to `True`.
            Default to `False`.
            * `transformer`: a function transforming the coordinates of geometry object. It takes two floats (`x`
            and `y`) as arguments and retrieves the transformed coordinates `x_transformed`, `y_transformed`. Default to
            `None`.
            * `geojson`: when set to `False`, the behaviour of mapbox-vector-tile version 1.* is used. When set
            to `False`, the retrieved dictionary is a valid geojson file. Default to `True`.
    """"""
<mask>:
        warnings.warn('`decode` signature has changed, use `default_options` instead', DeprecationWarning, stacklevel=2)
        default_options = {**kwargs, **(default_options or {})}
    vector_tile = decoder.TileData(pbf_data=tile, per_layer_options=per_layer_options, default_options=default_options)
    message = vector_tile.get_message()
    return message",kwargs,200,kwargs,True,100.00000000000004,N/A
"def add_layer(self, name, features, options=None):
<mask>:
        raise ValueError(f'A layer name can not be empty. {name!r} was provided.')
    if name in self.seen_layer_names:
        raise ValueError(f'The layer name {name!r} already exists in the vector tile.')
    self.seen_layer_names.add(name)
    self.layer = self.tile.layers.add()
    self.layer_options = get_encode_options(layer_options=options, default_options=self.default_options)
    self.layer.name = name
    self.layer.version = 2
    self.layer.extent = self.layer_options['extents']
    self.key_idx = 0
    self.val_idx = 0
    self.seen_keys_idx = {}
    self.seen_values_idx = {}
    self.seen_values_bool_idx = {}
    for feature in features:
        geometry_spec = feature.get('geometry')
        if geometry_spec is None:
            continue
        shape = self._load_geometry(geometry_spec)
        if shape is None:
            raise NotImplementedError(""Can't do geometries that are not wkt, wkb, or shapely geometries"")
        if shape.is_empty:
            continue
        if self.layer_options['quantize_bounds']:
            shape = self.quantize(shape)
        if self.layer_options['check_winding_order']:
            shape = self.enforce_winding_order(shape)
        if shape is not None and (not shape.is_empty):
            self.add_feature(feature, shape)",not name,118,name.is_empty,False,10.682175159905848,N/A
"def enforce_winding_order(self, shape, n_try=1):
<mask>:
        shape = self.enforce_multipolygon_winding_order(shape=shape, n_try=n_try)
    elif shape.geom_type == 'Polygon':
        shape = self.enforce_polygon_winding_order(shape=shape, n_try=n_try)
    return shape",shape.geom_type == 'MultiPolygon',19,shape.geom_type == 'Multipolygon',False,84.08964152537145,N/A
"def handle_shape_validity(self, shape, n_try):
<mask>:
        return shape
    if n_try >= self.layer_options['max_geometry_validate_tries']:
        return None
    if self.layer_options['on_invalid_geometry']:
        shape = self.layer_options['on_invalid_geometry'](shape)
        if shape is not None and (not shape.is_empty):
            shape = self.enforce_winding_order(shape=shape, n_try=n_try + 1)
    return shape",shape.is_valid,34,shape is not None,False,14.794015674776452,N/A
"def enforce_multipolygon_winding_order(self, shape, n_try):
    assert shape.geom_type == 'MultiPolygon'
    parts = []
    for part in shape.geoms:
        part = self.enforce_polygon_winding_order(shape=part, n_try=n_try)
<mask>:
            if part.geom_type == 'MultiPolygon':
                parts.extend(part.geoms)
            else:
                parts.append(part)
    if not parts:
        return None
    oriented_shape = parts[0] if len(parts) == 1 else MultiPolygon(parts)
    oriented_shape = self.handle_shape_validity(oriented_shape, n_try)
    return oriented_shape",part is not None and (not part.is_empty),47,part,False,0.0006144212353328212,N/A
"def enforce_polygon_winding_order(self, shape, n_try):
    assert shape.geom_type == 'Polygon'

    def fn(point):
        x, y = point
        return (round(x), round(y))
    exterior = self.apply_map(fn, shape.exterior.coords)
    rings = None
<mask>:
        rings = [self.apply_map(fn, ring.coords) for ring in shape.interiors]
    sign = 1.0 if self.layer_options['y_coord_down'] else -1.0
    oriented_shape = orient(Polygon(exterior, rings), sign=sign)
    oriented_shape = self.handle_shape_validity(oriented_shape, n_try)
    return oriented_shape",len(shape.interiors) > 0,51,shape.interiors,False,18.887560283756194,N/A
"def coords_on_grid(self, x, y):
    """"""Snap coordinates on the grid with integer coordinates""""""
<mask>:
        x = int(round(x))
    if isinstance(y, float):
        y = int(round(y))
    if not self._y_coord_down:
        y = self._extents - y
    return (x, y)","isinstance(x, float)",33,"isinstance(x, float)",True,100.00000000000004,N/A
"def encode_arc(self, coords):
    """"""Appends commands to _geometry to create an arc.
        - Returns False if nothing was added
        - Returns True and moves _last_x, _last_y if
            some points where added
        """"""
    last_x, last_y = (self._last_x, self._last_y)
    float_x, float_y = next(coords)
    x, y = self.coords_on_grid(float_x, float_y)
    dx, dy = (x - last_x, y - last_y)
    cmd_encoded = self.encode_cmd_length(CMD_MOVE_TO, 1)
    commands = [cmd_encoded, zig_zag_encode(dx), zig_zag_encode(dy), CMD_FAKE]
    pairs_added = 0
    last_x, last_y = (x, y)
    for float_x, float_y in coords:
        x, y = self.coords_on_grid(float_x, float_y)
        dx, dy = (x - last_x, y - last_y)
<mask>:
            continue
        commands.append(zig_zag_encode(dx))
        commands.append(zig_zag_encode(dy))
        last_x, last_y = (x, y)
        pairs_added += 1
    if pairs_added == 0:
        return False
    cmd_encoded = self.encode_cmd_length(CMD_LINE_TO, pairs_added)
    commands[3] = cmd_encoded
    self._geometry.extend(commands)
    self._last_x, self._last_y = (last_x, last_y)
    return True",dx == 0 and dy == 0,124,dx == 0 or dy == 0,False,59.694917920196445,N/A
"def encode_ring(self, ring):
    coords = self.omit_last(iter(ring.coords))
<mask>:
        return False
    cmd_seg_end = self.encode_cmd_length(CMD_SEG_END, 1)
    self._geometry.append(cmd_seg_end)
    return True",not self.encode_arc(coords),16,coords is None,False,3.7238938287986976,N/A
"def encode_polygon(self, shape):
<mask>:
        return
    for arc in shape.interiors:
        self.encode_ring(arc)",not self.encode_ring(shape.exterior),10,shape.interiors is None,False,6.434818657591886,N/A
"def encode(self, shape):
<mask>:
        pass
    elif shape.geom_type == 'Point':
        x, y = self.coords_on_grid(shape.x, shape.y)
        cmd_encoded = self.encode_cmd_length(CMD_MOVE_TO, 1)
        self._geometry = [cmd_encoded, zig_zag_encode(x), zig_zag_encode(y)]
    elif shape.geom_type == 'MultiPoint':
        self.encode_multipoint(shape.geoms)
    elif shape.geom_type == 'LineString':
        coords = iter(shape.coords)
        self.encode_arc(coords)
    elif shape.geom_type == 'MultiLineString':
        self.encode_multilinestring(shape)
    elif shape.geom_type == 'Polygon':
        self.encode_polygon(shape)
    elif shape.geom_type == 'MultiPolygon':
        self.encode_multipolygon(shape)
    else:
        raise NotImplementedError(f""Can't do {shape.geom_type} geometries"")
    return self._geometry",shape.geom_type == 'GeometryCollection',59,shape.geom_type == 'Point',False,84.08964152537145,N/A
"def _get_options(layer_options, default_options, global_default_options, operation_name):
    """"""Get the entire options dictionary filled using: first, the provided `layer_options`, then the provided
    `default_options` and finally filled using the provided `global_default_options`.

    Args:
        layer_options:
            The options for the current layer.

        default_options:
            The default options of the operation.

        global_default_options:
            The global default options for the operation.

        operation_name:
            The name of the current operation.

    Returns:
        The options to use to operate the layer.
    """"""
<mask>:
        default_options = global_default_options
    if layer_options is None:
        layer_options = default_options
    result = global_default_options.copy()
    result.update(default_options)
    result.update(layer_options)
    result_keys = set(result.keys())
    expected_keys = set(global_default_options.keys())
    extra_keys = result_keys.difference(expected_keys)
    if extra_keys:
        extra_keys_msg = ', '.join((f'{str(x)!r}' for x in sorted(extra_keys)))
        raise ValueError(f'The following options are not allowed for {operation_name} a tile: {extra_keys_msg}.')
    return result",default_options is None,116,default_options is None,True,100.00000000000004,N/A
"def get_encode_options(layer_options, default_options):
    """"""Get the entire encoding options dictionary filled using: first, the provided `layer_options`, then the provided
    `default_options` and finally filled using the global default options

    Args:
        layer_options:
            The options for the current layer.

        default_options:
            The default options of the encoding operation.

    Returns:
        The options to use for encoding the layer.
    """"""
    result = _get_options(layer_options=layer_options, default_options=default_options, global_default_options=DEFAULT_ENCODE_OPTIONS, operation_name='encoding')
    extents = result['extents']
    max_geometry_validate_tries = result['max_geometry_validate_tries']
<mask>:
        raise ValueError(f'The extents must be positive. {extents} provided.')
    if max_geometry_validate_tries <= 0:
        raise ValueError(f'The max_geometry_validate_tries must be positive. {max_geometry_validate_tries} provided.')
    return result",extents <= 0,88,extents <= 0,True,100.00000000000004,N/A
"def _decode_lines(geom):
    """"""
    Decode a linear MVT geometry into a list of Lines.

    Each individual linestring in the MVT is extracted to a separate entry in the list of lines.
    """"""
    lines = []
    current_line = []
    current_moveto = None
    x = 0
    y = 0
    end = len(geom)
    i = 0
    while i < end:
        header = geom[i]
        cmd = header & 7
        run_length = header // 8
<mask>:
            if current_moveto:
                lines.append(Line(current_moveto, EndsAt(x, y), current_line))
                current_line = []
            assert run_length == 1
            x += zig_zag_decode(geom[i + 1])
            y += zig_zag_decode(geom[i + 2])
            i += 3
            current_moveto = MoveTo(x, y)
        elif cmd == CMD_LINE_TO:
            assert current_moveto
            next_i = i + 1 + run_length * 2
            current_line.extend(geom[i:next_i])
            for j in range(run_length):
                dx = zig_zag_decode(geom[i + 1 + 2 * j])
                dy = zig_zag_decode(geom[i + 2 + 2 * j])
                x += dx
                y += dy
            i = next_i
        else:
            raise ValueError(f'Unhandled command: {cmd}')
    if current_line:
        assert current_moveto
        lines.append(Line(current_moveto, EndsAt(x, y), current_line))
    return lines",cmd == CMD_MOVE_TO,162,cmd == CMD_LINE_FROM,False,54.10822690539397,N/A
"def _reorder_lines(lines):
    """"""
    Reorder lines so that the distance from the end of one to the beginning of the next is minimized.
    """"""
    x = 0
    y = 0
    new_lines = []
    while lines:
        min_dist = None
        min_i = None
        for i, line in enumerate(lines):
            moveto, _, _ = line
            dist = abs(moveto.x - x) + abs(moveto.y - y)
<mask>:
                min_dist = dist
                min_i = i
        assert min_i is not None
        line = lines.pop(min_i)
        _, endsat, _ = line
        x = endsat.x
        y = endsat.y
        new_lines.append(line)
    return new_lines",min_dist is None or dist < min_dist,88,dist < min_dist,False,30.119421191220216,N/A
"def optimise_multilinestring(geom):
    lines = _decode_lines(geom)
<mask>:
        lines = _reorder_lines(lines)
        _rewrite_geometry(geom, lines)",len(lines) > 1,11,lines,False,0.673794699908547,N/A
"def optimise_tile(tile_bytes):
    """"""
    Decode a sequence of bytes as an MVT tile and reorder the string table of its layers and the order of its
    multilinestrings to save a few bytes.
    """"""
    t = vector_tile.tile()
    t.ParseFromString(tile_bytes)
    for layer in t.layers:
        sto = StringTableOptimiser()
        for feature in layer.features:
<mask>:
                optimise_multilinestring(feature.geometry)
            sto.add_tags(feature.tags)
        sto.update_string_table(layer)
    return t.SerializeToString()",feature.type == LINESTRING,53,feature.geometry,False,20.24518585186855,N/A
"def parse(self):
    with open(self._csvname, 'r') as fcsv:
        for line in fcsv:
<mask>:
                print(line)
            else:
                self._header = line.split(',')
                self._packages_available = self._header[8:]
                self._packages_dict = {}
                i = 8
                for package in self._packages_available:
                    self._packages_dict[package.strip()] = i
                    i += 1
                break
        if listpackage:
            return
        if self._package not in self._packages_available:
            raise MachXO3toKipartError('No package named {} in {}'.format(self._package, self._packages_available))
        self._pinoutdict = {}
        power_index = 10000
        pindex = self._packages_dict[self._package]
        for line in fcsv:
            sline = line.split(',')
            index = int(sline[0])
            if sline[pindex] != '-':
                if index != 0:
                    self._pinoutdict[index] = sline
                else:
                    self._pinoutdict[power_index] = sline
                    power_index += 1
        self.count_bank()","line[0] == '#' or line[0] == ','",91,self._verbose,False,0.0,N/A
"def output(self, filename=None):
<mask>:
        raise MachXO3toKipartError(""can't write output on stdout"")
    with open(filename, 'w') as foutput:
        if self._partname is None:
            foutput.write(self._csvname.split('.')[0] + '\n')
        else:
            foutput.write('{}\n'.format(self._partname))
        foutput.write('\n')
        foutput.write('Pin, Unit, Name, Side\n')
        bank = {}
        for rawpin in self._pinoutdict.keys():
            pin = self._pinoutdict[rawpin]
            bank[pin[2]] = bank.get(pin[2], 0) + 1
            pindex = self._packages_dict[self._package]
            if pin[pindex] != '-':
                if bank[pin[2]] <= self._bankcount[pin[2]] / 2:
                    side = 'left'
                else:
                    side = 'right'
                foutput.write('{}, BANK{}, {}{}{}, {}\n'.format(pin[pindex], pin[2], pin[1] if pin[1] != '-' else '', '_' + pin[2] if pin[2] != '-' else '', '_' + pin[3] if pin[3] != '-' else '', side))",filename is None,96,filename is None,True,100.00000000000004,N/A
"def parse_csv_file(csv_file):
    """"""Parses the CSV file and returns a list of pins in the form of (number, 'name', 'type')""""""
    pins = []
    reader = csv.reader(csv_file, delimiter=',', quotechar='""')
    next(reader, None)
    for row in reader:
        number, name, ptype, signal, label = row
<mask>:
            name += '/' + label
        elif signal:
            name += '/' + signal
        name = name.replace(' ', '_')
        pin_type = type_mappings.get(ptype, 'inout')
        pins.append((number, name, pin_type))
    return pins",label,67,label,True,100.00000000000004,N/A
"def parse_portpin(name):
    """"""Finds the port name and number of a pin in a string. If found
    returns a tuple in the form of ('port_name', port_number).
    Otherwise returns `None`.
    """"""
    m = re.search('P([A-Z])(\\d+)', name)
<mask>:
        port_name, port_number = m.groups()
        return (port_name, int(port_number))",m,41,m,True,100.00000000000004,N/A
"def group_pins(pins):
    """"""Groups pins together per their port name and functions. Returns a
    dictionary of {'port': [pin]}.""""""
    ports = defaultdict(list)
    power_names = ['VDD', 'VSS', 'VCAP', 'VBAT', 'VREF', 'V12PHYHS']
    config_names = ['RCC_OSC', 'NRST', 'PDR', 'SWCLK', 'SWDIO', 'BOOT']
    for pin in pins:
        number, name, ptype = pin
<mask>:
            ports['power'].append(pin)
        elif any((pn in name for pn in config_names)):
            ports['config'].append(pin)
        else:
            m = parse_portpin(name)
            if m:
                port_name, port_number = m
                ports[port_name].append(pin)
            else:
                ports['other'].append(pin)
    for port in ports:
        if port in ['config', 'power', 'other']:
            ports[port] = sorted(ports[port], key=itemgetter(1))
        else:
            ports[port] = sorted(ports[port], key=lambda p: parse_portpin(p[1])[1])
    return ports",any((pn in name for pn in power_names)),92,any((pn in name for pn in power_names)),True,100.00000000000004,N/A
"def stm32cube_reader(part_data_file, part_data_file_name, part_data_file_type='.csv'):
    """"""Reader for STM32CubeMx pin list output.

    STM32CubeMx is a tool for creating firmware projects for STM32
    MCUs. It also includes a pin layout designer which can export the
    list of pins in the form of a CSV file. This will read the csv
    file and return a dictionary of pin data.

    An example output of the STM32CubeMx tool can be seen below:

    ""Position"",""Name"",""Type"",""Signal"",""Label""
    ""1"",""VBAT"",""Power"","""",""""
    ""2"",""PC13-ANTI_TAMP"",""I/O"","""",""""
    ""3"",""PC14-OSC32_IN"",""I/O"",""RCC_OSC32_IN"",""""
    ""4"",""PC15-OSC32_OUT"",""I/O"",""RCC_OSC32_OUT"",""""
    ...

    Pin names for the symbols will be constructed as ""Name/Signal"". If
    user defined label is specified it will be used instead of
    ""Signal"" column.

    All IO pins will be grouped to units per their ports.
    Configuration related pins such as boot, clock etc will be grouped
    as a separate unit. Power pins will be a separate unit as well.
    """"""
<mask>:
        part_data_file = convert_xlsx_to_csv(part_data_file)
    csv_file = part_data_file
    pins = parse_csv_file(csv_file)
    ports = group_pins(pins)
    pin_data = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))
    index = 0
    for port_name in ports:
        for p in ports[port_name]:
            pin = copy.copy(DEFAULT_PIN)
            pin.index = index = index + 1
            pin.num = p[0]
            pin.name = p[1]
            pin.type = p[2]
            pin.unit = port_name
            pin_data[pin.unit][pin.side][pin.name].append(pin)
    part_name = os.path.splitext(os.path.split(csv_file.name)[1])[0]
    yield (part_name, 'U', '', '', '', '', pin_data)",part_data_file_type == '.xlsx',195,part_data_file_type == '.xlsx',True,100.00000000000004,N/A
"def xilinx7_reader(part_data_file, part_data_file_name, part_data_file_type='.csv'):
    """"""Extract the pin data from a Xilinx CSV file and return a dictionary of pin data.""""""
<mask>:
        part_data_file = convert_xlsx_to_csv(part_data_file)
    csv_file = part_data_file
    pin_data = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))
    part_num = None
    try:
        while True:
            line = csv_file.readline()
            if re.match('^,*$', line):
                break
            elif line.startswith('""#') or line.startswith('#'):
                device = re.search('#\\s+Device\\s*:\\s*(\\w+)', line)
                if device:
                    part_num = device.group(1)
            else:
                _, part_num, date, time, _ = re.split('\\s+', line)
    except Exception:
        return
    if part_num is None:
        return
    csv_reader = csv.DictReader(csv_file, skipinitialspace=True)
    for index, row in enumerate(csv_reader):
        try:
            if row['Pin'] == '':
                break
        except KeyError:
            return
        pin = copy.copy(DEFAULT_PIN)
        pin.index = index
        pin.name = fix_pin_data(row['Pin Name'], part_num)
        pin.num = fix_pin_data(row['Pin'], part_num)
        pin.unit = fix_pin_data(row['Bank'], part_num)
        DEFAULT_PIN_TYPE = 'input'
        PIN_TYPE_PREFIXES = [('VCC', 'power_in'), ('GND', 'power_in'), ('IO_', 'bidirectional'), ('DONE', 'output'), ('VREF[PN]_', 'input'), ('TCK', 'input'), ('TDI', 'input'), ('TDO', 'output'), ('TMS', 'input'), ('CCLK', 'input'), ('M0', 'input'), ('M1', 'input'), ('M2', 'input'), ('INIT_B', 'input'), ('PROG', 'input'), ('NC', 'no_connect'), ('VP_', 'input'), ('VN_', 'input'), ('DXP_', 'passive'), ('DXN_', 'passive'), ('CFGBVS_', 'input'), ('MGTZ?REFCLK[0-9]+[NP]_', 'input'), ('MGTZ_OBS_CLK_[PN]_', 'input'), ('MGT[ZPHX]TX[NP][0-9]+_', 'output'), ('MGT[ZPHX]RX[NP][0-9]+_', 'input'), ('MGTAVTTRCAL_', 'passive'), ('MGTRREF_', 'passive'), ('MGTVCCAUX_?', 'power_in'), ('MGTAVTT_?', 'power_in'), ('MGTZ_THERM_IN_', 'input'), ('MGTZ_THERM_OUT_', 'input'), ('MGTZ?A(VCC|GND)_?', 'power_in'), ('MGTZVCC[LH]_', 'power_in'), ('MGTZ_SENSE_(A?VCC|A?GND)[LH]?_', 'power_in'), ('RSVD(VCC[1-3]|GND)', 'power_in'), ('PS_CLK_', 'input'), ('PS_POR_B', 'input'), ('PS_SRST_B', 'input'), ('PS_DDR_CK[PN]_', 'output'), ('PS_DDR_CKE_', 'output'), ('PS_DDR_CS_B_', 'output'), ('PS_DDR_RAS_B_', 'output'), ('PS_DDR_CAS_B_', 'output'), ('PS_DDR_WE_B_', 'output'), ('PS_DDR_BA[0-9]+_', 'output'), ('PS_DDR_A[0-9]+_', 'output'), ('PS_DDR_ODT_', 'output'), ('PS_DDR_DRST_B_', 'output'), ('PS_DDR_DQ[0-9]+_', 'bidirectional'), ('PS_DDR_DM[0-9]+_', 'output'), ('PS_DDR_DQS_[PN][0-9]+_', 'bidirectional'), ('PS_DDR_VR[PN]_', 'power_out'), ('PS_DDR_VREF[0-9]+_', 'power_in'), ('PS_MIO_VREF_', 'power_in'), ('PS_MIO[0-9]+_', 'bidirectional')]
        for prefix, typ in PIN_TYPE_PREFIXES:
            if re.match(prefix, pin.name, re.IGNORECASE):
                pin.type = typ
                break
        else:
            issue('No match for {} on {}, assigning as {}'.format(pin.name, part_num[:4], DEFAULT_PIN_TYPE))
            pin.type = DEFAULT_PIN_TYPE
        pin.type = fix_pin_data(pin.type, part_num)
        pin_data[pin.unit][pin.side][pin.name].append(pin)
    yield (part_num, 'U', '', '', '', part_num, pin_data)",part_data_file_type == '.xlsx',268,part_data_file_type == '.csv',False,90.36020036098445,N/A
"def xilinx6v_reader(part_data_file, part_data_file_name, part_data_file_type='.txt'):
    """"""Extract the pin data from a Xilinx Virtex-6 TXT file and return a dictionary of pin data.""""""
<mask>:
        part_data_file = convert_xlsx_to_csv(part_data_file)
    txt_file = part_data_file
    pin_data = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))
    part_num = txt_file.readline().split()[1]
    _ = txt_file.readline()
    pin_list = txt_file.readlines()
    for index, line in enumerate(pin_list, 4):
        pin = copy.copy(DEFAULT_PIN)
        pin.index = index
        fields = line.split()
        fields = [fix_pin_data(d, part_num) for d in fields]
        if len(fields) == 0:
            break
        pin.num = fields[0]
        if fields[1].upper() == 'NOPAD/UNCONNECTED':
            pin.unit = 'NA'
            pin.name = 'NC'
        else:
            pin.unit = fields[1]
            pin.name = fields[2]
        DEFAULT_PIN_TYPE = 'input'
        PIN_TYPE_PREFIXES = [('VCC', 'power_in'), ('GND', 'power_in'), ('IO_', 'bidirectional'), ('VREF[PN]_', 'input'), ('NC', 'no_connect'), ('VP_', 'input'), ('VN_', 'input'), ('DXP_', 'passive'), ('DXN_', 'passive'), ('CCLK', 'input'), ('CSI_B', 'input'), ('DIN', 'input'), ('DOUT_BUSY', 'output'), ('HSWAPEN', 'input'), ('RDWR_B', 'input'), ('M0', 'input'), ('M1', 'input'), ('M2', 'input'), ('INIT_B', 'input'), ('PROGRAM_B', 'input'), ('DONE', 'output'), ('TCK', 'input'), ('TDI', 'input'), ('TDO', 'output'), ('TMS', 'input'), ('VFS', 'power_in'), ('RSVD', 'nc'), ('VREF[NP]', 'power_in'), ('VBATT', 'power_in'), ('A(VDD|VSS)_', 'power_in'), ('MGTA(VCC|VTT)', 'power_in'), ('MGTHA(VCC|GND|VTT)', 'power_in'), ('MGTRBIAS_', 'passive'), ('MGTREFCLK[0-9]?[NP]_', 'input'), ('MGTRX[NP][0-9]+_', 'input'), ('MGTTX[NP][0-9]+_', 'output'), ('MGTRREF_', 'passive')]
        for prefix, typ in PIN_TYPE_PREFIXES:
            if re.match(prefix, pin.name, re.IGNORECASE):
                pin.type = typ
                break
        else:
            issue('No match for {} on {}, assigning as {}'.format(pin.name, part_num[:4], DEFAULT_PIN_TYPE))
            pin.type = DEFAULT_PIN_TYPE
        pin_data[pin.unit][pin.side][pin.name].append(pin)
    yield (part_num, 'U', '', '', '', part_num, pin_data)",part_data_file_type == '.xlsx',206,part_data_file_type == '.txt',False,90.36020036098445,N/A
"def generic_reader(part_data_file, part_data_file_name, part_data_file_type):
    """"""Extract pin data from a CSV/text/Excel file and return a dictionary of pin data.
    The file contains one or more groups of rows formatted as follows:
        A row with a part info fields containing the part number, prefix, footprint id, etc.
        Zero or more blank rows.
        A row containing the column headers:
            'Pin', 'Unit', 'Type', 'Style', 'Side', and 'Name'.
            (Only 'Pin' and 'Name' are required. The order of
            the columns is not important.)
        Each succeeding row should contain:
            The 'Pin' column should contain the pin number.
            The 'Unit' column specifies the bank or unit number for the pin.
            The 'Type' column specifies the pin type (input, output,...).
            The 'Style' column specifies the pin's schematic style.
            The 'Side' column specifies the side of the symbol the pin is on.
            The 'Name' column contains the pin name.
        A blank row terminates the pin data for the part and begins
        a new group of rows for another part.
    """"""
<mask>:
        part_data_file = convert_xlsx_to_csv(part_data_file)
    while True:
        pin_data = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))
        csv_reader = csv.reader(part_data_file, skipinitialspace=True)
        part_num, part_ref_prefix, part_footprint, part_manf_num, part_datasheet, part_desc = get_part_info(csv_reader)
        if part_num is None:
            break
        headers = get_nonblank_row(csv_reader)
        headers = clean_headers(headers)
        for index, row in enumerate(part_data_file):
            if len(row.strip()) == 0:
                break
            dictreader = csv.DictReader([row], headers, skipinitialspace=True)
            row_dict = next(dictreader)
            if num_row_elements(list(row_dict.values())) == 0:
                break
            pin = copy.copy(DEFAULT_PIN)
            pin.index = index
            for c, a in list(COLUMN_NAMES.items()):
                try:
                    setattr(pin, a, fix_pin_data(row_dict[c], part_num))
                except KeyError:
                    pass
            if pin.num is None:
                issue('ERROR: No pin number on row {index} of {part_num}'.format(index=index, part_num=part_num), level='error')
            if not pin.side:
                pin.side = DEFAULT_PIN.side
            pin_data[pin.unit][pin.side.lower()][pin.name].append(pin)
        yield (part_num, part_ref_prefix, part_footprint, part_manf_num, part_datasheet, part_desc, pin_data)
    part_data_file.close()",part_data_file_type == '.xlsx',267,part_data_file_type == 'xlsx',False,72.05745450576255,N/A
"def get_nonblank_row(csv_reader):
    """"""Return the first non-blank row encountered from the current point in a CSV file.""""""
    for row in csv_reader:
<mask>:
            return row
    return []",num_row_elements(row) > 0,25,row.strip() == '',False,5.660233915657916,N/A
"def get_part_info(csv_reader):
    """"""Get the part number, ref prefix, footprint, MPN, datasheet link, and description from a row of the CSV file.""""""
    part_num, part_ref_prefix, part_footprint, part_manf_num, part_datasheet, part_desc = list(get_nonblank_row(csv_reader) + [None] * 6)[:6]
<mask>:
        part_ref_prefix = 'U'
    if part_num and part_num.lower() in list(COLUMN_NAMES.keys()):
        issue('Row with part number is missing in CSV file.', 'error')
    return (part_num, part_ref_prefix, part_footprint, part_manf_num, part_datasheet, part_desc)","part_ref_prefix in (None, '', ' ')",60,part_ref_prefix == '',False,25.55891661822957,N/A
"def find_closest_match(name, name_dict, fuzzy_match, threshold=0.0):
    """"""Approximate matching subroutine""""""
    scrubber = re.compile('[\\W.]+')
    name = scrubber.sub('', name).lower()
<mask>:
        try:
            return name_dict[name]
        except KeyError:
            issue(""Can't find match of '{name}' among allowed substitutions."".format(**locals()))
            return name
    match = difflib.get_close_matches(name, list(name_dict.keys()), 1, threshold)[0]
    return name_dict[match]",fuzzy_match == False,39,fuzzy_match,False,36.78794411714425,N/A
"def issue(msg, level='warning'):
<mask>:
        print('Warning: {}'.format(msg))
    elif level == 'error':
        print('ERROR: {}'.format(msg))
        raise Exception('Unrecoverable error')
    else:
        print(msg)",level == 'warning',17,level == 'warning',True,100.00000000000004,N/A
"def fix_pin_data(pin_data, part_num):
    """"""Fix common errors in pin data.""""""
    try:
        fixed_pin_data = pin_data.strip()
<mask>:
            fixed_pin_data = re.sub('\\s', '_', fixed_pin_data)
            issue(""Replaced whitespace with '_' in pin '{pin_data}' of part {part_num}."".format(**locals()))
    except Exception as e:
        issue(""Something went wrong with pin '{pin_data}' of part {part_num}: {e}"".format(**locals()))
        fixed_pin_data = pin_data
    return fixed_pin_data","re.search('\\s', fixed_pin_data) is not None",48,fixed_pin_data.startswith('_'),False,23.549928338544667,N/A
"def xilinx6s_reader(part_data_file, part_data_file_name, part_data_file_type='.txt'):
    """"""Extract the pin data from a Xilinx Spartan-6 TXT file and return a dictionary of pin data.""""""
<mask>:
        part_data_file = convert_xlsx_to_csv(part_data_file)
    txt_file = part_data_file
    pin_data = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))
    part_num = txt_file.readline().split()[1]
    _ = txt_file.readline()
    _ = txt_file.readline()
    _ = txt_file.readline()
    pin_list = txt_file.readlines()
    for index, line in enumerate(pin_list, 4):
        pin = copy.copy(DEFAULT_PIN)
        pin.index = index
        fields = line.split()
        fields = [fix_pin_data(d, part_num) for d in fields]
        if len(fields) == 0:
            break
        pin.num = fields[0]
        if fields[1].upper() == 'NOPAD/UNCONNECTED':
            pin.unit = 'NA'
            pin.name = 'NC'
        else:
            pin.unit = fields[1]
            pin.name = fields[3]
        DEFAULT_PIN_TYPE = 'input'
        PIN_TYPE_PREFIXES = [('CMPCS_B', 'input'), ('DONE', 'output'), ('VCC', 'power_in'), ('GND', 'power_in'), ('IO_', 'bidirectional'), ('MGTAVCC', 'power_in'), ('MGTAVTTRCAL_', 'passive'), ('MGTREFCLK[0-9]?[NP]_', 'input'), ('MGTRX[NP][0-9]+_', 'input'), ('MGTRREF_', 'passive'), ('MGTAVTT[RT]_?', 'power_in'), ('MGTTX[NP][0-9]+_', 'output'), ('NC', 'no_connect'), ('PROGRAM_B', 'input'), ('RFUSE', 'input'), ('SUSPEND', 'input'), ('TCK', 'input'), ('TDI', 'input'), ('TDO', 'output'), ('TMS', 'input'), ('VFS', 'power_in'), ('VBATT', 'power_in')]
        for prefix, typ in PIN_TYPE_PREFIXES:
            if re.match(prefix, pin.name, re.IGNORECASE):
                pin.type = typ
                break
        else:
            issue('No match for {} on {}, assigning as {}'.format(pin.name, part_num[:4], DEFAULT_PIN_TYPE))
            pin.type = DEFAULT_PIN_TYPE
        pin_data[pin.unit][pin.side][pin.name].append(pin)
    yield (part_num, 'U', '', '', '', part_num, pin_data)",part_data_file_type == '.xlsx',182,part_data_file_type == '.txt',False,90.36020036098445,N/A
"def _parse_lib_V6(lib_filename):
    """"""
    Return an object storing the contents of a KiCad V6 symbol library.
    """"""

    class SymbolLib(dict):

        def __init__(self, *args, **kwargs):
            super().__init__(self, *args, **kwargs)

        def append(self, part):
            self[part.name] = part

        @property
        def parts(self):
            return self.values()

    class Part:

        def __init__(self, name, ref_id, pins):
            self.name = name
            self.ref_id = ref_id
            self.pins = pins[:]

    class Pin:

        def __init__(self, unit_id='', name='', number='', type='', style='', orientation=0, hide=False):
            self.unit = unit_id
            self.name = name
            self.num = number
            self.type = type
            self.style = style
            self.orientation = orientation
            self.hide = hide
    lib = sx.load(open(lib_filename, 'r'))
    symbols = {item[1]: item[2:] for item in lib[1:] if item[0].value().lower() == 'symbol'}
    symbol_lib = SymbolLib()
    for sym_name, sym_data in symbols.items():
        properties = {}
        pins = []
        for item in sym_data:
<mask>:
                parent_symbol = symbol_lib[item[1]]
                properties['Value'] = parent_symbol.name
                properties['Reference'] = parent_symbol.ref_id
                pins.extend(parent_symbol.pins)
        properties.update({item[1]: item[2] for item in sym_data if item[0].value().lower() == 'property'})
        assert sym_name == properties['Value']
        units = {item[1]: item[2:] for item in sym_data if item[0].value().lower() == 'symbol'}
        for unit_id, unit_data in units.items():
            unit_pins = [item[1:] for item in unit_data if item[0].value().lower() == 'pin']
            for pin_data in unit_pins:
                pin = Pin()
                pin.unit = unit_id
                pin.type = pin_data[0].value().lower()
                pin.style = pin_data[1].value().lower()
                pin.hide = False
                for data in pin_data[2:]:
                    if not isinstance(data, list):
                        if data.value().lower() == 'hide':
                            pin.hide = True
                        continue
                    else:
                        label = data[0].value().lower()
                        if label == 'at':
                            pin.orientation = data[3]
                        elif label == 'name':
                            pin.name = data[1]
                        elif label == 'number':
                            pin.num = data[1]
                pins.append(pin)
        symbol_lib.append(Part(properties['Value'], properties['Reference'], pins))
    return symbol_lib",item[0].value().lower() == 'extends',237,item[0].value().lower() == 'symbol',False,92.53911813809742,N/A
"def _gen_csv(parsed_lib):
    """"""Return multi-line CSV string for the parts in a parsed schematic library.""""""
    is_v5 = isinstance(parsed_lib, ParseResults)
    type_tbl = {'I': 'in', 'O': 'out', 'B': 'bidir', 'T': 'tri', 'P': 'passive', 'U': 'unspecified', 'W': 'pwr', 'w': 'pwr_out', 'C': 'open_collector', 'E': 'open_emitter', 'N': 'NC', 'input': 'in', 'output': 'out', 'bidirectional': 'bidir', 'tri_state': 'tri', 'passive': 'passive', 'free': 'free', 'unspecified': 'unspecified', 'power_in': 'pwr', 'power_out': 'pwr_out', 'open_collector': 'open_collector', 'open_emitter': 'open_emitter', 'no_connect': 'NC'}
    orientation_tbl = {'R': 'left', 'L': 'right', 'U': 'bottom', 'D': 'top', 0: 'left', 180: 'right', 90: 'bottom', 270: 'top'}
    style_tbl = {'': '', 'I': 'inv', 'C': 'clk', 'IC': 'inv_clk', 'L': 'input_low', 'CL': 'clk_low', 'V': 'output_low', 'F': 'falling_clk', 'X': 'non_logic', 'line': '', 'inverted': 'inv', 'clock': 'clk', 'inverted_clock': 'inv_clk', 'input_low': 'input_low', 'clock_low': 'clk_low', 'output_low': 'output_low', 'edge_clock_high': 'falling_clk', 'non_logic': 'non_logic'}
    csv = ''
    for part in parsed_lib.parts:
        csv += '{part.name},{part.ref_id},,,,,\n'.format(**locals())
        csv += 'Pin,Name,Type,Side,Unit,Style,Hidden\n'

        def zero_pad_nums(s):
            try:
                return re.sub('\\d+', lambda mtch: '0' * (8 - len(mtch.group(0))) + mtch.group(0), s)
            except TypeError:
                return s

        def num_key(pin):
            """"""Generate a key from a pin's number so they are sorted by position on the package.""""""
            return zero_pad_nums(pin.unit) + zero_pad_nums(pin.num)
        for p in sorted(part.pins, key=num_key):
<mask>:
                p['num'] = re.sub(',', ';', p.num)
                p['name'] = re.sub(',', ';', p.name)
                p['unit'] = re.sub(',', ';', p.unit)
            else:
                p.num = re.sub(',', ';', p.num)
                p.name = re.sub(',', ';', p.name)
                p.unit = re.sub(',', ';', p.unit)
            is_hidden = ''
            if is_v5:
                if p.style.find('N') != -1:
                    p['style'] = p.style.replace('N', '')
                    is_hidden = 'Y'
            elif p.hide:
                is_hidden = 'Y'
            csv += ','.join([p.num, p.name, type_tbl[p.type], orientation_tbl[p.orientation], p.unit, style_tbl[p.style], is_hidden]) + '\n'
        csv += ',,,,,,\n'
    csv += '\n'
    return csv",is_v5,253,part.type == 'P',False,0.0,N/A
"def main():
    parser = ap.ArgumentParser(description='Convert a KiCad schematic symbol library file into a CSV file for KiPart.')
    parser.add_argument('-v', '--version', action='version', version='kilib2csv ' + __version__)
    parser.add_argument('input_files', nargs='+', type=str, metavar='file.lib', help='KiCad schematic symbol library.')
    parser.add_argument('-o', '--output', nargs='?', type=str, metavar='file.csv', help='CSV file created from schematic library file.')
    parser.add_argument('-a', '--append', action='store_true', help='Append to an existing CSV file.')
    parser.add_argument('-w', '--overwrite', action='store_true', help='Allow overwriting of an existing CSV file.')
    args = parser.parse_args()
    check_file_exists = True
    for input_file in args.input_files:
<mask>:
            output_file = args.output or os.path.splitext(input_file)[0] + '.csv'
            if os.path.isfile(output_file):
                if args.overwrite:
                    csv = ''
                elif args.append:
                    csv = read_csv_file(output_file)
                else:
                    print('Output file {} already exists! Use the --overwrite option to replace it or the --append option to append to it.'.format(output_file))
                    sys.exit(1)
            else:
                csv = ''
        check_file_exists = not args.output
        file_ext = os.path.splitext(input_file)[-1]
        if input_file.endswith('.lib'):
            parsed_lib = _parse_lib_V5(input_file)
            csv += _gen_csv(parsed_lib)
        elif input_file.endswith('.kicad_sym'):
            parsed_lib = _parse_lib_V6(input_file)
            csv += _gen_csv(parsed_lib)
        else:
            continue
        if not args.output:
            with open(output_file, 'w') as out_fp:
                out_fp.write(csv)
    if args.output:
        with open(output_file, 'w') as out_fp:
            out_fp.write(csv)",check_file_exists or not args.output,161,input_file.endswith('.csv'),False,9.287528999566801,N/A
"def annotate_pins(unit_pins, annotation_style):
    """"""Annotate pin names to indicate special information.""""""
    for name, pins in unit_pins:
        name_suffix = SINGLE_PIN_SUFFIX
<mask>:
            if annotation_style == 'count':
                name_suffix = '[{}]'.format(len(pins))
            elif annotation_style == 'range':
                name_suffix = '[{}:0]'.format(len(pins) - 1)
        for pin in pins:
            pin.name += name_suffix",len(pins) > 1,42,"name_suffix not in ['count', 'range']",False,0.0,N/A
"def count_pin_slots(unit_pins):
    """"""Count the number of vertical pin slots needed for a column of pins.""""""
    num_slots = 0
    pin_num_len = 0
    for name, pins in unit_pins:
        pin_spacer = 0
        pin_num_len = 0
        for pin in pins:
            pin_num, pin_spacer = get_pin_num_and_spacer(pin)
            pin_num_len = max(pin_num_len, len(pin_num))
        num_slots += pin_spacer
<mask>:
            num_slots += 1
    return num_slots",pin_num_len > 0,53,pin_num_len == pin_num_len,False,31.702331385234313,N/A
"def pins_bbox(unit_pins, pin_length):
    """"""Return the bounding box of a column of pins and their names.""""""
<mask>:
        return [[XO, YO], [XO, YO]]
    width = max((len(up[1][0].name) for up in unit_pins))
    width = width * PIN_NAME_SIZE + pin_length + 2 * PIN_NAME_OFFSET
    width = math.ceil(float(width) / PIN_SPACING) * PIN_SPACING
    height = count_pin_slots(unit_pins) * PIN_SPACING
    return [[XO, YO + PIN_SPACING], [XO + width, YO - height]]",len(unit_pins) == 0,62,not unit_pins,False,17.035677145427364,N/A
"def balance_bboxes(bboxes):
    """"""
    Make the symbol more balanced by adjusting the bounding boxes of the pins on each side.
    """"""
    X, Y = (0, 1)
    non_empty_sides = []
    for side, bbox in bboxes.items():
<mask>:
            non_empty_sides.append(side)
    num_sides = len(non_empty_sides)
    if num_sides == 4:
        lr_bbox = find_bbox_bbox(bboxes['left'], bboxes['right'])
        lr_hgt = abs(lr_bbox[0][Y] - lr_bbox[1][Y])
        tb_bbox = find_bbox_bbox(bboxes['top'], bboxes['bottom'])
        tb_hgt = abs(tb_bbox[0][Y] - tb_bbox[1][Y])
        if 0.75 * tb_hgt <= lr_hgt <= 1.33 * tb_hgt:
            bal_bbox = find_bbox_bbox(*list(bboxes.values()))
            for side in bboxes:
                bboxes[side] = deepcopy(bal_bbox)
        else:
            bboxes['left'] = deepcopy(lr_bbox)
            bboxes['right'] = deepcopy(lr_bbox)
            bboxes['top'] = deepcopy(tb_bbox)
            bboxes['bottom'] = deepcopy(tb_bbox)
    elif num_sides == 3:
        if 'top' in non_empty_sides and 'bottom' in non_empty_sides:
            bal_bbox = find_bbox_bbox(bboxes['top'], bboxes['bottom'])
            bboxes['top'] = deepcopy(bal_bbox)
            bboxes['bottom'] = deepcopy(bal_bbox)
        elif 'left' in non_empty_sides and 'right' in non_empty_sides:
            bal_bbox = find_bbox_bbox(bboxes['left'], bboxes['right'])
            bboxes['left'] = deepcopy(bal_bbox)
            bboxes['right'] = deepcopy(bal_bbox)
    elif num_sides == 2:
        if 'left' in non_empty_sides and 'right' in non_empty_sides:
            bal_bbox = find_bbox_bbox(bboxes['left'], bboxes['right'])
            bboxes['left'][0][Y] = bal_bbox[0][Y]
            bboxes['left'][1][Y] = bal_bbox[1][Y]
            bboxes['right'][0][Y] = bal_bbox[0][Y]
            bboxes['right'][1][Y] = bal_bbox[1][Y]
        elif 'top' in non_empty_sides and 'bottom' in non_empty_sides:
            bal_bbox = find_bbox_bbox(bboxes['top'], bboxes['bottom'])
            bboxes['top'][0][Y] = bal_bbox[0][Y]
            bboxes['top'][1][Y] = bal_bbox[1][Y]
            bboxes['bottom'][0][Y] = bal_bbox[0][Y]
            bboxes['bottom'][1][Y] = bal_bbox[1][Y]",bbox[0][X] != bbox[1][X] and bbox[0][Y] != bbox[1][Y],185,side not in non_empty_sides,False,0.0,N/A
"def draw_pins(unit_num, unit_pins, bbox, transform, side, push, fuzzy_match, pin_length):
    """"""Draw a column of pins rotated/translated by the transform matrix.""""""
    pin_defn = ''
    Y = 1
    pins_bb = pins_bbox(unit_pins, pin_length)
    height_offset = abs(bbox[0][Y] - bbox[1][Y]) - abs(pins_bb[0][Y] - pins_bb[1][Y])
    push = min(max(0.0, push), 1.0)
<mask>:
        push = 1.0 - push
    height_offset *= push
    height_offset -= height_offset % PIN_SPACING
    x = XO
    y = YO - height_offset
    for name, pins in unit_pins:
        pin_spacer = 0
        pin_num_len = 0
        for pin in pins:
            pin_num, pin_spacer = get_pin_num_and_spacer(pin)
            pin_num_len = max(pin_num_len, len(pin_num))
        y -= pin_spacer * PIN_SPACING
        if pin_num_len == 0:
            continue
        draw_x, draw_y = transform * (x, y)
        pin_type = find_closest_match(pins[0].type, PIN_TYPES, fuzzy_match)
        pin_style = find_closest_match(pins[0].style, PIN_STYLES, fuzzy_match)
        pin_side = find_closest_match(pins[0].side, PIN_ORIENTATIONS, fuzzy_match)
        if pins[0].hidden.lower().strip() in ['y', 'yes', 't', 'true', '1']:
            pin_style = 'N' + pin_style
        num_size = PIN_NUM_SIZE if pin_type != 'N' else 0
        for index, pin in enumerate(pins):
            pin_num, _ = get_pin_num_and_spacer(pin)
            pin_defn += PIN.format(name=pin.name, num=pin_num, x=int(draw_x), y=int(draw_y), length=pin_length, orientation=pin_side, num_sz=num_size, name_sz=PIN_NAME_SIZE, unit_num=unit_num, pin_type=pin_type, pin_style=pin_style)
            pin_style = 'N' + pin_style.lstrip('N')
            if pin_type in 'wW':
                pin_type = 'P'
            if pin_type == 'N':
                draw_x, draw_y = transform * (x + index, y + 1)
        y -= PIN_SPACING
    return pin_defn","side in ('right', 'top')",196,push < 1.0,False,0.0,N/A
"def test_original_holiday(self):
    """"""
        独自の休み
        """"""

    class TestHoliday(jpholiday.OriginalHolidayCheckerInterface):

        def is_holiday(self, date: datetime.date) -> bool:
<mask>:
                return True
            if date == datetime.date(2020, 2, 9):
                return True
            return False

        def holiday_name(self, date: datetime.date) -> str:
            return '特別休暇'
    jpholiday.register(TestHoliday())
    self.assertEqual(jpholiday.is_holiday_name(datetime.date(2020, 2, 3)), '特別休暇')
    self.assertEqual(jpholiday.is_holiday(datetime.date(2020, 2, 4)), False)
    self.assertEqual(jpholiday.is_holiday_name(datetime.date(2020, 2, 5)), '特別休暇')
    self.assertEqual(jpholiday.is_holiday_name(datetime.date(2020, 2, 9)), '特別休暇')
    self.assertEqual(jpholiday.is_holiday(datetime.date(2020, 2, 10)), False)
    jpholiday.unregister(TestHoliday())
    jpholiday.register(TestHoliday())
    self.assertEqual(jpholiday.is_holiday_name(datetime.date(2020, 2, 3)), '特別休暇')
    self.assertEqual(jpholiday.is_holiday(datetime.date(2020, 2, 4)), False)
    self.assertEqual(jpholiday.is_holiday_name(datetime.date(2020, 2, 5)), '特別休暇')
    self.assertEqual(jpholiday.is_holiday_name(datetime.date(2020, 2, 9)), '特別休暇')
    self.assertEqual(jpholiday.is_holiday(datetime.date(2020, 2, 10)), False)
    jpholiday.unregister(TestHoliday())","date == datetime.date(2020, 2, 3) or date == datetime.date(2020, 2, 5)",78,"date == datetime.date(2020, 2, 8)",False,28.69441433223828,N/A
"def is_holiday_name(date: Union[datetime.date, datetime.datetime]) -> Optional[str]:
    """"""
    その日の祝日名を返します。
    """"""
    result = new_api.holidays(date)
<mask>:
        return None
    return result[0].name",len(result) == 0,17,result[0].name is None,False,6.567274736060395,N/A
"def holidays(self, date: Union[datetime.date, datetime.datetime]) -> list[Holiday]:
    """"""
        その日の祝日名を返します。
        """"""
    date = self._to_date(date)
    cache = self._cache.get(date)
<mask>:
        return cache
    holidays = []
    for holiday in self.registry.checkers():
        if holiday.is_holiday(date):
            holidays.append(Holiday(date, holiday.holiday_name(date)))
    self._cache.set(date, holidays)
    return holidays",cache is not None,34,cache,False,4.9787068367863965,N/A
"def is_holiday(self, date: Union[datetime.date, datetime.datetime]) -> bool:
    """"""
        その日が祝日かどうかを返します。
        """"""
    date = self._to_date(date)
<mask>:
        return True
    return False",len(self.holidays(date)) != 0,18,date.month == self.month,False,7.966506956353643,N/A
"def iter_year_holidays(self, year: int) -> Iterator[Holiday]:
    """"""
        指定された年の祝日日、祝日名をイテレーターで返します。
        """"""
    date = datetime.date(year, 1, 1)
    while date.year == year:
        result = self.holidays(date)
<mask>:
            for holiday in result:
                yield holiday
        date = date + datetime.timedelta(days=1)",result,33,result,True,100.00000000000004,N/A
"def iter_month_holidays(self, year: int, month: int) -> Iterator[Holiday]:
    """"""
        指定された月の祝日日、祝日名をイテレーターで返します。
        """"""
    date = datetime.date(year, month, 1)
    while date.month == month:
        result = self.holidays(date)
<mask>:
            for holiday in result:
                yield holiday
        date = date + datetime.timedelta(days=1)",result,35,result,True,100.00000000000004,N/A
"def iter_between(self, start_date: Union[datetime.date, datetime.datetime], end_date: Union[datetime.date, datetime.datetime]) -> Iterator[Holiday]:
    """"""
        指定された期間の祝日日、祝日名をイテレーターで返します。
        """"""
    current_date = self._to_date(start_date)
    end_date = self._to_date(end_date)
    while current_date <= end_date:
        result = self.holidays(current_date)
<mask>:
            for holiday in result:
                yield holiday
        current_date = current_date + datetime.timedelta(days=1)",result,38,result,True,100.00000000000004,N/A
"def _week_day(date, week, weekday):
    """"""
    特定の月の第1月曜日などを返します。
    """"""
<mask>:
        return None
    if weekday < 1 or weekday > 7:
        return None
    lines = calendar.monthcalendar(date.year, date.month)
    days = []
    for line in lines:
        if line[weekday - 1] == 0:
            continue
        days.append(line[weekday - 1])
    return datetime.date(date.year, date.month, days[week - 1])",week < 1 or week > 5,47,date is None,False,0.0,N/A
"def is_holiday(self, date: datetime.date) -> bool:
<mask>:
        return True
    elif date.year >= 2000 and date.month == 1 and (date.day == utils._week_day(date, 2, 1).day):
        return True
    return False",date.year <= 1999 and date.month == 1 and (date.day == 15),27,date.year == self.year and date.month == self.month,False,29.69339818653496,N/A
"def is_holiday(self, date: datetime.date) -> bool:
<mask>:
        return True
    elif date.year in range(1989, 2018 + 1) and date.month == 12 and (date.day == 23):
        return True
    elif date.year >= 2020 and date.month == 2 and (date.day == 23):
        return True
    return False","date.year in range(1948, 1988 + 1) and date.month == 4 and (date.day == 29)",42,date.year == 1989,False,0.9711282555136248,N/A
"def is_holiday(self, date: datetime.date) -> bool:
<mask>:
        return True
    return False",date.month == 3 and date.day == self._vernal_equinox_day(date.year),11,self.holiday_date.compare_and_swap(date),False,5.217509875878342,N/A
"@staticmethod
def _vernal_equinox_day(year):
    """"""
        春季皇霊祭: 1879-1947
        春分の日: 1948
        春分の日の日付を返します。
        http://mt-soft.sakura.ne.jp/kyozai/excel_high/200_jissen_kiso/60_syunbun.htm
        """"""
<mask>:
        return 0
    if year >= 1851 and year <= 1899:
        i = 19.8277
    elif year >= 1900 and year <= 1979:
        i = 20.8357
    elif year >= 1980 and year <= 2099:
        i = 20.8431
    elif year >= 2100 and year <= 2150:
        i = 21.851
    else:
        i = 0
    return math.floor(i + 0.242194 * (year - 1980) - math.floor((year - 1980) / 4))",year <= 1948,76,year == 0,False,18.99589214128981,N/A
"def is_holiday(self, date: datetime.date) -> bool:
<mask>:
        return True
    elif date.year >= 2007 and date.month == 5 and (date.day == 4):
        return True
    return False","date.year in range(1989, 2006 + 1) and date.month == 4 and (date.day == 29)",25,date.year == 0,False,0.9711282555136248,N/A
"@register('process_form_submission')
def email_submission(instance, form):
    """""" Send an email with the submission. """"""
<mask>:
        return
    addresses = [instance.advanced_settings.to_address]
    content = ['Please see below submission\n']
    from_address = settings.DEFAULT_FROM_EMAIL
    subject = 'New Form Submission : %s' % instance.title
    for field, value in form.cleaned_data.items():
        if field in form.files:
            count = len(form.files.getlist(field))
            value = '{} file{}'.format(count, pluralize(count))
        elif isinstance(value, list):
            value = ', '.join(value)
        content.append('{}: {}'.format(field, value))
    content = '\n'.join(content)
    email = EmailMessage(subject=subject, body=content, from_email=from_address, to=addresses)
    for field in form.files:
        for file in form.files.getlist(field):
            file.seek(0)
            email.attach(file.name, file.read(), file.content_type)
    email.send(fail_silently=True)","not hasattr(instance, 'advanced_settings')",84,instance.email_submitted,False,5.70796903405875,N/A
"def assertModelField(self, field, expected_class, null=False, blank=False, default=None):
    self.assertEqual(field.__class__, expected_class)
    self.assertEqual(field.null, null)
<mask>:
        self.assertEqual(field.blank, blank)
    if default:
        self.assertEqual(field.default, default)",expected_class not in self._non_blankable_fields,18,blank,False,0.0,N/A
"def assertModelPKField(self, field, rel_to, on_delete, null=False, blank=False, related_name=None):
    self.assertEqual(field.__class__, models.ForeignKey)
    self.assertEqual(field.remote_field.model, rel_to)
    self.assertEqual(field.remote_field.on_delete, on_delete)
    self.assertEqual(field.null, null)
    self.assertEqual(field.blank, blank)
<mask>:
        self.assertEqual(field.remote_field.related_name, related_name)",related_name,21,related_name,True,100.00000000000004,N/A
"def __init__(self, local_blocks=None, **kwargs):
    self._constructor_kwargs = kwargs
    super(blocks.BaseStreamBlock, self).__init__()
    self._child_blocks = self.base_blocks.copy()
    for name, field_class in get_fields().items():
<mask>:
            raise ImproperlyConfigured(""'%s' must be a subclass of '%s'"" % (field_class, BaseField))
        block = field_class().get_form_block()
        block.set_name(name)
        self._child_blocks[name] = block
    self._dependencies = self._child_blocks.values()","not issubclass(field_class, BaseField)",39,"not issubclass(field_class, BaseField)",True,100.00000000000004,N/A
"def register(hook_name, fn=None, order=0):
    """"""
    Register hook for ``hook_name``. Can be used as a decorator::
        @register('hook_name')
        def my_hook(...):
            pass
    or as a function call::
        def my_hook(...):
            pass
        register('hook_name', my_hook)
    """"""
<mask>:

        def decorator(fn):
            register(hook_name, fn, order=order)
            return fn
        return decorator
    if hook_name not in _hooks:
        _hooks[hook_name] = []
    _hooks[hook_name].append((fn, order))",fn is None,50,fn is None,True,100.00000000000004,N/A
"def get_hooks(hook_name):
    """"""Return the hooks function sorted by their order.""""""
    search_for_hooks()
    hooks = _hooks.get(hook_name, [])
    hooks = sorted(hooks, key=itemgetter(1))
    fncs = []
    builtin_hook_modules = ['wagtailstreamforms.wagtailstreamforms_hooks']
    builtin_enabled = get_setting('ENABLE_BUILTIN_HOOKS')
    for fn, _ in hooks:
<mask>:
            continue
        fncs.append(fn)
    return fncs",fn.__module__ in builtin_hook_modules and (not builtin_enabled),38,fn in builtin_hook_modules or builtin_enabled,False,26.624666746611076,N/A
"def register(field_name, cls=None):
    """"""
    Register field for ``field_name``. Can be used as a decorator::
        @register('singleline')
        class SingleLineTextField(BaseField):
            field_class = django.forms.CharField
    or as a function call::
        class SingleLineTextField(BaseField):
            field_class = django.forms.CharField
        register('singleline', SingleLineTextField)
    """"""
<mask>:

        def decorator(cls):
            register(field_name, cls)
            return cls
        return decorator
    _fields[field_name] = cls",cls is None,45,cls is None,True,100.00000000000004,N/A
"def get_formfield(self, block_value):
    """"""
        Get the form field. Its unlikely you will need to override this.

        :param block_value: The StreamValue for this field from the StreamField
        :return: An instance of a form field class, ie ``django.forms.CharField(**options)``
        """"""
<mask>:
        raise NotImplementedError('must provide a cls.field_class')
    options = self.get_options(block_value)
    if self.widget:
        return self.field_class(widget=self.widget, **options)
    return self.field_class(**options)",not self.field_class,53,"not hasattr(self, 'field_class')",False,6.27465531099474,N/A
"def get_db_prep_value(self, value, connection=None, prepared=False):
<mask>:
        return value
    elif isinstance(value, list):
        return ','.join(value)","isinstance(value, str)",13,"isinstance(value, str)",True,100.00000000000004,N/A
"def from_db_value(self, value, expression, connection, context=None, *args, **kwargs):
<mask>:
        return []
    return value.split(',')",value is None or value == '',13,value is None,False,18.887560283756194,N/A
"def get_options(self, block_value):
    options = super().get_options(block_value)
    choices: List[Tuple[str, str]] = []
    for c in block_value.get('choices'):
<mask>:
            choices.append((c['value'].strip(), c['value'].strip()))
        else:
            choices.append((c.strip(), c.strip()))
    if block_value.get('empty_label'):
        choices.insert(0, ('', block_value.get('empty_label')))
    options.update({'choices': choices})
    return options","isinstance(c, dict) and c.get('value')",30,c.get('value'),False,31.140322391459787,N/A
"def get_options(self, block_value):
    options = super().get_options(block_value)
    choices: List[Tuple[str, str]] = []
    for c in block_value.get('choices'):
<mask>:
            choices.append((c['value'].strip(), c['value'].strip()))
        else:
            choices.append((c.strip(), c.strip()))
    options.update({'choices': choices})
    return options","isinstance(c, dict) and c.get('value')",25,c.get('value'),False,31.140322391459787,N/A
"def clean(self, data, initial=None):
    single_file_clean = super().clean
<mask>:
        result = [single_file_clean(d, initial) for d in data]
    else:
        result = single_file_clean(data, initial)
    return result","data and isinstance(data, (list, tuple))",23,"isinstance(data, list)",False,20.687381245863396,N/A
"@property
def formfields(self):
    """"""Return a list of form fields from the registered fields.""""""
    formfields = OrderedDict()
    registered_fields = get_fields()
    for field in self.fields:
        field_type = field.get('type')
        field_value = field.get('value')
<mask>:
            raise AttributeError('Could not find a registered field of type %s' % field_type)
        registered_cls = registered_fields[field_type]()
        field_name = registered_cls.get_formfield_name(field_value)
        field_cls = registered_cls.get_formfield(field_value)
        formfields[field_name] = field_cls
    formfields['form_id'] = forms.CharField(widget=forms.HiddenInput)
    formfields['form_reference'] = forms.CharField(widget=forms.HiddenInput)
    return formfields",field_type not in registered_fields,62,field_type not in registered_fields,True,100.00000000000004,N/A
"def get_action_url(self, action, *args, **kwargs):
<mask>:
        return reverse('wagtailstreamforms:streamforms_%s' % action, args=args, kwargs=kwargs)
    return super().get_action_url(action, *args, **kwargs)","action in ['advanced', 'copy', 'submissions']",16,self.is_stream_form_action(action),False,3.3864985683445354,N/A
"def get_buttons_for_obj(self, obj, exclude=None, classnames_add=None, classnames_exclude=None):
    buttons = super().get_buttons_for_obj(obj, exclude, classnames_add, classnames_exclude)
    pk = getattr(obj, self.opts.pk.attname)
    ph = self.permission_helper
    usr = self.request.user
<mask>:
        buttons.append(self.button(pk, 'advanced', _('Advanced'), _('Advanced settings'), classnames_add, classnames_exclude))
    if ph.user_can_create(usr):
        buttons.append(self.button(pk, 'copy', _('Copy'), _('Copy this form'), classnames_add, classnames_exclude))
    buttons.append(self.button(pk, 'submissions', _('Submissions'), _('Submissions of this form'), classnames_add, classnames_exclude))
    return buttons","SettingsModel and (ph.user_can_create(usr) or ph.user_can_edit_obj(usr, obj))",51,ph.user_can_update(usr),False,9.84216864560483,N/A
"@hooks.register('before_serve_page')
def process_form(page, request, *args, **kwargs):
    """"""Process the form if there is one, if not just continue.""""""
<mask>:
        return
    if request.method == 'POST':
        form_def = get_form_instance_from_request(request)
        if form_def:
            form = form_def.get_form(request.POST, request.FILES, page=page, user=request.user)
            context = page.get_context(request, *args, **kwargs)
            if form.is_valid():
                form_def.process_form_submission(form)
                if form_def.success_message:
                    messages.success(request, form_def.success_message, fail_silently=True)
                redirect_page = form_def.post_redirect_page or page
                return redirect(redirect_page.get_url(request), context=context)
            else:
                context.update({'invalid_stream_form_reference': form.data.get('form_reference'), 'invalid_stream_form': form})
                if form_def.error_message:
                    messages.error(request, form_def.error_message, fail_silently=True)
                return TemplateResponse(request, page.get_template(request, *args, **kwargs), context)",not get_setting('ENABLE_FORM_PROCESSING'),71,not page.is_submitted,False,4.194930905450255,N/A
"def render(self, value, context=None):
    form = value.get('form')
<mask>:
        self.meta.template = form.template_name
    else:
        self.meta.template = 'streamforms/non_existent_form.html'
    return super().render(value, context)",form,18,form,True,100.00000000000004,N/A
"def get_context(self, value, parent_context=None):
    context = super().get_context(value, parent_context)
    form = value.get('form')
    form_reference = value.get('form_reference')
<mask>:
        invalid_form_reference = context.get('invalid_stream_form_reference')
        invalid_form = context.get('invalid_stream_form')
        if invalid_form_reference and invalid_form and (invalid_form_reference == form_reference):
            context['form'] = invalid_form
        else:
            context['form'] = form.get_form(initial={'form_id': form.id, 'form_reference': form_reference})
    return context",form,41,form and form_reference,False,10.682175159905848,N/A
"def clean(self, value):
    result = super().clean(value)
<mask>:
        result['form_reference'] = uuid.uuid4()
    return result",not result.get('form_reference'),12,value,False,0.0,N/A
"def get_advanced_settings_model():
    """"""
    Returns the advanced form settings model if one is defined
    """"""
    model = get_setting('ADVANCED_SETTINGS_MODEL')
<mask>:
        return

    def raise_error(msg):
        setting = 'WAGTAILSTREAMFORMS_ADVANCED_SETTINGS_MODEL'
        raise ImproperlyConfigured('%s %s' % (setting, msg))
    try:
        model_class = apps.get_model(model, require_ready=False)
        if issubclass(model_class, AbstractFormSetting):
            return model_class
        raise_error(""must inherit from 'wagtailstreamforms.models.AbstractFormSetting'"")
    except ValueError:
        raise_error(""must be of the form 'app_label.model_name'"")
    except LookupError:
        raise_error(""refers to model '%s' that has not been installed"" % model)",not model,65,model is None,False,27.516060407455225,N/A
"def get_form_instance_from_request(request):
    """"""Get the form class from the request.""""""
    form_id = request.POST.get('form_id')
<mask>:
        try:
            return Form.objects.get(pk=int(form_id))
        except Form.DoesNotExist:
            pass
    return None",form_id and form_id.isdigit(),21,form_id,False,6.948345122280157,N/A
"def copy(self):
    """"""Copy this form and its fields.""""""
    form_copy = Form(site=self.site, title=self.title, slug=uuid.uuid4(), template_name=self.template_name, fields=self.fields, submit_button_text=self.submit_button_text, success_message=self.success_message, error_message=self.error_message, post_redirect_page=self.post_redirect_page, process_form_submission_hooks=self.process_form_submission_hooks)
    form_copy.save()
    SettingsModel = get_advanced_settings_model()
<mask>:
        try:
            advanced = SettingsModel.objects.get(form=self)
            advanced.pk = None
            advanced.form = form_copy
            advanced.save()
        except SettingsModel.DoesNotExist:
            pass
    return form_copy",SettingsModel,41,SettingsModel,True,100.00000000000004,N/A
"def get_data_fields(self):
    """"""Returns a list of tuples with (field_name, field_label).""""""
    data_fields = [('submit_time', _('Submission date'))]
    data_fields += [(get_slug_from_string(field['value']['label']), field['value']['label']) for field in self.get_form_fields()]
<mask>:
        data_fields += [('form_reference', _('Form reference'))]
    return data_fields","getattr(settings, 'WAGTAILSTREAMFORMS_SHOW_FORM_REFERENCE', False)",31,self.form_reference,False,1.7657516777206852,N/A
"def get_form_fields(self):
    """"""Returns the form field's stream data.""""""
<mask>:
        form_fields = self.fields.raw_data
    else:
        form_fields = self.fields.stream_data
    for fn in hooks.get_hooks('construct_submission_form_fields'):
        form_fields = fn(form_fields)
    return form_fields","WAGTAIL_VERSION >= (2, 12)",25,self.fields.stream_data is None,False,4.266331692956901,N/A
"def process_form_submission(self, form):
    """"""Runs each hook if selected in the form.""""""
    for fn in hooks.get_hooks('process_form_submission'):
<mask>:
            fn(self, form)",fn.__name__ in self.process_form_submission_hooks,18,fn.selected(),False,1.9381301343713229,N/A
"def dispatch(self, request, *args, **kwargs):
    self.object = self.get_object()
<mask>:
        raise PermissionDenied
    return super().dispatch(request, *args, **kwargs)",not self.permission_helper.user_can_list(self.request.user),15,not self.object.can_change_user(request),False,11.099363079156142,N/A
"def get(self, request, *args, **kwargs):
    self.filter_form = SelectDateForm(request.GET)
<mask>:
        return self.csv()
    return super().get(request, *args, **kwargs)",request.GET.get('action') == 'CSV',15,self.filter_form.validate_on_submit(),False,4.368583925857938,N/A
"def get_queryset(self):
    submission_class = self.object.get_submission_class()
    self.queryset = submission_class._default_manager.filter(form=self.object)
<mask>:
        date_from = self.filter_form.cleaned_data.get('date_from')
        date_to = self.filter_form.cleaned_data.get('date_to')
        if date_from:
            self.queryset = self.queryset.filter(submit_time__gte=date_from)
        if date_to:
            date_to += datetime.timedelta(days=1)
            self.queryset = self.queryset.filter(submit_time__lte=date_to)
    return self.queryset.prefetch_related('files')",self.filter_form.is_valid(),30,self.filter_form.is_valid(),True,100.00000000000004,N/A
"def clean_slug(self):
    slug = self.cleaned_data['slug']
<mask>:
        raise forms.ValidationError('This slug is already in use')
    return slug",Form.objects.filter(slug=slug).exists(),15,slug in self.use,False,2.099844458473431,N/A
"def copy(self, request, *args, **kwargs):
    form = CopyForm(request.POST)
<mask>:
        copied = self.object.copy()
        copied.title = form.cleaned_data['title']
        copied.slug = form.cleaned_data['slug']
        copied.save()
        self.create_success_message(copied)
        return HttpResponseRedirect(self.get_success_url())
    context = self.get_context_data(object=self.object)
    context['form'] = form
    return self.render_to_response(context)",form.is_valid(),30,form.is_valid(),True,100.00000000000004,N/A
"def clear(self):
    self.reset()
<mask>:
        self.log('Restoring normal refresh frequency.')
        self.exchange.stop().start()
        self.chill = False",self.chill,12,self.chill,True,100.00000000000004,N/A
"def is_ok(self):
    max = self.count <= MAX_ERRORS
<mask>:
        self.log('Error limit reached. Cooling down for ' + str(REFRESH_INTERVAL) + ' seconds.')
        self.exchange.stop()
        GLib.timeout_add_seconds(REFRESH_INTERVAL, self.exchange.restart)
        self.chill = True
    else:
        self.chill = False
    return max",max is False,32,max,False,13.533528323661276,N/A
"def _base_changed(self, selection):
    model, iter = selection.get_selected()
<mask>:
        return
    self.quote_store.clear()
    self.current_base = model[iter][0]
    for quote in self.parent.coin.bases[self.current_base]:
        self.quote_store.append([quote])
    self.quote_store.set_sort_column_id(0, Gtk.SortType.ASCENDING)
    self.view_quotes.set_cursor(0)",iter is None,21,iter == -1,False,15.97357760615681,N/A
"def _quote_changed(self, selection):
    model, iter = selection.get_selected()
<mask>:
        return
    self.ex_store.clear()
    self.current_quote = model[iter][0]
    for exchange in self.parent.coin.bases[self.current_base][self.current_quote]:
        if exchange.active:
            self.ex_store.append([exchange.name, exchange.code])
    self.ex_store.set_sort_column_id(0, Gtk.SortType.ASCENDING)
    self.view_exchanges.set_cursor(0)",iter is None,24,iter == -1,False,15.97357760615681,N/A
"def _exchange_changed(self, selection):
    model, iter = selection.get_selected()
<mask>:
        return
    self.current_exchange = model[iter][1]",iter is None,12,iter == -1,False,15.97357760615681,N/A
"def _select_currents(self):

    def _select_and_scroll(store, view, current_value):
        for row in store:
<mask>:
                view.set_cursor(row.path)
                view.scroll_to_cell(row.path)
                break
    _select_and_scroll(self.base_store, self.view_bases, self.parent.exchange.asset_pair.get('base'))
    _select_and_scroll(self.quote_store, self.view_quotes, self.parent.exchange.asset_pair.get('quote'))
    _select_and_scroll(self.ex_store, self.view_exchanges, self.parent.exchange.name)",row[0] == current_value,23,row.path == current_value,False,47.750342648354646,N/A
"def start(self):
    self.indicator_widget = AppIndicator.Indicator.new('Coindicator_' + str(uuid1()), str(self.exchange.icon), AppIndicator.IndicatorCategory.APPLICATION_STATUS)
    self.indicator_widget.set_status(AppIndicator.IndicatorStatus.ACTIVE)
    self.indicator_widget.set_ordering_index(0)
    self.indicator_widget.set_menu(self._menu())
<mask>:
        self._start_exchange()
    else:
        self._stop_exchange()",self.exchange.active,16,self.exchange.is_active(),False,31.55984539112946,N/A
"def update_gui(self):
    logging.debug('Updating GUI, last response was: ' + str(self.latest_response))
    self.symbol = self.exchange.symbol
    self.volumecurrency = self.exchange.volume_currency
<mask>:
        label = self.symbol + self.prices.get(self.default_label)
    else:
        label = 'select default label'
    self.indicator_widget.set_label(label, label)
    for item, name in CATEGORIES:
        price_menu_item = self.price_menu_items.get(item)
        if self.prices.get(item):
            if item == self.default_label:
                price_menu_item.set_active(True)
                if self.alarm.active:
                    if self.alarm.check(float(self.prices.get(item))):
                        self.alarm.deactivate()
            price_menu_item.set_label(name + ':\t\t' + self.symbol + ' ' + self.prices.get(item))
            price_menu_item.show()
        else:
            price_menu_item.hide()
    if self.prices.get('vol'):
        self.volume_item.set_label('Vol:\t\t' + self.prices.get('vol') + ' ' + self.volumecurrency)
        self.volume_item.show()
    else:
        self.volume_item.hide()",self.prices.get(self.default_label),76,self.prices.get(self.default_label),True,100.00000000000004,N/A
"def _make_default_label(self, label):
    self.default_label = label
<mask>:
        new_label = self.prices.get(label)
        if new_label:
            self.indicator_widget.set_label(self.symbol + new_label, new_label)",self.price_menu_items.get(self.default_label),16,self.prices,False,0.7222266253934472,N/A
"def _menu_recents(self):
    recent_menu = Gtk.Menu()
<mask>:
        return
    for recent in self.config['settings'].get('recent'):
        exchange = self.coin.exchanges.get(recent.get('exchange'))
        if exchange is None:
            continue
        asset_pair = exchange.find_asset_pair_by_code(recent.get('asset_pair', 'None'))
        base = asset_pair.get('base', 'None')
        quote = asset_pair.get('quote', 'None')
        tabs = '\t' * (floor(abs(len(exchange.name) - 8) / 4) + 1)
        recent_string = exchange.name[0:8] + ':' + tabs + base + ' - ' + quote
        recent_item = Gtk.MenuItem(recent_string)
        recent_item.connect('activate', self._recent_change, base, quote, exchange)
        recent_menu.append(recent_item)
    recent_menu.show_all()
    return recent_menu",len(self.config['settings'].get('recent')) == 0,69,not self.config['settings'].get('recent'),False,59.56911543101374,N/A
"def check(self, price):
<mask>:
        if price > self.ceil:
            self.__notify(price, 'rose above', self.ceil)
            return True
    if self.floor:
        if price < self.floor:
            self.__notify(price, 'fell below', self.floor)
            return True
    return False",self.ceil,28,self.ceil,True,100.00000000000004,N/A
"def __notify(self, price, direction, threshold):
    exchange_name = self.parent.exchange.name
    asset_name = self.parent.exchange.asset_pair.get('base')
    title = asset_name + ' price alert: ' + self.parent.symbol + str(price)
    message = 'Price on ' + exchange_name + ' ' + direction + ' ' + self.parent.symbol + str(threshold)
<mask>:
        pygame.mixer.music.load(self.config['project_root'] / 'resources/ca-ching.wav')
        pygame.mixer.music.play()
    notifier = desktop_notifier.DesktopNotifier(app_name=self.config['app']['name'])
    notifier.send_sync(title, message, urgency=desktop_notifier.Urgency.Critical, icon=str(self.config['project_root'] / 'resources/icon_32px.png'))",pygame.init(),56,not pygame.mixer.music.is_available(),False,9.578464408619821,N/A
"def __init__(self, parent, price):
    Gtk.Window.__init__(self, title='Set price alert')
    self.parent = parent
    self.set_keep_above(True)
    self.set_border_width(5)
    self.set_position(Gtk.WindowPosition.MOUSE)
    self.connect('key-release-event', self._on_key_release)
    self.grid = Gtk.Grid()
    self.grid.set_column_homogeneous(True)
    self.grid.set_row_homogeneous(True)
    label = Gtk.Label('Alert if the active price is')
    hbox = Gtk.Box(spacing=2)
    radio_over = Gtk.RadioButton.new_with_label(None, 'above')
    radio_under = Gtk.RadioButton.new_with_label_from_widget(radio_over, 'below')
<mask>:
        if self.parent.alarm.ceil:
            price = self.parent.alarm.ceil
            radio_over.set_active(True)
        elif self.parent.alarm.floor:
            price = self.parent.alarm.floor
            radio_under.set_active(True)
    entry_price = Gtk.Entry()
    entry_price.set_text(str(price))
    entry_price.connect('activate', self._set_alarm, radio_over, entry_price)
    entry_price.connect('changed', self._strip_text)
    self.set_focus_child(entry_price)
    hbox.pack_start(label, False, False, 0)
    hbox.pack_start(radio_over, False, False, 0)
    hbox.pack_start(radio_under, False, False, 0)
    hbox.pack_start(entry_price, True, True, 0)
    buttonbox = Gtk.Box(spacing=2)
    button_set = Gtk.Button('Set Alert')
    button_clear = Gtk.Button('Delete Alert')
    button_cancel = Gtk.Button('Close')
    button_set.connect('clicked', self._set_alarm, radio_over, entry_price)
    button_set.set_can_default(True)
    button_set.get_style_context().add_class(Gtk.STYLE_CLASS_SUGGESTED_ACTION)
    button_clear.connect('clicked', self._clear_alarm)
    button_cancel.connect('clicked', self._close)
    buttonbox.pack_start(button_set, True, True, 0)
    buttonbox.pack_start(button_clear, True, True, 0)
    buttonbox.pack_start(button_cancel, True, True, 0)
    self.grid.attach(hbox, 0, 0, 50, 50)
    self.grid.attach(buttonbox, 0, 50, 50, 50)
    self.add(self.grid)
    self.set_accept_focus(True)
    self.show_all()
    self.present()
    entry_price.grab_focus()",self.parent.alarm.active,131,not price,False,0.0,N/A
"def _set_alarm(self, widget, radio_over, entry_price):
    above = radio_over.get_active()
    try:
        price = float(entry_price.get_text())
<mask>:
            self.parent.alarm.set_ceil(price)
            self.parent.alarm.set_floor(None)
        else:
            self.parent.alarm.set_floor(price)
            self.parent.alarm.set_ceil(None)
        self.destroy()
    except ValueError:
        entry_price.set_text('')
        entry_price.grab_focus()",above,23,above,True,100.00000000000004,N/A
"@property
def icon(self) -> str:
    asset = self.asset_pair.get('base', '').lower()
    asset_dir = Config()['icon_dir']
<mask>:
        return asset_dir / f'{asset}.png'
    else:
        fetched = CoinGeckoClient().get_icon(asset)
        if fetched is not None:
            return fetched
    return asset_dir / 'unknown-coin.png'",(asset_dir / f'{asset}.png').exists(),32,asset.endswith('.png'),False,8.993236413460203,N/A
"def set_asset_pair(self, base, quote):
    for ap in self.asset_pairs:
<mask>:
            self.asset_pair = ap
            break
    if not self.asset_pair:
        logging.warning('User.conf specifies unavailable asset pair, trying default.                 Run Asset Discovery again.')
        self.asset_pair = ap",ap.get('base').upper() == base.upper() and ap.get('quote').upper() == quote.upper(),30,ap['base'] == base and ap['quote'] == quote,False,5.335748978361823,N/A
"def set_asset_pair_from_code(self, code):
    for ap in self.asset_pairs:
<mask>:
            self.asset_pair = ap
            break
    if not self.asset_pair:
        logging.warning('User.conf specifies unavailable asset pair, trying default.                 Run Asset Discovery again.')
        self.asset_pair = {}",ap.get('pair').upper() == code.upper(),29,ap['code'] == code,False,6.295416788379057,N/A
"@classmethod
def find_asset_pair_by_code(cls, code):
    for ap in cls.asset_pairs:
<mask>:
            return ap
    return {}",ap.get('pair') == code,13,ap['code'] == code,False,19.740631366145518,N/A
"@classmethod
def find_asset_pair(cls, quote, base):
    for ap in cls.asset_pairs:
<mask>:
            return ap
    return {}",ap.get('quote') == quote and ap.get('base') == base,14,quote in ap.quote and ap.base == base,False,19.069363427734363,N/A
"def load_list(self):
    url = self.url + 'list'
    data = requests.get(url, timeout=10)
<mask>:
        self.coingecko_list = [{'id': item['id'], 'symbol': item['symbol']} for item in data.json()]
    else:
        logging.warning('CoinGecko API Error <%d>: %s' % (data.status_code, data.text))",data.status_code == 200,31,data.status_code == 200,True,100.00000000000004,N/A
"def get_icon(self, asset):
<mask>:
        self.load_list()
    url = ''
    for coin in self.coingecko_list:
        if asset == coin.get('symbol'):
            url = self.url + coin.get('id')
            break
    if url == '':
        return
    data = requests.get(url, timeout=5)
    if data.status_code != 200:
        logging.error('Coingecko returned %d while fetching symbol details' % data.status_code)
        return
    img_url = data.json().get('image').get('small')
    img = requests.get(img_url, stream=True, timeout=5)
    if img.status_code != 200:
        logging.error('Coingecko returned %d while fetching symbol icon' % img.status_code)
        return
    img_file = self.config['icon_dir'] / f'{asset}.png'
    with open(img_file, 'wb') as f:
        img.raw.decode_content = True
        shutil.copyfileobj(img.raw, f)
    return img_file",len(self.coingecko_list) == 0,84,not self.coingecko_list,False,33.02232277439296,N/A
"def _load_assets(self):
    self.assets = {}
    for exchange in self.exchanges.values():
<mask>:
            if not exchange.asset_pairs:
                exchange.discover_assets(DownloadService(), lambda *args: None)
            self.assets[exchange.code] = exchange.asset_pairs
    bases = {}
    for exchange in self.assets.keys():
        for asset_pair in self.assets.get(exchange):
            base = asset_pair.get('base')
            quote = asset_pair.get('quote')
            if base not in bases:
                bases[base] = {}
            if quote not in bases[base]:
                bases[base][quote] = []
            bases[base][quote].append(self.exchanges[exchange])
    self.bases = bases",exchange.active,57,exchange.code not in self.assets,False,11.044795567078939,N/A
"def _load_settings(self):
    for plugin in self.config['settings'].get('plugins', {}):
        for code, active in plugin.items():
            exchange = self.exchanges.get(code)
<mask>:
                exchange.active = active
    if not self.config['settings'].get('tickers'):
        first_exchange = next(iter(self.exchanges.values()))
        first_code = first_exchange.code
        self.config['settings']['tickers'] = [{'exchange': first_code, 'asset_pair': self.assets[first_code][0].get('pair'), 'refresh': 3, 'default_label': first_exchange.default_label}]
    if not self.config['settings'].get('recent'):
        self.config['settings']['recent'] = []",exchange is not None,44,exchange,False,4.9787068367863965,N/A
"def add_new_recent(self, asset_pair, exchange_code):
    for recent in self.config['settings']['recent']:
<mask>:
            self.config['settings']['recent'].remove(recent)
    self.config['settings']['recent'] = self.config['settings']['recent'][0:4]
    new_recent = {'asset_pair': asset_pair, 'exchange': exchange_code}
    self.config['settings']['recent'].insert(0, new_recent)
    for instance in self.instances:
        instance.rebuild_recents_menu()",recent.get('asset_pair') == asset_pair and recent.get('exchange') == exchange_code,26,recent in self.config['settings']['recent'],False,1.2482247181508341,N/A
"def remove_ticker(self, indicator):
<mask>:
        Gtk.main_quit()
    else:
        indicator.exchange.stop()
        indicator.indicator_widget.set_status(AppIndicator.IndicatorStatus.PASSIVE)
        self.instances.remove(indicator)
        self.save_settings()",len(self.instances) == 1,10,indicator.exchange is None,False,4.79981069911921,N/A
"def _discover_assets(self, _widget):
<mask>:
        return
    self.main_item.set_icon_full(str(self.config.get('project_root') / 'resources/loading.png'), 'Discovering assets')
    for indicator in self.instances:
        if indicator.asset_selection_window:
            indicator.asset_selection_window.destroy()
    for exchange in self.exchanges.values():
        if exchange.active and exchange.discovery:
            exchange.discover_assets(self.downloader, self.update_assets)",len([ex for ex in self.exchanges.values() if ex.active and ex.discovery]) == 0,27,not self.config.get('project_root'),False,2.2008358123711007,N/A
"@staticmethod
def _parse_discovery(result):
    asset_pairs = []
    assets = result.get('symbols')
    for asset in assets:
        base = asset.get('commodity')
        quote = asset.get('currency')
        names = {'IOTA': 'IOT', 'MAN': 'MANA'}
<mask>:
            base = names[base]
        if quote in names:
            quote = names[quote]
        asset_pair = {'pair': asset.get('symbol'), 'base': base, 'quote': quote, 'name': base + ' to ' + quote, 'currency': quote.lower(), 'volumecurrency': base}
        asset_pairs.append(asset_pair)
    return asset_pairs",base in names,59,base in names,True,100.00000000000004,N/A
"@staticmethod
def _parse_discovery(result):
    asset_pairs = []
    assets = result.get('result')
    for asset in assets:
<mask>:
            continue
        asset_data = assets.get(asset)
        names = asset_data.get('wsname').split('/')
        base = names[0]
        quote = names[1]
        kraken_names = {'XBT': 'BTC', 'XZC': 'ZEC'}
        if base in kraken_names:
            base = kraken_names[base]
        if quote in kraken_names:
            quote = kraken_names[quote]
        asset_pair = {'pair': asset, 'base': base, 'quote': quote, 'name': base + ' to ' + quote, 'currency': quote.lower(), 'volumecurrency': base}
        asset_pairs.append(asset_pair)
    return asset_pairs",asset[-2:] == '.d',70,asset.get('type') != 'asset',False,5.61480827173619,N/A
"@staticmethod
def _parse_discovery(result):
    asset_pairs = []
    for asset in result:
        base = asset[0:3].upper()
        quote = asset[-3:].upper()
        names = {'DSH': 'DASH', 'TRST': 'TRUST', 'XZC': 'ZEC'}
<mask>:
            base = names[base]
        if quote in names:
            quote = names[quote]
        asset_pair = {'pair': 't' + asset.upper(), 'base': base, 'quote': quote, 'name': base + ' to ' + quote, 'currency': quote.lower(), 'volumecurrency': base}
        asset_pairs.append(asset_pair)
    return asset_pairs",base in names,60,base in names,True,100.00000000000004,N/A
"@staticmethod
def _parse_discovery(result):
    asset_pairs = []
    for asset in result:
        base = asset.get('baseCurrencySymbol')
        quote = asset.get('quoteCurrencySymbol')
        market = asset.get('symbol')
        names = {'SWIFT': 'SWFTC', 'DSH': 'DASH', 'TRST': 'TRUST', 'XZC': 'ZEC', 'GAM': 'GAME', 'BCC': 'BCH'}
<mask>:
            base = names[base]
        if quote in names:
            quote = names[quote]
        asset_pair = {'pair': market, 'base': base, 'quote': quote, 'name': base + ' to ' + quote, 'currency': quote.lower(), 'volumecurrency': base}
        asset_pairs.append(asset_pair)
    return asset_pairs",base in names,67,base in names,True,100.00000000000004,N/A
"@staticmethod
def _parse_discovery(result):
    asset_pairs = []
    success = result.get('error') == 0
<mask>:
        return []
    assets = result.get('result')
    for asset in assets:
        symbol = asset.get('symbol')
        split_sym = str(symbol).strip().split('_')
        base = split_sym[1]
        quote = split_sym[0]
        asset_pair = {'pair': symbol, 'base': base, 'quote': quote, 'name': asset.get('info'), 'currency': quote.lower(), 'volumecurrency': base}
        asset_pairs.append(asset_pair)
    return asset_pairs",not success,50,not success,True,100.00000000000004,N/A
"@staticmethod
def _parse_discovery(result):
    asset_pairs = []
    assets = result.get('symbols')
    for asset in assets:
        base = asset.get('baseAsset')
        quote = asset.get('quoteAsset')
        names = {'XZC': 'ZEC', 'BCC': 'BCH', 'IOTA': 'IOT'}
<mask>:
            base = names[base]
        if quote in names:
            quote = names[quote]
        asset_pair = {'pair': asset.get('symbol'), 'base': base, 'quote': quote, 'name': base + ' to ' + quote, 'currency': quote.lower(), 'volumecurrency': base}
        asset_pairs.append(asset_pair)
    return asset_pairs",base in names,61,base in names,True,100.00000000000004,N/A
"def __init__(self, *args, **kwargs):
    self.dict = {}
    for arg in args:
<mask>:
            for k, v in arg.items():
                self[k] = v
        else:
            for k, v in arg:
                self[k] = v
    for k, v in kwargs.items():
        self[k] = v","hasattr(arg, 'items')",37,"isinstance(arg, dict)",False,32.46679154750991,N/A
"def copy_file(stream, target, maxread=-1, buffer_size=2 ** 16):
    """""" Read from :stream and write to :target until :maxread or EOF. """"""
    size, read = (0, stream.read)
    while True:
        to_read = buffer_size if maxread < 0 else min(buffer_size, maxread - size)
        part = read(to_read)
<mask>:
            return size
        target.write(part)
        size += len(part)",not part,49,not part,True,100.00000000000004,N/A
"def header_quote(val):
    """""" Quote header option values if necessary.

        Note: This is NOT the way modern browsers quote field names or filenames
        in Content-Disposition headers. See :func:`content_disposition_quote`
    """"""
<mask>:
        return val
    return '""' + val.replace('\\', '\\\\').replace('""', '\\""') + '""'",_re_istoken.match(val),39,not val,False,1.509869171115925,N/A
"def compact_events(self):
    current = None
    data = []
    for event in self.events:
<mask>:
            current = event
        elif event:
            data.append(event)
        else:
            yield (current, b''.join(data))
            current = None
            data = []
    if current:
        yield (current, b''.join(data))","isinstance(event, multipart.MultipartSegment)",34,current is None,False,0.0,N/A
"def get_segment(self, index_or_name):
    allnames = []
    for i, (segment, body) in enumerate(self.compact_events()):
        allnames.append(segment.name)
<mask>:
            return (segment, body)
    self.fail(f'Segment {index_or_name!r} not found in {allnames!r}')",index_or_name == i or index_or_name == segment.name,23,index_or_name in allnames,False,11.071861994616217,N/A
"def write_end(self, force=False):
    end = b'--' + to_bytes(self.boundary) + b'--'
<mask>:
        return
    if self.data.tell() > 0:
        self.write(b'\r\n')
    self.write(end)",not force and self.data.getvalue().endswith(end),18,force and self.data.tell() == 0,False,36.54499360464565,N/A
"def write_header(self, header, value, **opts):
    line = to_bytes(header) + b': ' + to_bytes(value)
    for opt, val in opts.items():
<mask>:
            line += b'; ' + to_bytes(opt) + b'=' + to_bytes(multipart.content_disposition_quote(val))
    self.write(line + b'\r\n')",val is not None,32,opt != 'content-disposition',False,0.0,N/A
"def write_field(self, name, data, filename=None, content_type=None):
    self.write_boundary()
    self.write_header('Content-Disposition', 'form-data', name=name, filename=filename)
<mask>:
        self.write_header('Content-Type', content_type)
    self.write(b'\r\n')
    self.write(data)",content_type,16,content_type,True,100.00000000000004,N/A
"def updateConfigFile(self):
<mask>:
        from configparser import ConfigParser
        old_config = ConfigParser()
        old_config.read(CONFIG_PATH)
        new_config = ConfigParser()
        new_config.read('lyrics/lyrics.cfg')
        skip = True
        for section in old_config.sections():
            old_keys = {o for o in old_config[section]}
            new_keys = {n for n in new_config[section]}
            changes = new_keys ^ old_keys
            if len(changes) == 0:
                continue
            else:
                skip = False
            for option in new_keys:
                fallback = new_config[section].get(option)
                new_config[section][option] = old_config[section].get(option, fallback)
        if not skip:
            with open(CONFIG_PATH, 'w') as file:
                new_config.write(file)",CONFIG_PATH.exists(),70,os.path.exists(CONFIG_PATH),False,27.301208627090666,N/A
"def set_constants(self):
    for key, value in self.dict.items():
<mask>:
            self.dict[key] = KEYS[value]
        else:
            try:
                value = int(value)
                self.dict[key] = value
            except ValueError:
                if len(value) == 1:
                    self.dict[key] = ord(value)",value in KEYS.keys(),28,value in KEYS,False,26.359713811572682,N/A
"def input(self, window, key):
<mask>:
        window.update_track()
    elif key == self.binds['down']:
        window.scroll_down()
    elif key == self.binds['step-down']:
        window.scroll_down(self.binds['step-size'])
        window.stdscr.erase()
    elif key == self.binds['up']:
        window.scroll_up()
    elif key == self.binds['step-up']:
        window.scroll_up(self.binds['step-size'])
        window.stdscr.erase()
    elif key == self.binds['cycle-source']:
        window.player.refresh(cycle_source=True, cache=False)
        window.current_pos = 0
        window.update_track(True)
    elif key == self.binds['left']:
        window.player.track.alignment = 1
        window.player.track.reset_width()
        window.update_track()
    elif key == self.binds['center']:
        window.player.track.alignment = 0
        window.player.track.reset_width()
        window.update_track()
    elif key == self.binds['right']:
        window.player.track.alignment = 2
        window.player.track.reset_width()
        window.update_track()
    elif key == self.binds['delete']:
        if window.player.track.delete_lyrics():
            window.stdscr.addstr(window.height - 1, 1, ' Deleted ', curses.A_REVERSE)
    elif key == self.binds['help']:
        window.stdscr.erase()
        HelpPage(self.binds)
        window.height, window.width = window.stdscr.getmaxyx()
    elif key == self.binds['edit']:
        curses.endwin()
        window.player.track.edit_lyrics()
        window.stdscr = curses.initscr()
        window.current_pos = 0
        window.player.refresh(cache=True)
        window.update_track()
    elif key == self.binds['find']:
        window.find()
    elif key == self.binds['autoswitchtoggle']:
        window.player.autoswitch = not window.player.autoswitch
        window.stdscr.addstr(window.height - 1, 1, f"" Autoswitch: {('on' if window.player.autoswitch else 'off')} "", curses.A_REVERSE)",key == curses.KEY_RESIZE,129,key == self.binds['up'],False,19.070828081828378,N/A
"def add_config(self, i, j, config, _keys):
    for k, v in config.items():
<mask>:
            v = _keys[v]
        elif k not in ['step-size', 'interval', 'mpd_port']:
            if isinstance(v, int):
                v = chr(v)
        self.win.addstr(i, j, f'{k:18} {v}')
        i += 1
    return i",v in _keys.keys(),37,"k in ['step-size', 'interval', 'mpd_port']",False,4.456882760699063,N/A
"def add_text(self):
    self.win.refresh()
    h, w = self.win.getmaxyx()
    self.win.addstr(2, 3, f""{'v' + __version__:>{w - 5}}"")
    self.win.addstr(3, 3, 'Help Page', curses.A_BOLD | curses.A_UNDERLINE)
    self.win.addstr(h - 2, 3, f""{'Press any key to exit...':>{w - 5}}"")
    keys = {curses.KEY_UP: '↑', curses.KEY_DOWN: '↓', curses.KEY_LEFT: '←', curses.KEY_RIGHT: '→'}
    i, j = (6, 3)
    self.win.addstr(i, j, 'Keybindings', curses.A_UNDERLINE)
    i += 2
    i = self.add_config(i, j, self.keybinds, keys)
<mask>:
        i, j = (6, w // 2)
    else:
        i += 2
    self.win.addstr(i, j, 'Default Options', curses.A_UNDERLINE)
    i += 2
    self.add_config(i, j, self.options, keys)",w // 2 >= 30,84,w % 2,False,9.138402379955025,N/A
"def set_up(self):
    self.stdscr.clear()
    curses.curs_set(0)
    self.current_pos = 0
<mask>:
        self.update_track()
        self.set_titlebar()
        self.stdscr.refresh()
        self.scroll_pad.refresh(self.current_pos, 0, 4, self.pad_offset, self.height - 2, self.width - 1)
    else:
        self.stdscr.addstr(0, 1, f'{self.player.player_name} is not running!')
        self.stdscr.refresh()",self.player.running,29,self.player.is_running(),False,31.55984539112946,N/A
"def set_statusbar(self):
<mask>:
        lines = self.player.track.length
        if self.current_pos < 0:
            self.current_pos = 0
        pct_progress = f' {round(self.current_pos * 100 / lines) + 1}% '
        self.stdscr.insstr(self.height - 1, self.width - len(pct_progress), pct_progress, curses.A_DIM)",self.options['statusbar'] == 'on',32,self.player.track,False,9.599621398238423,N/A
"def get_lyrics(self, source, cycle_source=False, cache=True):
    """""" fetch lyrics off the internet
        """"""
<mask>:
        self.source = source or self.sources[0]
    if cycle_source is True and self.source not in self.sources:
        self.source = self.sources[0]
    if cycle_source:
        curr_source = self.sources.index(self.source)
        next_source = (curr_source + 1) % len(self.sources)
        source = self.sources[next_source]
        cache = False
    else:
        source = 'any'
    self.lyrics, self.source = util.get_lyrics(self.track_name, source, cache=cache)
    self.width = len(max(self.lyrics, key=len))
    self.length = len(self.lyrics)",self.source is None or self.source == 'cache',65,self.source is None,False,24.659696394160658,N/A
"def get_text(self, wrap=False, width=0):
    """""" returns lyrics text seperated by '\\n'
        """"""
<mask>:
        lyrics = util.wrap_text(self.lyrics, width)
    else:
        lyrics = self.lyrics
    self.width = len(max(lyrics, key=len))
    self.length = len(lyrics)
    lyrics = util.align(lyrics, self.width, self.alignment)
    return '\n'.join((line for line in lyrics))",wrap,39,wrap,True,100.00000000000004,N/A
"def get_az_html(html: str) -> str | None:
    """""" finds azlyrics website link and
        returns html text from azlyrics

        if azlyrics link not found returns None
    """"""
    regex = re.compile('(http[s]?://www.azlyrics.com/lyrics(?:.*?))&amp')
    az_url = regex.search(html)
<mask>:
        return None
    header = {'User-Agent': 'Mozilla/5.0 Firefox/70.0'}
    az_url = az_url.group(1)
    az_html = get_html(az_url, header)
    if isinstance(az_html, tuple):
        return None
    return az_html",az_url == None,54,not az_url,False,36.06452879987789,N/A
"def get_genius_html(html: str) -> str | None:
    """""" finds genius website link and
        returns html text from genius

        if genius link not found returns None
    """"""
    regex = re.compile('(http[s]?://genius.com/(?!albums)(?:.*?))&amp')
    gns_url = regex.search(html)
<mask>:
        return None
    gns_url = gns_url.group(1)
    gns_html = get_html(gns_url, header={})
    if isinstance(gns_html, tuple):
        return None
    return gns_html",gns_url == None,49,gns_url is None,False,34.98330125272253,N/A
"def parse_genius(html: str | None) -> List[str] | None:
    """""" parses lyrics from genius html
        returns list if strings of lyrics or None if lyrics not found
    """"""
<mask>:
        return None
    gns_regex = re.compile('<div data-lyrics-container=""true"" (?:.*?)(>.*?<)/div>', re.S)
    ly = gns_regex.findall(html)
    if ly == None:
        return None
    lyrics_lines = ''
    for ly_section in ly:
        ly_section = ly_section.replace('&#x27;', ""'"")
        line_regex = re.compile('>([^<]+?)<', re.S)
        lines = line_regex.findall(ly_section)
        lyrics_lines += '\n'.join(lines)
    lyrics_lines = re.sub('\\n{2,}', '\n', lyrics_lines)
    lyrics_lines = lyrics_lines.replace('\n[', '\n\n[').replace('&quot;', '""')
    lyrics_lines = lyrics_lines.split('\n')
    return lyrics_lines",html == None,82,html is None,False,24.840753130578644,N/A
"def parse_azlyrics(html: str | None) -> List[str] | None:
    """""" parses lyrics from azlyrics html
        returns list if strings of lyrics

        if lyrics not found returns error string
    """"""
<mask>:
        return None
    az_regex = re.compile('Sorry about that. -->(.*)(?:<script>(.*))<!-- MxM banner -->', re.S)
    ly = az_regex.search(html)
    if ly == None:
        return None
    ly = re.sub('<[/]?\\w*?>', '', ly.group(1)).strip()
    rep = {'&quot;': '""', '&amp;': '&', '\r': ''}
    replace_patten = '|'.join(rep.keys())
    ly = re.sub(replace_patten, lambda match: rep[match.group(0)], ly)
    lyrics_lines = ly.split('\n')
    return lyrics_lines",html == None,79,not html,False,18.393972058572114,N/A
"def parse_google(html: str) -> List[str] | None:
    """""" parses google result html
        returns list of strings or None if no lyrics found
    """"""
    html_regex = re.compile('<div class=""{}"">([^>]*?)</div>'.format(CLASS_NAME), re.S)
    text_list = html_regex.findall(html)
<mask>:
        return None
    lyrics_lines = []
    for l in text_list[1:]:
        if l.count('\n') > 2:
            lyrics_lines += l.split('\n')
    if len(lyrics_lines) < 5:
        return None
    return lyrics_lines",len(text_list) < 2,56,text_list == None,False,21.64910073203448,N/A
"@ErrorHandler
def init_pager(stdscr=None):
    defaults = Config('OPTIONS')
<mask>:
        player_name = sys.argv[1].strip()
        autoswitch = False
    else:
        player_name = defaults['player'].strip()
        autoswitch = defaults.getboolean('autoswitch')
    align = defaults['alignment']
    if align == 'center':
        align = 0
    elif align == 'right':
        align = 2
    else:
        align = 1
    interval = defaults['interval']
    source = defaults['source']
    mpd_connect = [defaults['mpd_host'], defaults['mpd_port'], defaults['mpd_pass']]
    player = Player(player_name, source, autoswitch, mpd_connect, align=align)
    win = Window(stdscr, player, timeout=interval)
    win.main()",len(sys.argv) >= 2,65,len(sys.argv) > 1,False,74.20884818558928,N/A
"def main():
<mask>:
        if sys.argv[1] == '-t':
            try:
                artist = sys.argv[2].strip()
                title = sys.argv[3].strip()
            except IndexError:
                print('Please provide track info in format ""-t {artist} {title}"".')
                exit(1)
            from lyrics.track import Track
            track = Track(artist=artist, title=title)
            track.get_lyrics('any')
            print(track.track_name)
            print('-' * track.width, '\n')
            print(track.get_text())
            exit(0)
    init_pager()",len(sys.argv) >= 2,43,len(sys.argv) > 1,False,74.20884818558928,N/A
"def check_playing(self):
    """""" checks playing status of current player
        """"""
<mask>:
        status = self.player_interface.Get('org.mpris.MediaPlayer2.Player', 'PlaybackStatus')
        self.running = status == 'Playing'",self.player_interface,20,self.player_interface is not None,False,51.697315395717055,N/A
"def get_active_player(self):
    """""" returns name of playing media source/player
        """"""
    session_bus = dbus.SessionBus()
    for service in session_bus.list_names():
<mask>:
            obj = session_bus.get_object(service, '/org/mpris/MediaPlayer2')
            interface = dbus.Interface(obj, 'org.freedesktop.DBus.Properties')
            status = interface.Get('org.mpris.MediaPlayer2.Player', 'PlaybackStatus')
            if status == 'Playing':
                self.player_name = service.split('MediaPlayer2.')[-1]
                self.player_interface = interface
                self.running = True
                return","re.findall('org.mpris.MediaPlayer2|plasma-browser-integration', service, re.IGNORECASE)",44,service.startswith('org.mpris.MediaPlayer2'),False,19.850986204168244,N/A
"def mpd_active(self):
    """""" Check if mpd is active and get metadata """"""
    client = mpd()
    try:
        client.connect(self.mpd_host, self.mpd_port)
    except Exception as e:
        return False
<mask>:
        client.password(self.mpd_pass)
    if client.status()['state'] == 'play':
        self.player_name = 'mpd'
        self.running = True
        currentsong = client.currentsong()
        if 'album' in currentsong:
            album = currentsong['album']
        else:
            album = ''
        title = currentsong['title']
        artist = currentsong['artist']
        album = album
        trackid = currentsong['id']
        if self.track.title != title:
            self.track.update(artist, title, album, trackid)
            self.refresh()
            return True
    else:
        self.running = False
    return False",self.mpd_pass != '',79,self.mpd_pass,False,54.88116360940266,N/A
"def get_bus(self):
    """""" gets dbus session bus and player interface
        """"""
    try:
<mask>:
            self.get_active_player()
        else:
            session_bus = dbus.SessionBus()
            player_bus = session_bus.get_object(f'org.mpris.MediaPlayer2.{self.player_name}', '/org/mpris/MediaPlayer2')
            self.player_interface = dbus.Interface(player_bus, 'org.freedesktop.DBus.Properties')
            self.running = True
    except dbus.exceptions.DBusException:
        self.running = False
        self.player_interface = None",self.autoswitch,37,self.running,False,55.03212081491043,N/A
"def update(self):
    """""" checks if player or track have changed or not
        """"""
    try:
<mask>:
            self.check_playing()
        if not self.running:
            self.get_bus()
        metadata = self.player_interface.Get('org.mpris.MediaPlayer2.Player', 'Metadata')
        self.running = True
    except Exception as e:
        self.running = False
        self.player_interface = None
    if self.running:
        try:
            title = metadata['xesam:title']
            if title.strip() == '':
                return False
            artist = ''
            if 'xesam:artist' in metadata:
                artist = metadata['xesam:artist']
            artist = artist[0] if isinstance(artist, list) else artist
            if re.search('chromium|plasma', self.player_name) and '-' in title:
                artist, title, *_ = title.split('-')
            title = title.strip()
            artist = artist.strip()
            album = metadata.get('xesam:album')
            album = '' if album is None else album
            trackid = metadata.get('mpris:trackid')
            trackid = title if trackid is None else trackid
        except (IndexError, KeyError) as e:
            self.running = False
            return False
        if trackid.find('spotify:ad') != -1:
            self.running = False
        elif self.track.trackid != trackid or self.track.title != title:
            self.track.update(artist, title, album, trackid)
            self.refresh()
            return True
    elif MPD_ENABLED:
        return self.mpd_active()
    return False",self.autoswitch,147,self.player_interface is not None,False,11.044795567078939,N/A
"def __init__(self, initial_dict: typing.Dict, on_change_callback: typing.Callable):
    self._on_change_callback = on_change_callback
    for k, v in initial_dict.items():
<mask>:
            initial_dict[k] = WhistleBlowerDict(v, on_change_callback)
    super().__init__(initial_dict)","isinstance(v, dict)",20,"isinstance(v, WhistleBlowerDict)",False,53.7284965911771,N/A
"def __setitem__(self, item, value):
<mask>:
        value = WhistleBlowerDict(value, self._on_change_callback)
    self._on_change_callback()
    super().__setitem__(item, value)","isinstance(value, dict)",12,"isinstance(value, dict)",True,100.00000000000004,N/A
"def to_plain_dict(d: typing.Dict) -> typing.Dict:
    plain = {}
    for k, v in d.items():
<mask>:
            plain[k] = to_plain_dict(v.data)
        elif isinstance(v, dict):
            plain[k] = to_plain_dict(v)
        else:
            plain[k] = v
    return plain","isinstance(v, collections.UserDict)",29,"isinstance(v, dict)",False,38.49815007763549,N/A
"@classmethod
def from_dict(cls, obj_dict: typing.Dict):
    """"""Creates an object from its dictionary representation.

    The dictionary representation of a VirusTotal API object has the following
    structure::

      {
        ""type"": <object type>,
        ""id"": <object id>,
        ""links"": {
          ""self"": ""https://www.virustotal.com/api/v3/<collection name>/<obj id>""
        },
        ""attributes"": {
          ...
        }
      }

    At least `type` and `id` are required to be present in the dictionary, if
    not, an exception is raised.
    """"""
<mask>:
        raise ValueError(f'Expecting dictionary, got: {type(obj_dict).__name__}')
    for field in ('type', 'id'):
        if field not in obj_dict:
            raise ValueError(f'Object {field} not found')
    obj = cls(obj_dict.get('type'), obj_dict.get('id'), obj_dict.get('attributes'))
    if 'context_attributes' in obj_dict:
        obj._context_attributes = obj_dict['context_attributes']
    if 'relationships' in obj_dict:
        obj._relationships = obj_dict['relationships']
    if 'error' in obj_dict:
        obj._error = obj_dict['error']
    return obj","not isinstance(obj_dict, dict)",112,"not isinstance(obj_dict, dict)",True,100.00000000000004,N/A
"def __init__(self, obj_type: str, obj_id: typing.Optional[str]=None, obj_attributes: typing.Optional[typing.Dict]=None):
    """"""Initializes a VirusTotal API object.""""""
<mask>:
        raise ValueError('Object attributes must be a dictionary')
    self._type = obj_type
    self._id = obj_id
    if obj_attributes:
        for attr, value in obj_attributes.items():
            setattr(self, attr, value)
    self._modified_attrs = []
    self._modified_data = {}
    self._error = None","not isinstance(obj_attributes, (dict, type(None)))",46,"obj_attributes and (not isinstance(obj_attributes, dict))",False,43.56033805378097,N/A
"def __init__(self, apikey: str, agent: str='unknown', host: typing.Optional[str]=None, trust_env: bool=False, timeout: int=300, proxy: typing.Optional[str]=None, headers: typing.Optional[typing.Dict]=None, verify_ssl: bool=True, connector: aiohttp.BaseConnector=None):
    """"""Initialize the client with the provided API key.""""""
<mask>:
        raise ValueError('API key must be a string')
    if not apikey:
        raise ValueError('API key can not be an empty string')
    self._host = host or _API_HOST
    self._apikey = apikey
    self._agent = agent
    self._session = None
    self._trust_env = trust_env
    self._timeout = timeout
    self._proxy = proxy
    self._user_headers = headers
    self._verify_ssl = verify_ssl
    if connector is not None:
        self._connector = connector
    else:
        try:
            event_loop = asyncio.get_event_loop()
        except RuntimeError:
            event_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(event_loop)
        self._connector = aiohttp.TCPConnector(ssl=self._verify_ssl, loop=event_loop)","not isinstance(apikey, str)",100,"not isinstance(apikey, str)",True,100.00000000000004,N/A
"def _full_url(self, path: str, *args: typing.Any) -> str:
    try:
        path = path.format(*args)
    except IndexError as exc:
        raise ValueError('Not enough arguments to fill all placeholders in path') from exc
<mask>:
        return path
    return self._host + _ENDPOINT_PREFIX + path",path.startswith('http'),37,self._host is None,False,8.116697886877475,N/A
"def _get_session(self) -> aiohttp.ClientSession:
<mask>:
        headers = {'X-Apikey': self._apikey, 'Accept-Encoding': 'gzip', 'User-Agent': _USER_AGENT_FMT.format_map({'agent': self._agent, 'version': __version__})}
        if self._user_headers:
            headers.update(self._user_headers)
        self._session = aiohttp.ClientSession(connector=self._connector, headers=headers, trust_env=self._trust_env, timeout=aiohttp.ClientTimeout(total=self._timeout))
    return self._session",not self._session,27,self._session is None,False,50.81327481546149,N/A
"def _extract_data_from_json(self, json_response: typing.Any) -> typing.Any:
<mask>:
        raise ValueError('response does not returns a data field')
    return json_response['data']",not 'data' in json_response,17,'data' not in json_response,False,56.234132519034915,N/A
"def __init__(self, client: 'Client', feed_type: FeedType, cursor: typing.Optional[str]=None):
    """"""Initializes a Feed object.

    This function is not intended to be called directly. Client.feed() is
    the preferred way for creating a feed.
    """"""
    self._client = client
    self._type = feed_type
    self._batch: typing.Optional[io.BytesIO] = None
    self._count = 0
    self._missing_batches_tolerancy = 1
<mask>:
        batch_time, _, batch_skip = cursor.partition('-')
        self._batch_time = datetime.strptime(batch_time, '%Y%m%d%H%M')
        self._batch_skip = int(batch_skip) if batch_skip else 0
    else:
        self._batch_time = datetime.utcnow() - timedelta(minutes=70)
        self._batch_skip = 0
    self._next_batch_time = self._batch_time",cursor,76,cursor,True,100.00000000000004,N/A
"def __init__(self, client: 'Client', path: str, params=None, cursor: typing.Optional[str]=None, limit: typing.Optional[int]=None, batch_size: int=0):
    """"""Initializes an iterator.

    This function is not intended to be called directly. Client.iterator() is
    the preferred way for creating an iterator.
    """"""
    self._client = client
    self._path = path
    self._params: typing.Dict = params or {}
    self._batch_size = batch_size
    self._limit = limit
    self._items: typing.List[typing.Dict] = []
    self._count = 0
    self._server_cursor: typing.Optional[str] = None
    self._batch_cursor = 0
    self._meta: typing.Optional[typing.Dict] = None
<mask>:
        raise ValueError('Do not pass ""cursor"" as a path param')
    if 'limit' in self._params:
        raise ValueError('Do not pass ""limit"" as a path param')
    if cursor:
        self._server_cursor, _, batch_cursor = cursor.rpartition('-')
        if not self._server_cursor:
            raise ValueError('invalid cursor')
        try:
            self._batch_cursor = int(batch_cursor)
        except ValueError as exc:
            raise ValueError('invalid cursor') from exc",'cursor' in self._params,120,'cursor' in self._params,True,100.00000000000004,N/A
"def _build_params(self) -> typing.Dict:
    params = self._params.copy()
<mask>:
        params['cursor'] = self._server_cursor
    if self._batch_size:
        params['limit'] = self._batch_size
    return params",self._server_cursor,18,self._server_cursor,True,100.00000000000004,N/A
"def _parse_response(self, json_resp: typing.Dict, batch_cursor: int) -> typing.Tuple[typing.List[typing.Dict], typing.Dict]:
<mask>:
        raise ValueError(f'{self._path} is not a collection')
    meta = json_resp.get('meta', {})
    items = json_resp['data'][batch_cursor:]
    return (items, meta)","not isinstance(json_resp.get('data'), list)",26,batch_cursor not in self.collections,False,3.4331054109918173,N/A
"@property
def cursor(self) -> typing.Optional[str]:
    """"""Cursor indicating the last returned object.

    This cursor can be used for creating a new iterator that continues where
    the current one left.
    """"""
<mask>:
        return None
    return self._server_cursor + '-' + str(self._batch_cursor)",not self._server_cursor or not self._count,38,self._server_cursor is None or self._batch_cursor is None,False,36.821398145189974,N/A
"@pytest.mark.usefixtures('iterator_response')
def test_next(httpserver):
    """"""Tests iterator's next with a limit higher than the total of elements.""""""
    with new_client(httpserver) as client:
        it = client.iterator('/dummy_collection/foo', limit=10, batch_size=3)
        assert next(it).id == 'dummy_id_1'
        assert next(it).id == 'dummy_id_2'
        assert it._batch_cursor == 2
        last = None
        for i, obj in enumerate(it):
            assert obj.id == f'dummy_id_{i + 3}'
<mask>:
                assert obj.error['code'] == 'NotFoundError'
            else:
                assert obj.order == 0
                assert obj.error is None
            last = obj
        assert last.id == 'dummy_id_5'
        assert it._count == 5
        assert it._batch_cursor == 2
        with pytest.raises(StopIteration):
            next(it)
        for obj in it:
            pytest.fail('Iteration should already be finished')",obj.id == 'dummy_id_5',91,obj.order == 0,False,11.786767588753092,N/A
"def process_item(item):
    """"""Processes a fetched item from the feed.""""""
    try:
        tags = item.tags
    except AttributeError:
        tags = []
    try:
        processes_created = item.processes_created
    except AttributeError:
        processes_created = []
<mask>:
        print(item.id.split('_')[0])",'executes-dropped-file' in tags or 'powershell.exe' in '\n'.join(processes_created),29,len(tags) == len(processes_created),False,20.125156761245176,N/A
"def print_results(res, netloc):
    """"""Print results for a given netloc.

  Results are only printed if there's a suspicious sighting.
  """"""
<mask>:
        n_spaces = 50 - len(netloc)
        print(f""{netloc}{' ' * n_spaces}{'  '.join((f'{n} detected {t} [max:{m}]' for m, n, t in res if m))}"")","any((x is not None for x, _, _ in res))",41,len(res) > 50,False,2.8722725093023906,N/A
"def main():
    parser = argparse.ArgumentParser(description='Get files from the VirusTotal feed. For each file in the feed a <sha256>.json file is created in the output directory containing information about the file. Additionally you can download the actual file with the --download-files option.')
    parser.add_argument('--apikey', required=True, help='your VirusTotal API key')
    parser.add_argument('--output', default='./file-feed', help='path to output directory')
    parser.add_argument('--download-files', action='store_true', help='download files')
    parser.add_argument('--cursor', required=False, help='cursor indicating where to start')
    args = parser.parse_args()
<mask>:
        os.makedirs(args.output)
    with vt.Client(args.apikey) as client:
        for file_obj in client.feed(vt.FeedType.FILES, cursor=args.cursor):
            file_path = os.path.join(args.output, file_obj.id)
            with open(file_path + '.json', mode='w', encoding='utf-8') as f:
                f.write(json.dumps(file_obj.to_dict()))
            if args.download_files:
                download_url = file_obj.context_attributes['download_url']
                response = client.get(download_url)
                with open(file_path, mode='wb') as f:
                    f.write(response.read())
            print(file_obj.id)",not os.path.exists(args.output),106,not os.path.exists(args.output),True,100.00000000000004,N/A
"@staticmethod
def create_download_folder(path=None):
    """"""Create the folder where the downloaded files will be put.""""""
    local_path = path or DEFAULT_PATH
    folder_name = time.strftime('%Y%m%dT%H%M%S')
    folder_path = os.path.join(local_path, folder_name)
<mask>:
        os.mkdir(local_path)
    if not os.path.exists(folder_path):
        os.mkdir(folder_path)
    return folder_path",not os.path.exists(local_path),33,not os.path.exists(local_path),True,100.00000000000004,N/A
"def print_results(self):
    """"""Print results of network infrastructure and commonalities analysis.""""""
    print('TOP CONTACTED DOMAINS')
    print('Num. Requests\tDomain')
    for domain_tuple in sorted(self.networking_counters['domains'].items(), key=lambda x: -x[1]):
        print(f'{domain_tuple[1]:>12}\t{domain_tuple[0]:>5}')
    print('TOP CONTACTED IPs')
    print('Num. Requests\tIP')
    for ip_tuple in sorted(self.networking_counters['ips'].items(), key=lambda x: -x[1]):
        print(f'{ip_tuple[1]:>12}\t{ip_tuple[0]:>12}')
    print('TOP CONTACTED URLs')
    print('Num. Requests\tURL')
    for url_tuple in sorted(self.networking_counters['urls'].items(), key=lambda x: -x[1]):
        print(f'{url_tuple[1]:>12}\t{url_tuple[0]:>12}')
    print('\nNETWORK INFRASTRUCTURE')
    for file_network in self.networking_infrastructure.items():
        contacted_addresses = file_network[1].values()
<mask>:
            print(f'File Hash: {file_network[0]}')
            for network_inf in file_network[1].items():
                if network_inf[1]:
                    print(f'\t{network_inf[0]}')
                    for address in network_inf[1]:
                        if address['type'] in ('domain', 'ip_address'):
                            print(f""\t\t{address['id']}"")
                        else:
                            print(f""\t\t{address['context_attributes']['url']}"")
    print('\nCOMMONALITIES BETWEEN FILES')
    for key, commonality in self.files_commonalities.items():
        comm_items = [(k, v) for k, v in commonality.items() if len(v) > self.MIN_IN_COMMON]
        if comm_items:
            print(f'{key}')
            for value_in_common, files_having_it_in_common in comm_items:
                value_in_common = str(value_in_common)
                print(f'\t{value_in_common:<32}\tFiles having it in common:')
                for file_hash in files_having_it_in_common:
                    print(f""\t{'':<32}\t{file_hash:<32}"")",any(contacted_addresses),123,len(contacted_addresses) > 0,False,51.697315395717055,N/A
"def main():
    parser = argparse.ArgumentParser(description='Get files from the VirusTotal feed. For each file in the feed a <sha256>.json file is created in the output directory containing information about the file. Additionally you can download the actual file with the --download-files option.')
    parser.add_argument('--apikey', required=True, help='your VirusTotal API key')
    parser.add_argument('--cursor', required=False, help='cursor indicating where to start')
    parser.add_argument('--output', default='./file-feed', help='path to output directory')
    parser.add_argument('--download-files', action='store_true', help='download files')
    parser.add_argument('--num_workers', type=int, required=False, help='number of concurrent workers', default=4)
    args = parser.parse_args()
<mask>:
        os.makedirs(args.output)
    feed_reader = FeedReader(args.apikey, args.output, num_workers=args.num_workers, download_files=args.download_files, cursor=args.cursor)
    feed_reader.run()
    print(f'\ncontinuation cursor: {feed_reader.cursor()}')",not os.path.exists(args.output),88,not os.path.exists(args.output),True,100.00000000000004,N/A
"def download_from_vt(file_id):
<mask>:
        logging.warning('Unsupported checksum length - %d', len(file_id))
        return
    with open(file_id, 'wb') as f:
        try:
            with vt.Client(os.environ.get(API_KEY_ENV_VAR)) as vt_client:
                vt_client.download_file(file_id, f)
        except Exception as e:
            logging.error('Exception while downloading file with a hash %s: %s', file_id, e)
        else:
            logging.info('Successfully downloaded %s', file_id)",len(file_id) not in SUPPORTED_CHECKSUM_LENS,42,len(file_id) != vt.checksum_len,False,39.553325358771794,N/A
"def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('user_input', help='File checksum (SHA-256, SHA-1, MD5) or text file containing list of checksums')
    args = parser.parse_args()
<mask>:
        logging.critical('Please set %s environment variable', API_KEY_ENV_VAR)
        return
    if os.path.isfile(args.user_input):
        logging.info('Treating input as a file, going to extract hashes...')
        with open(args.user_input, encoding='utf-8') as f:
            for line in f:
                download_from_vt(file_id=line.rstrip())
    else:
        logging.info('Treating input as a checksum...')
        download_from_vt(file_id=args.user_input)",os.environ.get(API_KEY_ENV_VAR) is None,57,args.user_input is None or args.user_input == '',False,6.437165254072419,N/A
"def print_results(self):
    """"""Print results of the network infrastructure analysis.""""""
    print('TOP CONTACTED DOMAINS')
    print('Num. Requests\tDomain')
    sorted_domains = sorted(self.networking_counters['domains'].items(), key=lambda x: -x[1])
    for domain_tuple in sorted_domains:
        print(f'{domain_tuple[1]:>12}\t{domain_tuple[0]:>5}')
    print('TOP CONTACTED IPs')
    print('Num. Requests\tIP')
    sorted_ips = sorted(self.networking_counters['domains'].items(), key=lambda x: -x[1])
    for ip_tuple in sorted_ips:
        print(f'{ip_tuple[1]:>12}\t{ip_tuple[0]:>12}')
    print('TOP CONTACTED URLs')
    print('Num. Requests\tURL')
    sorted_urls = sorted(self.networking_counters['domains'].items(), key=lambda x: -x[1])
    for url_tuple in sorted_urls:
        print(f'{url_tuple[1]:>12}\t{url_tuple[0]:>12}')
    print('\nNETWORK INFRASTRUCTURE')
    for file_network in self.networking_infrastructure.items():
        contacted_addresses = file_network[1].values()
<mask>:
            print(f'File Hash: {file_network[0]}')
            for network_inf in file_network[1].items():
                if network_inf[1]:
                    print(f'\t{network_inf[0]}')
                    for address in network_inf[1]:
                        if address['type'] in ('domain', 'ip_address'):
                            print(f""\t\t{address['id']}"")
                        else:
                            print(f""\t\t{address['context_attributes']['url']}"")",any(contacted_addresses),89,file_network[0] in contacted_addresses,False,15.619699684601283,N/A
"def create_reference(url, creation_date, title, author, client, iocs):
    """"""Creates a reference in VirusTotal.

  Args:
    url: Reference url.
    creation_date: Reference creation date (YY-MM-DD HH:mm:ss).
    title: Reference title.
    author: Author
    client: VirusTotal client.
    iocs: Dict with the different IOCs to add to the reference.

  Returns:
    The new reference object.
  """"""
    payload = {'attributes': {'url': url, 'creation_date': creation_date, 'title': title, 'author': author}, 'relationships': {}, 'type': 'reference', 'id': ''}
    add_iocs_to_reference_payload(iocs, payload)
    ref_url = base64.b64encode(url.encode()).decode().strip('=')
    response_get_reference = client.get(f'/references/{ref_url}')
    exists = response_get_reference.status == 200
<mask>:
        payload['id'] = response_get_reference.json()['data']['id']
    reference_obj = vt.Object.from_dict(payload)
    if exists:
        print(f'Patching reference {url}...')
        return client.patch_object(f'/references/{ref_url}', obj=reference_obj)
    else:
        print(f'Posting reference {url}...')
        return client.post_object('/references', obj=reference_obj)",exists,99,exists,True,100.00000000000004,N/A
"def add_iocs_to_reference_payload(iocs, reference_payload):
    """"""Adds IOCs relationships to a given reference.

  Args:
    iocs: Dict with the different IOCs to add to the reference.
    reference_payload: Reference payload
  """"""
    for relationship_name in ['files', 'domains', 'urls', 'ip_addresses']:
<mask>:
            continue
        if relationship_name == 'urls':
            descriptors = [{'type': 'url', 'url': u} for u in iocs['urls']]
        else:
            type_name = 'ip_address' if relationship_name == 'ip_addresses' else relationship_name[:-1]
            descriptors = [{'type': type_name, 'id': i} for i in iocs[relationship_name]]
        reference_payload['relationships'][relationship_name] = {'data': descriptors}",relationship_name not in iocs,73,relationship_name not in iocs,True,100.00000000000004,N/A
"def main():
    parser = argparse.ArgumentParser(description='Create references and add IOCs to them.')
    parser.add_argument('--apikey', required=True, help='your VirusTotal API key')
    args = parser.parse_args()
    client = vt.Client(args.apikey)
    url = 'https://blog.google/threat-analysis-group/new-campaign-targeting-security-researchers/'
    iocs = {'files': ['4c3499f3cc4a4fdc7e67417e055891c78540282dccc57e37a01167dfe351b244', '68e6b9d71c727545095ea6376940027b61734af5c710b2985a628131e47c6af7', '25d8ae4678c37251e7ffbaeddc252ae2530ef23f66e4c856d98ef60f399fa3dc', 'a75886b016d84c3eaacaf01a3c61e04953a7a3adf38acf77a4a2e3a8f544f855', 'a4fb20b15efd72f983f0fb3325c0352d8a266a69bb5f6ca2eba0556c3e00bd15'], 'urls': ['https://angeldonationblog.com/image/upload/upload.php', 'https://codevexillium.org/image/download/download.asp', 'https://investbooking.de/upload/upload.asp', 'https://transplugin.io/upload/upload.asp', 'https://www.dronerc.it/forum/uploads/index.php', 'https://www.dronerc.it/shop_testbr/Core/upload.php', 'https://www.dronerc.it/shop_testbr/upload/upload.php', 'https://www.edujikim.com/intro/blue/insert.asp', 'https://www.fabioluciani.com/es/include/include.asp', 'http://trophylab.com/notice/images/renewal/upload.asp', 'http://www.colasprint.com/_vti_log/upload.asp'], 'domains': ['angeldonationblog.comcodevexillium.org', 'investbooking.de', 'krakenfolio.com', 'opsonew3org.sg', 'transferwiser.io', 'transplugin.io', 'trophylab.com', 'www.colasprint.com', 'www.dronerc.it', 'www.edujikim.com', 'www.fabioluciani.com'], 'ip_addresses': ['193.70.64.169']}
    reference_obj = create_reference(url=url, creation_date='2021-01-25 00:00:00', title='New campaign targeting security researchers', author='Google Threat Analysis Group', iocs=iocs, client=client)
    client.close()
<mask>:
        pprint(reference_obj.to_dict())",reference_obj,79,not os.path.exists(url),False,0.0,N/A
"def render_template(entity, domains):
    domain_list = json.dumps(domains, indent=1)
    template = ''
    body_template = os.path.join(TEMPLATE_DIR, '_body.yara')
    with open(body_template, encoding='utf-8') as f:
        template += f.read().replace('${domain_list_json}', domain_list)
        template += '\n'
    kind_template = os.path.join(TEMPLATE_DIR, entity + '.yara')
    escaped_domains = {}
    with open(kind_template, encoding='utf-8') as f:
        rule_block = f.read()
        for domain in domains:
            domain_escaped = domain.lower()
            domain_escaped = re.compile('[^[a-z\\d]').sub('_', domain_escaped)
            domain_escaped = re.compile('(_(?i:_)+)').sub('_', domain_escaped)
<mask>:
                escaped_domains[domain_escaped] = 0
            escaped_domains[domain_escaped] += 1
            if escaped_domains[domain_escaped] > 1:
                domain_escaped = f'{domain_escaped}_{escaped_domains[domain_escaped]}'
            template += rule_block.replace('${domain}', domain).replace('${domain_escaped}', domain_escaped)
            template += '\n'
    return template",not domain_escaped in escaped_domains,82,domain_escaped not in escaped_domains,False,51.69731539571708,N/A
"def load_bulk_file_domains(filename):
<mask>:
        print(f'Error: File {filename} does not exists.')
        sys.exit(1)
    domains = []
    with open(filename, encoding='utf-8') as bulk_file:
        for line in bulk_file.read().split('\n'):
            if not line:
                continue
            domains.append(line)
    return domains",not os.path.isfile(filename),29,not os.path.exists(filename),False,59.694917920196445,N/A
"def print_results(self):
    """"""Pretty print network IoCs for the given VTI search query.""""""
    print('\n\n=== Results: ===')
    for item in self.networking_infrastructure.items():
        contacted_addr = item[1].values()
<mask>:
            for inf in item[1].items():
                for key in inf[1]:
                    k = key['type'].upper()
                    v = key.get('context_attributes', {}).get('url') or key.get('id')
                    print(f'{k}: {v}')",any(contacted_addr),42,contacted_addr[0] == 'tier',False,17.747405280050266,N/A
"def main():
    parser = argparse.ArgumentParser(description='Scan file privately using VirusTotal API')
    parser.add_argument('--apikey', help='VirusTotal API key')
    parser.add_argument('--file_path', help='Path to file to scan')
    parser.add_argument('--wait', action='store_true', help='Wait for scan completion')
    args = parser.parse_args()
    file_path = Path(args.file_path)
<mask>:
        console.print(f'[red]Error: File {file_path} not found[/red]')
        sys.exit(1)
    if not file_path.is_file():
        console.print(f'[red]Error: {file_path} is not a file[/red]')
        sys.exit(1)
    asyncio.run(scan_file_private(args.apikey, file_path, args.wait))",not file_path.exists(),52,not file_path.exists(),True,100.00000000000004,N/A
"def iterate(data, path):
    """"""Generator that returns values in data matching the given JsonPath.""""""
<mask>:
        return
    parsed_path = _PARSED_PATH_CACHE.get(path)
    if not parsed_path:
        parsed_path = jsonpath_ng.parse(path)
        _PARSED_PATH_CACHE[path] = parsed_path
    for item in parsed_path.find(data):
        yield item.value",not data,33,path not in _PARSED_PATH_CACHE,False,4.767707020457095,N/A
"def get(data, path, default=None):
    """"""Returns the value in data matching the given JsonPath.

  If the path matches more than one value an exception is raised.
  Args:
    data: Data to be queried.
    path: Query string in JsonPath format.
    default: Value returned when there are no results.

  Returns:
    The value in data matching the given path.
  Raises:
    Exception: If the provided path matches more than one value.
  """"""
    result = get_all(data, path)
<mask>:
        return default
    if len(result) > 1:
        raise DictPathException(f'JsonPath {path} returning more than one value')
    return result[0]",not result,87,result is None,False,27.516060407455225,N/A
"def __call__(self, headers: Dict) -> Any:
    v = headers.get(self.key, self.default)
<mask>:
        return self.default
    return v",self.allowed_values is not None and v not in self.allowed_values,15,v is None,False,0.37318062716230643,N/A
"def from_header(key: str, allowed_values: Optional[Iterable]=None) -> Callable:
    """"""returns a function that retrieves a header value from a request.
    The returned function can be passed to the `labels` argument of PrometheusMiddleware
    to label metrics using a header value.

    `key`: header key
    `allowed_values`: an iterable (e.g. list or tuple) containing an allowlist of values. Any
    header value not in allowed_values will result in an empty string being returned.  Use
    this to constrain the potential label values.

    example:

    ```
        PrometheusMiddleware(
            labels={
                ""host"": from_header(""X-User"", allowed_values=(""frank"", ""estelle""))
            }
        )
    ```
    """"""

    def inner(r: Request):
        v = r.headers.get(key, '')
<mask>:
            return ''
        return v
    return inner",allowed_values is not None and v not in allowed_values,100,v is None,False,1.4157233641833757,N/A
"def handle_metrics(request: Request) -> Response:
    """"""A handler to expose Prometheus metrics
    Example usage:

        ```
        app.add_middleware(PrometheusMiddleware)
        app.add_route(""/metrics"", handle_metrics)
        ```
    """"""
    registry = REGISTRY
<mask>:
        registry = CollectorRegistry()
        multiprocess.MultiProcessCollector(registry)
    headers = {'Content-Type': CONTENT_TYPE_LATEST}
    return Response(generate_latest(registry), status_code=200, headers=headers)",'prometheus_multiproc_dir' in os.environ or 'PROMETHEUS_MULTIPROC_DIR' in os.environ,35,request.method == 'POST',False,0.9298395269585749,N/A
"def handle_openmetrics(request: Request) -> Response:
    """"""A handler to expose Prometheus metrics in OpenMetrics format.
    This is required to expose metrics with exemplars.
    Example usage:

        ```
        app.add_middleware(PrometheusMiddleware)
        app.add_route(""/metrics"", openmetrics_handler)
        ```
    """"""
    registry = REGISTRY
<mask>:
        registry = CollectorRegistry()
        multiprocess.MultiProcessCollector(registry)
    headers = {'Content-Type': openmetrics_content_type_latest}
    return Response(openmetrics_generate_latest(registry), status_code=200, headers=headers)",'prometheus_multiproc_dir' in os.environ or 'PROMETHEUS_MULTIPROC_DIR' in os.environ,46,request.method == 'POST',False,0.9298395269585749,N/A
"def get_matching_route_path(scope: Dict[Any, Any], routes: List[BaseRoute], route_name: Optional[str]=None) -> Optional[str]:
    """"""
    Find a matching route and return its original path string

    Will attempt to enter mounted routes and subrouters.

    Credit to https://github.com/elastic/apm-agent-python
    """"""
    for route in routes:
        match, child_scope = route.matches(scope)
<mask>:
            route_name = getattr(route, 'path', None)
            if route_name is None:
                return None
            if isinstance(route, BaseRoute) and getattr(route, 'routes', None):
                child_scope = {**scope, **child_scope}
                child_route_name = get_matching_route_path(child_scope, getattr(route, 'routes'), route_name)
                if child_route_name is None:
                    route_name = None
                else:
                    route_name += child_route_name
            return route_name
        elif match == Match.PARTIAL and route_name is None:
            route_name = getattr(route, 'path', None)
    return None",match == Match.FULL,98,match == Match.ROOT and route_name is None,False,31.702331385234313,N/A
"def __init__(self, app: ASGIApp, group_paths: bool=True, app_name: str='starlette', prefix: str='starlette', buckets: Optional[Sequence[Union[float, str]]]=None, filter_unhandled_paths: bool=True, skip_paths: Optional[List[str]]=None, skip_methods: Optional[List[str]]=None, optional_metrics: Optional[List[str]]=None, always_use_int_status: bool=False, labels: Optional[Mapping[str, Union[str, Callable]]]=None, exemplars: Optional[Callable]=None, group_unhandled_paths: bool=False):
    self.app = app
    self.app_name = app_name
    self.prefix = prefix
    self.group_paths = group_paths
<mask>:
        filter_unhandled_paths = False
        warnings.warn('filter_unhandled_paths was set to True but has been changed to False because group_unhandled_paths is True and these settings are mutually exclusive', UserWarning)
    self.group_unhandled_paths = group_unhandled_paths
    self.filter_unhandled_paths = filter_unhandled_paths
    self.kwargs = {}
    if buckets is not None:
        self.kwargs['buckets'] = buckets
    self.skip_paths: List[re.Pattern] = []
    if skip_paths is not None:
        self.skip_paths = [re.compile(path) for path in skip_paths]
    self.skip_methods = []
    if skip_methods is not None:
        self.skip_methods = skip_methods
    self.optional_metrics_list = []
    if optional_metrics is not None:
        self.optional_metrics_list = optional_metrics
    self.always_use_int_status = always_use_int_status
    self.exemplars = exemplars
    self._exemplars_req_kw = ''
    if self.exemplars:
        exemplar_sig = inspect.signature(self.exemplars)
        for p in exemplar_sig.parameters.values():
            if p.annotation is Request:
                self._exemplars_req_kw = p.name
                break
        else:
            if 'request' in exemplar_sig.parameters:
                self._exemplars_req_kw = 'request'
    self.request_labels = OrderedDict({})
    self.response_labels: OrderedDict[str, ResponseHeaderLabel] = OrderedDict({})
    if labels is not None:
        for k, v in labels.items():
            if isinstance(v, ResponseHeaderLabel):
                self.response_labels[k] = v
            else:
                self.request_labels[k] = v",group_unhandled_paths and filter_unhandled_paths,186,group_unhandled_paths and filter_unhandled_paths,True,100.00000000000004,N/A
"@property
def request_count(self):
    metric_name = f'{self.prefix}_requests_total'
<mask>:
        PrometheusMiddleware._metrics[metric_name] = Counter(metric_name, 'Total HTTP requests', ('method', 'path', 'status_code', 'app_name', *self.request_labels.keys(), *self.response_labels.keys()))
    return PrometheusMiddleware._metrics[metric_name]",metric_name not in PrometheusMiddleware._metrics,21,metric_name not in PrometheusMiddleware._metrics,True,100.00000000000004,N/A
"@property
def response_body_size_count(self):
    """"""
        Optional metric for tracking the size of response bodies.
        If using gzip middleware, you should test that the starlette_exporter middleware computes
        the proper response size value. Please post any feedback on this metric as an issue
        at https://github.com/stephenhillier/starlette_exporter.

        """"""
<mask>:
        metric_name = f'{self.prefix}_response_body_bytes_total'
        if metric_name not in PrometheusMiddleware._metrics:
            PrometheusMiddleware._metrics[metric_name] = Counter(metric_name, 'Total HTTP response body bytes', ('method', 'path', 'status_code', 'app_name', *self.request_labels.keys(), *self.response_labels.keys()))
        return PrometheusMiddleware._metrics[metric_name]
    else:
        pass",self.optional_metrics_list is not None and optional_metrics.response_body_size in self.optional_metrics_list,70,self.gzip,False,0.013228042066535462,N/A
"@property
def request_body_size_count(self):
    """"""
        Optional metric tracking the received content-lengths of request bodies
        """"""
<mask>:
        metric_name = f'{self.prefix}_request_body_bytes_total'
        if metric_name not in PrometheusMiddleware._metrics:
            PrometheusMiddleware._metrics[metric_name] = Counter(metric_name, 'Total HTTP request body bytes', ('method', 'path', 'status_code', 'app_name', *self.request_labels.keys(), *self.response_labels.keys()))
        return PrometheusMiddleware._metrics[metric_name]
    else:
        pass",self.optional_metrics_list is not None and optional_metrics.request_body_size in self.optional_metrics_list,41,self.prefix,False,0.013228042066535462,N/A
"@pytest.fixture
def testapp():
    """"""create a test app with various endpoints for the test scenarios""""""
    collectors = list(REGISTRY._collector_to_names.keys())
    for collector in collectors:
        REGISTRY.unregister(collector)
    PrometheusMiddleware._metrics = {}

    def _testapp(**middleware_options):
        app = Starlette()
        app.add_middleware(starlette_exporter.PrometheusMiddleware, **middleware_options)
        app.add_route('/metrics', handle_metrics)
        app.add_route('/openmetrics', handle_openmetrics)

        def normal_response(_):
            return JSONResponse({'message': 'Hello World'}, headers={'foo': 'baz'})
        app.add_route('/200', normal_response, methods=['GET', 'POST', 'OPTIONS'])
        app.add_route('/200/{test_param}', normal_response, methods=['GET', 'POST', 'OPTIONS'])

        def httpstatus_response(request):
            """"""
            Returns a JSON Response using status_code = HTTPStatus.OK if the param is set to OK
            otherewise it returns a JSON response with status_code = 200
            """"""
<mask>:
                return Response(status_code=HTTPStatus.OK)
            else:
                return Response(status_code=200)
        app.add_route('/200_or_httpstatus/{test_param}', httpstatus_response, methods=['GET', 'OPTIONS'])

        async def error(request):
            raise HTTPException(status_code=500, detail='this is a test error', headers={'foo': 'baz'})
        app.add_route('/500', error)
        app.add_route('/500/{test_param}', error)

        async def unhandled(request):
            test_dict = {'yup': 123}
            return JSONResponse({'message': test_dict['value_error']})
        app.add_route('/unhandled', unhandled)
        app.add_route('/unhandled/{test_param}', unhandled)

        async def background(request):

            def backgroundtask():
                time.sleep(0.1)
            task = BackgroundTask(backgroundtask)
            return JSONResponse({'message': 'task started'}, background=task)
        app.add_route('/background', background)

        def healthcheck(request):
            return JSONResponse({'message': 'Healthcheck route'})
        app.add_route('/health', healthcheck)

        async def test_mounted_function(request):
            return JSONResponse({'message': 'Hello World'})

        async def test_mounted_function_param(request):
            return JSONResponse({'message': request.path_params.get('item')})
        mounted_routes = Mount('/', routes=[Route('/test/{item}', test_mounted_function_param, methods=['GET']), Route('/test', test_mounted_function)])
        app.mount('/mounted', mounted_routes)
        app.mount('/static', app=StaticFiles(directory='tests/static'), name='static')
        return app
    return _testapp",request.path_params['test_param'] == 'OK',177,request.method == 'POST',False,7.1490359214859,N/A
"@pytest.mark.parametrize('annotated', [False, True])
def test_exemplar_request(self, testapp, annotated: bool) -> None:
    """"""test setting exemplar with request injection""""""
<mask>:

        def exemplar_fn(r: Request):
            return {'trace_id': r.headers.get('trace-id', '')}
    else:

        def exemplar_fn(request):
            return {'trace_id': request.headers.get('trace-id', '')}
    labels = {'test': 'exemplar'}
    client = TestClient(testapp(exemplars=exemplar_fn, labels=labels))
    client.get('/200', headers={'Trace-ID': 'abc123'})
    metrics = client.get('/openmetrics', headers={'Accept': 'application/openmetrics-text', 'Trace-ID': 'abc123'}).content.decode()
    assert 'starlette_requests_total{app_name=""starlette"",method=""GET"",path=""/200"",status_code=""200"",test=""exemplar""} 1.0 # {trace_id=""abc123""}' in metrics, metrics",annotated,57,annotated,True,100.00000000000004,N/A
"def json_response(data, status=200, **kwargs):
    kwargs['headers'] = {'Access-Control-Allow-Origin': '*'}
    kwargs['text'] = json.dumps(data, indent=2, sort_keys=True)
<mask>:
        kwargs['content_type'] = 'application/json'
    return web.Response(status=status, **kwargs)",'content_type' not in kwargs,20,status == 400,False,0.0,N/A
"def format_validator_info(node_data):
    node_aliases = list(node_data.keys())
    node_aliases.sort()
    ret = []
    for node in node_aliases:
        try:
            reply = json.loads(node_data[node])
        except json.JSONDecodeError:
            data = {'Node_info': {'Name': node}, 'error': node_data[node]}
        else:
<mask>:
                data = reply['result']['data']
            elif 'reason' in reply:
                data = {'Node_info': {'Name': node}, 'error': reply['reason']}
            else:
                data = {'Node_info': {'Name': node}, 'error': 'unknown error'}
        ret.append(data)
    return ret",'result' in reply,54,'result' in reply,True,100.00000000000004,N/A
"def seed_as_bytes(seed):
<mask>:
        return seed
    if len(seed) != 32:
        return base64.b64decode(seed)
    return seed.encode('ascii')","not seed or isinstance(seed, bytes)",13,not seed,False,3.0197383422318516,N/A
"def sign_request(self, req: ledger.Request, apply_taa: bool=True):
<mask>:
        raise AnchorException('Cannot sign request: no DID')
    if apply_taa and self._taa_accept:
        req.set_txn_author_agreement_acceptance(self._taa_accept)
    key = nacl.signing.SigningKey(self._seed)
    signed = key.sign(req.signature_input)
    req.set_signature(signed.signature)
    return req",not self._did,27,not req.did,False,16.37226966703825,N/A
"def compare_txns(self, txnA: dict, txnB: dict) -> bool:
    match = True
    for k in ('txn', 'txnMetadata', 'reqSignature'):
<mask>:
            match = False
            print(txnA)
            print('<<<>>>')
            print(txnB)
            break
    return match",txnA[k] != txnB[k],27,not self.txnA[k] == txnB[k],False,48.44273237963865,N/A
"def txn_extract_terms(txn_json):
    data = json.loads(txn_json)
    result = {}
    txntype = None
    ledger_size = None
<mask>:
        ledger_size = data.get('ledgerSize')
        txnmeta = data.get('txnMetadata', {})
        result['txnid'] = txnmeta.get('txnId')
        txn = data.get('txn', {})
        txntype = txn.get('type')
        meta = txn.get('metadata', {})
        result['sender'] = meta.get('from')
        if txntype == '1':
            result['ident'] = txn['data']['dest']
            result['alias'] = txn['data'].get('alias')
            short_verkey = None
            if 'verkey' in txn['data']:
                verkey = txn['data']['verkey']
                try:
                    did = base58.b58decode(txn['data']['dest'])
                    if verkey[0] == '~':
                        short_verkey = verkey
                        suffix = base58.b58decode(verkey[1:])
                        verkey = base58.b58encode(did + suffix).decode('ascii')
                    else:
                        long = base58.b58decode(verkey)
                        if long[0:16] == did:
                            short_verkey = '~' + base58.b58encode(long[16:]).decode('ascii')
                except ValueError:
                    LOGGER.error('Error decoding verkey: %s', verkey)
                result['short_verkey'] = short_verkey
                result['verkey'] = verkey
            else:
                result['short_verkey'] = None
                result['verkey'] = None
            role_id = txn['data'].get('role')
            result['data'] = INDY_ROLE_TYPES.get(role_id)
        elif txntype == '100':
            result['ident'] = txn['data']['dest']
            raw_data = txn['data'].get('raw', '{}')
            data = json.loads(raw_data) or {}
            result['alias'] = data.get('endpoint', {}).get('endpoint')
        elif txntype == '101':
            result['ident'] = '{} {}'.format(txn['data']['data']['name'], txn['data']['data']['version'])
            result['data'] = ' '.join(txn['data']['data']['attr_names'])
        elif txntype == '102':
            result['data'] = ' '.join(txn['data']['data']['primary'].keys())
    return (txntype, result, ledger_size)",data,162,data.get('ledgerSize'),False,8.116697886877475,N/A
"@pytest.mark.parametrize('header, user, method, status, user_name', [('X-User', 'alice', 'GET', 200, 'X-User'), ('X-USER', 'alice', 'GET', 200, 'x-user'), ('x-user', 'alice', 'GET', 200, 'X-USER'), ('X-User', 'alice', 'GET', 200, 'X-USER'), ('X-User', 'alice', 'GET', 200, 'X-Not-A-Header'), ('X-User', 'alice', 'POST', 201, None), ('X-User', 'alice', 'DELETE', 202, None), ('X-User', 'bob', 'GET', 200, None), ('X-User', 'bob', 'POST', 401, None), ('X-User', 'bob', 'DELETE', 401, None), ('X-Idp-Groups', 'admin', 'GET', 401, 'X-User'), ('X-Idp-Groups', 'group with space, users', 'GET', 200, None), ('X-Idp-Groups', 'noexist,testnoexist,users', 'GET', 200, None), ('X-Idp-Groups', 'noexist, testnoexist, users', 'GET', 200, None), ('X-Idp-Groups', 'group with space', 'GET', 200, None), ('X-Idp-Groups', 'somegroup, group with space', 'GET', 200, None), ('Authorization', 'Basic Ym9iOnBhc3N3b3Jk', 'GET', 200, 'Authorization'), ('Authorization', 'Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpZGVudGl0eSI6ImJvYiJ9.LM-CqxAM2MtT2uT3AO69rZ3WJ81nnyMQicizh4oqBwk', 'GET', 200, None), ('Authorization', 'Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJleHAiOjE2MTUxMDg0OTIuNTY5MjksImlkZW50aXR5IjoiQm9iIn0.CAeMpG-gKbucHU7-KMiqM7H_gTkHSRvXSjNtlvh5DlE', 'GET', 401, None), ('Authorization', 'Unsupported Ym9iOnBhc3N3b3Jk', 'GET', 401, None), ('Authorization', 'Unsupported Ym9iOnBhc3N3b3Jk', 'GET', 401, None)])
def test_enforcer(app_fixture, enforcer, header, user, method, status, user_name):
<mask>:
        enforcer.user_name_headers = {user_name}

    @app_fixture.route('/')
    @enforcer.enforcer
    def index():
        return (jsonify({'message': 'passed'}), 200)

    @app_fixture.route('/item', methods=['GET', 'POST', 'DELETE'])
    @enforcer.enforcer
    def item():
        if request.method == 'GET':
            return (jsonify({'message': 'passed'}), 200)
        elif request.method == 'POST':
            return (jsonify({'message': 'passed'}), 201)
        elif request.method == 'DELETE':
            return (jsonify({'message': 'passed'}), 202)
    headers = {header: user}
    c = app_fixture.test_client()
    rv = c.get('/')
    assert rv.status_code == 401
    caller = getattr(c, method.lower())
    rv = caller('/item', headers=headers)
    assert rv.status_code == status",user_name,202,status == 'ok',False,0.0,N/A
"@pytest.mark.parametrize('header, user, method, status', [('X-User', 'alice', 'GET', 200), ('X-User', 'alice', 'POST', 201), ('X-User', 'alice', 'DELETE', 202), ('X-User', 'bob', 'GET', 200), ('X-User', 'bob', 'POST', 401), ('X-User', 'bob', 'DELETE', 401), ('X-Idp-Groups', 'admin', 'GET', 401), ('X-Idp-Groups', 'users', 'GET', 200), ('Authorization', 'Basic Ym9iOnBhc3N3b3Jk', 'GET', 200), ('Authorization', 'Unsupported Ym9iOnBhc3N3b3Jk', 'GET', 401)])
def test_enforcer_with_watcher(app_fixture, enforcer, header, user, method, status, watcher):
    enforcer.set_watcher(watcher())

    @app_fixture.route('/')
    @enforcer.enforcer
    def index():
        return (jsonify({'message': 'passed'}), 200)

    @app_fixture.route('/item', methods=['GET', 'POST', 'DELETE'])
    @enforcer.enforcer
    def item():
<mask>:
            return (jsonify({'message': 'passed'}), 200)
        elif request.method == 'POST':
            return (jsonify({'message': 'passed'}), 201)
        elif request.method == 'DELETE':
            return (jsonify({'message': 'passed'}), 202)
    headers = {header: user}
    c = app_fixture.test_client()
    rv = c.get('/')
    assert rv.status_code == 401
    caller = getattr(c, method.lower())
    rv = caller('/item', headers=headers)
    assert rv.status_code == status",request.method == 'GET',117,request.method == 'GET',True,100.00000000000004,N/A
"@pytest.mark.parametrize('owner, method, status', [(['alice'], 'GET', 200), (['alice'], 'POST', 201), (['alice'], 'DELETE', 202), (['bob'], 'GET', 200), (['bob'], 'POST', 401), (['bob'], 'DELETE', 401), (['admin'], 'GET', 401), (['users'], 'GET', 200), (['alice', 'bob'], 'POST', 201), (['noexist', 'testnoexist'], 'POST', 401)])
def test_enforcer_with_owner_loader(app_fixture, enforcer, owner, method, status):

    @app_fixture.route('/')
    @enforcer.enforcer
    def index():
        return (jsonify({'message': 'passed'}), 200)

    @app_fixture.route('/item', methods=['GET', 'POST', 'DELETE'])
    @enforcer.enforcer
    def item():
<mask>:
            return (jsonify({'message': 'passed'}), 200)
        elif request.method == 'POST':
            return (jsonify({'message': 'passed'}), 201)
        elif request.method == 'DELETE':
            return (jsonify({'message': 'passed'}), 202)

    @enforcer.owner_loader
    def owner_loader():
        return owner
    c = app_fixture.test_client()
    rv = c.get('/')
    assert rv.status_code == 401
    caller = getattr(c, method.lower())
    rv = caller('/item')
    assert rv.status_code == status",request.method == 'GET',103,request.method == 'GET',True,100.00000000000004,N/A
"@pytest.mark.parametrize('header, user, method, status, user_name', [('X-User', 'alice', 'GET', 200, 'X-User'), ('X-USER', 'alice', 'GET', 200, 'x-user'), ('x-user', 'alice', 'GET', 200, 'X-USER'), ('X-User', 'alice', 'GET', 200, 'X-USER'), ('X-User', 'alice', 'GET', 200, 'X-Not-A-Header'), ('X-User', 'alice', 'POST', 201, None), ('X-User', 'alice', 'DELETE', 202, None), ('X-User', 'bob', 'GET', 200, None), ('X-User', 'bob', 'POST', 401, None), ('X-User', 'bob', 'DELETE', 401, None), ('X-Idp-Groups', 'admin', 'GET', 401, 'X-User'), ('X-Idp-Groups', 'users', 'GET', 200, None), ('X-Idp-Groups', 'noexist,testnoexist,users', 'GET', 200, None), ('X-Idp-Groups', 'noexist, testnoexist, users', 'GET', 200, None), ('Authorization', 'Basic Ym9iOnBhc3N3b3Jk', 'GET', 200, 'Authorization'), ('Authorization', 'Unsupported Ym9iOnBhc3N3b3Jk', 'GET', 401, None)])
def test_enforcer(app_fixture, enforcer, header, user, method, status, user_name):
<mask>:
        enforcer.user_name_headers = {user_name}

    @app_fixture.route('/')
    @enforcer.enforcer
    def index():
        return (jsonify({'message': 'passed'}), 200)

    @app_fixture.route('/item', methods=['GET', 'POST', 'DELETE'])
    @enforcer.enforcer
    def item():
        if request.method == 'GET':
            return (jsonify({'message': 'passed'}), 200)
        elif request.method == 'POST':
            return (jsonify({'message': 'passed'}), 201)
        elif request.method == 'DELETE':
            return (jsonify({'message': 'passed'}), 202)
    headers = {header: user}
    c = app_fixture.test_client()
    rv = c.get('/')
    assert rv.status_code == 401
    caller = getattr(c, method.lower())
    rv = caller('/item', headers=headers)
    assert rv.status_code == status",user_name,166,user_name,True,100.00000000000004,N/A
"def __init__(self, app=None, adapter=None, watcher=None):
    """"""
        Args:
            app (object): Flask App object to get Casbin Model
            adapter (object): Casbin Adapter
        """"""
    self.app = app
    self.adapter = adapter
    self.e = None
    self.watcher = watcher
    self._owner_loader = None
    self.user_name_headers = None
<mask>:
        self.init_app(self.app)",self.app is not None,41,self.app is not None,True,100.00000000000004,N/A
"def init_app(self, app):
    self.app = app
    self.e = casbin.Enforcer(app.config.get('CASBIN_MODEL'), self.adapter)
<mask>:
        self.e.set_watcher(self.watcher)
    self.user_name_headers = app.config.get('CASBIN_USER_NAME_HEADERS', None)",self.watcher,16,self.watcher is not None,False,30.213753973567677,N/A
"def enforcer(self, func, delimiter=','):

    @wraps(func)
    def wrapper(*args, **kwargs):
<mask>:
            self.e.watcher.update_callback()
        owner_audit = ''
        self.app.logger.debug('Enforce Headers Config: %s\nRequest Headers: %s' % (self.app.config.get('CASBIN_OWNER_HEADERS'), request.headers))
        uri = str(request.path)
        if self._owner_loader:
            self.app.logger.info('Get owner from owner_loader')
            for owner in self._owner_loader():
                owner = owner.strip('""') if isinstance(owner, str) else owner
                if self.e.enforce(owner, uri, request.method):
                    return func(*args, **kwargs)
        for header in map(str.lower, self.app.config.get('CASBIN_OWNER_HEADERS')):
            if header in request.headers:
                if header == 'authorization':
                    try:
                        owner = authorization_decoder(self.app.config, request.headers.get(header))
                    except UnSupportedAuthType:
                        self.app.logger.info('Authorization header type requested for decoding is unsupported by flask-casbin at this time')
                        continue
                    except Exception as e:
                        self.app.logger.info(e)
                        continue
                    if self.user_name_headers and header in map(str.lower, self.user_name_headers):
                        owner_audit = owner
                    if self.e.enforce(owner, uri, request.method):
                        self.app.logger.info('access granted: method: %s resource: %s%s' % (request.method, uri, '' if not self.user_name_headers and owner_audit != '' else ' to user: %s' % owner_audit))
                        return func(*args, **kwargs)
                else:
                    for owner in self.sanitize_group_headers(request.headers.get(header), delimiter):
                        self.app.logger.debug('Enforce against owner: %s header: %s' % (owner.strip('""'), header))
                        if self.user_name_headers and header in map(str.lower, self.user_name_headers):
                            owner_audit = owner
                        if self.e.enforce(owner.strip('""'), uri, request.method):
                            self.app.logger.info('access granted: method: %s resource: %s%s' % (request.method, uri, '' if not self.user_name_headers and owner_audit != '' else ' to user: %s' % owner_audit))
                            return func(*args, **kwargs)
        else:
            self.app.logger.error('Unauthorized attempt: method: %s resource: %s%s' % (request.method, uri, '' if not self.user_name_headers and owner_audit != '' else ' by user: %s' % owner_audit))
            return (jsonify({'message': 'Unauthorized'}), 401)
    return wrapper",self.e.watcher and self.e.watcher.should_reload(),218,self.e.watcher,False,9.071795328941255,N/A
"@staticmethod
def sanitize_group_headers(headers_str, delimiter=',') -> list:
    """"""
        Sanitizes group header string so that it is easily parsable by enforcer
        removes extra spaces, and converts comma delimited or white space
        delimited list into a list.

        Default delimiter: "","" (comma)

        Returns:
            list
        """"""
<mask>:
        return [string.strip() for string in shlex.split(headers_str) if string != '']
    return [string.strip() for string in headers_str.split(delimiter) if string != '']","delimiter == ' ' and (headers_str.startswith(""'"") and headers_str.endswith(""'"") or (headers_str.startswith('""') and headers_str.endswith('""')))",62,"delimiter == ','",False,0.021292725087545785,N/A
"def __init__(self, message, status_code=None, payload=None, errors=None):
    Exception.__init__(self)
    self.message = message
<mask>:
        self.status_code = status_code
    self.payload = payload
    self.errors = errors",status_code is not None,20,status_code is not None,True,100.00000000000004,N/A
"def to_dict(self):
    rv = dict(self.payload or ())
    rv['message'] = self.message
<mask>:
        rv['errors'] = self.errors
    return rv",self.errors is not None,16,self.errors,False,36.78794411714425,N/A
"def authorization_decoder(config, auth_str: str):
    """"""
    Authorization token decoder based on type. This will decode the token and
    only return the owner
    Args:
        config: app.config object
        auth_str: Authorization string should be in ""<type> <token>"" format
    Returns:
        decoded owner from token
    """"""
    type, token = auth_str.split()
<mask>:
        'Basic format <user>:<password> return only the user'
        return b64decode(token).decode().split(':')[0]
    elif type == 'Bearer':
        'return only the identity， depends on JWT 2.x'
        decoded_jwt = jwt.decode(token, config.get('JWT_SECRET_KEY'), algorithms=config.get('JWT_HASH'))
        return decoded_jwt.get('identity', '')
    else:
        raise UnSupportedAuthType('%s Authorization is not supported' % type)",type == 'Basic',83,type == 'Basic',True,100.00000000000004,N/A
"def __init__(self, hs, config):
    self.cache_directory = hs.config.media.media_store_path
    self.bucket = config['bucket']
    self.prefix = config['prefix']
    self.extra_args = config['extra_args']
    self.api_kwargs = {}
<mask>:
        self.api_kwargs['region_name'] = config['region_name']
    if 'endpoint_url' in config:
        self.api_kwargs['endpoint_url'] = config['endpoint_url']
    if 'access_key_id' in config:
        self.api_kwargs['aws_access_key_id'] = config['access_key_id']
    if 'secret_access_key' in config:
        self.api_kwargs['aws_secret_access_key'] = config['secret_access_key']
    if 'session_token' in config:
        self.api_kwargs['aws_session_token'] = config['session_token']
    self._s3_client = None
    self._s3_client_lock = threading.Lock()
    threadpool_size = config.get('threadpool_size', 40)
    self._s3_pool = ThreadPool(name='s3-pool', maxthreads=threadpool_size)
    self._s3_pool.start()
    reactor.addSystemEventTrigger('during', 'shutdown', self._s3_pool.stop)",'region_name' in config,69,'region_name' in config,True,100.00000000000004,N/A
"def _get_s3_client(self):
    s3 = self._s3_client
<mask>:
        return s3
    with self._s3_client_lock:
        s3 = self._s3_client
        if not s3:
            b3_session = boto3.session.Session()
            self._s3_client = s3 = b3_session.client('s3', **self.api_kwargs)
        return s3",s3,27,s3,True,100.00000000000004,N/A
"@staticmethod
def parse_config(config):
    """"""Called on startup to parse config supplied. This should parse
        the config and raise if there is a problem.

        The returned value is passed into the constructor.

        In this case we return a dict with fields, `bucket`, `prefix` and `storage_class`
        """"""
    bucket = config['bucket']
    prefix = config.get('prefix', '')
    storage_class = config.get('storage_class', 'STANDARD')
    assert isinstance(bucket, string_types)
    assert storage_class in _VALID_STORAGE_CLASSES
    result = {'bucket': bucket, 'prefix': prefix, 'extra_args': {'StorageClass': storage_class}}
<mask>:
        result['region_name'] = config['region_name']
    if 'endpoint_url' in config:
        result['endpoint_url'] = config['endpoint_url']
    if 'access_key_id' in config:
        result['access_key_id'] = config['access_key_id']
    if 'secret_access_key' in config:
        result['secret_access_key'] = config['secret_access_key']
    if 'session_token' in config:
        result['session_token'] = config['session_token']
    if 'sse_customer_key' in config:
        result['extra_args']['SSECustomerKey'] = config['sse_customer_key']
        result['extra_args']['SSECustomerAlgorithm'] = config.get('sse_customer_algo', 'AES256')
    return result",'region_name' in config,116,'region_name' in config,True,100.00000000000004,N/A
"def s3_download_task(s3_client, bucket, key, extra_args, deferred, parent_logcontext):
    """"""Attempts to download a file from S3.

    Args:
        s3_client: boto3 s3 client
        bucket (str): The S3 bucket which may have the file
        key (str): The key of the file
        deferred (Deferred[_S3Responder|None]): If file exists
            resolved with an _S3Responder instance, if it doesn't
            exist then resolves with None.
        parent_logcontext (LoggingContext): the logcontext to report logs and metrics
            against.
    """"""
    with LoggingContext(parent_context=parent_logcontext):
        logger.info('Fetching %s from S3', key)
        try:
<mask>:
                resp = s3_client.get_object(Bucket=bucket, Key=key, SSECustomerKey=extra_args['SSECustomerKey'], SSECustomerAlgorithm=extra_args['SSECustomerAlgorithm'])
            else:
                resp = s3_client.get_object(Bucket=bucket, Key=key)
        except botocore.exceptions.ClientError as e:
            if e.response['Error']['Code'] in ('404', 'NoSuchKey'):
                logger.info('Media %s not found in S3', key)
                reactor.callFromThread(deferred.callback, None)
                return
            reactor.callFromThread(deferred.errback, Failure())
            return
        producer = _S3Responder()
        reactor.callFromThread(deferred.callback, producer)
        _stream_to_producer(reactor, producer, resp['Body'], timeout=90.0)",'SSECustomerKey' in extra_args and 'SSECustomerAlgorithm' in extra_args,116,extra_args,False,6.948345122280157,N/A
"def _stream_to_producer(reactor, producer, body, status=None, timeout=None):
    """"""Streams a file like object to the producer.

    Correctly handles producer being paused/resumed/stopped.

    Args:
        reactor
        producer (_S3Responder): Producer object to stream results to
        body (file like): The object to read from
        status (_ProducerStatus|None): Used to track whether we're currently
            paused or not. Used for testing
        timeout (float|None): Timeout in seconds to wait for consume to resume
            after being paused
    """"""
    wakeup_event = producer.wakeup_event
    stop_event = producer.stop_event
<mask>:
        status = _ProducerStatus()
    try:
        while not stop_event.is_set():
            if not wakeup_event.is_set():
                status.set_paused(True)
                ret = wakeup_event.wait(timeout)
                if not ret:
                    raise Exception('Timed out waiting to resume')
                status.set_paused(False)
            if stop_event.is_set():
                return
            chunk = body.read(READ_CHUNK_SIZE)
            if not chunk:
                return
            reactor.callFromThread(producer._write, chunk)
    except Exception:
        reactor.callFromThread(producer._error, Failure())
    finally:
        reactor.callFromThread(producer._finish)
        if body:
            body.close()",not status,118,status is None,False,27.516060407455225,N/A
"def read(self, _):
    val = self._queue.get()
<mask>:
        raise val
    return val","isinstance(val, Exception)",11,val is None,False,10.122592925934278,N/A
"def menu(title, choices):
    while True:
        print(title)
        for i, choice in enumerate(choices):
            print(f'  {i + 1} = {choice}')
        str_choice = input(': ')
        try:
            choice = int(str_choice)
        except ValueError:
            print('Invalid input, please enter a number')
            continue
<mask>:
            return choices[choice - 1]
        else:
            print('Invalid input, please enter a valid number')","choice in range(1, len(choices) + 1)",47,choice > 0,False,0.9816077559181531,N/A
"def bump_version(version, major: bool=False, minor: bool=False, patch: bool=False, alpha: bool=False, beta: bool=False):
    major_number = version.major
    minor_number = version.minor
    patch_number = version.patch
    modifier = version.modifier or ''
<mask>:
        major_number = int(major_number) + 1
        minor_number = 0
        patch_number = 0
        modifier = ''
    elif minor:
        minor_number = int(minor_number) + 1
        patch_number = 0
        modifier = ''
    elif patch:
        patch_number = int(patch_number) + 1
        modifier = ''
    if alpha:
        if not modifier:
            alpha_version = 0
        elif version.alpha:
            if (match := re.search('\\d+$', modifier)):
                alpha_version = int(match.group()) + 1
        else:
            raise ValueError('Bumping a non-alpha version (e.g. beta) to an alpha modifier is not supported')
        modifier = f'a{alpha_version}'
    elif beta:
        if not modifier or version.alpha:
            beta_version = 0
        elif version.beta:
            if (match := re.search('\\d+$', modifier)):
                beta_version = int(match.group()) + 1
        else:
            raise ValueError('Bumping a non-beta version (e.g. rc) to a beta modifier is not supported')
        modifier = f'b{beta_version}'
    return AwesomeVersion(f'{major_number}.{minor_number}.{patch_number}{modifier}')",major,144,major,True,100.00000000000004,N/A
"def get_versions():
    version_tags = subprocess.check_output(['git', 'tag', '-l', 'v*'])
    awesome_versions = []
    for version_tag in version_tags.decode('utf-8').split('\n'):
        version = AwesomeVersion(version_tag[1:])
<mask>:
            awesome_versions.append(version)
    awesome_versions.sort()
    return awesome_versions",version.valid,23,version.get_version() != '0',False,8.392229812593097,N/A
"def get_integration_name():
    dir_list = [name for name in os.listdir('custom_components') if os.path.isdir(os.path.join('custom_components', name)) and name != '__pycache__']
<mask>:
        raise ValueError(f""Expected one directory below custom_components, but found {', '.join(dir_list)}"")
    return dir_list[0]",len(dir_list) != 1,29,len(dir_list) != 1,True,100.00000000000004,N/A
"def main(args):
    branch = Git.get_current_branch()
<mask>:
        raise ValueError(f'Unexpected branch: {branch.name}, should be dev or release/x.y.z')
    if not Git.workarea_is_clean():
        logging.error('Workarea is not clean')
        exit(1)
    Git.fetch_tags()
    manifest_version = get_version_from_manifest()
    print(f'Manifest version is {manifest_version}')
    bump_version_after_release = None
    if branch.is_dev:
        last_released_version = get_last_released_version()
        print(f'Last released version was {last_released_version}')
        if not last_released_version:
            print('First release, nice!')
            last_released_version = AwesomeVersion('0.0.0')
        release_type = enum_menu('What type of release is this?', ReleaseType)
        release_type_modifier = enum_menu('Create releasebranch for alpha or beta?', ReleaseTypeModifier)
        next_version = bump_version(last_released_version, major=release_type == ReleaseType.MAJOR, minor=release_type == ReleaseType.MINOR, patch=release_type == ReleaseType.PATCH, alpha=release_type_modifier == ReleaseTypeModifier.ALPHA, beta=release_type_modifier == ReleaseTypeModifier.BETA)
        release_branch_name = f""release/{AwesomeVersion(f'{next_version.major}.{next_version.minor}.{next_version.patch}')}""
    if branch.is_release:
        release_branch_name = branch.name
        release_type_modifier = enum_menu('Bump alpha or beta (no = release to master)?', ReleaseTypeModifier)
        if release_type_modifier == ReleaseTypeModifier.NO:
            next_version = AwesomeVersion(f'{manifest_version.major}.{manifest_version.minor}.{manifest_version.patch}')
        else:
            next_manifest_version = bump_version(manifest_version, alpha=release_type_modifier == ReleaseTypeModifier.ALPHA, beta=release_type_modifier == ReleaseTypeModifier.BETA)
            if manifest_version.alpha and next_manifest_version.beta:
                next_version = next_manifest_version
            else:
                next_version = manifest_version
    if release_type_modifier != ReleaseTypeModifier.NO:
        bump_version_after_release = bump_version(next_version, alpha=release_type_modifier == ReleaseTypeModifier.ALPHA, beta=release_type_modifier == ReleaseTypeModifier.BETA)
    tag_name = f'v{next_version}'
    logging.debug(f'Tag name: {tag_name}')
    print(f'On branch: {branch.name}')
    print(f'Release branch to use: {release_branch_name}')
    if bump_version_after_release:
        print(f'Bump version after release: {bump_version_after_release}')
    print(' ')
    if input(f'Confirm release of version {next_version}? [y/N]: ') != 'y':
        exit(1)
    if branch.is_dev:
        Git.create_branch(release_branch_name)
        Git.checkout(release_branch_name)
    if branch.is_dev or (branch.is_release and (not next_version.modifier)):
        update_manifest_version_number(next_version)
        Git.add_changes()
        Git.commit_changes(f'Update version to {next_version}')
    if not next_version.modifier:
        Git.checkout(MASTER)
        Git.pull()
        subprocess.run(['git', 'merge', '--no-ff', release_branch_name, '--strategy-option', 'theirs', '-m', f'Release v{next_version}'], check=True)
    Git.create_tag(tag_name)
    if bump_version_after_release:
        assert Git.get_current_branch() != MASTER
        update_manifest_version_number(bump_version_after_release)
        Git.add_changes()
        Git.commit_changes(f'Update version to {bump_version_after_release}')
    if input('Push to origin? [Y/n]: ') != 'n':
        if Git.get_current_branch() == MASTER:
            Git.push_to_origin(MASTER)
        Git.push_to_origin(release_branch_name)
        Git.push_to_origin(tag_name)
    else:
        print(""Don't forget to push later or revert changes!"")
    print('Done!')
    print(f'Currently on branch: {Git.get_current_branch().name}')",not (branch.is_dev or branch.is_release),258,"branch.is_release and branch.name not in ['dev', 'release', 'release', 'release', 'release_x', 'y', 'z']",False,15.310245441182444,N/A
"def get_sync_mode(api: aiohuesyncbox.HueSyncBox):
    """"""Get mode""""""
    mode = api.execution.mode
<mask>:
        mode = api.execution.last_sync_mode
    return mode",not api.execution.mode in SYNC_MODES,14,api.execution.last_sync_mode is not None,False,24.384183193426086,N/A
"def current_input(api: aiohuesyncbox.HueSyncBox):
    for input_id in INPUTS:
<mask>:
            return getattr(api.hdmi, input_id).name",input_id == api.execution.hdmi_source,11,input_id in api.hdmi.inputs,False,18.690535088645685,N/A
"def current_entertainment_area(api: aiohuesyncbox.HueSyncBox):
    hue_target = api.execution.hue_target
    id_ = hue_target.replace('groups/', '')
    selected_area = None
    for group in api.hue.groups:
<mask>:
            selected_area = group.name
            break
    return selected_area",group.id == id_,24,group.id == id_,True,100.00000000000004,N/A
"@property
def options(self) -> list[str]:
<mask>:
        return self.entity_description.options_fn(self.coordinator.api)
    return super().options",self.entity_description.options_fn is not None,10,self.entity_description.options_fn,False,71.65313105737896,N/A
"def get_group_from_area_name(api: aiohuesyncbox.HueSyncBox, area_name):
    """"""Get the group object by entertainment area name.""""""
    for group in api.hue.groups:
<mask>:
            return group
    return None",group.name == area_name,21,group.entertainment_area_name == area_name,False,46.92470064105597,N/A
"def __init__(self, returnValueDict={}, rawResponse=None):
<mask>:
        try:
            returnValueDict = rawResponse.json()
        except ValueError:
            returnValueDict = {'BaseResponse': {'Ret': -1004, 'ErrMsg': 'Unexpected return value'}, 'Data': rawResponse.content}
    for k, v in returnValueDict.items():
        self[k] = v
    if not 'BaseResponse' in self:
        self['BaseResponse'] = {'ErrMsg': 'no BaseResponse in raw response', 'Ret': -1000}
    if TRANSLATE:
        self['BaseResponse']['RawMsg'] = self['BaseResponse'].get('ErrMsg', '')
        self['BaseResponse']['ErrMsg'] = TRANSLATION[TRANSLATE].get(self['BaseResponse'].get('Ret', '')) or self['BaseResponse'].get('ErrMsg', u'No ErrMsg')
        self['BaseResponse']['RawMsg'] = self['BaseResponse']['RawMsg'] or self['BaseResponse']['ErrMsg']",rawResponse,64,rawResponse,True,100.00000000000004,N/A
"def set_logging(self, showOnCmd=True, loggingFile=None, loggingLevel=logging.INFO):
<mask>:
        if showOnCmd:
            self.logger.addHandler(self.cmdHandler)
        else:
            self.logger.removeHandler(self.cmdHandler)
        self.showOnCmd = showOnCmd
    if loggingFile != self.loggingFile:
        if self.loggingFile is not None:
            self.logger.removeHandler(self.fileHandler)
            self.fileHandler.close()
        if loggingFile is not None:
            self.fileHandler = logging.FileHandler(loggingFile)
            self.logger.addHandler(self.fileHandler)
        self.loggingFile = loggingFile
    if loggingLevel != self.loggingLevel:
        self.logger.setLevel(loggingLevel)
        self.loggingLevel = loggingLevel",showOnCmd != self.showOnCmd,45,self.cmdHandler != None,False,22.957488466614336,N/A
"def emoji_formatter(d, k):
    """""" _emoji_deebugger is for bugs about emoji match caused by wechat backstage
    like :face with tears of joy: will be replaced with :cat face with tears of joy:
    """"""

    def _emoji_debugger(d, k):
        s = d[k].replace('<span class=""emoji emoji1f450""></span', '<span class=""emoji emoji1f450""></span>')

        def __fix_miss_match(m):
            return '<span class=""emoji emoji%s""></span>' % {'1f63c': '1f601', '1f639': '1f602', '1f63a': '1f603', '1f4ab': '1f616', '1f64d': '1f614', '1f63b': '1f60d', '1f63d': '1f618', '1f64e': '1f621', '1f63f': '1f622'}.get(m.group(1), m.group(1))
        return emojiRegex.sub(__fix_miss_match, s)

    def _emoji_formatter(m):
        s = m.group(1)
<mask>:
            return ('\\U%s\\U%s' % (s[:2].rjust(8, '0'), s[2:].rjust(8, '0'))).encode('utf8').decode('unicode-escape', 'replace')
        elif len(s) == 10:
            return ('\\U%s\\U%s' % (s[:5].rjust(8, '0'), s[5:].rjust(8, '0'))).encode('utf8').decode('unicode-escape', 'replace')
        else:
            return ('\\U%s' % m.group(1).rjust(8, '0')).encode('utf8').decode('unicode-escape', 'replace')
    d[k] = _emoji_debugger(d, k)
    d[k] = emojiRegex.sub(_emoji_formatter, d[k])",len(s) == 6,113,len(s) == 3,False,80.91067115702207,N/A
"def print_qr(fileDir):
<mask>:
        subprocess.call(['open', fileDir])
    elif config.OS == 'Linux':
        subprocess.call(['xdg-open', fileDir])
    else:
        os.startfile(fileDir)",config.OS == 'Darwin',13,config.OS == 'Windows',False,75.98356856515926,N/A
"def print_cmd_qr(qrText, white=BLOCK, black='  ', enableCmdQR=True):
    blockCount = int(enableCmdQR)
<mask>:
        blockCount = 1
    white *= abs(blockCount)
    if blockCount < 0:
        white, black = (black, white)
    sys.stdout.write(' ' * 50 + '\r')
    sys.stdout.flush()
    qr = qrText.replace('0', white).replace('1', black)
    sys.stdout.write(qr)
    sys.stdout.flush()",abs(blockCount) == 0,39,blockCount < 1,False,7.253154775624655,N/A
"def search_dict_list(l, key, value):
    """""" Search a list of dict
        * return dict with specific value & key """"""
    for i in l:
<mask>:
            return i",i.get(key) == value,26,i['key'] == key and i['value'] == value,False,11.114924776032012,N/A
"def print_line(msg, oneLine=False):
<mask>:
        sys.stdout.write(' ' * 40 + '\r')
        sys.stdout.flush()
    else:
        sys.stdout.write('\n')
    sys.stdout.write(msg.encode(sys.stdin.encoding or 'utf8', 'replace').decode(sys.stdin.encoding or 'utf8', 'replace'))
    sys.stdout.flush()",oneLine,21,oneLine,True,100.00000000000004,N/A
"def loads(self, j):
    self.userName = j.get('userName', None)
    self.nickName = j.get('nickName', None)
    del self.memberList[:]
    for i in j.get('memberList', []):
        self.memberList.append(i)
    del self.mpList[:]
    for i in j.get('mpList', []):
        self.mpList.append(i)
    del self.chatroomList[:]
    for i in j.get('chatroomList', []):
        self.chatroomList.append(i)
    for chatroom in self.chatroomList:
<mask>:
            for member in chatroom['MemberList']:
                member.core = chatroom.core
                member.chatroom = chatroom
        if 'Self' in chatroom:
            chatroom['Self'].core = chatroom.core
            chatroom['Self'].chatroom = chatroom
    self.lastInputUserName = j.get('lastInputUserName', None)",'MemberList' in chatroom,64,'memoryList' in chatroom,False,55.03212081491043,N/A
"def search_friends(self, name=None, userName=None, remarkName=None, nickName=None, wechatAccount=None):
    with self.updateLock:
<mask>:
            return copy.deepcopy(self.memberList[0])
        elif userName:
            for m in self.memberList:
                if m['UserName'] == userName:
                    return copy.deepcopy(m)
        else:
            matchDict = {'RemarkName': remarkName, 'NickName': nickName, 'Alias': wechatAccount}
            for k in ('RemarkName', 'NickName', 'Alias'):
                if matchDict[k] is None:
                    del matchDict[k]
            if name:
                contact = []
                for m in self.memberList:
                    if any([m.get(k) == name for k in ('RemarkName', 'NickName', 'Alias')]):
                        contact.append(m)
            else:
                contact = self.memberList[:]
            if matchDict:
                friendList = []
                for m in contact:
                    if all([m.get(k) == v for k, v in matchDict.items()]):
                        friendList.append(m)
                return copy.deepcopy(friendList)
            else:
                return copy.deepcopy(contact)",(name or userName or remarkName or nickName or wechatAccount) is None,93,len(self.memberList) == 1,False,3.6353588668522963,N/A
"def search_chatrooms(self, name=None, userName=None):
    with self.updateLock:
<mask>:
            for m in self.chatroomList:
                if m['UserName'] == userName:
                    return copy.deepcopy(m)
        elif name is not None:
            matchList = []
            for m in self.chatroomList:
                if name in m['NickName']:
                    matchList.append(copy.deepcopy(m))
            return matchList",userName is not None,36,userName is not None,True,100.00000000000004,N/A
"def search_mps(self, name=None, userName=None):
    with self.updateLock:
<mask>:
            for m in self.mpList:
                if m['UserName'] == userName:
                    return copy.deepcopy(m)
        elif name is not None:
            matchList = []
            for m in self.mpList:
                if name in m['NickName']:
                    matchList.append(copy.deepcopy(m))
            return matchList",userName is not None,36,userName is not None,True,100.00000000000004,N/A
"def set_default_value(self, initFunction=None, contactClass=None):
<mask>:
        self.contactInitFn = initFunction
    if hasattr(contactClass, '__call__'):
        self.contactClass = contactClass","hasattr(initFunction, '__call__')",14,initFunction,False,0.0016701700790245667,N/A
"def append(self, value):
    contact = self.contactClass(value)
    contact.core = self.core
<mask>:
        contact = self.contactInitFn(self, contact) or contact
    super(ContactList, self).append(contact)",self.contactInitFn is not None,18,self.contactInitFn,False,36.78794411714425,N/A
"def update(self):
    r = self.core.update_friend(self.userName)
<mask>:
        update_info_dict(self, r)
    return r",r,10,self.info,False,0.0,N/A
"def __init__(self, *args, **kwargs):
    super(Chatroom, self).__init__(*args, **kwargs)
    memberList = ContactList()
    userName = self.get('UserName', '')
    refSelf = ref(self)

    def init_fn(parentList, d):
        d.chatroom = refSelf() or parentList.core.search_chatrooms(userName=userName)
    memberList.set_default_value(init_fn, ChatroomMember)
<mask>:
        for member in self.memberList:
            memberList.append(member)
    self['MemberList'] = memberList",'MemberList' in self,36,self.memberList,False,27.516060407455225,N/A
"def update(self, detailedMember=False):
    r = self.core.update_chatroom(self.userName, detailedMember)
<mask>:
        update_info_dict(self, r)
        self['MemberList'] = r['MemberList']
    return r",r,15,r,True,100.00000000000004,N/A
"def __getitem__(self, value):
<mask>:
        v = value[0].upper() + value[1:]
        logger.debug('%s is expired in 1.3.0, use %s instead.' % (value, v))
        value = v
    return super(Message, self).__getitem__(value)","value in ('isAdmin', 'isAt')",26,self.is_expired,False,0.0,N/A
"def auto_login(self, hotReload=False, statusStorageDir='itchat.pkl', enableCmdQR=False, picDir=None, qrCallback=None, loginCallback=None, exitCallback=None):
<mask>:
        logger.info(""You can't get access to internet or wechat domain, so exit."")
        sys.exit()
    self.useHotReload = hotReload
    self.hotReloadDir = statusStorageDir
    if hotReload:
        if self.load_login_status(statusStorageDir, loginCallback=loginCallback, exitCallback=exitCallback):
            return
        self.login(enableCmdQR=enableCmdQR, picDir=picDir, qrCallback=qrCallback, loginCallback=loginCallback, exitCallback=exitCallback)
        self.dump_login_status(statusStorageDir)
    else:
        self.login(enableCmdQR=enableCmdQR, picDir=picDir, qrCallback=qrCallback, loginCallback=loginCallback, exitCallback=exitCallback)",not test_connect(),47,not self.is_internet() or not self.is_wechat(),False,5.439330544349821,N/A
"def configured_reply(self):
    """""" determine the type of message and reply if its method is defined
        however, I use a strange way to determine whether a msg is from massive platform
        I haven't found a better solution here
        The main problem I'm worrying about is the mismatching of new friends added on phone
        If you have any good idea, pleeeease report an issue. I will be more than grateful.
    """"""
    try:
        msg = self.msgList.get(timeout=1)
    except Queue.Empty:
        pass
    else:
<mask>:
            replyFn = self.functionDict['FriendChat'].get(msg['Type'])
        elif isinstance(msg['User'], templates.MassivePlatform):
            replyFn = self.functionDict['MpChat'].get(msg['Type'])
        elif isinstance(msg['User'], templates.Chatroom):
            replyFn = self.functionDict['GroupChat'].get(msg['Type'])
        if replyFn is None:
            r = None
        else:
            try:
                r = replyFn(msg)
                if r is not None:
                    self.send(r, msg.get('FromUserName'))
            except:
                logger.warning(traceback.format_exc())","isinstance(msg['User'], templates.User)",114,"isinstance(msg['User'], templates.FriendPlatform)",False,80.70557274927978,N/A
"def msg_register(self, msgType, isFriendChat=False, isGroupChat=False, isMpChat=False):
    """""" a decorator constructor
        return a specific decorator based on information given """"""
<mask>:
        msgType = [msgType]

    def _msg_register(fn):
        for _msgType in msgType:
            if isFriendChat:
                self.functionDict['FriendChat'][_msgType] = fn
            if isGroupChat:
                self.functionDict['GroupChat'][_msgType] = fn
            if isMpChat:
                self.functionDict['MpChat'][_msgType] = fn
            if not any((isFriendChat, isGroupChat, isMpChat)):
                self.functionDict['FriendChat'][_msgType] = fn
        return fn
    return _msg_register","not (isinstance(msgType, list) or isinstance(msgType, tuple))",56,"not isinstance(msgType, list)",False,23.246837589676872,N/A
"def run(self, debug=False, blockThread=True):
    logger.info('Start auto replying.')
<mask>:
        set_logging(loggingLevel=logging.DEBUG)

    def reply_fn():
        try:
            while self.alive:
                self.configured_reply()
        except KeyboardInterrupt:
            if self.useHotReload:
                self.dump_login_status()
            self.alive = False
            logger.debug('itchat received an ^C and exit.')
            logger.info('Bye~')
    if blockThread:
        reply_fn()
    else:
        replyThread = threading.Thread(target=reply_fn)
        replyThread.setDaemon(True)
        replyThread.start()",debug,39,debug,True,100.00000000000004,N/A
"def load_login_status(self, fileDir, loginCallback=None, exitCallback=None):
    try:
        with open(fileDir, 'rb') as f:
            j = pickle.load(f)
    except Exception as e:
        logger.debug('No such file, loading login status failed.')
        return ReturnValue({'BaseResponse': {'ErrMsg': 'No such file, loading login status failed.', 'Ret': -1002}})
<mask>:
        logger.debug(('you have updated itchat from %s to %s, ' + 'so cached status is ignored') % (j.get('version', 'old version'), VERSION))
        return ReturnValue({'BaseResponse': {'ErrMsg': 'cached status ignored because of version', 'Ret': -1005}})
    self.loginInfo = j['loginInfo']
    self.loginInfo['User'] = templates.User(self.loginInfo['User'])
    self.loginInfo['User'].core = self
    self.s.cookies = requests.utils.cookiejar_from_dict(j['cookies'])
    self.storageClass.loads(j['storage'])
    try:
        msgList, contactList = self.get_msg()
    except:
        msgList = contactList = None
    if (msgList or contactList) is None:
        self.logout()
        load_last_login_status(self.s, j['cookies'])
        logger.debug('server refused, loading login status failed.')
        return ReturnValue({'BaseResponse': {'ErrMsg': 'server refused, loading login status failed.', 'Ret': -1003}})
    else:
        if contactList:
            for contact in contactList:
                if '@@' in contact['UserName']:
                    update_local_chatrooms(self, [contact])
                else:
                    update_local_friends(self, [contact])
        if msgList:
            msgList = produce_msg(self, msgList)
            for msg in msgList:
                self.msgList.put(msg)
        self.start_receiving(exitCallback)
        logger.debug('loading login status succeeded.')
        if hasattr(loginCallback, '__call__'):
            loginCallback()
        return ReturnValue({'BaseResponse': {'ErrMsg': 'loading login status succeeded.', 'Ret': 0}})","j.get('version', '') != VERSION",164,"j.get('version', 'old version') != VERSION",False,63.40466277046863,N/A
"def get_download_fn(core, url, msgId):

    def download_fn(downloadDir=None):
        params = {'msgid': msgId, 'skey': core.loginInfo['skey']}
        headers = {'User-Agent': config.USER_AGENT}
        r = core.s.get(url, params=params, stream=True, headers=headers)
        tempStorage = io.BytesIO()
        for block in r.iter_content(1024):
            tempStorage.write(block)
<mask>:
            return tempStorage.getvalue()
        with open(downloadDir, 'wb') as f:
            f.write(tempStorage.getvalue())
        tempStorage.seek(0)
        return ReturnValue({'BaseResponse': {'ErrMsg': 'Successfully downloaded', 'Ret': 0}, 'PostFix': utils.get_image_postfix(tempStorage.read(20))})
    return download_fn",downloadDir is None,51,downloadDir is None,True,100.00000000000004,N/A
"def produce_group_chat(core, msg):
    r = re.match('(@[0-9a-z]*?):<br/>(.*)$', msg['Content'])
<mask>:
        actualUserName, content = r.groups()
        chatroomUserName = msg['FromUserName']
    elif msg['FromUserName'] == core.storageClass.userName:
        actualUserName = core.storageClass.userName
        content = msg['Content']
        chatroomUserName = msg['ToUserName']
    else:
        msg['ActualUserName'] = core.storageClass.userName
        msg['ActualNickName'] = core.storageClass.nickName
        msg['IsAt'] = False
        utils.msg_formatter(msg, 'Content')
        return
    chatroom = core.storageClass.search_chatrooms(userName=chatroomUserName)
    member = utils.search_dict_list((chatroom or {}).get('MemberList') or [], 'UserName', actualUserName)
    if member is None:
        chatroom = core.update_chatroom(chatroomUserName)
        member = utils.search_dict_list((chatroom or {}).get('MemberList') or [], 'UserName', actualUserName)
    if member is None:
        logger.debug('chatroom member fetch failed with %s' % actualUserName)
        msg['ActualNickName'] = ''
        msg['IsAt'] = False
    else:
        msg['ActualNickName'] = member.get('DisplayName', '') or member['NickName']
        atFlag = '@' + (chatroom['Self'].get('DisplayName', '') or core.storageClass.nickName)
        msg['IsAt'] = atFlag + (u'\u2005' if u'\u2005' in msg['Content'] else ' ') in msg['Content'] or msg['Content'].endswith(atFlag)
    msg['ActualUserName'] = actualUserName
    msg['Content'] = content
    utils.msg_formatter(msg, 'Content')",r,126,r,True,100.00000000000004,N/A
"def _prepare_file(fileDir, file_=None):
    fileDict = {}
<mask>:
        if hasattr(file_, 'read'):
            file_ = file_.read()
        else:
            return ReturnValue({'BaseResponse': {'ErrMsg': 'file_ param should be opened file', 'Ret': -1005}})
    else:
        if not utils.check_file(fileDir):
            return ReturnValue({'BaseResponse': {'ErrMsg': 'No file found in specific dir', 'Ret': -1002}})
        with open(fileDir, 'rb') as f:
            file_ = f.read()
    fileDict['fileSize'] = len(file_)
    fileDict['fileMd5'] = hashlib.md5(file_).hexdigest()
    fileDict['file_'] = io.BytesIO(file_)
    return fileDict",file_,59,file_,True,100.00000000000004,N/A
"def upload_file(self, fileDir, isPicture=False, isVideo=False, toUserName='filehelper', file_=None, preparedFile=None):
    logger.debug('Request to upload a %s: %s' % ('picture' if isPicture else 'video' if isVideo else 'file', fileDir))
<mask>:
        preparedFile = _prepare_file(fileDir, file_)
        if not preparedFile:
            return preparedFile
    fileSize, fileMd5, file_ = (preparedFile['fileSize'], preparedFile['fileMd5'], preparedFile['file_'])
    fileSymbol = 'pic' if isPicture else 'video' if isVideo else 'doc'
    chunks = int((fileSize - 1) / 524288) + 1
    clientMediaId = int(time.time() * 10000.0)
    uploadMediaRequest = json.dumps(OrderedDict([('UploadType', 2), ('BaseRequest', self.loginInfo['BaseRequest']), ('ClientMediaId', clientMediaId), ('TotalLen', fileSize), ('StartPos', 0), ('DataLen', fileSize), ('MediaType', 4), ('FromUserName', self.storageClass.userName), ('ToUserName', toUserName), ('FileMd5', fileMd5)]), separators=(',', ':'))
    r = {'BaseResponse': {'Ret': -1005, 'ErrMsg': 'Empty file detected'}}
    for chunk in range(chunks):
        r = upload_chunk_file(self, fileDir, fileSymbol, fileSize, file_, chunk, chunks, uploadMediaRequest)
    file_.close()
    if isinstance(r, dict):
        return ReturnValue(r)
    return ReturnValue(rawResponse=r)",not preparedFile,122,file_,False,0.0,N/A
"def login(self, enableCmdQR=False, picDir=None, qrCallback=None, loginCallback=None, exitCallback=None):
<mask>:
        logger.warning('itchat has already logged in.')
        return
    self.isLogging = True
    while self.isLogging:
        uuid = push_login(self)
        if uuid:
            qrStorage = io.BytesIO()
        else:
            logger.info('Getting uuid of QR code.')
            while not self.get_QRuuid():
                time.sleep(1)
            logger.info('Downloading QR code.')
            qrStorage = self.get_QR(enableCmdQR=enableCmdQR, picDir=picDir, qrCallback=qrCallback)
            logger.info('Please scan the QR code to log in.')
        isLoggedIn = False
        while not isLoggedIn:
            status = self.check_login()
            if hasattr(qrCallback, '__call__'):
                qrCallback(uuid=self.uuid, status=status, qrcode=qrStorage.getvalue())
            if status == '200':
                isLoggedIn = True
            elif status == '201':
                if isLoggedIn is not None:
                    logger.info('Please press confirm on your phone.')
                    isLoggedIn = None
            elif status != '408':
                break
        if isLoggedIn:
            break
        elif self.isLogging:
            logger.info('Log in time out, reloading QR code.')
    else:
        return
    logger.info('Loading the contact, this may take a little while.')
    self.web_init()
    self.show_mobile_login()
    self.get_contact(True)
    if hasattr(loginCallback, '__call__'):
        r = loginCallback()
    else:
        utils.clear_screen()
        if os.path.exists(picDir or config.DEFAULT_QR):
            os.remove(picDir or config.DEFAULT_QR)
        logger.info('Login successfully as %s' % self.storageClass.nickName)
    self.start_receiving(exitCallback)
    self.isLogging = False",self.alive or self.isLogging,149,self.isLogging,False,26.359713811572682,N/A
"def push_login(core):
    cookiesDict = core.s.cookies.get_dict()
<mask>:
        url = '%s/cgi-bin/mmwebwx-bin/webwxpushloginurl?uin=%s' % (config.BASE_URL, cookiesDict['wxuin'])
        headers = {'User-Agent': config.USER_AGENT}
        r = core.s.get(url, headers=headers).json()
        if 'uuid' in r and r.get('ret') in (0, '0'):
            core.uuid = r['uuid']
            return r['uuid']
    return False",'wxuin' in cookiesDict,36,'wxuin' in cookiesDict,True,100.00000000000004,N/A
"def get_QRuuid(self):
    url = '%s/jslogin' % config.BASE_URL
    params = {'appid': 'wx782c26e4c19acffb', 'fun': 'new'}
    headers = {'User-Agent': config.USER_AGENT}
    r = self.s.get(url, params=params, headers=headers)
    regx = 'window.QRLogin.code = (\\d+); window.QRLogin.uuid = ""(\\S+?)"";'
    data = re.search(regx, r.text)
<mask>:
        self.uuid = data.group(2)
        return self.uuid",data and data.group(1) == '200',40,data,False,0.004539992976248487,N/A
"def get_QR(self, uuid=None, enableCmdQR=False, picDir=None, qrCallback=None):
    uuid = uuid or self.uuid
    picDir = picDir or config.DEFAULT_QR
    qrStorage = io.BytesIO()
    qrCode = QRCode('https://login.weixin.qq.com/l/' + uuid)
    qrCode.png(qrStorage, scale=10)
<mask>:
        qrCallback(uuid=uuid, status='0', qrcode=qrStorage.getvalue())
    else:
        with open(picDir, 'wb') as f:
            f.write(qrStorage.getvalue())
        if enableCmdQR:
            utils.print_cmd_qr(qrCode.text(1), enableCmdQR=enableCmdQR)
        else:
            utils.print_qr(picDir)
    return qrStorage","hasattr(qrCallback, '__call__')",45,qrCallback,False,0.0016701700790245667,N/A
"def check_login(self, uuid=None):
    uuid = uuid or self.uuid
    url = '%s/cgi-bin/mmwebwx-bin/login' % config.BASE_URL
    localTime = int(time.time())
    params = 'loginicon=true&uuid=%s&tip=1&r=%s&_=%s' % (uuid, int(-localTime / 1579), localTime)
    headers = {'User-Agent': config.USER_AGENT}
    r = self.s.get(url, params=params, headers=headers)
    regx = 'window.code=(\\d+)'
    data = re.search(regx, r.text)
<mask>:
        if process_login_info(self, r.text):
            return '200'
        else:
            return '400'
    elif data:
        return data.group(1)
    else:
        return '400'",data and data.group(1) == '200',57,data is None,False,1.9119108411650758,N/A
"def update_chatroom(self, userName, detailedMember=False):
<mask>:
        userName = [userName]
    url = '%s/webwxbatchgetcontact?type=ex&r=%s' % (self.loginInfo['url'], int(time.time()))
    headers = {'ContentType': 'application/json; charset=UTF-8', 'User-Agent': config.USER_AGENT}
    data = {'BaseRequest': self.loginInfo['BaseRequest'], 'Count': len(userName), 'List': [{'UserName': u, 'ChatRoomId': ''} for u in userName]}
    chatroomList = json.loads(self.s.post(url, data=json.dumps(data), headers=headers).content.decode('utf8', 'replace')).get('ContactList')
    if not chatroomList:
        return ReturnValue({'BaseResponse': {'ErrMsg': 'No chatroom found', 'Ret': -1001}})
    if detailedMember:

        def get_detailed_member_info(encryChatroomId, memberList):
            url = '%s/webwxbatchgetcontact?type=ex&r=%s' % (self.loginInfo['url'], int(time.time()))
            headers = {'ContentType': 'application/json; charset=UTF-8', 'User-Agent': config.USER_AGENT}
            data = {'BaseRequest': self.loginInfo['BaseRequest'], 'Count': len(memberList), 'List': [{'UserName': member['UserName'], 'EncryChatRoomId': encryChatroomId} for member in memberList]}
            return json.loads(self.s.post(url, data=json.dumps(data), headers=headers).content.decode('utf8', 'replace'))['ContactList']
        MAX_GET_NUMBER = 50
        for chatroom in chatroomList:
            totalMemberList = []
            for i in range(int(len(chatroom['MemberList']) / MAX_GET_NUMBER + 1)):
                memberList = chatroom['MemberList'][i * MAX_GET_NUMBER:(i + 1) * MAX_GET_NUMBER]
                totalMemberList += get_detailed_member_info(chatroom['EncryChatRoomId'], memberList)
            chatroom['MemberList'] = totalMemberList
    update_local_chatrooms(self, chatroomList)
    r = [self.storageClass.search_chatrooms(userName=c['UserName']) for c in chatroomList]
    return r if 1 < len(r) else r[0]","not isinstance(userName, list)",142,"not isinstance(userName, list)",True,100.00000000000004,N/A
"def update_friend(self, userName):
<mask>:
        userName = [userName]
    url = '%s/webwxbatchgetcontact?type=ex&r=%s' % (self.loginInfo['url'], int(time.time()))
    headers = {'ContentType': 'application/json; charset=UTF-8', 'User-Agent': config.USER_AGENT}
    data = {'BaseRequest': self.loginInfo['BaseRequest'], 'Count': len(userName), 'List': [{'UserName': u, 'EncryChatRoomId': ''} for u in userName]}
    friendList = json.loads(self.s.post(url, data=json.dumps(data), headers=headers).content.decode('utf8', 'replace')).get('ContactList')
    update_local_friends(self, friendList)
    r = [self.storageClass.search_friends(userName=f['UserName']) for f in friendList]
    return r if len(r) != 1 else r[0]","not isinstance(userName, list)",58,"not isinstance(userName, list)",True,100.00000000000004,N/A
"@contact_change
def update_local_chatrooms(core, l):
    """"""
        get a list of chatrooms for updating local chatrooms
        return a list of given chatrooms with updated info
    """"""
    for chatroom in l:
        utils.emoji_formatter(chatroom, 'NickName')
        for member in chatroom['MemberList']:
<mask>:
                utils.emoji_formatter(member, 'NickName')
            if 'DisplayName' in member:
                utils.emoji_formatter(member, 'DisplayName')
            if 'RemarkName' in member:
                utils.emoji_formatter(member, 'RemarkName')
        oldChatroom = utils.search_dict_list(core.chatroomList, 'UserName', chatroom['UserName'])
        if oldChatroom:
            update_info_dict(oldChatroom, chatroom)
            memberList = chatroom.get('MemberList', [])
            oldMemberList = oldChatroom['MemberList']
            if memberList:
                for member in memberList:
                    oldMember = utils.search_dict_list(oldMemberList, 'UserName', member['UserName'])
                    if oldMember:
                        update_info_dict(oldMember, member)
                    else:
                        oldMemberList.append(member)
        else:
            core.chatroomList.append(chatroom)
            oldChatroom = utils.search_dict_list(core.chatroomList, 'UserName', chatroom['UserName'])
        if len(chatroom['MemberList']) != len(oldChatroom['MemberList']) and chatroom['MemberList']:
            existsUserNames = [member['UserName'] for member in chatroom['MemberList']]
            delList = []
            for i, member in enumerate(oldChatroom['MemberList']):
                if member['UserName'] not in existsUserNames:
                    delList.append(i)
            delList.sort(reverse=True)
            for i in delList:
                del oldChatroom['MemberList'][i]
        if oldChatroom.get('ChatRoomOwner') and oldChatroom.get('MemberList'):
            owner = utils.search_dict_list(oldChatroom['MemberList'], 'UserName', oldChatroom['ChatRoomOwner'])
            oldChatroom['OwnerUin'] = (owner or {}).get('Uin', 0)
        if 'OwnerUin' in oldChatroom and oldChatroom['OwnerUin'] != 0:
            oldChatroom['IsAdmin'] = oldChatroom['OwnerUin'] == int(core.loginInfo['wxuin'])
        else:
            oldChatroom['IsAdmin'] = None
        newSelf = utils.search_dict_list(oldChatroom['MemberList'], 'UserName', core.storageClass.userName)
        oldChatroom['Self'] = newSelf or copy.deepcopy(core.loginInfo['User'])
    return {'Type': 'System', 'Text': [chatroom['UserName'] for chatroom in l], 'SystemInfo': 'chatrooms', 'FromUserName': core.storageClass.userName, 'ToUserName': core.storageClass.userName}",'NickName' in member,180,'NickName' in member,True,100.00000000000004,N/A
"@contact_change
def update_local_friends(core, l):
    """"""
        get a list of friends or mps for updating local contact
    """"""
    fullList = core.memberList + core.mpList
    for friend in l:
<mask>:
            utils.emoji_formatter(friend, 'NickName')
        if 'DisplayName' in friend:
            utils.emoji_formatter(friend, 'DisplayName')
        if 'RemarkName' in friend:
            utils.emoji_formatter(friend, 'RemarkName')
        oldInfoDict = utils.search_dict_list(fullList, 'UserName', friend['UserName'])
        if oldInfoDict is None:
            oldInfoDict = copy.deepcopy(friend)
            if oldInfoDict['VerifyFlag'] & 8 == 0:
                core.memberList.append(oldInfoDict)
            else:
                core.mpList.append(oldInfoDict)
        else:
            update_info_dict(oldInfoDict, friend)",'NickName' in friend,65,'NickName' in friend,True,100.00000000000004,N/A
"@contact_change
def update_local_uin(core, msg):
    """"""
        content contains uins and StatusNotifyUserName contains username
        they are in same order, so what I do is to pair them together

        I caught an exception in this method while not knowing why
        but don't worry, it won't cause any problem
    """"""
    uins = re.search('<username>([^<]*?)<', msg['Content'])
    usernameChangedList = []
    r = {'Type': 'System', 'Text': usernameChangedList, 'SystemInfo': 'uins'}
<mask>:
        uins = uins.group(1).split(',')
        usernames = msg['StatusNotifyUserName'].split(',')
        if 0 < len(uins) == len(usernames):
            for uin, username in zip(uins, usernames):
                if not '@' in username:
                    continue
                fullContact = core.memberList + core.chatroomList + core.mpList
                userDicts = utils.search_dict_list(fullContact, 'UserName', username)
                if userDicts:
                    if userDicts.get('Uin', 0) == 0:
                        userDicts['Uin'] = uin
                        usernameChangedList.append(username)
                        logger.debug('Uin fetched: %s, %s' % (username, uin))
                    elif userDicts['Uin'] != uin:
                        logger.debug('Uin changed: %s, %s' % (userDicts['Uin'], uin))
                else:
                    if '@@' in username:
                        core.storageClass.updateLock.release()
                        update_chatroom(core, username)
                        core.storageClass.updateLock.acquire()
                        newChatroomDict = utils.search_dict_list(core.chatroomList, 'UserName', username)
                        if newChatroomDict is None:
                            newChatroomDict = utils.struct_friend_info({'UserName': username, 'Uin': uin, 'Self': copy.deepcopy(core.loginInfo['User'])})
                            core.chatroomList.append(newChatroomDict)
                        else:
                            newChatroomDict['Uin'] = uin
                    elif '@' in username:
                        core.storageClass.updateLock.release()
                        update_friend(core, username)
                        core.storageClass.updateLock.acquire()
                        newFriendDict = utils.search_dict_list(core.memberList, 'UserName', username)
                        if newFriendDict is None:
                            newFriendDict = utils.struct_friend_info({'UserName': username, 'Uin': uin})
                            core.memberList.append(newFriendDict)
                        else:
                            newFriendDict['Uin'] = uin
                    usernameChangedList.append(username)
                    logger.debug('Uin fetched: %s, %s' % (username, uin))
        else:
            logger.debug('Wrong length of uins & usernames: %s, %s' % (len(uins), len(usernames)))
    else:
        logger.debug('No uins in 51 message')
        logger.debug(msg['Content'])
    return r",uins,215,uins,True,100.00000000000004,N/A
"def run(self, view):
    s = sublime.load_settings('Git.sublime-settings')
<mask>:
        self.run_command(['git', 'rev-parse', '--abbrev-ref', 'HEAD'], self.branch_done, show_status=False, no_save=True, error_suppresses_output=True)
    else:
        self.branch_done(False)
    if s.get('statusbar_status'):
        self.run_command(['git', 'status', '--porcelain'], self.status_done, show_status=False, no_save=True, error_suppresses_output=True)
    else:
        self.status_done(False)",s.get('statusbar_branch'),28,s.get('branchbar_branch'),False,50.000000000000014,N/A
"def branch_done(self, result):
<mask>:
        self.view.set_status('git-branch', '')
    else:
        self.view.set_status('git-branch', 'Git branch: ' + result.strip())",result is False,13,result == '',False,15.97357760615681,N/A
"def status_done(self, result):
<mask>:
        self.view.set_status('git-status-index', '')
        self.view.set_status('git-status-working', '')
    else:
        lines = [line for line in result.splitlines() if re.match('^[ MADRCU?!]{1,2}\\s+.*', line)]
        index = [line[0] for line in lines if not line[0].isspace()]
        working = [line[1] for line in lines if not line[1].isspace()]
        self.view.set_status('git-status-index', 'index: ' + self.status_string(index))
        self.view.set_status('git-status-working', 'working: ' + self.status_string(working))",result is False,50,result is None,False,55.03212081491043,N/A
"def status_string(self, statuses):
    s = sublime.load_settings('Git.sublime-settings')
    symbols = s.get('statusbar_status_symbols')
<mask>:
        return symbols['clean']
    status = []
    if statuses.count('M'):
        status.append('%d%s' % (statuses.count('M'), symbols['modified']))
    if statuses.count('A'):
        status.append('%d%s' % (statuses.count('A'), symbols['added']))
    if statuses.count('D'):
        status.append('%d%s' % (statuses.count('D'), symbols['deleted']))
    if statuses.count('?'):
        status.append('%d%s' % (statuses.count('?'), symbols['untracked']))
    if statuses.count('U'):
        status.append('%d%s' % (statuses.count('U'), symbols['conflicts']))
    if statuses.count('R'):
        status.append('%d%s' % (statuses.count('R'), symbols['renamed']))
    if statuses.count('C'):
        status.append('%d%s' % (statuses.count('C'), symbols['copied']))
    return symbols['separator'].join(status)",not statuses,59,statuses is None,False,27.516060407455225,N/A
"def run(self):
    working_dir = git_root(self.get_working_dir())
    config_file = os.path.join(working_dir, '.git/config')
<mask>:
        self.window.open_file(config_file)
    else:
        sublime.status_message('No config found')",os.path.exists(config_file),15,os.path.exists(config_file),True,100.00000000000004,N/A
"def url_done(self, result):
    results = [r for r in result.rstrip().split('\n') if r.startswith('http')]
<mask>:
        url = results[0]
        user_end = url.index('@')
        if user_end > -1:
            user_start = url.index('//') + 1
            user = url[user_start + 1:user_end + 1]
            url = url.replace(user, '')
        self.window.run_command('open_url', {'url': url})
    else:
        sublime.status_message('No url to open')",len(results),47,len(results) > 0,False,50.81327481546149,N/A
"def panel_followup(self, picked_status, picked_file, picked_index):
    working_dir = git_root(self.get_working_dir())
<mask>:
        command = ['git', 'add', '--update']
    elif picked_index == 1:
        command = ['git', 'add', '--all']
    else:
        command = ['git']
        picked_file = picked_file.strip('""')
        if os.path.exists(working_dir + '/' + picked_file):
            command += ['add']
        else:
            command += ['rm']
        command += ['--', picked_file]
    self.run_command(command, self.rerun, working_dir=working_dir)",picked_index == 0,50,picked_status == 'update',False,22.957488466614336,N/A
"def cull_diff(self, result):
    selection = []
    for sel in self.view.sel():
        selection.append({'start': self.view.rowcol(sel.begin())[0] + 1, 'end': self.view.rowcol(sel.end())[0] + 1})
    hunks = [{'diff': ''}]
    i = 0
    matcher = re.compile('^@@ -([0-9]*)(?:,([0-9]*))? \\+([0-9]*)(?:,([0-9]*))? @@')
    for line in result.splitlines():
<mask>:
            i += 1
            match = matcher.match(line)
            start = int(match.group(3))
            end = match.group(4)
            if end:
                end = start + int(end)
            else:
                end = start
            hunks.append({'diff': '', 'start': start, 'end': end})
        hunks[i]['diff'] += line + '\n'
    diffs = hunks[0]['diff']
    hunks.pop(0)
    selection_is_hunky = False
    for hunk in hunks:
        for sel in selection:
            if sel['end'] < hunk['start']:
                continue
            if sel['start'] > hunk['end']:
                continue
            diffs += hunk['diff']
            selection_is_hunky = True
    if selection_is_hunky:
        self.run_command(['git', 'apply', '--cached'], stdin=diffs)
    else:
        sublime.status_message('No selected hunk')",line.startswith('@@'),111,not i % 2,False,0.0,N/A
"def panel_followup(self, picked_status, picked_file, picked_index):
    working_dir = git_root(self.get_working_dir())
    command = ['git']
    picked_file = picked_file.strip('""')
<mask>:
        command += ['update-index', '--assume-unchanged']
    command += ['--', picked_file]
    self.run_command(command, self.rerun, working_dir=working_dir)",os.path.exists(working_dir + '/' + picked_file),26,picked_status == 'updated',False,2.1969512149331583,N/A
"def status_done(self, result):
    self.results = list(filter(self.status_filter, result.rstrip().split('\n')))
<mask>:
        self.show_status_list()
    else:
        sublime.status_message('Nothing to show')",len(self.results),13,len(self.results) > 0,False,68.037493331712,N/A
"def status_filter(self, item):
<mask>:
        return False
    return len(item) > 0","not re.match('^h\\s+.*', item)",10,"not isinstance(item, list)",False,2.0401632902006996,N/A
"def panel_done(self, picked):
<mask>:
        return
    root = git_root(self.get_working_dir())
    picked_file = self.results[picked]
    if isinstance(picked_file, (list, tuple)):
        picked_file = picked_file[0]
    picked_file = picked_file[2:]
    self.run_command(['git', 'update-index', '--no-assume-unchanged', picked_file.strip('""')], self.rerun, working_dir=root)",0 > picked < len(self.results),27,picked == 'Nothing to do',False,4.167251645138561,N/A
"def run(self, edit, target=None):
<mask>:
        target = self.get_file_name()
    self.get_window().show_input_panel('Message', '', functools.partial(self.on_input, target), None, None)",target is None,14,target is None,True,100.00000000000004,N/A
"def on_input(self, target, message):
<mask>:
        self.panel('No commit message provided')
        return
    if target:
        command = ['git', 'add']
        if target == '*':
            command.append('--all')
        else:
            command.extend(('--', target))
        self.run_command(command, functools.partial(self.add_done, message))
    else:
        self.add_done(message, '')",message.strip() == '',30,not message,False,2.489353418393197,N/A
"def add_done(self, message, result):
<mask>:
        sublime.error_message('Error adding file:\n' + result)
        return
    self.run_command(['git', 'commit', '-m', message])",result.strip(),15,result != 'OK',False,12.44023474812678,N/A
"def porcelain_status_done(self, result):
    has_staged_files = False
    result_lines = result.rstrip().split('\n')
    for line in result_lines:
<mask>:
            has_staged_files = True
            break
    if not has_staged_files and self.quit_when_nothing_staged:
        self.panel('Nothing to commit')
        return
    s = sublime.load_settings('Git.sublime-settings')
    if s.get('verbose_commits'):
        self.run_command(['git', 'diff', '--staged', '--no-color'], self.diff_done)
    else:
        self.run_command(['git', 'status'], self.diff_done)",line and (not line[0].isspace()),41,line.strip() == self.output_file,False,7.410494411527525,N/A
"def diff_done(self, result):
    settings = sublime.load_settings('Git.sublime-settings')
    historySize = settings.get('history_size')
    rulers = settings.get('commit_rulers')

    def format(line):
        return '# ' + line.replace('\n', ' ')
<mask>:
        self.lines = ['', '']
    self.lines.extend(map(format, history[:historySize]))
    self.lines.extend(['# --------------', '# Please enter the commit message for your changes. Everything below', '# this paragraph is ignored, and an empty message aborts the commit.', '# Just close the window to accept your message.', result.strip()])
    template = '\n'.join(self.lines)
    msg = self.window.new_file()
    msg.set_scratch(True)
    msg.set_name('COMMIT_EDITMSG')
    if rulers:
        msg.settings().set('rulers', rulers)
    self._output_to_view(msg, template, syntax=plugin_file('syntax/Git Commit Message.tmLanguage'))
    msg.sel().clear()
    msg.sel().add(sublime.Region(0, 0))
    GitCommitCommand.active_message = self",not len(self.lines),86,"not hasattr(self, 'lines')",False,15.619699684601283,N/A
"def find_plugin_directory():
<mask>:
        match = re.search('([^\\\\/]+)\\.sublime-package', __file__)
        if match:
            return 'Packages/' + match.group(1)
    if __file__.startswith('./') or __file__.startswith('.\\'):
        full = os.getcwd()
    else:
        full = os.path.normpath(os.path.join(os.path.dirname(__file__), '..'))
    dirname = os.path.split(full)[-1]
    return 'Packages/' + dirname.replace('.sublime-package', '')",'.sublime-package' in __file__,33,'.' not in __file__,False,58.14307369682194,N/A
"def git_root(directory):
    global git_root_cache
    retval = False
    leaf_dir = directory
<mask>:
        return git_root_cache[leaf_dir]['retval']
    while directory:
        if os.path.exists(os.path.join(directory, '.git')):
            retval = directory
            break
        parent = os.path.realpath(os.path.join(directory, os.path.pardir))
        if parent == directory:
            retval = False
            break
        directory = parent
    git_root_cache[leaf_dir] = {'retval': retval, 'expires': time.time() + 5}
    return retval",leaf_dir in git_root_cache and git_root_cache[leaf_dir]['expires'] > time.time(),47,leaf_dir in git_root_cache,False,10.836802322189591,N/A
"def do_when(conditional, command, *args, **kwargs):
<mask>:
        return command(*args, **kwargs)
    sublime.set_timeout(functools.partial(do_when, conditional, command, *args, **kwargs), 50)",conditional(),15,not conditional,False,30.326532985631665,N/A
"def find_binary(cmd):
    path = os.environ.get('PATH', '').split(os.pathsep)
<mask>:
        cmd = cmd + '.exe'
    path = _test_paths_for_executable(path, cmd)
    if not path:
        if os.name == 'nt':
            extra_paths = (os.path.join(os.environ.get('ProgramFiles', ''), 'Git', 'bin'), os.path.join(os.environ.get('ProgramFiles(x86)', ''), 'Git', 'bin'))
        else:
            extra_paths = ('/usr/local/bin', '/usr/local/git/bin')
        path = _test_paths_for_executable(extra_paths, cmd)
    return path",os.name == 'nt',44,cmd.endswith('.exe'),False,5.522397783539471,N/A
"def is_visible(self):
    s = sublime.load_settings('Git.sublime-settings')
<mask>:
        return True
    return False",s.get('flow'),10,"s.get('git_visible', False)",False,27.77619034011791,N/A
"def is_notag(self):
    s = sublime.load_settings('Git.sublime-settings')
<mask>:
        return True
    return False",s.get('flow-notag'),10,s['GitVersion'] == '2.0',False,6.567274736060395,N/A
"def panel_done(self, picked):
<mask>:
        return
    picked_feature = self.results[picked]
    if picked_feature.startswith('*'):
        picked_feature = picked_feature.strip('*')
    picked_feature = picked_feature.strip()
    self.run_command(['git', 'flow', 'feature', 'finish', picked_feature])",0 > picked < len(self.results),21,not picked in self.results,False,16.669006580554246,N/A
"def panel_done(self, picked):
<mask>:
        return
    picked_release = self.results[picked]
    if picked_release.startswith('*'):
        picked_release = picked_release.strip('*')
    picked_release = picked_release.strip()
    if self.is_notag():
        self.run_command(['git', 'flow', 'release', 'finish', '-n', picked_release])
    else:
        self.picked_release = picked_release
        self.get_window().show_input_panel('Enter Tag message:', '', self.tag_message_done, None, None)",0 > picked < len(self.results),35,picked == self.current_release,False,9.51934081834847,N/A
"def panel_done(self, picked):
<mask>:
        return
    picked_hotfix = self.results[picked]
    if picked_hotfix.startswith('*'):
        picked_hotfix = picked_hotfix.strip('*')
    picked_hotfix = picked_hotfix.strip()
    if self.is_notag():
        self.run_command(['git', 'flow', 'hotfix', 'finish', '-n', picked_hotfix])
    else:
        self.picked_hotfix = picked_hotfix
        self.get_window().show_input_panel('Enter Tag message:', '', self.tag_message_done, None, None)",0 > picked < len(self.results),35,picked == self.last_picked,False,9.51934081834847,N/A
"def stash_list_done(self, result):
<mask>:
        self.panel('No stash found')
        return
    self.results = result.rstrip().split('\n')
    if len(self.results) == 1:
        self.stash_list_panel_done()
    else:
        self.quick_panel(self.results, self.stash_list_panel_done)",not result,19,not result,True,100.00000000000004,N/A
"def stash_list_panel_done(self, picked=0):
<mask>:
        return
    stash = self.results[picked].split(':')[0]
    self.run_command(['git', 'stash'] + self.command_to_run_after_list + [stash], self.handle_command or self.generic_done, stash=stash)",0 > picked < len(self.results),18,picked >= len(self.results),False,56.481980977130846,N/A
"def on_input(self, command):
    command = str(command)
<mask>:
        self.panel('No git command provided')
        return
    import shlex
    command_splitted = ['git'] + shlex.split(command)
    print(command_splitted)
    self.run_command(command_splitted)",command.strip() == '',21,not command,False,2.489353418393197,N/A
"def run(self, **args):
    self.command = str(args.get('command', ''))
    show_in = str(args.get('show_in', 'pane_below'))
<mask>:
        self.panel('No git command provided')
        return
    import shlex
    command_split = shlex.split(self.command)
    if args.get('append_current_file', False) and self.active_file_name():
        command_split.extend(('--', self.active_file_name()))
    print(command_split)
    self.may_change_files = bool(args.get('may_change_files', True))
    if show_in == 'pane_below':
        self.run_command(command_split)
    elif show_in == 'quick_panel':
        self.run_command(command_split, self.show_in_quick_panel)
    elif show_in == 'new_tab':
        self.run_command(command_split, self.show_in_new_tab)
    elif show_in == 'suppress':
        self.run_command(command_split, self.do_nothing)
    view = self.active_view()
    view.run_command('git_branch_status')",self.command.strip() == '',61,not self.command,False,13.267398701010466,N/A
"def show_in_quick_panel(self, result):
    self.results = list(result.rstrip().split('\n'))
<mask>:
        self.quick_panel(self.results, self.do_nothing, sublime.MONOSPACE_FONT)
    else:
        sublime.status_message('Nothing to show')",len(self.results),14,len(self.results) > 0,False,68.037493331712,N/A
"def run(self, edit, output='', output_file=None, clear=False):
<mask>:
        region = sublime.Region(0, self.view.size())
        self.view.erase(edit, region)
    self.view.insert(edit, 0, output)",clear,16,clear,True,100.00000000000004,N/A
"def temp_file(view, key):
<mask>:
        fd, filepath = tempfile.mkstemp(prefix='git_annotations_')
        os.close(fd)
        view.settings().set('git_annotation_temp_%s' % key, filepath)
    return view.settings().get('git_annotation_temp_%s' % key)","not view.settings().get('git_annotation_temp_%s' % key, False)",17,not view.settings().get('git_annotation_temp_%s' % key),False,86.67865171073738,N/A
"def run(self, view):
<mask>:
        self.git_tmp = temp_file(self.active_view(), 'head')
        self.buffer_tmp = temp_file(self.active_view(), 'buffer')
    self.active_view().settings().set('live_git_annotations', True)
    root = git_root(self.get_working_dir())
    repo_file = os.path.relpath(self.view.file_name(), root).replace('\\', '/')
    self.run_command(['git', 'show', 'HEAD:{0}'.format(repo_file)], show_status=False, no_save=True, callback=self.compare_tmp)","not hasattr(self, 'git_tmp')",28,not self.git_tmp or not self.buffer_tmp,False,4.065425428798724,N/A
"def path(self, folderpath):
    project_file_name = self.view.window().project_file_name()
<mask>:
        return os.path.join(os.path.dirname(project_file_name), folderpath)
    return folderpath",project_file_name,12,project_file_name,True,100.00000000000004,N/A
"def ignored_files_found(self, result, folder_index):
    self.count -= 1
    self.process_ignored_files(result, folder_index)
<mask>:
        self.all_ignored_files_found()",self.count == 0,11,self.count == 0,True,100.00000000000004,N/A
"def process_ignored_files(self, result, folder_index):
    data = self.view.window().project_data()
    folder = data['folders'][folder_index]
<mask>:
        return
    if not result or result.isspace():
        return
    root = self.path(folder['path'])
    exclude_folders = self.excludes[folder_index]['folders']
    exclude_files = self.excludes[folder_index]['files']
    subroot = ''
    for line in result.strip().split('\n'):
        if line.startswith('Entering'):
            subroot = line.replace('Entering ', '').replace(""'"", '')
        if not line.startswith('!!'):
            continue
        path = os.path.join(subroot, line.replace('!! ', ''))
        if os.path.isdir(os.path.join(root, path)):
            exclude_folders.add(path.rstrip('\\/'))
        else:
            exclude_files.add(path)
    return (exclude_files, exclude_folders)",not folder,61,not folder,True,100.00000000000004,N/A
"def run(self, **args):
    filename = self.relative_active_file_path()
    branch, leaf = os.path.split(filename)
<mask>:
        sublime.error_message(leaf + ' is read-only')
    panel = self.get_window().show_input_panel('New path / name', filename, self.on_input, None, None)
    if branch:
        branch = branch + os.path.sep
    name, ext = os.path.splitext(leaf)
    panel.sel().clear()
    panel.sel().add(sublime.Region(len(branch), len(branch) + len(name)))","not os.access(self.active_file_path(), os.W_OK)",42,self.is_read_only(),False,3.933852380954249,N/A
"def on_input(self, newpath):
    newpath = str(newpath)
<mask>:
        return self.panel('No input received')
    working_dir = git_root(self.get_working_dir())
    newpath = os.path.join(working_dir, newpath)
    command = ['git', 'mv', '--', self.active_file_path(), newpath]
    self.run_command(command, functools.partial(self.on_done, newpath), working_dir=working_dir)",not newpath.strip(),29,not newpath,False,13.533528323661276,N/A
"def git_init(self, directory):
<mask>:
        self.run_command(['git', 'init'], self.git_inited, working_dir=directory)
    else:
        sublime.status_message('Directory does not exist.')",os.path.exists(directory),13,os.path.exists(directory),True,100.00000000000004,N/A
"def panel_done(self, picked):
<mask>:
        return
    picked_branch = self.results[picked]
    if picked_branch.startswith('*'):
        return
    picked_branch = picked_branch.strip()
    self.run_command(['git'] + self.command_to_run_after_branch + [picked_branch], self.update_status)",0 > picked < len(self.results),20,picked == 'All',False,3.564186929405141,N/A
"def on_input(self, branchname):
<mask>:
        self.panel('No branch name provided')
        return
    self.run_command(['git', 'checkout', '-b', branchname], self.branch_done)",branchname.strip() == '',14,not branchname,False,2.489353418393197,N/A
"def on_input(self, tagname):
<mask>:
        self.panel('No branch name provided')
        return
    self.run_command(['git', 'tag', tagname])",not tagname.strip(),12,not tagname,False,13.533528323661276,N/A
"def run(self, edit=None, ignore_whitespace=False, word_diff=False):
    command = ['git', 'diff', '--no-color']
<mask>:
        command.extend(('--ignore-all-space', '--ignore-blank-lines'))
    command.extend(('--', self.get_file_name()))
    self.run_command(command, self.diff_done)
    if word_diff:
        command.append('--word-diff')",ignore_whitespace,20,ignore_whitespace,True,100.00000000000004,N/A
"def diff_done(self, result):
<mask>:
        self.panel('No output')
        return
    s = sublime.load_settings('Git.sublime-settings')
    syntax = s.get('diff_syntax', 'Packages/Git/syntax/Git Diff.sublime-syntax')
    if s.get('diff_panel'):
        self.panel(result, syntax=syntax)
    else:
        self.scratch(result, title='Git Diff', syntax=syntax)",not result.strip(),24,not result,False,13.533528323661276,N/A
"def run(self, edit=None, ignore_whitespace=False, word_diff=False):
    command = ['git', 'diff', '--cached', '--no-color']
<mask>:
        command.extend(('--ignore-all-space', '--ignore-blank-lines'))
    if word_diff:
        command.extend('--word-diff')
    self.run_command(command, self.diff_done)",ignore_whitespace,19,ignore_whitespace,True,100.00000000000004,N/A
"def diff_done(self, result):
<mask>:
        self.panel('No output')
        return
    s = sublime.load_settings('Git.sublime-settings')
    syntax = s.get('diff_syntax', 'Packages/Git/syntax/Git Diff.sublime-syntax')
    self.scratch(result, title='Git Diff', syntax=syntax)",not result.strip(),19,not result,False,13.533528323661276,N/A
"def run(self, edit):
    v = self.view
    view_scope_name = v.scope_name(v.sel()[0].a)
    scope_markup_inserted = 'markup.inserted.diff' in view_scope_name
    scope_markup_deleted = 'markup.deleted.diff' in view_scope_name
<mask>:
        return
    beg = v.sel()[0].a
    pt = v.line(beg).a
    self.column = beg - pt - 1
    self.file_name = None
    hunk_line = None
    line_offset = 0
    while pt > 0:
        line = v.line(pt)
        lineContent = v.substr(line)
        if lineContent.startswith('@@'):
            if not hunk_line:
                hunk_line = lineContent
        elif lineContent.startswith('+++ b/'):
            self.file_name = v.substr(sublime.Region(line.a + 6, line.b)).strip()
            break
        elif not hunk_line and (not lineContent.startswith('-')):
            line_offset = line_offset + 1
        pt = v.line(pt - 1).a
    hunk = re.match('^@@ -(\\d+),(\\d+) \\+(\\d+),(\\d+) @@.*', hunk_line)
    if not hunk:
        sublime.status_message('No hunk info')
        return
    hunk_start_line = hunk.group(3)
    self.goto_line = int(hunk_start_line) + line_offset - 1
    git_root_dir = v.settings().get('git_root_dir')
    if not git_root_dir:
        working_dir = get_open_folder_from_window(v.window())
        git_root_dir = git_root(working_dir) if working_dir else None
    full_path_file_name = self.file_name
    if git_root_dir:
        full_path_file_name = os.path.join(git_root_dir, self.file_name)
    else:
        git_root_dir = ''
    if not os.path.isfile(full_path_file_name):
        caption = ""Enter base directory for file '%s':"" % self.file_name
        v.window().show_input_panel(caption, git_root_dir, self.on_path_confirmed, None, None)
    else:
        self.on_path_confirmed(git_root_dir)",not scope_markup_inserted and (not scope_markup_deleted),160,not scope_markup_inserted or not scope_markup_deleted,False,65.26220818377338,N/A
"def panel_done(self, picked):
<mask>:
        return
    picked_file = self.results[picked]
    if isinstance(picked_file, (list, tuple)):
        picked_file = picked_file[0]
    picked_status = picked_file[:2]
    picked_file = picked_file[3:]
    self.panel_followup(picked_status, picked_file, picked)",0 > picked < len(self.results),24,not picked in self.results,False,16.669006580554246,N/A
"def panel_followup(self, picked_status, picked_file, picked_index):
    s = sublime.load_settings('Git.sublime-settings')
    root = git_root(self.get_working_dir())
<mask>:
        file_name = os.path.join(root, picked_file)
        if os.path.isfile(file_name):
            sublime.set_timeout(lambda: self.window.open_file(file_name), 0)
    elif s.get('diff_tool'):
        self.run_command(['git', 'difftool', '--', picked_file.strip('""')], working_dir=root)
    else:
        self.run_command(['git', 'diff', '--no-color', '--', picked_file.strip('""')], self.diff_done, working_dir=root)",picked_status == '??' or s.get('status_opens_file') or self.force_open,36,s.get('diff'),False,1.9167100299515254,N/A
"def run(self, edit):
    command = ['git', 'blame', '-w', '-M', '-C']
    line_ranges = [self.get_lines(selection) for selection in self.view.sel() if not selection.empty()]
<mask>:
        for range_start, range_end, *line_range_len in line_ranges:
            command.extend(('-L', str(range_start) + ',' + str(range_end)))
        callback = self.blame_done
    else:
        callback = functools.partial(self.blame_done, focused_line=self.get_current_line())
    command.append(self.get_file_name())
    self.run_command(command, callback)",line_ranges,44,len(line_ranges) > 0,False,20.556680845025987,N/A
"def get_lines(self, selection):
<mask>:
        return False
    begin_line, begin_column = self.view.rowcol(selection.begin())
    end_line, end_column = self.view.rowcol(selection.end())
    if end_line > begin_line and end_column == 0:
        end_line -= 1
    return (begin_line + 1, end_line + 1)",selection.empty(),32,not selection,False,11.15650800742149,N/A
"def log_panel_done(self, picked):
<mask>:
        return
    item = self.results[picked]
    ref = item[0].split(' ')[-1].strip('()')
    self.log_result(ref)",0 > picked < len(self.results),13,picked >= len(self.results),False,56.481980977130846,N/A
"def panel_done(self, picked):
<mask>:
        return
    item = self.results[picked]
    ref = item[0].split(' ')[-1].strip('()')
    self.run_command(['git', 'show', '%s:%s' % (ref, self.get_relative_file_path())], self.details_done, ref=ref)",0 > picked < len(self.results),20,picked >= len(self.results),False,56.481980977130846,N/A
"def show_done(self, result, commit):
<mask>:
        self.panel(result)
        return
    self.scratch(result, title='Git Commit: %s' % commit, syntax=plugin_file('syntax/Git Commit View.tmLanguage'))",result.startswith('fatal:'),16,commit == 'All',False,0.0,N/A
"def normalize_language(language):
    for lookup_key in ('alpha_2', 'alpha_3'):
        try:
            lang = languages.get(**{lookup_key: language})
<mask>:
                language = lang.name.lower()
        except KeyError:
            pass
    return language.lower()",lang,21,lang,True,100.00000000000004,N/A
"def cached_property(getter):
    """"""
    Decorator that converts a method into memoized property.
    The decorator works as expected only for classes with
    attribute '__dict__' and immutable properties.
    """"""

    @wraps(getter)
    def decorator(self):
        key = '_cached_property_' + getter.__name__
<mask>:
            setattr(self, key, getter(self))
        return getattr(self, key)
    return property(decorator)","not hasattr(self, key)",43,"not hasattr(self, key)",True,100.00000000000004,N/A
"def __call__(self, sequence):
<mask>:
        if self._value.endswith('%'):
            total_count = len(sequence)
            percentage = int(self._value[:-1])
            count = max(1, total_count * percentage // 100)
            return sequence[:count]
        else:
            return sequence[:int(self._value)]
    elif isinstance(self._value, (int, float)):
        return sequence[:int(self._value)]
    else:
        raise ValueError(""Unsuported value of items count '%s'."" % self._value)","isinstance(self._value, string_types)",41,"isinstance(self._value, str)",False,60.10525952194528,N/A
"def unicode_compatible(cls):
    """"""
    Decorator for unicode compatible classes. Method ``__unicode__``
    has to be implemented to work decorator as expected.
    """"""
<mask>:
        cls.__str__ = cls.__unicode__
        cls.__bytes__ = lambda self: self.__str__().encode('utf-8')
    else:
        cls.__str__ = lambda self: self.__unicode__().encode('utf-8')
    return cls",PY3,37,sys.version_info[0] == 2,False,0.0,N/A
"def to_bytes(object):
<mask>:
        return object
    elif isinstance(object, unicode):
        return object.encode('utf-8')
    else:
        return instance_to_bytes(object)","isinstance(object, bytes)",13,"isinstance(object, bytes)",True,100.00000000000004,N/A
"def to_unicode(object):
<mask>:
        return object
    elif isinstance(object, bytes):
        return object.decode('utf-8')
    else:
        return instance_to_unicode(object)","isinstance(object, unicode)",13,"isinstance(object, str)",False,53.7284965911771,N/A
"def instance_to_bytes(instance):
<mask>:
        if hasattr(instance, '__bytes__'):
            return bytes(instance)
        elif hasattr(instance, '__str__'):
            return unicode(instance).encode('utf-8')
    elif hasattr(instance, '__str__'):
        return bytes(instance)
    elif hasattr(instance, '__unicode__'):
        return unicode(instance).encode('utf-8')
    return to_bytes(repr(instance))",PY3,25,PY2,False,0.0,N/A
"def instance_to_unicode(instance):
<mask>:
        if hasattr(instance, '__str__'):
            return unicode(instance)
        elif hasattr(instance, '__bytes__'):
            return bytes(instance).decode('utf-8')
    elif hasattr(instance, '__unicode__'):
        return unicode(instance)
    elif hasattr(instance, '__str__'):
        return bytes(instance).decode('utf-8')
    return to_unicode(repr(instance))",PY3,25,"isinstance(instance, (str, bytes))",False,0.0,N/A
"def main(args=None):
    args = docopt(to_string(__doc__), args, version=__version__)
    summarizer, parser, items_count = handle_arguments(args)
    for sentence in summarizer(parser.document, items_count):
<mask>:
            print(to_unicode(sentence))
        else:
            print(to_bytes(sentence))
    return 0",PY3,23,PY2,False,0.0,N/A
"def handle_arguments(args, default_input_stream=sys.stdin):
    document_format = args['--format']
<mask>:
        raise ValueError('Unsupported format of input document. Possible values are: %s. Given: %s.' % (', '.join(PARSERS.keys()), document_format))
    if args['--url'] is not None:
        parser = PARSERS[document_format or 'html']
        document_content = fetch_url(args['--url'])
    elif args['--file'] is not None:
        parser = PARSERS[document_format or 'plaintext']
        with open(args['--file'], 'rb') as file:
            document_content = file.read()
    elif args['--text'] is not None:
        parser = PARSERS[document_format or 'plaintext']
        document_content = args['--text']
    else:
        parser = PARSERS[document_format or 'plaintext']
        document_content = default_input_stream.read()
    items_count = ItemsCount(args['--length'])
    language = args['--language']
    if args['--stopwords']:
        stop_words = read_stop_words(args['--stopwords'])
    else:
        stop_words = get_stop_words(language)
    parser = parser(document_content, Tokenizer(language))
    stemmer = Stemmer(language)
    summarizer_class = next((cls for name, cls in AVAILABLE_METHODS.items() if args[name]))
    summarizer = build_summarizer(summarizer_class, stop_words, stemmer, parser)
    return (summarizer, parser, items_count)",document_format is not None and document_format not in PARSERS,118,document_format not in PARSERS,False,31.140322391459787,N/A
"def build_summarizer(summarizer_class, stop_words, stemmer, parser):
    summarizer = summarizer_class(stemmer)
<mask>:
        summarizer.null_words = stop_words
        summarizer.bonus_words = parser.significant_words
        summarizer.stigma_words = parser.stigma_words
    else:
        summarizer.stop_words = stop_words
    return summarizer",summarizer_class is EdmundsonSummarizer,24,parser,False,0.0,N/A
"@cached_property
def significant_words(self):
    words = []
    for paragraph in self.document.paragraphs:
        for heading in paragraph.headings:
            words.extend(heading.words)
<mask>:
        return tuple(words)
    else:
        return self.SIGNIFICANT_WORDS",words,21,words,True,100.00000000000004,N/A
"@cached_property
def document(self):
    current_paragraph = []
    paragraphs = []
    for line in self._text.splitlines():
        line = line.strip()
<mask>:
            heading = Sentence(line, self._tokenizer, is_heading=True)
            current_paragraph.append(heading)
        elif not line and current_paragraph:
            sentences = self._to_sentences(current_paragraph)
            paragraphs.append(Paragraph(sentences))
            current_paragraph = []
        elif line:
            current_paragraph.append(line)
    sentences = self._to_sentences(current_paragraph)
    paragraphs.append(Paragraph(sentences))
    return ObjectDocumentModel(paragraphs)",line.isupper(),44,line,False,1.8315638888734187,N/A
"def _to_sentences(self, lines):
    text = ''
    sentence_objects = []
    for line in lines:
<mask>:
            if text:
                sentence_objects.extend(self._to_sentence_objects(text))
            sentence_objects.append(line)
            text = ''
        else:
            text += ' ' + line
    text = text.strip()
    if text:
        sentence_objects.extend(self._to_sentence_objects(text))
    return sentence_objects","isinstance(line, Sentence)",36,not line.startswith('#'),False,6.27465531099474,N/A
"@cached_property
def significant_words(self):
    words = []
    for paragraph in self._article.main_text:
        for text, annotations in paragraph:
<mask>:
                words.extend(self.tokenize_words(text))
    if words:
        return tuple(words)
    else:
        return self.SIGNIFICANT_WORDS","self._contains_any(annotations, *self.SIGNIFICANT_TAGS)",24,"self._article.is_sentence(text, annotations)",False,12.014605158792346,N/A
"@cached_property
def stigma_words(self):
    words = []
    for paragraph in self._article.main_text:
        for text, annotations in paragraph:
<mask>:
                words.extend(self.tokenize_words(text))
    if words:
        return tuple(words)
    else:
        return self.STIGMA_WORDS","self._contains_any(annotations, 'a', 'strike', 's')",24,"self._article.is_stigma_sentence(text, annotations)",False,12.500763055889768,N/A
"def _contains_any(self, sequence, *args):
<mask>:
        return False
    for item in args:
        if item in sequence:
            return True
    return False",sequence is None,19,not sequence,False,30.326532985631665,N/A
"@cached_property
def document(self):
    annotated_text = self._article.main_text
    paragraphs = []
    for paragraph in annotated_text:
        sentences = []
        current_text = ''
        for text, annotations in paragraph:
<mask>:
                sentences.append(Sentence(text, self._tokenizer, is_heading=True))
            elif not (annotations and 'pre' in annotations):
                current_text += '' + text if text[0] in punctuation else ' ' + text
        new_sentences = self.tokenize_sentences(current_text)
        sentences.extend((Sentence(s, self._tokenizer) for s in new_sentences))
        paragraphs.append(Paragraph(sentences))
    return ObjectDocumentModel(paragraphs)",annotations and ('h1' in annotations or 'h2' in annotations or 'h3' in annotations),61,annotations and 'heading' in annotations,False,4.088986951654114,N/A
"def __call__(self, document, sentences_count):
    self._ensure_dependecies_installed()
    dictionary = self._create_dictionary(document)
<mask>:
        return ()
    matrix = self._create_matrix(document, dictionary)
    matrix = self._compute_term_frequency(matrix)
    u, sigma, v = singular_value_decomposition(matrix, full_matrices=False)
    ranks = iter(self._compute_ranks(sigma, v))
    return self._get_best_sentences(document.sentences, sentences_count, lambda s: next(ranks))",not dictionary,34,not document.sentences or not dictionary,False,13.134549472120788,N/A
"def _ensure_dependecies_installed(self):
<mask>:
        raise ValueError(""LSA summarizer requires NumPy. Please, install it by command 'pip install numpy'."")",numpy is None,16,not self.is_numpy,False,9.652434877402245,N/A
"def _create_matrix(self, document, dictionary):
    """"""
        Creates matrix of shape |unique words|×|sentences| where cells
        contains number of occurrences of words (rows) in sentences (cols).
        """"""
    sentences = document.sentences
    words_count = len(dictionary)
    sentences_count = len(sentences)
<mask>:
        message = 'Number of words (%d) is lower than number of sentences (%d). LSA algorithm may not work properly.'
        warn(message % (words_count, sentences_count))
    matrix = numpy.zeros((words_count, sentences_count))
    for col, sentence in enumerate(sentences):
        for word in map(self.stem_word, sentence.words):
            if word in dictionary:
                row = dictionary[word]
                matrix[row, col] += 1
    return matrix",words_count < sentences_count,84,words_count < sentences_count,True,100.00000000000004,N/A
"def _compute_term_frequency(self, matrix, smooth=0.4):
    """"""
        Computes TF metrics for each sentence (column) in the given matrix.
        You can read more about smoothing parameter at URL below:
        http://nlp.stanford.edu/IR-book/html/htmledition/maximum-tf-normalization-1.html
        """"""
    assert 0.0 <= smooth < 1.0
    max_word_frequencies = numpy.max(matrix, axis=0)
    rows, cols = matrix.shape
    for row in range(rows):
        for col in range(cols):
            max_word_frequency = max_word_frequencies[col]
<mask>:
                frequency = matrix[row, col] / max_word_frequency
                matrix[row, col] = smooth + (1.0 - smooth) * frequency
    return matrix",max_word_frequency != 0,72,max_word_frequency > 0,False,55.780028607687655,N/A
"def _rate_sentences_edge(self, words1, words2):
    rank = 0
    for w1 in words1:
        for w2 in words2:
            rank += int(w1 == w2)
<mask>:
        return 0.0
    assert len(words1) > 0 and len(words2) > 0
    norm = math.log(len(words1)) + math.log(len(words2))
    return 0.0 if norm == 0.0 else rank / norm",rank == 0,46,rank == 0,True,100.00000000000004,N/A
"def _count_words(self, words):
    """"""
        Counts number of bonus/stigma words.

        :param iterable words:
            Collection of words.
        :returns pair:
            Tuple with number of words (bonus words, stigma words).
        """"""
    bonus_words_count = 0
    stigma_words_count = 0
    for word in words:
<mask>:
            bonus_words_count += 1
        if word in self._stigma_words:
            stigma_words_count += 1
    return (bonus_words_count, stigma_words_count)",word in self._bonus_words,51,word in self._bonus_words,True,100.00000000000004,N/A
"@staticmethod
def _compute_average_probability_of_words(word_freq_in_doc, content_words_in_sentence):
    content_words_count = len(content_words_in_sentence)
<mask>:
        word_freq_sum = sum([word_freq_in_doc[w] for w in content_words_in_sentence])
        word_freq_avg = word_freq_sum / content_words_count
        return word_freq_avg
    else:
        return 0",content_words_count > 0,25,content_words_count > 0,True,100.00000000000004,N/A
"def _find_index_of_best_sentence(self, word_freq, sentences_as_words):
    min_possible_freq = -1
    max_value = min_possible_freq
    best_sentence_index = 0
    for i, words in enumerate(sentences_as_words):
        word_freq_avg = self._compute_average_probability_of_words(word_freq, words)
<mask>:
            max_value = word_freq_avg
            best_sentence_index = i
    return best_sentence_index",word_freq_avg > max_value,31,word_freq_avg > max_value,True,100.00000000000004,N/A
"def _joint_freq(self, word_list_1, word_list_2):
    total_len = len(word_list_1) + len(word_list_2)
    wc1 = self._compute_word_freq(word_list_1)
    wc2 = self._compute_word_freq(word_list_2)
    joint = wc1.copy()
    for k in wc2:
<mask>:
            joint[k] += wc2[k]
        else:
            joint[k] = wc2[k]
    for k in joint:
        joint[k] /= float(total_len)
    return joint",k in joint,39,joint[k] < joint[k],False,5.669791110976001,N/A
"@staticmethod
def _kl_divergence(summary_freq, doc_freq):
    """"""
        Note: Could import scipy.stats and use scipy.stats.entropy(doc_freq, summary_freq)
        but this gives equivalent value without the import
        """"""
    sum_val = 0
    for w in summary_freq:
        frequency = doc_freq.get(w)
<mask>:
            sum_val += frequency * math.log(frequency / summary_freq[w])
    return sum_val",frequency,42,frequency is not None,False,15.97357760615681,N/A
"def _ensure_correct_weights(self, *weights):
    for w in weights:
<mask>:
            raise ValueError('Negative weights are not allowed.')",w < 0.0,14,w < 0,False,55.03212081491043,N/A
"def __call__(self, document, sentences_count):
    ratings = defaultdict(int)
<mask>:
        method = self._build_cue_method_instance()
        ratings = self._update_ratings(ratings, method.rate_sentences(document))
    if self._key_weight > 0.0:
        method = self._build_key_method_instance()
        ratings = self._update_ratings(ratings, method.rate_sentences(document))
    if self._title_weight > 0.0:
        method = self._build_title_method_instance()
        ratings = self._update_ratings(ratings, method.rate_sentences(document))
    if self._location_weight > 0.0:
        method = self._build_location_method_instance()
        ratings = self._update_ratings(ratings, method.rate_sentences(document))
    return self._get_best_sentences(document.sentences, sentences_count, ratings)",self._cue_weight > 0.0,52,self._cue_weight > 0.0,True,100.00000000000004,N/A
"def __check_bonus_words(self):
<mask>:
        raise ValueError(""Set of bonus words is empty. Please set attribute 'bonus_words' with collection of words."")",not self._bonus_words,18,not self.bonus_words,False,43.01250851313264,N/A
"def __check_stigma_words(self):
<mask>:
        raise ValueError(""Set of stigma words is empty. Please set attribute 'stigma_words' with collection of words."")",not self._stigma_words,18,not self.stigma_words,False,43.01250851313264,N/A
"def __check_null_words(self):
<mask>:
        raise ValueError(""Set of null words is empty. Please set attribute 'null_words' with collection of words."")",not self._null_words,18,not self.null_words,False,43.01250851313264,N/A
"def __call__(self, document, sentences_count):
    self._ensure_dependencies_installed()
    sentences_words = [self._to_words_set(s) for s in document.sentences]
<mask>:
        return tuple()
    tf_metrics = self._compute_tf(sentences_words)
    idf_metrics = self._compute_idf(sentences_words)
    matrix = self._create_matrix(sentences_words, self.threshold, tf_metrics, idf_metrics)
    scores = self.power_method(matrix, self.epsilon)
    ratings = dict(zip(document.sentences, scores))
    return self._get_best_sentences(document.sentences, sentences_count, ratings)",not sentences_words,39,not sentences_words,True,100.00000000000004,N/A
"@staticmethod
def _ensure_dependencies_installed():
<mask>:
        raise ValueError(""LexRank summarizer requires NumPy. Please, install it by command 'pip install numpy'."")",numpy is None,17,not _is_numpy,False,12.703318703865365,N/A
"@staticmethod
def _compute_idf(sentences):
    idf_metrics = {}
    sentences_count = len(sentences)
    for sentence in sentences:
        for term in sentence:
<mask>:
                n_j = sum((1 for s in sentences if term in s))
                idf_metrics[term] = math.log(sentences_count / (1 + n_j))
    return idf_metrics",term not in idf_metrics,38,term not in idf_metrics,True,100.00000000000004,N/A
"def _create_matrix(self, sentences, threshold, tf_metrics, idf_metrics):
    """"""
        Creates matrix of shape |sentences|×|sentences|.
        """"""
    sentences_count = len(sentences)
    matrix = numpy.zeros((sentences_count, sentences_count))
    degrees = numpy.zeros((sentences_count,))
    for row, (sentence1, tf1) in enumerate(zip(sentences, tf_metrics)):
        for col, (sentence2, tf2) in enumerate(zip(sentences, tf_metrics)):
            matrix[row, col] = self.cosine_similarity(sentence1, sentence2, tf1, tf2, idf_metrics)
<mask>:
                matrix[row, col] = 1.0
                degrees[row] += 1
            else:
                matrix[row, col] = 0
    for row in range(sentences_count):
        for col in range(sentences_count):
            if degrees[row] == 0:
                degrees[row] = 1
            matrix[row][col] = matrix[row][col] / degrees[row]
    return matrix","matrix[row, col] > threshold",80,"matrix[row, col] == 0",False,58.73949094699213,N/A
"@staticmethod
def cosine_similarity(sentence1, sentence2, tf1, tf2, idf_metrics):
    """"""
        We compute idf-modified-cosine(sentence1, sentence2) here.
        It's cosine similarity of these two sentences (vectors) A, B computed as cos(x, y) = A . B / (|A| . |B|)
        Sentences are represented as vector TF*IDF metrics.

        :param sentence1:
            Iterable object where every item represents word of 1st sentence.
        :param sentence2:
            Iterable object where every item represents word of 2nd sentence.
        :type tf1: dict
        :param tf1:
            Term frequencies of words from 1st sentence.
        :type tf2: dict
        :param tf2:
            Term frequencies of words from 2nd sentence
        :type idf_metrics: dict
        :param idf_metrics:
            Inverted document metrics of the sentences. Every sentence is treated as document for this algorithm.
        :rtype: float
        :return:
            Returns -1.0 for opposite similarity, 1.0 for the same sentence and zero for no similarity between sentences.
        """"""
    unique_words1 = frozenset(sentence1)
    unique_words2 = frozenset(sentence2)
    common_words = unique_words1 & unique_words2
    numerator = 0.0
    for term in common_words:
        numerator += tf1[term] * tf2[term] * idf_metrics[term] ** 2
    denominator1 = sum(((tf1[t] * idf_metrics[t]) ** 2 for t in unique_words1))
    denominator2 = sum(((tf2[t] * idf_metrics[t]) ** 2 for t in unique_words2))
<mask>:
        return numerator / (math.sqrt(denominator1) * math.sqrt(denominator2))
    else:
        return 0.0",denominator1 > 0 and denominator2 > 0,190,denominator1 != 0 and denominator2 != 0,False,20.164945583740657,N/A
"def _get_chunk_ratings(self, sentence, significant_stems):
    chunks = []
    NONSIGNIFICANT_CHUNK = [0] * self.max_gap_size
    in_chunk = False
    for order, word in enumerate(sentence.words):
        stem = self.stem_word(word)
<mask>:
            in_chunk = True
            chunks.append([1])
        elif in_chunk:
            is_significant_word = int(stem in significant_stems)
            chunks[-1].append(is_significant_word)
        if chunks and chunks[-1][-self.max_gap_size:] == NONSIGNIFICANT_CHUNK:
            in_chunk = False
    return tuple(map(self._get_chunk_rating, chunks))",stem in significant_stems and (not in_chunk),48,not stem,False,0.4764448014328884,N/A
"def _get_chunk_rating(self, chunk):
    chunk = self.__remove_trailing_zeros(chunk)
    words_count = len(chunk)
    assert words_count > 0
    significant_words = sum(chunk)
<mask>:
        return 0
    else:
        return significant_words ** 2 / words_count",significant_words == 1,26,significant_words == 0,False,75.98356856515926,N/A
"def __call__(self, document, sentences_count):
    self._ensure_dependencies_installed()
<mask>:
        return ()
    ratings = self.rate_sentences(document)
    return self._get_best_sentences(document.sentences, sentences_count, ratings)",not document.sentences,15,not document,False,36.78794411714425,N/A
"@staticmethod
def _rate_sentences_edge(words1, words2):
    rank = sum((words2.count(w) for w in words1))
<mask>:
        return 0.0
    assert len(words1) > 0 and len(words2) > 0
    norm = math.log(len(words1)) + math.log(len(words2))
    if numpy.isclose(norm, 0.0):
        assert rank in (0, 1)
        return float(rank)
    else:
        return rank / norm",rank == 0,42,rank == 0,True,100.00000000000004,N/A
"def __init__(self, stemmer=null_stemmer):
<mask>:
        raise ValueError('Stemmer has to be a callable object')
    self._stemmer = stemmer",not callable(stemmer),15,"not isinstance(stemmer, callable)",False,16.515821590069034,N/A
"@staticmethod
def _get_best_sentences(sentences, count, rating, *args, **kwargs):
    rate = rating
<mask>:
        assert not args and (not kwargs)

        def rate(s):
            return rating[s]
    infos = (SentenceInfo(s, o, rate(s, *args, **kwargs)) for o, s in enumerate(sentences))
    infos = sorted(infos, key=attrgetter('rating'), reverse=True)
    if not callable(count):
        count = ItemsCount(count)
    infos = count(infos)
    infos = sorted(infos, key=attrgetter('order'))
    return tuple((i.sentence for i in infos))","isinstance(rating, dict)",57,not rating,False,6.7667641618306344,N/A
"def _rate_sentences(self, document, significant_words, w_h, w_p1, w_p2, w_s1, w_s2):
    rated_sentences = {}
    paragraphs = document.paragraphs
    for paragraph_order, paragraph in enumerate(paragraphs):
        sentences = paragraph.sentences
        for sentence_order, sentence in enumerate(sentences):
            rating = self._rate_sentence(sentence, significant_words)
            rating *= w_h
<mask>:
                rating += w_p1
            elif paragraph_order == len(paragraphs) - 1:
                rating += w_p2
            if sentence_order == 0:
                rating += w_s1
            elif sentence_order == len(sentences) - 1:
                rating += w_s2
            rated_sentences[sentence] = rating
    return rated_sentences",paragraph_order == 0,69,paragraph_order == 0,True,100.00000000000004,N/A
"def _compute_significant_words(self, document, weight):
    words = map(self.stem_word, document.words)
    words = filter(self._is_bonus_word, words)
    word_counts = Counter(words)
    word_frequencies = word_counts.values()
<mask>:
        return ()
    max_word_frequency = max(word_frequencies)
    return tuple((word for word, frequency in word_counts.items() if frequency / max_word_frequency > weight))",not word_frequencies,37,len(word_frequencies) == 0,False,17.747405280050266,N/A
"def __init__(self, words, tokenizer=None):
<mask>:
        raise ValueError('Tokenizer has to be given if ``words`` is not a sequence.')
    elif isinstance(words, string_types):
        words = tokenizer.to_words(to_unicode(words))
    elif not isinstance(words, Sequence):
        raise ValueError('Parameter ``words`` has to be sequence or string with tokenizer given.')
    self._terms = Counter(map(unicode.lower, words))
    self._max_frequency = max(self._terms.values()) if self._terms else 1","isinstance(words, string_types) and tokenizer is None",50,"not isinstance(tokenser, Sequence)",False,7.64649370538093,N/A
"def most_frequent_terms(self, count=0):
    """"""
        Returns ``count`` of terms sorted by their frequency
        in descending order.

        :parameter int count:
            Max. number of returned terms. Value 0 means no limit (default).
        """"""
    terms = sorted(self._terms.items(), key=lambda i: -i[1])
    terms = tuple((i[0] for i in terms))
<mask>:
        return terms
    elif count > 0:
        return terms[:count]
    else:
        raise ValueError('Only non-negative values are allowed for count of terms.')",count == 0,63,count == 0,True,100.00000000000004,N/A
"def __init__(self, sentences):
    sentences = tuple(sentences)
    for sentence in sentences:
<mask>:
            raise TypeError(""Only instances of class 'Sentence' are allowed."")
    self._sentences = sentences","not isinstance(sentence, Sentence)",22,"not isinstance(sentence, Sentence)",True,100.00000000000004,N/A
"def _get_sentence_tokenizer(self, language):
<mask>:
        return self.SPECIAL_SENTENCE_TOKENIZERS[language]
    try:
        path = to_string('tokenizers/punkt/%s.pickle') % to_string(language)
        return nltk.data.load(path)
    except (LookupError, zipfile.BadZipfile) as e:
        raise LookupError('NLTK tokenizers are missing or the language is not supported.\nDownload them by following command: python -c ""import nltk; nltk.download(\'punkt\')""\nOriginal error was:\n' + str(e))",language in self.SPECIAL_SENTENCE_TOKENIZERS,43,language in self.SPECIAL_SENTENCE_TOKENIZERS,True,100.00000000000004,N/A
"def to_sentences(self, paragraph):
<mask>:
        extra_abbreviations = self.LANGUAGE_EXTRA_ABREVS.get(self._language, [])
        self._sentence_tokenizer._params.abbrev_types.update(extra_abbreviations)
    sentences = self._sentence_tokenizer.tokenize(to_unicode(paragraph))
    return tuple(map(unicode.strip, sentences))","hasattr(self._sentence_tokenizer, '_params')",15,self._language,False,6.26707538823693,N/A
"def stem_word(word):
    """"""
    Stem_word() checks if the associated word has the correct tag and stems it.
    If it does not have the correct one then it throws an exception.
    So we check all available POS tags, until we get the correct one.
    In the future a working greek POS tagger can substitute the code of this function.
    """"""
    try:
        from greek_stemmer import stemmer as gr_stemmer
    except ImportError:
        raise ValueError(""Greek stemmer requires greek_stemmer. Please, install it by command 'pip install greek-stemmer-pos'."")
    stem_candidates = set()
<mask>:
        return word.lower()
    for tag in _TOTAL_TAGS:
        try:
            stemmed = gr_stemmer.stem_word(word.lower(), tag)
            if stemmed and stemmed[-1].upper() in _CONSONANTS:
                stem_candidates.add(stemmed)
        except (TypeError, ValueError):
            pass
    return min(stem_candidates or [word], key=len).lower()",len(word) < 4,111,not word,False,6.7667641618306344,N/A
"def stem_word(word, aggressive=False):
<mask>:
        word = word.decode('utf-8')
    if not WORD_PATTERN.match(word):
        return word
    if not word.islower() and (not word.istitle()) and (not word.isupper()):
        warn('skipping word with mixed case: ' + word)
        return word
    stem = word.lower()
    stem = _remove_case(stem)
    stem = _remove_possessives(stem)
    if aggressive:
        stem = _remove_comparative(stem)
        stem = _remove_diminutive(stem)
        stem = _remove_augmentative(stem)
        stem = _remove_derivational(stem)
    if word.isupper():
        return stem.upper()
    if word.istitle():
        return stem.title()
    return stem","not isinstance(word, unicode)",64,"isinstance(word, bytes)",False,45.48019047027906,N/A
"def _remove_case(word):
<mask>:
        return word[:-5]
    if len(word) > 6:
        if word.endswith('ětem'):
            return _palatalize(word[:-3])
        if word.endswith('atům'):
            return word[:-4]
    if len(word) > 5:
        if word[-3:] in ('ech', 'ich', 'ích', 'ého', 'ěmi', 'emi', 'ému', 'ete', 'eti', 'iho', 'ího', 'ími', 'imu'):
            return _palatalize(word[:-2])
        if word[-3:] in ('ách', 'ata', 'aty', 'ých', 'ama', 'ami', 'ové', 'ovi', 'ými'):
            return word[:-3]
    if len(word) > 4:
        if word.endswith('em'):
            return _palatalize(word[:-1])
        if word[-2:] in ('es', 'ém', 'ím'):
            return _palatalize(word[:-2])
        if word[-2:] in ('ům', 'at', 'ám', 'os', 'us', 'ým', 'mi', 'ou'):
            return word[:-2]
    if len(word) > 3:
        if word[-1] in 'eiíě':
            return _palatalize(word)
        if word[-1] in 'uyůaoáéý':
            return word[:-1]
    return word",len(word) > 7 and word.endswith('atech'),100,word.endswith('á'),False,16.731227054577023,N/A
"def _remove_possessives(word):
<mask>:
        if word[-2:] in ('ov', 'ův'):
            return word[:-2]
        if word.endswith('in'):
            return _palatalize(word[:-1])
    return word",len(word) > 5,16,word.startswith('â'),False,10.682175159905853,N/A
"def _remove_comparative(word):
<mask>:
        if word[-3:] in ('ejš', 'ějš'):
            return _palatalize(word[:-2])
    return word",len(word) > 5,12,word.startswith('á'),False,10.682175159905853,N/A
"def _remove_diminutive(word):
<mask>:
        return word[:-5]
    if len(word) > 6:
        if word[-4:] in ('eček', 'éček', 'iček', 'íček', 'enek', 'ének', 'inek', 'ínek'):
            return _palatalize(word[:-3])
        if word[-4:] in ('áček', 'aček', 'oček', 'uček', 'anek', 'onek', 'unek', 'ánek'):
            return _palatalize(word[:-4])
    if len(word) > 5:
        if word[-3:] in ('ečk', 'éčk', 'ičk', 'íčk', 'enk', 'énk', 'ink', 'ínk'):
            return _palatalize(word[:-3])
        if word[-3:] in ('áčk', 'ačk', 'očk', 'učk', 'ank', 'onk', 'unk', 'átk', 'ánk', 'ušk'):
            return word[:-3]
    if len(word) > 4:
        if word[-2:] in ('ek', 'ék', 'ík', 'ik'):
            return _palatalize(word[:-1])
        if word[-2:] in ('ák', 'ak', 'ok', 'uk'):
            return word[:-1]
    if len(word) > 3 and word[-1] == 'k':
        return word[:-1]
    return word",len(word) > 7 and word.endswith('oušek'),101,len(word) > 7,False,31.140322391459787,N/A
"def __init__(self, language):
    language = normalize_language(language)
    self._stemmer = null_stemmer
<mask>:
        self._stemmer = self.SPECIAL_STEMMERS[language.lower()]
        return
    stemmer_classname = language.capitalize() + 'Stemmer'
    try:
        stemmer_class = getattr(nltk_stemmers_module, stemmer_classname)
    except AttributeError:
        raise LookupError('Stemmer is not available for language %s.' % language)
    self._stemmer = stemmer_class().stem",language.lower() in self.SPECIAL_STEMMERS,39,language in self.SPECIAL_STEMMERS,False,47.486944442513455,N/A
"def stem_word(word):
    """"""
    Based on https://drupal.org/project/ukstemmer and ported to Python https://github.com/Amice13/ukr_stemmer
    """"""
    word = _preprocess(word)
<mask>:
        return word
    p = re.search(_RVRE, word)
    start = word[0:p.span()[1]]
    suffix = word[p.span()[1]:]
    updated, suffix = _update_suffix(suffix, _PERFECTIVE_GROUND, '')
    if not updated:
        _, suffix = _update_suffix(suffix, _REFLEXIVE, '')
        updated, suffix = _update_suffix(suffix, _ADJECTIVE, '')
        if updated:
            updated, suffix = _update_suffix(suffix, _PARTICIPLE, '')
        else:
            updated, suffix = _update_suffix(suffix, _VERB, '')
            if not updated:
                _, suffix = _update_suffix(suffix, _NOUN, '')
    updated, suffix = _update_suffix(suffix, 'и$', '')
    if re.search(_DERIVATIONAL, suffix):
        updated, suffix = _update_suffix(suffix, 'ость$', '')
    updated, suffix = _update_suffix(suffix, 'ь$', '')
    if updated:
        _, suffix = _update_suffix(suffix, 'ейше?$', '')
        _, suffix = _update_suffix(suffix, 'нн$', u'н')
    return start + suffix","not re.search('[аеиоуюяіїє]', word)",112,not word,False,0.2889783797297444,N/A
"def _split_into_words(sentences):
    full_text_words = []
    for s in sentences:
<mask>:
            raise ValueError('Object in collection must be of type Sentence')
        full_text_words.extend(s.words)
    return full_text_words","not isinstance(s, Sentence)",22,"not isinstance(s, Sentence)",True,100.00000000000004,N/A
"def _lcs(x, y):
    """"""
    Computes the length of the longest common subsequence (lcs) between two
    strings. The implementation below uses a DP programming algorithm and runs
    in O(nm) time where n = len(x) and m = len(y).
    Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence

    :param x: collection of words
    :param y: collection of words
    :returns table: dictionary of coord and len lcs
    """"""
    n, m = _get_index_of_lcs(x, y)
    table = dict()
    for i in range(n + 1):
        for j in range(m + 1):
<mask>:
                table[i, j] = 0
            elif x[i - 1] == y[j - 1]:
                table[i, j] = table[i - 1, j - 1] + 1
            else:
                table[i, j] = max(table[i - 1, j], table[i, j - 1])
    return table",i == 0 or j == 0,116,x[i - 1] == y[j - 1],False,6.754312828675707,N/A
"def _recon_lcs(x, y):
    """"""
    Returns the Longest Subsequence between x and y.
    Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence

    :param x: sequence of words
    :param y: sequence of words
    :returns sequence: LCS of x and y
    """"""
    table = _lcs(x, y)

    def _recon(i, j):
<mask>:
            return []
        elif x[i - 1] == y[j - 1]:
            return _recon(i - 1, j - 1) + [(x[i - 1], i)]
        elif table[i - 1, j] > table[i, j - 1]:
            return _recon(i - 1, j)
        else:
            return _recon(i, j - 1)
    i, j = _get_index_of_lcs(x, y)
    recon_tuple = tuple(map(lambda r: r[0], _recon(i, j)))
    return recon_tuple",i == 0 or j == 0,97,i == 0 or j == 0,True,100.00000000000004,N/A
"def rouge_n(evaluated_sentences, reference_sentences, n=2):
    """"""
    Computes ROUGE-N of two text collections of sentences.
    Sourece: http://research.microsoft.com/en-us/um/people/cyl/download/
    papers/rouge-working-note-v1.3.1.pdf

    :param evaluated_sentences:
        The sentences that have been picked by the summarizer
    :param reference_sentences:
        The sentences from the reference set
    :param n: Size of ngram.  Defaults to 2.
    :returns:
        float 0 <= ROUGE-N <= 1, where 0 means no overlap and 1 means
        exactly the same.
    :raises ValueError: raises exception if a param has len <= 0
    """"""
<mask>:
        raise ValueError('Collections must contain at least 1 sentence.')
    evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)
    reference_ngrams = _get_word_ngrams(n, reference_sentences)
    reference_count = len(reference_ngrams)
    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)
    overlapping_count = len(overlapping_ngrams)
    return overlapping_count / reference_count",len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0,103,len(evaluated_sentences) < 1,False,21.26119097464094,N/A
"def rouge_l_sentence_level(evaluated_sentences, reference_sentences):
    """"""
    Computes ROUGE-L (sentence level) of two text collections of sentences.
    http://research.microsoft.com/en-us/um/people/cyl/download/papers/
    rouge-working-note-v1.3.1.pdf

    Calculated according to:
    R_lcs = LCS(X,Y)/m
    P_lcs = LCS(X,Y)/n
    F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)

    where:
    X = reference summary
    Y = Candidate summary
    m = length of reference summary
    n = length of candidate summary

    :param evaluated_sentences:
        The sentences that have been picked by the summarizer
    :param reference_sentences:
        The sentences from the reference set
    :returns float: F_lcs
    :raises ValueError: raises exception if a param has len <= 0
    """"""
<mask>:
        raise ValueError('Collections must contain at least 1 sentence.')
    reference_words = _split_into_words(reference_sentences)
    evaluated_words = _split_into_words(evaluated_sentences)
    m = len(reference_words)
    n = len(evaluated_words)
    lcs = _len_lcs(evaluated_words, reference_words)
    return _f_lcs(lcs, m, n)",len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0,120,len(reference_sentences) == 0,False,22.397465712826726,N/A
"def f_score(evaluated_sentences, reference_sentences, weight=1.0):
    """"""
    Computation of F-Score measure. It is computed as
    F(E) = ( (W^2 + 1) * P(E) * R(E) ) / ( W^2 * P(E) + R(E) ), where:

    - P(E) is precision metrics of extract E.
    - R(E) is recall metrics of extract E.
    - W is a weighting factor that favours P(E) metrics
      when W > 1 and favours R(E) metrics when W < 1.

    If W = 1.0 (default value) basic F-Score is computed.
    It is equivalent to F(E) = (2 * P(E) * R(E)) / (P(E) + R(E)).

    :parameter iterable evaluated_sentences:
        Sentences of evaluated extract.
    :parameter iterable reference_sentences:
        Sentences of reference extract.
    :returns float:
        Returns 0.0 <= P(E) <= 1.0
    """"""
    p = precision(evaluated_sentences, reference_sentences)
    r = recall(evaluated_sentences, reference_sentences)
    weight **= 2
    denominator = weight * p + r
<mask>:
        return 0.0
    else:
        return (weight + 1) * p * r / denominator",denominator == 0.0,151,denominator == 0.0,True,100.00000000000004,N/A
"def _divide_evaluation(numerator_sentences, denominator_sentences):
    denominator_sentences = frozenset(denominator_sentences)
    numerator_sentences = frozenset(numerator_sentences)
<mask>:
        raise ValueError('Both collections have to contain at least 1 sentence.')
    common_count = len(denominator_sentences & numerator_sentences)
    choosen_count = len(denominator_sentences)
    assert choosen_count != 0
    return common_count / choosen_count",len(numerator_sentences) == 0 or len(denominator_sentences) == 0,36,len(denominator_sentences) < 1,False,17.202572313558196,N/A
"def cosine_similarity(evaluated_model, reference_model):
    """"""
    Computes cosine similarity of two text documents. Each document
    has to be represented as TF model of non-empty document.

    :returns float:
        0 <= cos <= 1, where 0 means independence and 1 means
        exactly the same.
    """"""
<mask>:
        raise ValueError(""Arguments has to be instances of 'sumy.models.TfDocumentModel'"")
    terms = frozenset(evaluated_model.terms) | frozenset(reference_model.terms)
    numerator = 0.0
    for term in terms:
        numerator += evaluated_model.term_frequency(term) * reference_model.term_frequency(term)
    denominator = evaluated_model.magnitude * reference_model.magnitude
    if denominator == 0.0:
        raise ValueError(""Document model can't be empty. Given %r & %r"" % (evaluated_model, reference_model))
    return numerator / denominator","not (isinstance(evaluated_model, TfModel) and isinstance(reference_model, TfModel))",93,"not isinstance(evaluated_model, TfDocumentModel)",False,18.59347207517099,N/A
"def unit_overlap(evaluated_model, reference_model):
    """"""
    Computes unit overlap of two text documents. Documents
    has to be represented as TF models of non-empty document.

    :returns float:
        0 <= overlap <= 1, where 0 means no match and 1 means
        exactly the same.
    """"""
<mask>:
        raise ValueError(""Arguments has to be instances of 'sumy.models.TfDocumentModel'"")
    terms1 = frozenset(evaluated_model.terms)
    terms2 = frozenset(reference_model.terms)
    if not terms1 and (not terms2):
        raise ValueError(""Documents can't be empty. Please pass the valid documents."")
    common_terms_count = len(terms1 & terms2)
    return common_terms_count / (len(terms1) + len(terms2) - common_terms_count)","not (isinstance(evaluated_model, TfModel) and isinstance(reference_model, TfModel))",85,"not isinstance(evaluated_model, TfDocumentModel) or not isinstance(reference_model, TfDocumentModel)",False,50.041090726431214,N/A
"def main(args=None):
    args = docopt(to_string(__doc__), args, version=__version__)
    summarizer, document, items_count, reference_summary = handle_arguments(args)
    evaluated_sentences = summarizer(document, items_count)
    reference_document = PlaintextParser.from_string(reference_summary, Tokenizer(args['--language']))
    reference_sentences = reference_document.document.sentences
    for name, evaluate_document, evaluate in AVAILABLE_EVALUATIONS:
<mask>:
            result = evaluate(evaluated_sentences, document.sentences)
        else:
            result = evaluate(evaluated_sentences, reference_sentences)
        print('%s: %f' % (name, result))
    return 0",evaluate_document,47,document.sentences,False,27.516060407455225,N/A
"def handle_arguments(args):
    document_format = args['--format']
<mask>:
        raise ValueError('Unsupported format of input document. Possible values are: %s. Given: %s.' % (', '.join(PARSERS.keys()), document_format))
    if args['--url'] is not None:
        parser = PARSERS['html']
        document_content = fetch_url(args['--url'])
    elif args['--file'] is not None:
        parser = PARSERS.get(document_format, PlaintextParser)
        with open(args['--file'], 'rb') as file:
            document_content = file.read()
    else:
        parser = PARSERS['plaintext']
        document_content = sys.stdin.read()
    summarizer_builder = AVAILABLE_METHODS['luhn']
    for method, builder in AVAILABLE_METHODS.items():
        if args[method]:
            summarizer_builder = builder
            break
    items_count = ItemsCount(args['--length'])
    parser = parser(document_content, Tokenizer(args['--language']))
    with open(args['<reference_summary>'], 'rb') as file:
        reference_summmary = file.read().decode('utf-8')
    return (summarizer_builder(parser, args['--language']), parser.document, items_count, reference_summmary)",document_format is not None and document_format not in PARSERS,92,document_format not in PARSERS,False,31.140322391459787,N/A
"def build_document_from_string(string):
    sentences = []
    paragraphs = []
    for line in string.strip().splitlines():
        line = line.lstrip()
<mask>:
            sentences.append(build_sentence(line[2:], is_heading=True))
        elif not line:
            paragraphs.append(Paragraph(sentences))
            sentences = []
        else:
            sentences.append(build_sentence(line))
    paragraphs.append(Paragraph(sentences))
    return ObjectDocumentModel(paragraphs)",line.startswith('# '),30,line.startswith('#'),False,100.00000000000004,N/A
"def test_less_than_10_words_should_be_returned():
    """"""https://github.com/miso-belica/sumy/issues/159""""""
    document = build_document_from_string(""\n        # Heading one\n        First sentence.\n        Second sentence.\n        Third sentence.\n\n        # Heading two\n        I like sentences but this one is really long.\n        They are so wordy\n        And have many many letters\n        And are green in my editor\n        But someone doesn't like them :(\n    "")
    summarizer = RandomSummarizer()

    def count(max_words, sentence_infos):
        results = []
        words_count = 0
        for info in sentence_infos:
            words_count += len(info.sentence.words)
<mask>:
                return results
            else:
                results.append(info)
        return results
    sentences = summarizer(document, partial(count, 10))
    assert 0 < sum((len(s.words) for s in sentences)) <= 10",words_count > max_words,90,words_count > max_words,True,100.00000000000004,N/A
"def display_np_arrays_as_images():

    def np_to_png(a):
<mask>:
            return fromarray(np.array(np.clip(a, 0, 1) * 255, dtype='uint8'))._repr_png_()
        else:
            return fromarray(np.zeros([1, 1], dtype='uint8'))._repr_png_()

    def np_to_text(obj, p, cycle):
        if len(obj.shape) < 2:
            print(repr(obj))
        if 2 <= len(obj.shape) <= 3:
            pass
        else:
            print('<array of shape {}>'.format(obj.shape))
    get_ipython().display_formatter.formatters['image/png'].for_type(np.ndarray, np_to_png)
    get_ipython().display_formatter.formatters['text/plain'].for_type(np.ndarray, np_to_text)",2 <= len(a.shape) <= 3,42,a is not None,False,2.1617886496312457,N/A
"def _reduce_axes(tensor, reduction_type: Reduction, reduced_axes: List[int], backend):
<mask>:
        return reduction_type(tensor, tuple(reduced_axes))
    else:
        assert reduction_type in _reductions
        if reduction_type == 'mean':
            if not backend.is_float_type(tensor):
                raise NotImplementedError('reduce_mean is not available for non-floating tensors')
        return backend.reduce(tensor, reduction_type, tuple(reduced_axes))",callable(reduction_type),35,"isinstance(reduction_type, Reduction)",False,36.55552228545123,N/A
"def _optimize_transformation(init_shapes, reduced_axes, axes_reordering, final_shapes):
    assert len(axes_reordering) + len(reduced_axes) == len(init_shapes)
    reduced_axes = tuple(sorted(reduced_axes))
    for i in range(len(reduced_axes) - 1)[::-1]:
<mask>:
            removed_axis = reduced_axes[i + 1]
            removed_length = init_shapes[removed_axis]
            init_shapes = init_shapes[:removed_axis] + init_shapes[removed_axis + 1:]
            init_shapes[removed_axis - 1] *= removed_length
            reduced_axes = reduced_axes[:i + 1] + tuple((axis - 1 for axis in reduced_axes[i + 2:]))

    def build_mapping():
        init_to_final = {}
        for axis in range(len(init_shapes)):
            if axis in reduced_axes:
                init_to_final[axis] = None
            else:
                after_reduction = sum((x is not None for x in init_to_final.values()))
                init_to_final[axis] = list(axes_reordering).index(after_reduction)
        return init_to_final
    init_axis_to_final_axis = build_mapping()
    for init_axis in range(len(init_shapes) - 1)[::-1]:
        if init_axis_to_final_axis[init_axis] is None:
            continue
        if init_axis_to_final_axis[init_axis + 1] is None:
            continue
        if init_axis_to_final_axis[init_axis] + 1 == init_axis_to_final_axis[init_axis + 1]:
            removed_axis = init_axis + 1
            removed_length = init_shapes[removed_axis]
            removed_axis_after_reduction = sum((x not in reduced_axes for x in range(removed_axis)))
            reduced_axes = tuple((axis if axis < removed_axis else axis - 1 for axis in reduced_axes))
            init_shapes = init_shapes[:removed_axis] + init_shapes[removed_axis + 1:]
            init_shapes[removed_axis - 1] *= removed_length
            old_reordering = axes_reordering
            axes_reordering = []
            for axis in old_reordering:
                if axis == removed_axis_after_reduction:
                    pass
                elif axis < removed_axis_after_reduction:
                    axes_reordering.append(axis)
                else:
                    axes_reordering.append(axis - 1)
            init_axis_to_final_axis = build_mapping()
    return (init_shapes, reduced_axes, axes_reordering, final_shapes)",reduced_axes[i] + 1 == reduced_axes[i + 1],194,reduced_axes[i] + 1 == init_shapes[i + 1],False,72.76817202342096,N/A
"def _reconstruct_from_shape_uncached(self: TransformRecipe, shape: List[int], axes_dims: FakeHashableAxesLengths) -> CookedRecipe:
    """"""
    Reconstruct all actual parameters using shape.
    Shape is a tuple that may contain integers, shape symbols (tf, theano) and UnknownSize (tf, previously mxnet)
    known axes can be integers or symbols, but not Nones.
    """"""
    need_init_reshape = False
    axes_lengths: List[int] = list(self.elementary_axes_lengths)
    for axis, dim in axes_dims:
        axes_lengths[self.axis_name2elementary_axis[axis]] = dim
    for input_axis, (known_axes, unknown_axes) in enumerate(self.input_composition_known_unknown):
        length = shape[input_axis]
<mask>:
            axes_lengths[unknown_axes[0]] = length
            continue
        known_product = 1
        for axis in known_axes:
            known_product *= axes_lengths[axis]
        if len(unknown_axes) == 0:
            if isinstance(length, int) and isinstance(known_product, int) and (length != known_product):
                raise EinopsError(f'Shape mismatch, {length} != {known_product}')
        else:
            if isinstance(length, int) and isinstance(known_product, int) and (length % known_product != 0):
                raise EinopsError(f""Shape mismatch, can't divide axis of length {length} in chunks of {known_product}"")
            unknown_axis = unknown_axes[0]
            inferred_length: int = length // known_product
            axes_lengths[unknown_axis] = inferred_length
        if len(known_axes) + len(unknown_axes) != 1:
            need_init_reshape = True
    init_shapes: Optional[List[int]] = axes_lengths[:len(self.axes_permutation)] if need_init_reshape else None
    need_final_reshape = False
    final_shapes: List[int] = []
    for grouping in self.output_composite_axes:
        lengths = [axes_lengths[elementary_axis] for elementary_axis in grouping]
        final_shapes.append(_product(lengths))
        if len(lengths) != 1:
            need_final_reshape = True
    added_axes: Dict[int, int] = {pos: axes_lengths[pos_in_elementary] for pos, pos_in_elementary in self.added_axes.items()}
    reduced_axes = list(range(self.first_reduced_axis, len(self.axes_permutation)))
    n_axes_after_adding_axes = len(added_axes) + len(self.axes_permutation)
    axes_reordering: Optional[List[int]] = self.axes_permutation
    if self.axes_permutation == list(range(len(self.axes_permutation))):
        axes_reordering = None
    _final_shapes = final_shapes if need_final_reshape else None
    return (init_shapes, axes_reordering, reduced_axes, added_axes, _final_shapes, n_axes_after_adding_axes)",len(known_axes) == 0 and len(unknown_axes) == 1,229,len(unknown_axes) == 1,False,32.91929878079057,N/A
"def _apply_recipe(backend, recipe: TransformRecipe, tensor: Tensor, reduction_type: Reduction, axes_lengths: HashableAxesLengths) -> Tensor:
    try:
        init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(recipe, backend.shape(tensor), axes_lengths)
    except TypeError:
        _result = _reconstruct_from_shape_uncached(recipe, backend.shape(tensor), axes_lengths)
        init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _result
<mask>:
        tensor = backend.reshape(tensor, init_shapes)
    if axes_reordering is not None:
        tensor = backend.transpose(tensor, axes_reordering)
    if len(reduced_axes) > 0:
        tensor = _reduce_axes(tensor, reduction_type=reduction_type, reduced_axes=reduced_axes, backend=backend)
    if len(added_axes) > 0:
        tensor = backend.add_axes(tensor, n_axes=n_axes_w_added, pos2len=added_axes)
    if final_shapes is not None:
        tensor = backend.reshape(tensor, final_shapes)
    return tensor",init_shapes is not None,82,len(init_shapes) > 0,False,20.556680845025987,N/A
"def _apply_recipe_array_api(xp, recipe: TransformRecipe, tensor: Tensor, reduction_type: Reduction, axes_lengths: HashableAxesLengths) -> Tensor:
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(recipe, tensor.shape, axes_lengths)
<mask>:
        tensor = xp.reshape(tensor, init_shapes)
    if axes_reordering is not None:
        tensor = xp.permute_dims(tensor, axes_reordering)
    if len(reduced_axes) > 0:
        if callable(reduction_type):
            tensor = reduction_type(tensor, tuple(reduced_axes))
        else:
            assert reduction_type in _reductions
            tensor = getattr(xp, reduction_type)(tensor, axis=tuple(reduced_axes))
    if len(added_axes) > 0:
        for axis_position, axis_length in added_axes.items():
            tensor = xp.expand_dims(tensor, axis=axis_position)
        final_shape = list(tensor.shape)
        for axis_position, axis_length in added_axes.items():
            final_shape[axis_position] = axis_length
        tensor = xp.broadcast_to(tensor, final_shape)
    if final_shapes is not None:
        tensor = xp.reshape(tensor, final_shapes)
    return tensor",init_shapes is not None,95,init_shapes is not None,True,100.00000000000004,N/A
"def reduce(tensor: Tensor, pattern: str, reduction: Reduction, **axes_lengths: int) -> Tensor:
<mask>:
        if len(tensor) == 0:
            raise TypeError(""Einops can't be applied to an empty list"")
        xp = tensor[0].__array_namespace__()
        tensor = xp.stack(tensor)
    else:
        xp = tensor.__array_namespace__()
    try:
        hashable_axes_lengths = tuple(axes_lengths.items())
        recipe = _prepare_transformation_recipe(pattern, reduction, axes_names=tuple(axes_lengths), ndim=tensor.ndim)
        return _apply_recipe_array_api(xp, recipe=recipe, tensor=tensor, reduction_type=reduction, axes_lengths=hashable_axes_lengths)
    except EinopsError as e:
        message = ' Error while processing {}-reduction pattern ""{}"".'.format(reduction, pattern)
        if not isinstance(tensor, list):
            message += '\n Input tensor shape: {}. '.format(tensor.shape)
        else:
            message += '\n Input is list. '
        message += 'Additional info: {}.'.format(axes_lengths)
        raise EinopsError(message + '\n {}'.format(e))","isinstance(tensor, list)",95,"isinstance(tensor, list)",True,100.00000000000004,N/A
"def pack(tensors: Sequence[Tensor], pattern: str) -> Tuple[Tensor, List[Shape]]:
    n_axes_before, n_axes_after, min_axes = analyze_pattern(pattern, 'pack')
    xp = tensors[0].__array_namespace__()
    reshaped_tensors: List[Tensor] = []
    packed_shapes: List[Shape] = []
    for i, tensor in enumerate(tensors):
        shape = tensor.shape
<mask>:
            raise EinopsError(f'packed tensor #{i} (enumeration starts with 0) has shape {shape}, while pattern {pattern} assumes at least {min_axes} axes')
        axis_after_packed_axes = len(shape) - n_axes_after
        packed_shapes.append(shape[n_axes_before:axis_after_packed_axes])
        reshaped_tensors.append(xp.reshape(tensor, (*shape[:n_axes_before], -1, *shape[axis_after_packed_axes:])))
    return (xp.concat(reshaped_tensors, axis=n_axes_before), packed_shapes)",len(shape) < min_axes,67,len(shape) < min_axes,True,100.00000000000004,N/A
"def unpack(tensor: Tensor, packed_shapes: List[Shape], pattern: str) -> List[Tensor]:
    xp = tensor.__array_namespace__()
    n_axes_before, n_axes_after, min_axes = analyze_pattern(pattern, opname='unpack')
    input_shape = tensor.shape
<mask>:
        raise EinopsError(f'unpack(..., {pattern}) received input of wrong dim with shape {input_shape}')
    unpacked_axis: int = n_axes_before
    lengths_of_composed_axes: List[int] = [-1 if -1 in p_shape else prod(p_shape) for p_shape in packed_shapes]
    n_unknown_composed_axes = sum((x == -1 for x in lengths_of_composed_axes))
    if n_unknown_composed_axes > 1:
        raise EinopsError(f""unpack(..., {pattern}) received more than one -1 in {packed_shapes} and can't infer dimensions"")
    split_positions = [0] * len(packed_shapes) + [input_shape[unpacked_axis]]
    if n_unknown_composed_axes == 0:
        for i, x in enumerate(lengths_of_composed_axes[:-1]):
            split_positions[i + 1] = split_positions[i] + x
    else:
        unknown_composed_axis: int = lengths_of_composed_axes.index(-1)
        for i in range(unknown_composed_axis):
            split_positions[i + 1] = split_positions[i] + lengths_of_composed_axes[i]
        for j in range(unknown_composed_axis + 1, len(lengths_of_composed_axes))[::-1]:
            split_positions[j] = split_positions[j + 1] - lengths_of_composed_axes[j]
    shape_start = input_shape[:unpacked_axis]
    shape_end = input_shape[unpacked_axis + 1:]
    slice_filler = (slice(None, None),) * unpacked_axis
    try:
        return [xp.reshape(tensor[*slice_filler, slice(split_positions[i], split_positions[i + 1]), ...], (*shape_start, *element_shape, *shape_end)) for i, element_shape in enumerate(packed_shapes)]
    except Exception:
        raise RuntimeError(f'Error during unpack(..., ""{pattern}""): could not split axis of size {split_positions[-1]} into requested {packed_shapes}')",len(input_shape) != n_axes_before + 1 + n_axes_after,178,input_shape != packed_shapes,False,5.327270801134887,N/A
"def __init__(self, value: str):
    self.value = int(value)
<mask>:
        if self.value == 1:
            raise EinopsError('No need to create anonymous axis of length 1. Report this as an issue')
        else:
            raise EinopsError('Anonymous axis should have positive length, not {}'.format(self.value))",self.value <= 1,37,self.value < 0,False,54.75182535069452,N/A
"def __init__(self, expression: str, *, allow_underscore: bool=False, allow_duplicates: bool=False):
    self.has_ellipsis: bool = False
    self.has_ellipsis_parenthesized: Optional[bool] = None
    self.identifiers: Set[str] = set()
    self.has_non_unitary_anonymous_axes: bool = False
    self.composition: List[Union[List[str], str]] = []
<mask>:
        if '...' not in expression:
            raise EinopsError('Expression may contain dots only inside ellipsis (...)')
        if str.count(expression, '...') != 1 or str.count(expression, '.') != 3:
            raise EinopsError('Expression may contain dots only inside ellipsis (...); only one ellipsis for tensor ')
        expression = expression.replace('...', _ellipsis)
        self.has_ellipsis = True
    bracket_group: Optional[List[str]] = None

    def add_axis_name(x):
        if x in self.identifiers:
            if not (allow_underscore and x == '_') and (not allow_duplicates):
                raise EinopsError('Indexing expression contains duplicate dimension ""{}""'.format(x))
        if x == _ellipsis:
            self.identifiers.add(_ellipsis)
            if bracket_group is None:
                self.composition.append(_ellipsis)
                self.has_ellipsis_parenthesized = False
            else:
                bracket_group.append(_ellipsis)
                self.has_ellipsis_parenthesized = True
        else:
            is_number = str.isdecimal(x)
            if is_number and int(x) == 1:
                if bracket_group is None:
                    self.composition.append([])
                else:
                    pass
                return
            is_axis_name, reason = self.check_axis_name_return_reason(x, allow_underscore=allow_underscore)
            if not (is_number or is_axis_name):
                raise EinopsError('Invalid axis identifier: {}\n{}'.format(x, reason))
            if is_number:
                x = AnonymousAxis(x)
            self.identifiers.add(x)
            if is_number:
                self.has_non_unitary_anonymous_axes = True
            if bracket_group is None:
                self.composition.append([x])
            else:
                bracket_group.append(x)
    current_identifier = None
    for char in expression:
        if char in '() ':
            if current_identifier is not None:
                add_axis_name(current_identifier)
            current_identifier = None
            if char == '(':
                if bracket_group is not None:
                    raise EinopsError('Axis composition is one-level (brackets inside brackets not allowed)')
                bracket_group = []
            elif char == ')':
                if bracket_group is None:
                    raise EinopsError('Brackets are not balanced')
                self.composition.append(bracket_group)
                bracket_group = None
        elif str.isalnum(char) or char in ['_', _ellipsis]:
            if current_identifier is None:
                current_identifier = char
            else:
                current_identifier += char
        else:
            raise EinopsError(""Unknown character '{}'"".format(char))
    if bracket_group is not None:
        raise EinopsError('Imbalanced parentheses in expression: ""{}""'.format(expression))
    if current_identifier is not None:
        add_axis_name(current_identifier)",'.' in expression,274,expression,False,1.8315638888734187,N/A
"def has_composed_axes(self) -> bool:
    for axes in self.composition:
<mask>:
            return True
    return False","isinstance(axes, list) and len(axes) > 1",13,axes.has_composed(),False,3.667862829704212,N/A
"@staticmethod
def check_axis_name_return_reason(name: str, allow_underscore: bool=False) -> Tuple[bool, str]:
<mask>:
        return (False, 'not a valid python identifier')
    elif name[0] == '_' or name[-1] == '_':
        if name == '_' and allow_underscore:
            return (True, '')
        return (False, 'axis name should should not start or end with underscore')
    else:
        if keyword.iskeyword(name):
            warnings.warn('It is discouraged to use axes names that are keywords: {}'.format(name), RuntimeWarning)
        if name in ['axis']:
            warnings.warn(""It is discouraged to use 'axis' as an axis name and will raise an error in future"", FutureWarning)
        return (True, '')",not str.isidentifier(name),86,not name or not name.startswith('_'),False,5.063996506781411,N/A
"@lru_cache(maxsize=128)
def analyze_pattern(pattern: str, opname: str) -> Tuple[int, int, int]:
    axes = pattern.split()
    axes_set = set(axes)
<mask>:
        raise EinopsError(f'Duplicates in axes names in {opname}(..., ""{pattern}"")')
    if '*' not in axes_set:
        raise EinopsError(f'No *-axis in {opname}(..., ""{pattern}"")')
    for axis in axes:
        if axis != '*':
            is_valid, reason = ParsedExpression.check_axis_name_return_reason(axis)
            if not is_valid:
                raise EinopsError(f'Invalid axis name {axis} in {opname}(..., ""{pattern}"")')
    n_axes_before = axes.index('*')
    n_axes_after = len(axes) - n_axes_before - 1
    min_axes = n_axes_before + n_axes_after
    return (n_axes_before, n_axes_after, min_axes)",len(axes) != len(axes_set),78,len(axes_set) > 1,False,41.26682571567718,N/A
"def pack(tensors: Sequence[Tensor], pattern: str) -> Tuple[Tensor, List[Shape]]:
    """"""
    Packs several tensors into one.
    See einops tutorial for introduction into packing (and how it replaces stack and concatenation).

    Parameters:
        tensors: tensors to be packed, can be of different dimensionality
        pattern: pattern that is shared for all inputs and output, e.g. ""i j * k"" or ""batch seq *""

    Returns:
        (packed_tensor, packed_shapes aka PS)

    Example:
    ```python
    >>> from numpy import zeros as Z
    >>> inputs = [Z([2, 3, 5]), Z([2, 3, 7, 5]), Z([2, 3, 7, 9, 5])]
    >>> packed, ps = pack(inputs, 'i j * k')
    >>> packed.shape, ps
    ((2, 3, 71, 5), [(), (7,), (7, 9)])
    ```

    In this example, axes were matched to: i=2, j=3, k=5 based on order (first, second, and last).
    All other axes were 'packed' and concatenated.
    PS (packed shapes) contains information about axes that were matched to '*' in every input.
    Resulting tensor has as many elements as all inputs in total.

    Packing can be reversed with unpack, which additionally needs PS (packed shapes) to reconstruct order.

    ```python
    >>> inputs_unpacked = unpack(packed, ps, 'i j * k')
    >>> [x.shape for x in inputs_unpacked]
    [(2, 3, 5), (2, 3, 7, 5), (2, 3, 7, 9, 5)]
    ```

    Read the tutorial for introduction and application scenarios.
    """"""
    n_axes_before, n_axes_after, min_axes = analyze_pattern(pattern, 'pack')
    backend = get_backend(tensors[0])
    reshaped_tensors: List[Tensor] = []
    packed_shapes: List[Shape] = []
    for i, tensor in enumerate(tensors):
        shape = backend.shape(tensor)
<mask>:
            raise EinopsError(f'packed tensor #{i} (enumeration starts with 0) has shape {shape}, while pattern {pattern} assumes at least {min_axes} axes')
        axis_after_packed_axes = len(shape) - n_axes_after
        packed_shapes.append(shape[n_axes_before:axis_after_packed_axes])
        reshaped_tensors.append(backend.reshape(tensor, (*shape[:n_axes_before], -1, *shape[axis_after_packed_axes:])))
    return (backend.concat(reshaped_tensors, axis=n_axes_before), packed_shapes)",len(shape) < min_axes,270,i == 0,False,0.0,N/A
"@staticmethod
def reduce(x: torch.Tensor, operation: str, reduced_axes: List[int]):
<mask>:
        return x.amin(dim=reduced_axes)
    elif operation == 'max':
        return x.amax(dim=reduced_axes)
    elif operation == 'sum':
        return x.sum(dim=reduced_axes)
    elif operation == 'mean':
        return x.mean(dim=reduced_axes)
    elif operation == 'prod':
        for i in list(sorted(reduced_axes))[::-1]:
            x = x.prod(dim=i)
        return x
    else:
        raise NotImplementedError('Unknown reduction ', operation)",operation == 'min',48,operation == 'min',True,100.00000000000004,N/A
"def apply_for_scriptable_torch(recipe: TransformRecipe, tensor: torch.Tensor, reduction_type: str, axes_dims: List[Tuple[str, int]]) -> torch.Tensor:
    backend = TorchJitBackend
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape_uncached(recipe, backend.shape(tensor), axes_dims=axes_dims)
<mask>:
        tensor = backend.reshape(tensor, init_shapes)
    if axes_reordering is not None:
        tensor = backend.transpose(tensor, axes_reordering)
    if len(reduced_axes) > 0:
        tensor = backend.reduce(tensor, operation=reduction_type, reduced_axes=reduced_axes)
    if len(added_axes) > 0:
        tensor = backend.add_axes(tensor, n_axes=n_axes_w_added, pos2len=added_axes)
    if final_shapes is not None:
        tensor = backend.reshape(tensor, final_shapes)
    return tensor",init_shapes is not None,68,init_shapes is not None,True,100.00000000000004,N/A
"def allow_ops_in_compiled_graph():
<mask>:
        return
    try:
        from torch._dynamo import allow_in_graph
    except ImportError:
        warnings.warn('allow_ops_in_compiled_graph failed to import torch: ensure pytorch >=2.0', ImportWarning)
        return
    from .einops import rearrange, reduce, repeat, einsum
    from .packing import pack, unpack
    allow_in_graph(rearrange)
    allow_in_graph(reduce)
    allow_in_graph(repeat)
    allow_in_graph(einsum)
    allow_in_graph(pack)
    allow_in_graph(unpack)
    global _ops_were_registered_in_torchdynamo
    _ops_were_registered_in_torchdynamo = True","hasattr(torch, '__version__') and torch.__version__[0] < '2'",44,_ops_were_registered_in_torchdynamo,False,1.400064844418452,N/A
"def get_backend(tensor) -> 'AbstractBackend':
    """"""
    Takes a correct backend (e.g. numpy backend if tensor is numpy.ndarray) for a tensor.
    If needed, imports package and creates backend
    """"""
    _type = type(tensor)
    _result = _type2backend.get(_type, None)
<mask>:
        return _result
    for framework_name, backend in list(_loaded_backends.items()):
        if backend.is_appropriate_type(tensor):
            _type2backend[_type] = backend
            return backend
    backend_subclasses = []
    backends = AbstractBackend.__subclasses__()
    while backends:
        backend = backends.pop()
        backends += backend.__subclasses__()
        backend_subclasses.append(backend)
    for BackendSubclass in backend_subclasses:
        if _debug_importing:
            print('Testing for subclass of ', BackendSubclass)
        if BackendSubclass.framework_name not in _loaded_backends:
            if BackendSubclass.framework_name in sys.modules:
                if _debug_importing:
                    print('Imported backend for ', BackendSubclass.framework_name)
                backend = BackendSubclass()
                _loaded_backends[backend.framework_name] = backend
                if backend.is_appropriate_type(tensor):
                    _type2backend[_type] = backend
                    return backend
    raise RuntimeError('Tensor type unknown to einops {}'.format(type(tensor)))",_result is not None,112,_result is not None,True,100.00000000000004,N/A
"def from_numpy(self, x):
    variable = self.torch.from_numpy(x)
<mask>:
        variable.requires_grad = True
    return variable",self.is_float_type(variable),12,self.requires_grad is None,False,10.175282441454787,N/A
"def reduce(self, x, operation, reduced_axes):
<mask>:
        return x.amin(dim=reduced_axes)
    elif operation == 'max':
        return x.amax(dim=reduced_axes)
    elif operation == 'sum':
        return x.sum(dim=reduced_axes)
    elif operation == 'mean':
        return x.mean(dim=reduced_axes)
    elif operation in ('any', 'all', 'prod'):
        for i in list(sorted(reduced_axes))[::-1]:
            x = getattr(x, operation)(dim=i)
        return x
    else:
        raise NotImplementedError('Unknown reduction ', operation)",operation == 'min',48,operation == 'min',True,100.00000000000004,N/A
"def shape(self, x):
<mask>:
        return tuple((UnknownSize() if d is None else int(d) for d in x.shape))
    else:
        static_shape = x.shape.as_list()
        tf_shape = self.tf.shape(x)
        shape = tuple([s or tf_shape[dim] for dim, s in enumerate(static_shape)])
        try:
            hash(shape)
            return shape
        except BaseException:
            return HashableTuple(shape)",self.tf.executing_eagerly(),41,"isinstance(x.shape, tuple)",False,6.413885305524152,N/A
"def from_numpy(self, x):
    variable = self.flow.from_numpy(x)
<mask>:
        variable.requires_grad = True
    return variable",self.is_float_type(variable),12,self.requires_grad is None,False,10.175282441454787,N/A
"def _create_parameters(self, weight_shape, weight_bound, bias_shape, bias_bound):
    self.weight = self.create_parameter(weight_shape, default_initializer=paddle.nn.initializer.Uniform(-weight_bound, weight_bound))
<mask>:
        self.bias = self.create_parameter(bias_shape, default_initializer=paddle.nn.initializer.Uniform(-bias_bound, bias_bound))
    else:
        self.bias = None",bias_shape is not None,21,bias_shape,False,36.78794411714425,N/A
"def _create_rearrange_layers(self, pre_reshape_pattern: Optional[str], pre_reshape_lengths: Optional[Dict], post_reshape_pattern: Optional[str], post_reshape_lengths: Optional[Dict]):
    self.pre_rearrange = None
<mask>:
        self.pre_rearrange = Rearrange(pre_reshape_pattern, **cast(dict, pre_reshape_lengths))
    self.post_rearrange = None
    if post_reshape_pattern is not None:
        self.post_rearrange = Rearrange(post_reshape_pattern, **cast(dict, post_reshape_lengths))",pre_reshape_pattern is not None,32,pre_reshape_pattern is not None,True,100.00000000000004,N/A
"def forward(self, input):
<mask>:
        input = self.pre_rearrange(input)
    result = paddle.einsum(self.einsum_pattern, input, self.weight)
    if self.bias is not None:
        result += self.bias
    if self.post_rearrange is not None:
        result = self.post_rearrange(result)
    return result",self.pre_rearrange is not None,30,self.pre_rearrange is not None,True,100.00000000000004,N/A
"def build(self, input_shape):
    [weight_shape, weight_bound, bias_shape, bias_bound] = self._params
    self.weight = self.add_weight(shape=weight_shape, initializer=tf.random_uniform_initializer(-weight_bound, weight_bound), trainable=True)
<mask>:
        self.bias = self.add_weight(shape=bias_shape, initializer=tf.random_uniform_initializer(-bias_bound, bias_bound), trainable=True)
    else:
        self.bias = None",bias_shape is not None,26,bias_shape,False,36.78794411714425,N/A
"def call(self, inputs):
<mask>:
        inputs = self.pre_rearrange(inputs)
    result = tf.einsum(self.einsum_pattern, inputs, self.weight)
    if self.bias is not None:
        result = result + self.bias
    if self.post_rearrange is not None:
        result = self.post_rearrange(result)
    return result",self.pre_rearrange is not None,32,self.pre_rearrange is not None,True,100.00000000000004,N/A
"def __repr__(self):
    params = repr(self.pattern)
    params += f"", '{self.weight_shape}'""
<mask>:
        params += f"", '{self.bias_shape}'""
    for axis, length in self.axes_lengths.items():
        params += ', {}={}'.format(axis, length)
    return '{}({})'.format(self.__class__.__name__, params)",self.bias_shape is not None,27,self.bias_shape,False,54.88116360940266,N/A
"def _create_parameters(self, weight_shape, weight_bound, bias_shape, bias_bound):
    self.weight = torch.nn.Parameter(torch.zeros(weight_shape).uniform_(-weight_bound, weight_bound), requires_grad=True)
<mask>:
        self.bias = torch.nn.Parameter(torch.zeros(bias_shape).uniform_(-bias_bound, bias_bound), requires_grad=True)
    else:
        self.bias = None",bias_shape is not None,21,"bias_shape != [0, 1]",False,15.619699684601283,N/A
"def forward(self, input):
<mask>:
        input = self.pre_rearrange(input)
    result = torch.einsum(self.einsum_pattern, input, self.weight)
    if self.bias is not None:
        result += self.bias
    if self.post_rearrange is not None:
        result = self.post_rearrange(result)
    return result",self.pre_rearrange is not None,30,self.pre_rearrange is not None,True,100.00000000000004,N/A
"def _create_parameters(self, weight_shape, weight_bound, bias_shape, bias_bound):
    self.weight = flow.nn.Parameter(flow.zeros(weight_shape).uniform_(-weight_bound, weight_bound), requires_grad=True)
<mask>:
        self.bias = flow.nn.Parameter(flow.zeros(bias_shape).uniform_(-bias_bound, bias_bound), requires_grad=True)
    else:
        self.bias = None",bias_shape is not None,21,bias_shape,False,36.78794411714425,N/A
"def forward(self, input):
<mask>:
        input = self.pre_rearrange(input)
    result = flow.einsum(self.einsum_pattern, input, self.weight)
    if self.bias is not None:
        result += self.bias
    if self.post_rearrange is not None:
        result = self.post_rearrange(result)
    return result",self.pre_rearrange is not None,30,self.pre_rearrange is not None,True,100.00000000000004,N/A
"def _create_parameters(self, weight_shape, weight_bound, bias_shape, bias_bound):
    self.weight = self.param('weight', jax.nn.initializers.uniform(weight_bound), weight_shape)
<mask>:
        self.bias = self.param('bias', jax.nn.initializers.uniform(bias_bound), bias_shape)
    else:
        self.bias = None",bias_shape is not None,21,bias_shape,False,36.78794411714425,N/A
"def _create_rearrange_layers(self, pre_reshape_pattern: Optional[str], pre_reshape_lengths: Optional[Dict], post_reshape_pattern: Optional[str], post_reshape_lengths: Optional[Dict]):
    self.pre_rearrange = None
<mask>:
        self.pre_rearrange = Rearrange(pre_reshape_pattern, sizes=cast(dict, pre_reshape_lengths))
    self.post_rearrange = None
    if post_reshape_pattern is not None:
        self.post_rearrange = Rearrange(post_reshape_pattern, sizes=cast(dict, post_reshape_lengths))",pre_reshape_pattern is not None,32,pre_reshape_pattern is not None,True,100.00000000000004,N/A
"def __call__(self, input):
<mask>:
        input = self.pre_rearrange(input)
    result = jnp.einsum(self.einsum_pattern, input, self.weight)
    if self.bias is not None:
        result += self.bias
    if self.post_rearrange is not None:
        result = self.post_rearrange(result)
    return result",self.pre_rearrange is not None,30,self.pre_rearrange is not None,True,100.00000000000004,N/A
"def main():
    _executable, *args = sys.argv
    frameworks = [x for x in args if x != '--pip-install']
    pip_install_is_set = '--pip-install' in args
    framework_name2installation = {'numpy': ['numpy'], 'torch': ['torch --index-url https://download.pytorch.org/whl/cpu'], 'jax': ['jax[cpu]', 'flax'], 'tensorflow': ['tensorflow'], 'cupy': ['cupy'], 'paddle': ['paddlepaddle'], 'oneflow': ['oneflow==0.9.0'], 'pytensor': ['pytensor']}
    usage = f'\n    Usage:   python -m einops.tests.run_tests <frameworks> [--pip-install]\n    Example: python -m einops.tests.run_tests numpy pytorch --pip-install\n\n    Available frameworks: {list(framework_name2installation)}\n    When --pip-install is set, auto-installs requirements with pip.\n     (make sure which pip points to right pip)\n    '
<mask>:
        print(usage)
        return
    else:
        synonyms = {'tf': 'tensorflow', 'pytorch': 'torch', 'paddlepaddle': 'paddle'}
        frameworks = [synonyms.get(f, f) for f in frameworks]
        wrong_frameworks = [f for f in frameworks if f not in framework_name2installation]
        if wrong_frameworks:
            print(usage)
            raise RuntimeError(f'Unrecognized frameworks: {wrong_frameworks}')
    if pip_install_is_set:
        print('Install testing infra')
        other_dependencies = ['pytest']
        assert 0 == run('pip install {} --progress-bar off -q'.format(' '.join(other_dependencies)))
        for framework in frameworks:
            print(f'Installing {framework}')
            pip_instructions = framework_name2installation[framework]
            assert 0 == run('pip install {} --progress-bar off -q'.format(' '.join(pip_instructions)))
    from einops.tests import unparse_backends
    envvar_name, envvar_value = unparse_backends(backend_names=frameworks)
    return_code = run('python -m pytest .', **{envvar_name: envvar_value})
    assert return_code == 0",len(frameworks) == 0,175,not frameworks,False,4.104249931194939,N/A
"def test_reduce_imperative():
    for backend in collect_test_backends(symbolic=False, layers=True):
        print('Test layer for ', backend.framework_name)
        for reduction in REDUCTIONS:
            for pattern, axes_lengths, input_shape, wrong_shapes in reduction_patterns:
                print(backend, reduction, pattern, axes_lengths, input_shape, wrong_shapes)
                x = numpy.arange(1, 1 + numpy.prod(input_shape), dtype='float32').reshape(input_shape)
                x /= x.mean()
                result_numpy = reduce(x, pattern, reduction, **axes_lengths)
                layer = backend.layers().Reduce(pattern, reduction, **axes_lengths)
                for shape in wrong_shapes:
                    try:
                        layer(backend.from_numpy(numpy.zeros(shape, dtype='float32')))
                    except BaseException:
                        pass
                    else:
                        raise AssertionError('Failure expected')
                layer2 = pickle.loads(pickle.dumps(layer))
                result1 = backend.to_numpy(layer(backend.from_numpy(x)))
                result2 = backend.to_numpy(layer2(backend.from_numpy(x)))
                assert numpy.allclose(result_numpy, result1)
                assert numpy.allclose(result1, result2)
                just_sum = backend.layers().Reduce('...->', reduction='sum')
                variable = backend.from_numpy(x)
                result = just_sum(layer(variable))
                result.backward()
                grad = backend.to_numpy(variable.grad)
<mask>:
                    assert numpy.allclose(grad, 1)
                if reduction == 'mean':
                    assert numpy.allclose(grad, grad.min())
                if reduction in ['max', 'min']:
                    assert numpy.all(numpy.in1d(grad, [0, 1]))
                    assert numpy.sum(grad) > 0.5",reduction == 'sum',117,"reduction in ['max', 'min']",False,6.567274736060395,N/A
"def create_torch_model(use_reduce=False, add_scripted_layer=False):
<mask>:
        pytest.skip()
    else:
        from torch.nn import Sequential, Conv2d, MaxPool2d, Linear, ReLU
        from einops.layers.torch import Rearrange, Reduce, EinMix
        import torch.jit
        return Sequential(Conv2d(3, 6, kernel_size=(5, 5)), Reduce('b c (h h2) (w w2) -> b c h w', 'max', h2=2, w2=2) if use_reduce else MaxPool2d(kernel_size=2), Conv2d(6, 16, kernel_size=(5, 5)), Reduce('b c (h h2) (w w2) -> b c h w', 'max', h2=2, w2=2), torch.jit.script(Rearrange('b c h w -> b (c h w)')) if add_scripted_layer else Rearrange('b c h w -> b (c h w)'), Linear(16 * 5 * 5, 120), ReLU(), Linear(120, 84), ReLU(), EinMix('b c1 -> (b c2)', weight_shape='c1 c2', bias_shape='c2', c1=84, c2=84), EinMix('(b c2) -> b c3', weight_shape='c2 c3', bias_shape='c3', c2=84, c3=84), Linear(84, 10))",not is_backend_tested('torch'),116,torch.cuda.is_available(),False,11.339582221952005,N/A
"def test_torch_layer():
<mask>:
        pytest.skip()
    else:
        import torch
        import torch.jit
        model1 = create_torch_model(use_reduce=True)
        model2 = create_torch_model(use_reduce=False)
        input = torch.randn([10, 3, 32, 32])
        assert not torch.allclose(model1(input), model2(input))
        model2.load_state_dict(pickle.loads(pickle.dumps(model1.state_dict())))
        assert torch.allclose(model1(input), model2(input))
        model3 = torch.jit.trace(model2, example_inputs=input)
        torch.testing.assert_close(model1(input), model3(input), atol=0.001, rtol=0.001)
        torch.testing.assert_close(model1(input + 1), model3(input + 1), atol=0.001, rtol=0.001)
        model4 = torch.jit.trace(model2, example_inputs=input)
        torch.testing.assert_close(model1(input), model4(input), atol=0.001, rtol=0.001)
        torch.testing.assert_close(model1(input + 1), model4(input + 1), atol=0.001, rtol=0.001)",not is_backend_tested('torch'),61,torch.cuda.is_available(),False,11.339582221952005,N/A
"def test_torch_layers_scripting():
<mask>:
        pytest.skip()
    else:
        import torch
        for script_layer in [False, True]:
            model1 = create_torch_model(use_reduce=True, add_scripted_layer=script_layer)
            model2 = torch.jit.script(model1)
            input = torch.randn([10, 3, 32, 32])
            torch.testing.assert_close(model1(input), model2(input), atol=0.001, rtol=0.001)",not is_backend_tested('torch'),29,torch.cuda.is_available(),False,11.339582221952005,N/A
"def test_keras_layer():
<mask>:
        pytest.skip()
    else:
        import tensorflow as tf
        if tf.__version__ < '2.16.':
            pytest.skip()
        from tensorflow.keras.models import Sequential
        from tensorflow.keras.layers import Conv2D as Conv2d, Dense as Linear, ReLU
        from einops.layers.keras import Rearrange, Reduce, EinMix, keras_custom_objects

        def create_keras_model():
            return Sequential([Conv2d(6, kernel_size=5, input_shape=[32, 32, 3]), Reduce('b c (h h2) (w w2) -> b c h w', 'max', h2=2, w2=2), Conv2d(16, kernel_size=5), Reduce('b c (h h2) (w w2) -> b c h w', 'max', h2=2, w2=2), Rearrange('b c h w -> b (c h w)'), Linear(120), ReLU(), Linear(84), ReLU(), EinMix('b c1 -> (b c2)', weight_shape='c1 c2', bias_shape='c2', c1=84, c2=84), EinMix('(b c2) -> b c3', weight_shape='c2 c3', bias_shape='c3', c2=84, c3=84), Linear(10)])
        model1 = create_keras_model()
        model2 = create_keras_model()
        input = numpy.random.normal(size=[10, 32, 32, 3]).astype('float32')
        assert not numpy.allclose(model1.predict_on_batch(input), model2.predict_on_batch(input))
        tmp_model_filename = '/tmp/einops_tf_model.h5'
        print('temp_path_keras1', tmp_model_filename)
        tf.keras.models.save_model(model1, tmp_model_filename)
        model3 = tf.keras.models.load_model(tmp_model_filename, custom_objects=keras_custom_objects)
        numpy.testing.assert_allclose(model1.predict_on_batch(input), model3.predict_on_batch(input))
        weight_filename = '/tmp/einops_tf_model.weights.h5'
        model4 = tf.keras.models.model_from_json(model1.to_json(), custom_objects=keras_custom_objects)
        model1.save_weights(weight_filename)
        model4.load_weights(weight_filename)
        model2.load_weights(weight_filename)
        numpy.testing.assert_allclose(model1.predict_on_batch(input), model2.predict_on_batch(input))
        numpy.testing.assert_allclose(model1.predict_on_batch(input), model4.predict_on_batch(input))",not is_backend_tested('tensorflow'),150,tf.__version__ < '3.0.',False,4.456882760699063,N/A
"def unparse_backends(backend_names: List[str]) -> Tuple[str, str]:
    _known_backends = find_names_of_all_frameworks()
    for backend_name in backend_names:
<mask>:
            raise RuntimeError(f'Unknown framework: {backend_name}')
    return (ENVVAR_NAME, ','.join(backend_names))",backend_name not in _known_backends,21,backend_name not in _known_backends,True,100.00000000000004,N/A
"@lru_cache(maxsize=1)
def parse_backends_to_test() -> List[str]:
<mask>:
        raise RuntimeError(f'Testing frameworks were not specified, env var {ENVVAR_NAME} not set')
    parsed_backends = os.environ[ENVVAR_NAME].split(',')
    _known_backends = find_names_of_all_frameworks()
    for backend_name in parsed_backends:
        if backend_name not in _known_backends:
            raise RuntimeError(f'Unknown framework: {backend_name}')
    return parsed_backends",ENVVAR_NAME not in os.environ,38,ENVVAR_NAME not in os.environ,True,100.00000000000004,N/A
"def is_backend_tested(backend: str) -> bool:
    """"""Used to skip test if corresponding backend is not tested""""""
<mask>:
        raise RuntimeError(f'Unknown framework {backend}')
    return backend in parse_backends_to_test()",backend not in find_names_of_all_frameworks(),24,"backend not in ['tested', 'test']",False,9.710288466562174,N/A
"def collect_test_backends(symbolic=False, layers=False) -> List[_backends.AbstractBackend]:
    """"""
    :param symbolic: symbolic or imperative frameworks?
    :param layers: layers or operations?
    :return: list of backends satisfying set conditions
    """"""
<mask>:
        if not layers:
            backend_types = [_backends.NumpyBackend, _backends.JaxBackend, _backends.TorchBackend, _backends.TensorflowBackend, _backends.OneFlowBackend, _backends.PaddleBackend, _backends.CupyBackend]
        else:
            backend_types = [_backends.TorchBackend, _backends.OneFlowBackend, _backends.PaddleBackend]
    elif not layers:
        backend_types = [_backends.PyTensorBackend]
    else:
        backend_types = [_backends.TFKerasBackend]
    backend_names_to_test = parse_backends_to_test()
    result = []
    for backend_type in backend_types:
        if backend_type.framework_name not in backend_names_to_test:
            continue
        try:
            result.append(backend_type())
        except ImportError:
            warnings.warn('backend could not be initialized for tests: {}'.format(backend_type))
    return result",not symbolic,84,symbolic,False,36.78794411714425,N/A
"def unpack_and_pack_against_numpy(x, ps, pattern: str):
    capturer_backend = CaptureException()
    capturer_numpy = CaptureException()
    with capturer_backend:
        unpacked = unpack(x, ps, pattern)
        packed, ps2 = pack(unpacked, pattern=pattern)
    with capturer_numpy:
        x_np = asnumpy(x)
        unpacked_np = unpack(x_np, ps, pattern)
        packed_np, ps3 = pack(unpacked_np, pattern=pattern)
    assert type(capturer_numpy.exception) == type(capturer_backend.exception)
<mask>:
        return
    else:
        assert np.allclose(asnumpy(packed), asnumpy(x))
        assert np.allclose(asnumpy(packed_np), asnumpy(x))
        assert len(unpacked) == len(unpacked_np)
        for a, b in zip(unpacked, unpacked_np):
            assert np.allclose(asnumpy(a), b)",capturer_numpy.exception is not None,64,len(unpacked) == 0 or len(unpacked_np) == 0,False,2.2869567780619007,N/A
"def test_pack_unpack_against_numpy():
    for backend in collect_test_backends(symbolic=False, layers=False):
        print(f'test packing against numpy for {backend.framework_name}')
        check_zero_len = True
        for case in cases:
            unpack_and_pack = unpack_and_pack_against_numpy
            shape = case.shape
            pattern = case.pattern
            x = np.random.random(shape)
            x = backend.from_numpy(x)
            unpack_and_pack(x, [[2], [1], [2]], pattern)
            with pytest.raises(BaseException):
                unpack(x, [[2], [1], [1]], pattern)
            with pytest.raises(BaseException):
                unpack(x, [[4], [1], [1]], pattern)
            unpack_and_pack(x, [[2], [1], [-1]], pattern)
            unpack_and_pack(x, [[2], [-1], [2]], pattern)
            unpack_and_pack(x, [[-1], [1], [2]], pattern)
            with pytest.raises(BaseException):
                unpack(x, [[2], [4], [-1]], pattern)
            with pytest.raises(BaseException):
                unpack(x, [[-1], [1], [5]], pattern)
            unpack_and_pack(x, [[1, 2], [1, 1], [-1, 1]], pattern)
            unpack_and_pack(x, [[1, 2], [1, -1], [1, 1]], pattern)
            unpack_and_pack(x, [[2, -1], [1, 2], [1, 1]], pattern)
            with pytest.raises(BaseException):
                unpack(x, [[-1, 2], [1], [5]], pattern)
            with pytest.raises(BaseException):
                unpack(x, [[2, 2], [2], [5, -1]], pattern)
            with pytest.raises(BaseException):
                unpack(x, [[2, 1], [1], [3, -1]], pattern)
            with pytest.raises(BaseException):
                unpack(x, [[2, 1], [3, -1], [1]], pattern)
            with pytest.raises(BaseException):
                unpack(x, [[3, -1], [2, 1], [1]], pattern)
<mask>:
                unpack_and_pack(x, [[2], [3], [-1]], pattern)
                unpack_and_pack(x, [[0], [5], [-1]], pattern)
                unpack_and_pack(x, [[0], [-1], [5]], pattern)
                unpack_and_pack(x, [[-1], [5], [0]], pattern)
                unpack_and_pack(x, [[2, -1], [1, 5]], pattern)",check_zero_len,178,len(x) == 2,False,6.567274736060395,N/A
"def test_pack_unpack_array_api():
    from einops import array_api as AA
    import numpy as xp
<mask>:
        pytest.skip()
    for case in cases:
        shape = case.shape
        pattern = case.pattern
        x_np = np.random.random(shape)
        x_xp = xp.from_dlpack(x_np)
        for ps in [[[2], [1], [2]], [[1], [1], [-1]], [[1], [1], [-1, 3]], [[2, 1], [1, 1, 1], [-1]]]:
            x_np_split = unpack(x_np, ps, pattern)
            x_xp_split = AA.unpack(x_xp, ps, pattern)
            for a, b in zip(x_np_split, x_xp_split):
                assert np.allclose(a, AA.asnumpy(b + 0))
            x_agg_np, ps1 = pack(x_np_split, pattern)
            x_agg_xp, ps2 = AA.pack(x_xp_split, pattern)
            assert ps1 == ps2
            assert np.allclose(x_agg_np, AA.asnumpy(x_agg_xp))
        for ps in [[[2, 3]], [[1], [5]], [[1], [5], [-1]], [[1], [2, 3]], [[1], [5], [-1, 2]]]:
            with pytest.raises(BaseException):
                unpack(x_np, ps, pattern)",xp.__version__ < '2.0.0',109,len(cases) == 0,False,0.0,N/A
"def test_layer():
    for backend in collect_test_backends(layers=True, symbolic=False):
<mask>:
            layer_type = backend.layers().EinMix
            for args, in_shape, out_shape in test_layer_cases:
                layer = args(layer_type)
                print('Running', layer.einsum_pattern, 'for', backend.framework_name)
                input = np.random.uniform(size=in_shape).astype('float32')
                input_framework = backend.from_numpy(input)
                output_framework = layer(input_framework)
                output = backend.to_numpy(output_framework)
                assert output.shape == out_shape","backend.framework_name in ['tensorflow', 'torch', 'oneflow', 'paddle']",40,layers,False,0.0,N/A
"def test_functional():
    backends = filter(lambda x: x.framework_name in valid_backends_functional, collect_test_backends())
    for backend in backends:
        for einops_pattern, true_pattern, in_shapes, out_shape in test_functional_cases:
            print(f""Running '{einops_pattern}' for {backend.framework_name}"")
            predicted_pattern = _compactify_pattern_for_einsum(einops_pattern)
            assert predicted_pattern == true_pattern
            rstate = np.random.RandomState(0)
            in_arrays = [rstate.uniform(size=shape).astype('float32') for shape in in_shapes]
            in_arrays_framework = [backend.from_numpy(array) for array in in_arrays]
            for do_manual_call in [True, False]:
<mask>:
                    out_array = backend.einsum(predicted_pattern, *in_arrays_framework)
                else:
                    out_array = einsum(*in_arrays_framework, einops_pattern)
                if tuple(out_array.shape) != out_shape:
                    raise ValueError(f'Expected output shape {out_shape} but got {out_array.shape}')
                true_out_array = np.einsum(true_pattern, *in_arrays)
                predicted_out_array = backend.to_numpy(out_array)
                np.testing.assert_array_almost_equal(predicted_out_array, true_out_array, decimal=5)",do_manual_call,86,do_manual_call,True,100.00000000000004,N/A
"def test_functional_symbolic():
    backends = filter(lambda x: x.framework_name in valid_backends_functional, collect_test_backends(symbolic=True, layers=False))
    for backend in backends:
        for einops_pattern, true_pattern, in_shapes, out_shape in test_functional_cases:
            print(f""Running '{einops_pattern}' for symbolic {backend.framework_name}"")
            predicted_pattern = _compactify_pattern_for_einsum(einops_pattern)
            assert predicted_pattern == true_pattern
            rstate = np.random.RandomState(0)
            in_syms = [backend.create_symbol(in_shape) for in_shape in in_shapes]
            in_data = [rstate.uniform(size=in_shape).astype('float32') for in_shape in in_shapes]
            expected_out_data = np.einsum(true_pattern, *in_data)
            for do_manual_call in [True, False]:
<mask>:
                    predicted_out_symbol = backend.einsum(predicted_pattern, *in_syms)
                else:
                    predicted_out_symbol = einsum(*in_syms, einops_pattern)
                predicted_out_data = backend.eval_symbol(predicted_out_symbol, list(zip(in_syms, in_data)))
                if predicted_out_data.shape != out_shape:
                    raise ValueError(f'Expected output shape {out_shape} but got {predicted_out_data.shape}')
                np.testing.assert_array_almost_equal(predicted_out_data, expected_out_data, decimal=5)",do_manual_call,90,do_manual_call,True,100.00000000000004,N/A
"def check_op_against_numpy(backend, numpy_input, pattern, axes_lengths, reduction='rearrange', is_symbolic=False):
    """"""
    Helper to test result of operation (rearrange or transpose) against numpy
    if reduction == 'rearrange', rearrange op is tested, otherwise reduce
    """"""

    def operation(x):
<mask>:
            return rearrange(x, pattern, **axes_lengths)
        else:
            return reduce(x, pattern, reduction, **axes_lengths)
    numpy_result = operation(numpy_input)
    check_equal = numpy.array_equal
    p_none_dimension = 0.5
    if is_symbolic:
        symbol_shape = [d if numpy.random.random() >= p_none_dimension else None for d in numpy_input.shape]
        symbol = backend.create_symbol(shape=symbol_shape)
        result_symbol = operation(symbol)
        backend_result = backend.eval_symbol(result_symbol, [(symbol, numpy_input)])
    else:
        backend_result = operation(backend.from_numpy(numpy_input))
        backend_result = backend.to_numpy(backend_result)
    check_equal(numpy_result, backend_result)",reduction == 'rearrange',87,reduction == 'rearrange',True,100.00000000000004,N/A
"def test_rearrange_array_api():
    import numpy as xp
    from einops import array_api as AA
<mask>:
        pytest.skip()
    x = numpy.arange(2 * 3 * 4 * 5 * 6).reshape([2, 3, 4, 5, 6])
    for pattern in identity_patterns + list(itertools.chain(*equivalent_rearrange_patterns)):
        expected = rearrange(x, pattern)
        result = AA.rearrange(xp.from_dlpack(x), pattern)
        assert numpy.array_equal(AA.asnumpy(result + 0), expected)",xp.__version__ < '2.0.0',48,not HAS_DLPACK,False,4.576506607182439,N/A
"def test_reduce_array_api():
    import numpy as xp
    from einops import array_api as AA
<mask>:
        pytest.skip()
    x = numpy.arange(2 * 3 * 4 * 5 * 6).reshape([2, 3, 4, 5, 6])
    for pattern in itertools.chain(*equivalent_reduction_patterns):
        for reduction in ['min', 'max', 'sum']:
            expected = reduce(x, pattern, reduction=reduction)
            result = AA.reduce(xp.from_dlpack(x), pattern, reduction=reduction)
            assert numpy.array_equal(AA.asnumpy(np.asarray(result + 0)), expected)",xp.__version__ < '2.0.0',54,len(equivalent_reduction_patterns) == 0,False,4.456882760699063,N/A
"def test_reduction_imperatives():
    for backend in imp_op_backends:
        print('Reduction tests for ', backend.framework_name)
        for reduction in REDUCTIONS:
            input = numpy.arange(2 * 3 * 4 * 5 * 6, dtype='int64').reshape([2, 3, 4, 5, 6])
<mask>:
                input = input / input.astype('float64').mean()
            test_cases = [['a b c d e -> ', {}, getattr(input, reduction)()], ['a ... -> ', {}, getattr(input, reduction)()], ['(a1 a2) ... (e1 e2) -> ', dict(a1=1, e2=2), getattr(input, reduction)()], ['a b c d e -> (e c) a', {}, getattr(input, reduction)(axis=(1, 3)).transpose(2, 1, 0).reshape([-1, 2])], ['a ... c d e -> (e c) a', {}, getattr(input, reduction)(axis=(1, 3)).transpose(2, 1, 0).reshape([-1, 2])], ['a b c d e ... -> (e c) a', {}, getattr(input, reduction)(axis=(1, 3)).transpose(2, 1, 0).reshape([-1, 2])], ['a b c d e -> (e c a)', {}, getattr(input, reduction)(axis=(1, 3)).transpose(2, 1, 0).reshape([-1])], ['(a a2) ... -> (a2 a) ...', dict(a2=1), input]]
            for pattern, axes_lengths, expected_result in test_cases:
                result = reduce(backend.from_numpy(input.copy()), pattern, reduction=reduction, **axes_lengths)
                result = backend.to_numpy(result)
                assert numpy.allclose(result, expected_result), f'Failed at {pattern}'","reduction in ['mean', 'prod']",161,"not isinstance(input, numpy.ndarray)",False,4.767707020457095,N/A
"def test_reduction_symbolic():
    for backend in sym_op_backends:
        print('Reduction tests for ', backend.framework_name)
        for reduction in REDUCTIONS:
            input = numpy.arange(2 * 3 * 4 * 5 * 6, dtype='int64').reshape([2, 3, 4, 5, 6])
            input = input / input.astype('float64').mean()
            test_cases = [['a b c d e -> ', {}, getattr(input, reduction)()], ['a ... -> ', {}, getattr(input, reduction)()], ['(a a2) ... (e e2) -> ', dict(a2=1, e2=1), getattr(input, reduction)()], ['a b c d e -> (e c) a', {}, getattr(input, reduction)(axis=(1, 3)).transpose(2, 1, 0).reshape([-1, 2])], ['a ... c d e -> (e c) a', {}, getattr(input, reduction)(axis=(1, 3)).transpose(2, 1, 0).reshape([-1, 2])], ['a b c d e ... -> (e c) a', {}, getattr(input, reduction)(axis=(1, 3)).transpose(2, 1, 0).reshape([-1, 2])], ['a b c d e -> (e c a)', {}, getattr(input, reduction)(axis=(1, 3)).transpose(2, 1, 0).reshape([-1])], ['(a a2) ... -> (a2 a) ...', dict(a2=1), input]]
            for pattern, axes_lengths, expected_numpy_result in test_cases:
                shapes = [input.shape, [None for _ in input.shape]]
                for shape in shapes:
                    sym = backend.create_symbol(shape)
                    result_sym = reduce(sym, pattern, reduction=reduction, **axes_lengths)
                    result = backend.eval_symbol(result_sym, [(sym, input)])
                    assert numpy.allclose(result, expected_numpy_result)
<mask>:
                    shape = []
                    _axes_lengths = {**axes_lengths}
                    for axis, length in zip('abcde', input.shape):
                        if axis in pattern:
                            shape.append(None)
                            _axes_lengths[axis] = length
                        else:
                            shape.append(length)
                    sym = backend.create_symbol(shape)
                    result_sym = reduce(sym, pattern, reduction=reduction, **_axes_lengths)
                    result = backend.eval_symbol(result_sym, [(sym, input)])
                    assert numpy.allclose(result, expected_numpy_result)",True,214,axes_lengths is not None,False,0.0,N/A
"def test_backends_installed():
    """"""
    This test will fail if some of backends are not installed or can't be imported
    Other tests will just work and only test installed backends.
    """"""
    from . import parse_backends_to_test
    backends_to_test = parse_backends_to_test()
    errors = []
    for backend_type in AbstractBackend.__subclasses__():
<mask>:
            continue
        try:
            backend_type()
        except Exception as e:
            errors.append((backend_type.framework_name, e))
    assert len(errors) == 0, errors",backend_type.framework_name not in backends_to_test,58,backend_type.framework_name not in backends_to_test,True,100.00000000000004,N/A
"@pytest.mark.parametrize('backend', _SYMBOLIC_BACKENDS)
def test_parse_shape_symbolic_ellipsis(backend):
    for static_shape, shape, pattern, expected in [([10, 20], [None, None], '...', dict()), ([10], [None], '... a', dict(a=10)), ([10, 20], [None, None], '... a', dict(a=20)), ([10, 20, 30], [None, None, None], '... a', dict(a=30)), ([10, 20, 30, 40], [None, None, None, None], '... a', dict(a=40)), ([10], [None], 'a ...', dict(a=10)), ([10, 20], [None, None], 'a ...', dict(a=10)), ([10, 20, 30], [None, None, None], 'a ...', dict(a=10)), ([10, 20, 30, 40], [None, None, None, None], 'a ...', dict(a=10)), ([10, 20, 30, 40], [None, None, None, None], ' a ... b', dict(a=10, b=40)), ([10, 40], [None, None], ' a ... b ', dict(a=10, b=40))]:
        input_symbol = backend.create_symbol(shape)
        shape_placeholder = parse_shape(input_symbol, pattern)
        out_shape = {}
        for name, symbol in shape_placeholder.items():
<mask>:
                out_shape[name] = symbol
            else:
                out_shape[name] = backend.eval_symbol(symbol, [(input_symbol, numpy.zeros(static_shape))])
        assert out_shape == expected","isinstance(symbol, int)",134,"isinstance(symbol, str)",False,53.7284965911771,N/A
"def test_torch_compile():
    """"""
    Test ensures that allow_ops_in_compiled_graph allows compiling in a single graph
    Additionally we ensure that after compilation cache works properly
     (by changing shapes and patterns)
    We additionally check that pack/unpack still can be handled
     despite variable number of inputs/outputs
    """"""
<mask>:
        pytest.skip()
    import torch
    from torch import nn
    from einops import repeat, reduce, pack, unpack, einsum
    from einops._torch_specific import allow_ops_in_compiled_graph
    allow_ops_in_compiled_graph()

    class TorchModuleWithOperations(nn.Module):

        def __init__(self) -> None:
            super().__init__()

        def forward(self, x_abc, suffix=''):
            a, b, c = x_abc.shape

            def suf(pattern):
                parts = pattern.split()
                return ' '.join([p if p[-1] not in 'acd' else p + suffix for p in parts])
            x_abcd = repeat(x_abc, suf('a b c -> a b c 4'))
            x_abc = reduce(x_abcd, suf('a b c d -> a b c'), 'min')
            x_abdc, ps = pack([x_abc] * (2 + len(suffix)), suf('a b * c'))
            x_array = unpack(rearrange(x_abdc, suf('a b d c -> (a b ) 1 c d')), ps, 'ab one1 c *')
            x1 = x_array[0] + len(x_array)
            x1 = rearrange(x1, suf('(a b ) 1 c -> a b c'), b=b)
            addition = einsum(x_abc, x_abcd, suf('a b c , a b c d -> d'))[0]
            return x1 + addition
    original = TorchModuleWithOperations()
    compiled = torch.compile(original, fullgraph=True, backend='aot_eager')
    for size in [10, 20, 40]:
        x = torch.rand([size, size + 1, size + 2])
        for suffix in ['', 'suf1', 'other_suffix']:
            result1 = compiled(x, suffix)
            result2 = original(x, suffix)
            assert torch.allclose(result1, result2)",not is_backend_tested('torch'),230,torch.cuda.is_available(),False,11.339582221952005,N/A
"def tensor_train_example_numpy():
    x = numpy.ones([3, 4, 5, 6])
    rank = 4
<mask>:
        return
    Gs = [numpy.ones([d, d, rank, rank]) for d in x.shape]
    Gs[0] = Gs[0][:, :, :1, :]
    Gs[-1] = Gs[-1][:, :, :, :1]
    y = x.reshape((1,) + x.shape)
    for G in Gs:
        y = numpy.einsum('i j a b, a i ...  -> b ... j', G, y)
    y1 = y.reshape(-1)
    y = x.reshape(-1)
    for G in Gs:
        i, j, alpha, beta = G.shape
        y = rearrange(y, '(i rest alpha) -> rest (alpha i)', alpha=alpha, i=i)
        y = y @ rearrange(G, 'i j alpha beta -> (alpha i) (j beta)')
        y = rearrange(y, 'rest (beta j) -> (beta rest j)', beta=beta, j=j)
    y2 = y
    assert numpy.allclose(y1, y2)
    y = x
    for G in Gs:
        i, j, alpha, beta = G.shape
        y = rearrange(y, 'i ... (j alpha) -> ... j (alpha i)', alpha=alpha, i=i)
        y = y @ rearrange(G, 'i j alpha beta -> (alpha i) (j beta)')
    y3 = y.reshape(-1)
    assert numpy.allclose(y1, y3)",numpy.__version__ < '1.15.0',166,rank != 2,False,0.0,N/A
"def run_migrations_online():
    """"""Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """"""

    def process_revision_directives(context, revision, directives):
<mask>:
            script = directives[0]
            if script.upgrade_ops.is_empty():
                directives[:] = []
                logger.info('No changes in schema detected.')
    conf_args = current_app.extensions['migrate'].configure_args
    if conf_args.get('process_revision_directives') is None:
        conf_args['process_revision_directives'] = process_revision_directives
    connectable = current_app.extensions['migrate'].db.get_engine()
    with connectable.connect() as connection:
        context.configure(connection=connection, target_metadata=target_metadata, **conf_args)
        with context.begin_transaction():
            context.run_migrations()","getattr(config.cmd_opts, 'autogenerate', False)",65,revision == 'online',False,0.0,N/A
"def get_template_directory(self):
<mask>:
        return self.template_directory
    package_dir = os.path.abspath(os.path.dirname(__file__))
    return os.path.join(package_dir, 'templates')",self.template_directory,11,self.template_directory,True,100.00000000000004,N/A
"def __init__(self, app=None, db=None, directory='migrations', command='db', compare_type=True, render_as_batch=True, **kwargs):
    self.configure_callbacks = []
    self.db = db
    self.command = command
    self.directory = str(directory)
    self.alembic_ctx_kwargs = kwargs
    self.alembic_ctx_kwargs['compare_type'] = compare_type
    self.alembic_ctx_kwargs['render_as_batch'] = render_as_batch
<mask>:
        self.init_app(app, db, directory)",app is not None and db is not None,34,app is not None,False,28.650479686019022,N/A
"def init_app(self, app, db=None, directory=None, command=None, compare_type=None, render_as_batch=None, **kwargs):
    self.db = db or self.db
    self.command = command or self.command
    self.directory = str(directory or self.directory)
    self.alembic_ctx_kwargs.update(kwargs)
<mask>:
        self.alembic_ctx_kwargs['compare_type'] = compare_type
    if render_as_batch is not None:
        self.alembic_ctx_kwargs['render_as_batch'] = render_as_batch
    if not hasattr(app, 'extensions'):
        app.extensions = {}
    app.extensions['migrate'] = _MigrateConfig(self, self.db, **self.alembic_ctx_kwargs)
    from flask_migrate.cli import db as db_cli_group
    app.cli.add_command(db_cli_group, name=self.command)",compare_type is not None,57,compare_type is not None,True,100.00000000000004,N/A
"def get_config(self, directory=None, x_arg=None, opts=None):
<mask>:
        directory = self.directory
    directory = str(directory)
    config = Config(os.path.join(directory, 'alembic.ini'))
    config.set_main_option('script_location', directory)
    if config.cmd_opts is None:
        config.cmd_opts = argparse.Namespace()
    for opt in opts or []:
        setattr(config.cmd_opts, opt, True)
    if not hasattr(config.cmd_opts, 'x'):
        setattr(config.cmd_opts, 'x', [])
        for x in getattr(g, 'x_arg', []):
            config.cmd_opts.x.append(x)
        if x_arg is not None:
            if isinstance(x_arg, list) or isinstance(x_arg, tuple):
                for x in x_arg:
                    config.cmd_opts.x.append(x)
            else:
                config.cmd_opts.x.append(x_arg)
    return self.call_configure_callbacks(config)",directory is None,68,directory is None,True,100.00000000000004,N/A
"@catch_errors
def init(directory=None, multidb=False, template=None, package=False):
    """"""Creates a new migration repository""""""
<mask>:
        directory = current_app.extensions['migrate'].directory
    template_directory = None
    if template is not None and ('/' in template or '\\' in template):
        template_directory, template = os.path.split(template)
    config = Config(template_directory=template_directory)
    config.set_main_option('script_location', directory)
    config.config_file_name = os.path.join(directory, 'alembic.ini')
    config = current_app.extensions['migrate'].migrate.call_configure_callbacks(config)
    if multidb and template is None:
        template = 'flask-multidb'
    elif template is None:
        template = 'flask'
    command.init(config, directory, template=template, package=package)",directory is None,67,directory is None,True,100.00000000000004,N/A
"def run_migrations_online():
    """"""Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """"""

    def process_revision_directives(context, revision, directives):
<mask>:
            script = directives[0]
            if script.upgrade_ops.is_empty():
                directives[:] = []
                logger.info('No changes in schema detected.')
    conf_args = current_app.extensions['migrate'].configure_args
    if conf_args.get('process_revision_directives') is None:
        conf_args['process_revision_directives'] = process_revision_directives
    connectable = get_engine()
    with connectable.connect() as connection:
        context.configure(connection=connection, target_metadata=get_metadata(), **conf_args)
        with context.begin_transaction():
            context.run_migrations()","getattr(config.cmd_opts, 'autogenerate', False)",65,revision == 'online',False,0.0,N/A
"def get_metadata(bind):
    """"""Return the metadata for a bind.""""""
<mask>:
        bind = None
    if hasattr(target_db, 'metadatas'):
        return target_db.metadatas[bind]
    m = MetaData()
    for t in target_db.metadata.tables.values():
        if t.info.get('bind_key') == bind:
            t.tometadata(m)
    return m",bind == '',31,bind == 'auto',False,59.460355750136046,N/A
"def do_run_migrations(_, engines):

    def process_revision_directives(context, revision, directives):
<mask>:
            script = directives[0]
            if len(script.upgrade_ops_list) >= len(bind_names) + 1:
                empty = True
                for upgrade_ops in script.upgrade_ops_list:
                    if not upgrade_ops.is_empty():
                        empty = False
                if empty:
                    directives[:] = []
                    logger.info('No changes in schema detected.')
    conf_args = current_app.extensions['migrate'].configure_args
    if conf_args.get('process_revision_directives') is None:
        conf_args['process_revision_directives'] = process_revision_directives
    for name, rec in engines.items():
        rec['sync_connection'] = conn = rec['connection']._sync_connection()
        if USE_TWOPHASE:
            rec['transaction'] = conn.begin_twophase()
        else:
            rec['transaction'] = conn.begin()
    try:
        for name, rec in engines.items():
            logger.info('Migrating database %s' % (name or '<default>'))
            context.configure(connection=rec['sync_connection'], upgrade_token='%s_upgrades' % name, downgrade_token='%s_downgrades' % name, target_metadata=get_metadata(name), **conf_args)
            context.run_migrations(engine_name=name)
        if USE_TWOPHASE:
            for rec in engines.values():
                rec['transaction'].prepare()
        for rec in engines.values():
            rec['transaction'].commit()
    except:
        for rec in engines.values():
            rec['transaction'].rollback()
        raise
    finally:
        for rec in engines.values():
            rec['sync_connection'].close()","getattr(config.cmd_opts, 'autogenerate', False)",117,revision == 'all',False,0.0,N/A
"def do_run_migrations(connection):

    def process_revision_directives(context, revision, directives):
<mask>:
            script = directives[0]
            if script.upgrade_ops.is_empty():
                directives[:] = []
                logger.info('No changes in schema detected.')
    conf_args = current_app.extensions['migrate'].configure_args
    if conf_args.get('process_revision_directives') is None:
        conf_args['process_revision_directives'] = process_revision_directives
    context.configure(connection=connection, target_metadata=get_metadata(), **conf_args)
    with context.begin_transaction():
        context.run_migrations()","getattr(config.cmd_opts, 'autogenerate', False)",36,revision == '1',False,0.0,N/A
"def run_migrations_online():
    """"""Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """"""

    def process_revision_directives(context, revision, directives):
<mask>:
            script = directives[0]
            if len(script.upgrade_ops_list) >= len(bind_names) + 1:
                empty = True
                for upgrade_ops in script.upgrade_ops_list:
                    if not upgrade_ops.is_empty():
                        empty = False
                if empty:
                    directives[:] = []
                    logger.info('No changes in schema detected.')
    conf_args = current_app.extensions['migrate'].configure_args
    if conf_args.get('process_revision_directives') is None:
        conf_args['process_revision_directives'] = process_revision_directives
    engines = {'': {'engine': get_engine()}}
    for name in bind_names:
        engines[name] = rec = {}
        rec['engine'] = get_engine(bind_key=name)
    for name, rec in engines.items():
        engine = rec['engine']
        rec['connection'] = conn = engine.connect()
        if USE_TWOPHASE:
            rec['transaction'] = conn.begin_twophase()
        else:
            rec['transaction'] = conn.begin()
    try:
        for name, rec in engines.items():
            logger.info('Migrating database %s' % (name or '<default>'))
            context.configure(connection=rec['connection'], upgrade_token='%s_upgrades' % name, downgrade_token='%s_downgrades' % name, target_metadata=get_metadata(name), **conf_args)
            context.run_migrations(engine_name=name)
        if USE_TWOPHASE:
            for rec in engines.values():
                rec['transaction'].prepare()
        for rec in engines.values():
            rec['transaction'].commit()
    except:
        for rec in engines.values():
            rec['transaction'].rollback()
        raise
    finally:
        for rec in engines.values():
            rec['connection'].close()","getattr(config.cmd_opts, 'autogenerate', False)",158,revision == 'online',False,0.0,N/A
"def fbeta(y_true, y_pred, beta=1):
    from tensorflow.keras import backend as K
<mask>:
        raise ValueError('The lowest choosable beta is zero (only precision).')
    p = precision(y_true, y_pred)
    r = recall(y_true, y_pred)
    bb = beta ** 2
    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())
    return fbeta_score",beta < 0,52,beta == 0,False,18.99589214128981,N/A
"def epoch_entropy(self, history):
    """"""Called from logging/logging_run.py

    Computes the entropy for epoch metric
    variation. If validation is on,
    then returns KL divergence instead of
    simple Shannon entropy. When Keras
    validation_freq is on, Shannon entropy
    is returned. Basically, all experiments
    should use validation, so Shannon is
    provided mearly as a fallback.

    """"""
    import warnings
    from scipy.stats import entropy
    warnings.simplefilter('ignore')
    out = []
    mode = 'shannon'
<mask>:
        for i in range(len(self._metric_keys)):
            if len(history[self._metric_keys[i]]) == len(history[self._val_keys[i]]):
                mode = 'kl_divergence'
            else:
                break
    if mode == 'shannon':
        for i in range(len(self._metric_keys)):
            out.append(entropy(history[self._metric_keys[i]]))
    elif mode == 'kl_divergence':
        for i in range(len(self._metric_keys)):
            out.append(entropy(history[self._val_keys[i]], history[self._metric_keys[i]]))
    return out",len(self._metric_keys) == len(self._val_keys),98,self._validation_freq == on,False,6.880354479320768,N/A
"def on_epoch_end(self, epoch, logs={}):
<mask>:
        try:
            open(self.name, 'r')
        except FileNotFoundError:
            self.epoch_out.append('id')
            self.epoch_out.append('epoch')
            for key in logs.keys():
                self.epoch_out.append(key)
            self.final_out.append(self.epoch_out)
            self.epoch_out = []
    self.epoch_out.append(self.hash)
    self.epoch_out.append(epoch + 1)
    for key in logs.keys():
        rounded = round(logs[key], 4)
        self.epoch_out.append(rounded)
    self.final_out.append(self.epoch_out)",len(self.final_out) == 0,35,self.name,False,3.8238216823301503,N/A
"def activate_model(self, model_id, saved=False, custom_objects=None):
    """"""Loads the model from the json that is stored in the Scan object
    or from local

    model_id | int | the sequential id of the model
    saved | bool | if a model saved on local machine should be used
    custom_object | dict | if the model has a custom object, pass it here

    """"""
    import tensorflow as tf
    from tensorflow.keras.models import model_from_json
<mask>:
        file_path = self.details['experiment_name']
        file_path += '/' + self.details['experiment_id']
        file_path += '/' + str(model_id)
        model = tf.keras.models.load_model(file_path, custom_objects=custom_objects)
    else:
        model = model_from_json(self.saved_models[model_id])
        model.set_weights(self.saved_weights[model_id])
    return model",saved,93,saved,True,100.00000000000004,N/A
"def recover_best_model(x_train, y_train, x_val, y_val, experiment_log, input_model, metric, multi_input=False, x_cross=None, y_cross=None, n_models=5, task='multi_label'):
    """"""Recover best models from Talos experiment log.

    x_train | array | same as was used in the experiment
    y_train | array | same as was used in the experiment
    x_val | array | same as was used in the experiment
    y_val | array | same as was used in the experiment
    experiment_log | str | path to the Talos experiment log
    input_model | function | model used in the experiment
    metric | str | use this metric to pick evaluation candidates
    multi_input | bool | set to True if multi-input model
    x_cross | array | data for the cross-validation or None for use x_val
    y_cross | array | data for the cross-validation or None for use y_val
    n_models | int | number of models to cross-validate
    task | str | binary, multi_class, multi_label or continuous

    Returns a pandas dataframe with the cross-validation results
    and the models.

    """"""
    import pandas as pd
    import sklearn as sk
    import numpy as np
    from talos.utils.validation_split import kfold
    df = pd.read_csv(experiment_log)
<mask>:
        x_cross = x_val
        y_cross = y_val
    results = []
    models = []
    for i in range(n_models):
        params = df.sort_values(metric, ascending=False)
        params = params.drop(metric, axis=1).iloc[i].to_dict()
        _history, model = input_model(x_train, y_train, x_val, y_val, params)
        out = []
        folds = 5
        kx, ky = kfold(x_cross, y_cross, folds, True, multi_input)
        for i in range(folds):
            y_pred = model.predict(kx[i]).argmax(axis=1)
            if task == 'binary':
                y_pred = y_pred >= 0.5
                scores = sk.metrics.f1_score(y_pred, ky[i], average='binary')
            elif task == 'multi_class':
                y_pred = y_pred.argmax(axis=-1)
                scores = sk.metrics.f1_score(y_pred, ky[i], average='macro')
            if task == 'multi_label':
                y_pred = model.predict(kx[i]).argmax(axis=1)
                scores = sk.metrics.f1_score(y_pred, ky[i].argmax(axis=1), average='macro')
            elif task == 'continuous':
                y_pred = model.predict(kx[i])
                scores = sk.metrics.mean_absolute_error(y_pred, ky[i])
            out.append(scores)
        results.append(np.mean(out))
        models.append(model)
    out = df.sort_values(metric, ascending=False).head(n_models)
    out['crossval_mean_f1score'] = results
    return (out, models)",x_cross is None or y_cross is None,295,x_cross is None or y_cross is None,True,100.00000000000004,N/A
"def validation_split(self):
    """"""Defines the attributes `x_train`, `y_train`, `x_val` and `y_val`.
    The validation sets are determined by the attribute val_split,
    which is a number in (0, 1) which determines the proportion of
    the input data to be allocated for cross-validation.""""""
<mask>:
        raise TypeError('For multi-input x, set multi_input to True')
    if self.custom_val_split:
        self.x_train = self.x
        self.y_train = self.y
        return self
    import wrangle
    self.x, self.y = wrangle.array_random_shuffle(x=self.x, y=self.y, multi_input=self.multi_input)
    limit = int(len(self.y) * (1 - self.val_split))
    if self.multi_input:
        self.x_train = []
        self.x_val = []
        for ar in self.x:
            self.x_train.append(ar[:limit])
            self.x_val.append(ar[limit:])
    else:
        self.x_train = self.x[:limit]
        self.x_val = self.x[limit:]
    self.y_train = self.y[:limit]
    self.y_val = self.y[limit:]
    return self","isinstance(self.x, list) and self.multi_input is False",102,"self.multi_input and (not isinstance(self.x, list))",False,64.1975224568211,N/A
"def kfold(x, y, folds=10, shuffled=True, multi_input=False):
    import wrangle
<mask>:
        raise TypeError('For multi-input x, set multi_input to True')
    if shuffled is True:
        x, y = wrangle.array_random_shuffle(x, y, multi_input)
    out_x = []
    out_y = []
    y_len = len(y)
    step = int(y_len / folds)
    lo = 0
    hi = step
    for _i in range(folds):
        if multi_input:
            fold_x = []
            for ar in x:
                fold_x.append(ar[lo:hi])
            out_x.append(fold_x)
        else:
            out_x.append(x[lo:hi])
        out_y.append(y[lo:hi])
        lo += step
        hi += step
    return (out_x, out_y)","isinstance(x, list) and multi_input is False",74,"multi_input and (not isinstance(x, list))",False,49.73567356124543,N/A
"def generator(x, y, batch_size):
    """"""Creates a data generator for Keras fit_generator(). """"""
    import numpy as np
    samples_per_epoch = x.shape[0]
    number_of_batches = samples_per_epoch / batch_size
    counter = 0
    while 1:
        x_batch = np.array(x[batch_size * counter:batch_size * (counter + 1)]).astype('float32')
        y_batch = np.array(y[batch_size * counter:batch_size * (counter + 1)]).astype('float32')
        counter += 1
        yield (x_batch, y_batch)
<mask>:
            counter = 0",counter >= number_of_batches,57,counter >= number_of_batches,True,100.00000000000004,N/A
"def scan_finish(self):
    attrs_final = ['data', 'x', 'y', 'learning_entropy', 'round_times', 'params', 'saved_models', 'saved_weights', 'round_history']
    attrs_to_keep = attrs_final + ['random_method', 'grid_downsample', 'reduction_interval', 'reduce_loss', 'reduction_method', 'reduction_metric', 'reduction_threshold', 'reduction_window', 'experiment_name', 'round_history']
    import time
    import pandas as pd
    self.round_times = pd.DataFrame(self.round_times)
    self.round_times.columns = ['start', 'end', 'duration']
    self.learning_entropy = pd.DataFrame(self.epoch_entropy)
    self.learning_entropy.columns = self._metric_keys
    self.data = self.result
    _experiment_id = self._experiment_id
    keys = list(self.__dict__.keys())
    for key in keys:
<mask>:
            delattr(self, key)
    out = {}
    for key in list(self.__dict__.keys()):
        if key not in attrs_final:
            out[key] = self.__dict__[key]
    out['experiment_id'] = _experiment_id
    out['complete_time'] = time.strftime('%D/%H:%M')
    try:
        out['x_shape'] = self.x.shape
    except AttributeError:
        out['x_shape'] = 'multi-input'
    try:
        out['y_shape'] = self.y.shape
    except AttributeError:
        out['y_shape'] = 'multi-input'
    keys = list(self.__dict__.keys())
    for key in keys:
        if key not in attrs_final:
            delattr(self, key)
    self.details = pd.Series(out)
    from ..scan.scan_addon import func_best_model, func_evaluate
    self.best_model = func_best_model.__get__(self)
    self.evaluate_models = func_evaluate.__get__(self)
    self.data.index = range(len(self.data))
    self.data = self.round_times.merge(self.data, left_index=True, right_index=True)
    return self",key not in attrs_to_keep,140,key not in attrs_final,False,54.44460596606694,N/A
"def initialize_log(self):
    import time
    import os
    try:
        path = os.getcwd()
        os.mkdir(path + '/' + self.experiment_name)
    except FileExistsError:
        pass
    self._experiment_id = time.strftime('%D%H%M%S').replace('/', '')
<mask>:
        self._saved_models_path = self.experiment_name + '/' + self._experiment_id
        file_path = path + '/' + self._saved_models_path
        os.mkdir(file_path)
    _file_name = self._experiment_id + '.csv'
    _experiment_log = './' + self.experiment_name + '/' + _file_name
    f = open(_experiment_log, 'w')
    f.write('')
    f.close()
    return _experiment_log",self.save_models,60,not os.path.exists(self._experiment_id),False,6.285596338261262,N/A
"def scan_round(self):
    """"""The main operational function that manages the experiment
    on the level of execution of each round.""""""
    import time
    import gc
<mask>:
        print(self.round_params)
    round_start = time.strftime('%D-%H%M%S')
    start = time.time()
    from ..model.ingest_model import ingest_model
    self.model_history, self.round_model = ingest_model(self)
    self.round_history.append(self.model_history.history)
    from ..logging.logging_run import logging_run
    self = logging_run(self, round_start, start, self.model_history)
    from ..reducers.reduce_run import reduce_run
    self = reduce_run(self)
    if self.save_models:
        dir_name = str(len(self.round_history) - 1)
        file_path = self._saved_models_path + '/' + dir_name
        self.round_model.save(file_path)
    else:
        try:
            self.saved_models.append(self.round_model.to_json())
            if self.save_weights:
                self.saved_weights.append(self.round_model.get_weights())
            else:
                self.saved_weights.append(None)
        except AttributeError as e:
            if str(e) == ""'Model' object has no attribute 'to_json'"":
                if self.save_weights:
                    self.saved_models.append(self.round_model.state_dict())
                else:
                    self.saved_weights.append(None)
    if self.clear_session is True:
        del self.round_model
        gc.collect()
        try:
            from tensorflow.keras import backend as K
            K.clear_session()
        except ImportError:
            pass
    return self",self.print_params is True,117,self.round_params,False,20.252884954471366,N/A
"def scan_prepare(self):
    """"""Includes all preparation procedures up until starting the first scan
    through scan_run()""""""
    from .scan_utils import initialize_log
    self._experiment_log = initialize_log(self)
    self.custom_val_split = False
<mask>:
        raise RuntimeError('If x_val/y_val is inputted, other must as well.')
    elif self.x_val is not None and self.y_val is not None:
        self.custom_val_split = True
    if isinstance(self.params, dict):
        self._param_dict_keys = list(self.params.keys())
        from ..parameters.ParamSpace import ParamSpace
        self.param_object = ParamSpace(params=self.params, param_keys=self._param_dict_keys, random_method=self.random_method, fraction_limit=self.fraction_limit, round_limit=self.round_limit, time_limit=self.time_limit, boolean_limit=self.boolean_limit)
    elif 'talos.parameters.ParamSpace.ParamSpace' in str(type(self.params)):
        self._param_dict_keys = list(self.params.param_keys)
        self.param_object = self.params
    else:
        raise TypeError('params has to be either dict or ParamSpace object.')
    self.first_round = True
    self.round_history = []
    self.peak_epochs = []
    self.epoch_entropy = []
    self.round_times = []
    self.result = []
    self.saved_models = []
    self.saved_weights = []
    from ..utils.validation_split import validation_split
    self = validation_split(self)
    self._data_len = len(self.x)
    return self",self.x_val is not None and self.y_val is None or (self.x_val is None and self.y_val is not None),123,self.x_val is None and self.y_val is None,False,26.359713811572682,N/A
"def scan_run(self):
    """"""The high-level management of the scan procedures
    onwards from preparation. Manages round_run()""""""
    from tqdm import tqdm
    from .scan_prepare import scan_prepare
    self = scan_prepare(self)
    self.pbar = tqdm(total=len(self.param_object.param_index), disable=self.disable_progress_bar)
    while True:
        self.round_params = self.param_object.round_parameters()
<mask>:
            break
        from .scan_round import scan_round
        self = scan_round(self)
        self.pbar.update(1)
    self.pbar.close()
    from ..logging.logging_finish import logging_finish
    self = logging_finish(self)
    from .scan_finish import scan_finish
    self = scan_finish(self)",self.round_params is False,59,not self.round_params,False,64.31870218238025,N/A
"def hidden_layers(model, params, last_neuron):
    """"""HIDDEN LAYER Generator

    NOTE: 'shapes', 'first_neuron', 'dropout', and 'hidden_layers' need
    to be present in the params dictionary.

    Hidden layer generation for the cases where number
    of layers is used as a variable in the optimization process.
    Handles things in a way where any number of layers can be tried
    with matching hyperparameters.""""""
    from tensorflow.keras.layers import Dense, Dropout
    from .network_shape import network_shape
    from ..utils.exceptions import TalosParamsError
    required = ['shapes', 'first_neuron', 'dropout', 'hidden_layers', 'activation']
    for param in required:
<mask>:
            message = ""hidden_layers requires '"" + param + ""' in params""
            raise TalosParamsError(message)
    layer_neurons = network_shape(params, last_neuron)
    for i in range(params['hidden_layers']):
        model.add(Dense(layer_neurons[i], kernel_initializer=params.get('kernel_initializer', 'glorot_uniform'), kernel_regularizer=params.get('kernel_regularizer'), bias_initializer=params.get('bias_initializer', 'zeros'), bias_regularizer=params.get('bias_regularizer'), use_bias=params.get('use_bias', True), activity_regularizer=params.get('activity_regularizer'), kernel_constraint=params.get('kernel_constraint'), bias_constraint=params.get('bias_constraint'), activation=params.get('activation')))
        model.add(Dropout(params['dropout']))",param not in params,116,param not in params,True,100.00000000000004,N/A
"def lr_normalizer(lr, optimizer):
    """"""Assuming a default learning rate 1, rescales the learning rate
    such that learning rates amongst different optimizers are more or less
    equivalent.

    Parameters
    ----------
    lr : float
        The learning rate.
    optimizer : keras optimizer
        The optimizer. For example, Adagrad, Adam, RMSprop.
    """"""
    from tensorflow.keras.optimizers.legacy import SGD, Adam, Adagrad, Adamax, RMSprop
    from tensorflow.keras.optimizers.legacy import Adagrad
    from talos.utils.exceptions import TalosModelError
<mask>:
        lr /= 100.0
    elif optimizer == Adam or optimizer == RMSprop:
        lr /= 1000.0
    elif optimizer == Adamax:
        lr /= 500.0
    else:
        raise TalosModelError(str(optimizer) + ' is not supported by lr_normalizer')
    return lr",optimizer == SGD or optimizer == Adagrad,95,optimizer == SGD,False,28.650479686019022,N/A
"def output_layer(task, last_activation, y_train, y_val):
    import numpy as np
<mask>:
        activation = last_activation
        last_neuron = 1
    elif task == 'multi_class':
        activation = last_activation
        last_neuron = len(np.unique(np.hstack((y_train, y_val))))
    elif task == 'multi_label':
        activation = last_activation
        last_neuron = y_train.shape[1]
    elif task == 'continuous':
        activation = None
        last_neuron = 1
    return (activation, last_neuron)",task == 'binary',50,task == 'full',False,59.460355750136046,N/A
"def early_stopper(epochs=None, monitor='val_loss', mode='moderate', min_delta=None, patience=None):
    """"""EARLY STOP CALLBACK

    Helps prevent wasting time when loss is not becoming
    better. Offers two pre-determined settings 'moderate'
    and 'strict' and allows input of list with two values:

    `epochs` | int | The number of epochs for the permutation e.g. params['epochs']
    `monitor` | int | The metric to monitor for change
    `mode` | str | One of the presets `lazy`, `moderate`, `strict` or `None`
    `min_delta` | float | The limit for change at which point flag is raised
    `patience` | str | the number of epochs before termination from flag

    """"""
<mask>:
        _es_out = EarlyStopping(monitor=monitor, min_delta=0, patience=int(epochs / 3), verbose=0, mode='auto')
    if mode == 'moderate':
        _es_out = EarlyStopping(monitor=monitor, min_delta=0, patience=int(epochs / 10), verbose=0, mode='auto')
    elif mode == 'strict':
        _es_out = EarlyStopping(monitor=monitor, min_delta=0, patience=2, verbose=0, mode='auto')
    else:
        _es_out = EarlyStopping(monitor=monitor, min_delta=mode[0], patience=mode[1], verbose=0, mode='auto')
    return _es_out",mode == 'lazy',141,epochs is not None,False,0.0,N/A
"def network_shape(params, last_neuron):
    """"""Provides the ability to include network shape in experiments. If params
    dictionary for the round contains float value for params['shapes'] then
    a linear contraction towards the last_neuron value. The higher the value,
    the fewer layers it takes to reach lesser than last_neuron.

    Supports three inbuilt shapes 'brick', 'funnel', and 'triangle'.


    params : dict
         Scan() params for a single roundself.
    last_neuron : int
         Number of neurons on the output layer in the Keras model.
    """"""
    import numpy as np
    from ..utils.exceptions import TalosParamsError
    layers = params['hidden_layers']
    shape = params['shapes']
    first_neuron = params['first_neuron']
    out = []
    n = first_neuron
<mask>:
        return [0]
    if isinstance(shape, float):
        for i in range(layers):
            n *= 1 - shape
            if n > last_neuron:
                out.append(int(n))
            else:
                out.append(last_neuron)
    elif shape == 'brick':
        out = [first_neuron] * layers
    elif shape == 'funnel':
        for i in range(layers + 1):
            n -= int((first_neuron - last_neuron) / layers)
            out.append(n)
        out.pop(-1)
    elif shape == 'triangle':
        out = np.linspace(first_neuron, last_neuron, layers + 2, dtype=int).tolist()
        out.pop(0)
        out.pop(-1)
        out.reverse()
    else:
        message = ""'shapes' must be float or in ['funnel', 'brick', 'triangle']""
        raise TalosParamsError(message)
    return out",layers == 0,181,shape is None,False,0.0,N/A
"def __init__(self, params, param_keys, random_method='uniform_mersenne', fraction_limit=None, round_limit=None, time_limit=None, boolean_limit=None):
    self.params = params
    self.param_keys = param_keys
    self.fraction_limit = fraction_limit
    self.round_limit = round_limit
    self.time_limit = time_limit
    self.boolean_limit = boolean_limit
    self.random_method = random_method
    self.round_counter = 0
    self.p = self._param_input_conversion()
    self._params_temp = [list(self.p[key]) for key in self.param_keys]
    self.dimensions = np.prod([len(l) for l in self._params_temp])
    self.param_index = self._param_apply_limits()
    self.param_space = self._param_space_creation()
<mask>:
        index = self._convert_lambda(self.boolean_limit)(self.param_space)
        self.param_space = self.param_space[index]
    self.param_index = list(range(len(self.param_index)))",self.boolean_limit is not None,66,boolean_limit,False,18.887560283756194,N/A
"def _param_input_conversion(self):
    """"""Parameters may be input as lists of single or
        multiple values (discrete values) or tuples
        (range of values). This helper checks the format of
        each input and handles it accordingly.""""""
    out = {}
    for param in self.param_keys:
<mask>:
            out[param] = self._param_range_expansion(self.params[param])
        elif isinstance(self.params[param], list):
            out[param] = self.params[param]
    return out","isinstance(self.params[param], tuple)",51,"isinstance(self.params[param], range)",False,80.70557274927978,N/A
"def _param_apply_limits(self):
    from talos.reducers.sample_reducer import sample_reducer
<mask>:
        pass
    if self.time_limit is not None:
        pass
    if self.fraction_limit is not None:
        return sample_reducer(self.fraction_limit, self.dimensions, self.random_method)
    if self.round_limit is not None:
        return sample_reducer(self.round_limit, self.dimensions, self.random_method)
    return list(range(self.dimensions))",self.boolean_limit is not None,34,self.random_method is not None,False,27.054113452696992,N/A
"def _param_range_expansion(self, param_values):
    """"""Expands a range (tuple) input into discrete
        values. Helper for _param_input_conversion.
        Expects to have a input as (start, end, steps).
        """"""
    start = param_values[0]
    end = param_values[1]
    steps = param_values[2]
    out = np.arange(start, end, (end - start) / steps, dtype=float)
<mask>:
        out = out.astype(int)
        out = np.unique(out)
    return out","isinstance(start, int) and isinstance(end, int)",52,"not isinstance(out, np.ndarray)",False,7.270717733704594,N/A
"def _check_time_limit(self):
<mask>:
        return True
    stop = datetime.strptime(self.time_limit, '%Y-%m-%d %H:%M')
    return stop > datetime.now()",self.time_limit is None,14,self.time_limit is None,True,100.00000000000004,N/A
"def titanic(debug=False):
    from tensorflow.keras.optimizers.legacy import Adam, Adagrad
    p = {'lr': (0.5, 5, 10), 'first_neuron': [4, 8, 16], 'batch_size': [20, 30, 40], 'dropout': (0, 0.5, 5), 'optimizer': [Adam(), Adagrad()], 'epochs': [50, 100, 150], 'losses': ['LogCosh', 'binary_crossentropy'], 'shapes': ['brick', 'triangle', 0.2], 'hidden_layers': [0, 1, 2, 3, 4], 'activation': ['relu', 'elu'], 'last_activation': ['sigmoid']}
<mask>:
        p = {'lr': [0.1, 0.2], 'first_neuron': [4, 8], 'batch_size': [20, 30], 'dropout': [0.2, 0.3], 'optimizer': [Adam(), Adagrad()], 'epochs': [50, 100], 'losses': ['LogCosh', 'binary_crossentropy'], 'shapes': ['brick', 'triangle', 0.2], 'hidden_layers': [0, 1], 'activation': ['relu', 'elu'], 'last_activation': ['sigmoid']}
    return p",debug,88,debug,True,100.00000000000004,N/A
"def telco_churn(quantile=0.5):
    """"""Returns dataset in format x, [y1, y2]. This dataset
    is useful for demonstrating multi-output model or for
    experimenting with reduction strategy creation.

    The data is from hyperparameter optimization experiment with
    Kaggle telco churn dataset.

    x: features
    y1: val_loss
    y2: val_f1score

    quantile is for transforming the otherwise continuous y variables into
    labels so that higher value is stronger. If set to 0 then original
    continuous will be returned.""""""
    import wrangle
    import pandas as pd
    base_url = 'https://raw.githubusercontent.com/autonomio/'
    url = 'examples/master/telco_churn/telco_churn_for_sensitivity.csv'
    df = pd.read_csv(base_url + url)
    df = df.drop(['val_acc', 'loss', 'f1score', 'acc', 'round_epochs'], axis=1)
    for col in df.iloc[:, 2:].columns:
        df = wrangle.col_to_multilabel(df, col)
    df = wrangle.df_rename_cols(df)
<mask>:
        y1 = (df.C0 < df.C0.quantile(quantile)).astype(int).values
        y2 = (df.C1 > df.C1.quantile(quantile)).astype(int).values
    else:
        y1 = df.C0.values
        y2 = df.C1.values
    x = df.drop(['C0', 'C1'], axis=1).values
    return (x, [y1, y2])",quantile > 0,133,quantile > 0,True,100.00000000000004,N/A
"def mnist():
    """"""Note that this dataset, unlike other Talos datasets,returns:

    x_train, y_train, x_val, y_val""""""
    import tensorflow as tf
    import numpy as np
    (x_train, y_train), (x_val, y_val) = tf.keras.datasets.mnist.load_data()
    img_rows, img_cols = (28, 28)
<mask>:
        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
        x_val = x_val.reshape(x_val.shape[0], 1, img_rows, img_cols)
        input_shape = (1, img_rows, img_cols)
    else:
        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
        x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)
        input_shape = (img_rows, img_cols, 1)
    x_train = x_train.astype('float32')
    x_val = x_val.astype('float32')
    x_train /= 255
    x_val /= 255
    classes = len(np.unique(y_train))
    y_train = tf.keras.utils.to_categorical(y_train, classes)
    y_val = tf.keras.utils.to_categorical(y_val, classes)
    print('Use input_shape %s' % str(input_shape))
    return (x_train, y_train, x_val, y_val)",tf.keras.backend.image_data_format() == 'channels_first',102,img_rows > img_cols,False,1.6224821466016939,N/A
"def predict(self, x, metric, asc, model_id=None, saved=False, custom_objects=None):
    """"""Makes a probability prediction from input x. If model_id
        is not given, then best_model will be used.

        x | array | data to be used for the predictions
        model_id | int | the id of the model from the Scan() object
        metric | str | the metric to be used for picking best model
        asc | bool | True if `metric` is something to be minimized
        saved | bool | if a model saved on local machine should be used
        custom_objects | dict | if the model has a custom object,
                                pass it here

        """"""
<mask>:
        from ..utils.best_model import best_model
        model_id = best_model(self.scan_object, metric, asc)
    from ..utils.best_model import activate_model
    model = activate_model(self.scan_object, model_id, saved, custom_objects)
    return model.predict(x)",model_id is None,125,model_id is None,True,100.00000000000004,N/A
"def predict_classes(self, x, metric, asc, task, model_id=None, saved=False, custom_objects=None):
    """"""Makes a class prediction from input x. If model_id
        is not given, then best_model will be used.

        x | array | data to be used for the predictions
        model_id | int | the id of the model from the Scan() object
        metric | str | the metric to be used for picking best model
        asc | bool | True if `metric` is something to be minimized
        task | string | 'binary' or 'multi_label'
        saved | bool | if a model saved on local machine should be used
        custom_objects | dict | if the model has a custom object, pass it here
        """"""
    import numpy as np
<mask>:
        from ..utils.best_model import best_model
        model_id = best_model(self.scan_object, metric, asc)
    from ..utils.best_model import activate_model
    model = activate_model(self.scan_object, model_id, saved, custom_objects)
    preds = model.predict(x)
    if task == 'binary':
        return np.where(preds >= 0.5, 1, 0)
    elif task == 'multi_label':
        return np.argmax(preds, 1)
    else:
        msg = 'Only `binary` and `multi_label` are supported'
        raise AttributeError(msg)",model_id is None,166,model_id is None,True,100.00000000000004,N/A
"def __init__(self, source=None):
    """"""Takes as input a filename to the experiment
        log or the Scan object""""""
    import pandas as pd
<mask>:
        self.data = pd.read_csv(source)
    else:
        self.data = source.data","isinstance(source, str)",28,"isinstance(source, str)",True,100.00000000000004,N/A
"def table(self, metric, exclude=[], sort_by=None, ascending=False):
    """"""Shows a table with hyperparameters and a given metric

        EXAMPLE USE:

        ra1 = Reporting('diabetes_1.csv')
        ra1.table(sort_by='fmeasure_acc', ascending=False)

        PARAMS:

        metric | str or list | Column labels for the metric to correlate with
        exclude | list | Column label/s to be excluded from the correlation
        sort_by | str | The colunm name sorting should be based on
        ascending | bool | Set to True when `sort_by` is to be minimized eg. loss

        """"""
    cols = self._cols(metric, exclude)
<mask>:
        sort_by = metric
    out = self.data[cols].sort_values(sort_by, ascending=ascending)
    return out",sort_by is None,91,sort_by is None,True,100.00000000000004,N/A
"def _cols(self, metric, exclude):
    """"""Helper to remove other than desired metric from data table""""""
    cols = [col for col in self.data.columns if col not in exclude + [metric]]
<mask>:
        metric = [metric]
    for i, metric in enumerate(metric):
        cols.insert(i, metric)
    cols = list(set(cols))
    return cols","isinstance(metric, list) is False",44,"not isinstance(metric, list)",False,70.1396726799769,N/A
"def logging_run(self, round_start, start, model_history):
    import time
    self._round_seconds = time.time() - start
    round_end = time.strftime('%D-%H%M%S')
    self.round_times.append([round_start, round_end, self._round_seconds])
<mask>:
        self._all_keys = list(model_history.history.keys())
        self._metric_keys = [k for k in self._all_keys if 'val_' not in k]
        self._val_keys = [k for k in self._all_keys if 'val_' in k]
        _results_header = ['round_epochs'] + self._all_keys + self._param_dict_keys
        self.result.append(_results_header)
        from .results import save_result
        save_result(self)
        self.first_round = False
    from ..metrics.entropy import epoch_entropy
    self.epoch_entropy.append(epoch_entropy(self, model_history.history))
    from .results import run_round_results
    _round_results = run_round_results(self, model_history)
    self.result.append(_round_results)
    from .results import save_result
    save_result(self)
    return self",self.first_round,83,self.first_round,True,100.00000000000004,N/A
"def run_updates(self):
    for key in self.gamify_dict.keys():
        for val in self.gamify_dict[key].keys():
<mask>:
                label = list(self.params.keys())[int(key)]
                value = self.params[label][int(val)]
                self.gamify_dict[key][val] = self.updated_dict[key][val]
                self.scan_object.param_object.remove_is(label, value)
    return self.scan_object",self.gamify_dict[key][val] != self.updated_dict[key][val],24,int(key) in self.updated_dict[key],False,24.717593402616735,N/A
"def limit_by_metric(self):
    """"""Takes as input metric, threshold, and loss and
    and returs a True if metric threshold have been
    met and False if not.

    USE: space.check_metric(model_history)
    """"""
    metric = self.performance_target[0]
    threshold = self.performance_target[1]
    loss = self.performance_target[2]
<mask>:
        return self.model_history.history[metric][-1] <= threshold
    elif loss is False:
        return self.model_history.history[metric][-1] >= threshold",loss is True,49,loss is True,True,100.00000000000004,N/A
"def gamify(self):
    """"""Will apply reduction changes based on edits on the
    the produced .json file in the experiment folder""""""
<mask>:
        from .GamifyMap import GamifyMap
        g = GamifyMap(self)
        self._gamify_object = g
        g.export_json()
        return self
    self._gamify_object.import_json()
    self = self._gamify_object.run_updates()
    self._gamify_object.export_json()
    return self",self.param_object.round_counter == 1,40,not self._gamify_object,False,9.614956805006122,N/A
"def sample_reducer(limit, max_value, random_method):
    """"""Sample Reducer (Helper)

    NOTE: The Scan() object  is in self.main_self because
    the object being passed here is ParamGrid() object where
    the Scan() object is attached as self.main_self.

    Utilize 'grid_downsample', 'shuffle', and 'random_method'
    to reduce the param_grid before starting the experiment.
    This is the simplest method in Talos for dealing with curse
    of dimensionality.

    Options are uniform random, stratified random, latin hypercube
    sampling, and latin hypercube with sudoku style constraint.

    Returns the reduced param_grid as numpy array.

    """"""
    import chances as ch
<mask>:
        n = int(max_value * limit)
    if isinstance(limit, int):
        n = limit
    max_value = int(max_value)
    from ..utils.exceptions import TalosDataError
    if n < 1:
        raise TalosDataError('Limiters lead to < 1 permutations.')
    r = ch.Randomizer(max_value, n)
    if random_method == 'sobol':
        out = r.sobol()
    elif random_method == 'quantum':
        out = r.quantum()
    elif random_method == 'halton':
        out = r.halton()
    elif random_method == 'korobov_matrix':
        out = r.korobov_matrix()
    elif random_method == 'latin_sudoku':
        out = r.latin_sudoku()
    elif random_method == 'latin_matrix':
        out = r.latin_matrix()
    elif random_method == 'latin_improved':
        out = r.latin_improved()
    elif random_method == 'uniform_mersenne':
        out = r.uniform_mersenne()
    elif random_method == 'uniform_crypto':
        out = r.uniform_crypto()
    elif random_method == 'ambience':
        out = r.ambience()
    else:
        print('No eligble random_method found. Using uniform_mersenne.')
        out = r.uniform_mersenne()
    return out","isinstance(limit, float)",201,"isinstance(limit, float)",True,100.00000000000004,N/A
"def reduce_run(self):
    """"""The process run script for reduce
    procedures; takes care of everything
    related with reduction. When new
    reduction methods are added, they need
    to be added as options here.

    To add new reducers, create a file in /reducers
    which is where this file is located. In that file,
    take as input self from Scan() and give as output
    either False, which does nothing, or a tuple of
    'value' and 'label' where value is a parameter
    value and label is parameter name. For example
    batch_size and 128. Then add a reference to
    reduce_run.py and make sure that you process
    the self.param_object.param_index there before
    wrapping up.

    """"""
    from .correlation import correlation
    from .forrest import forrest
    from .trees import trees
    from .gamify import gamify
    from .local_strategy import local_strategy
    from .limit_by_metric import limit_by_metric
<mask>:
        status = limit_by_metric(self)
        if status == True:
            self.param_object.param_index = []
            print('Target %.3f have been met.' % self.performance_target[1])
    if self.reduction_method is None:
        return self
    left = self.param_object.round_counter + 1
    right = self.reduction_interval
    len_before_reduce = len(self.param_object.param_index)
    if self.reduction_method == 'gamify':
        self = gamify(self)
    if left % right == 0:
        if self.reduction_method in ['pearson', 'kendall', 'spearman']:
            self = correlation(self, self.reduction_method)
        if self.reduction_method == 'correlation':
            self = correlation(self, 'spearman')
        if self.reduction_method == 'forrest':
            self = forrest(self)
        if self.reduction_method == 'trees':
            self = trees(self)
        if self.reduction_method == 'local_strategy':
            self = local_strategy(self)
        total_reduced = len_before_reduce - len(self.param_object.param_index)
        total_reduced = max(0, total_reduced)
        self.pbar.update(total_reduced)
        if total_reduced > 0:
            drop_share = total_reduced / len_before_reduce * 100
            print('Total %.1f%% permutations reduced' % drop_share)
    return self",self.performance_target is not None,246,self.performance_target is not None,True,100.00000000000004,N/A
"def correlation(self, method):
    """"""This is called from reduce_run.py.

    Performs a spearman rank order correlation
    based reduction. First looks for a parameter
    that correlates with reduction_metric and
    correlation meets reduction_threshold and
    then converts the match parameter into
    a 2d multilabel shape. Then new correlation
    against reduction_metric is performed to identify
    which particular value is to be dropped.

    """"""
    import numpy as np
    from .reduce_utils import cols_to_multilabel
    data = cols_to_multilabel(self)
    corr_values = data.corr(method)[self.reduction_metric]
    corr_values.drop(self.reduction_metric, inplace=True)
    corr_values.dropna(inplace=True)
<mask>:
        return self
    corr_values.sort_values(ascending=self.minimize_loss, inplace=True)
    if abs(corr_values[-1]) < self.reduction_threshold:
        return self
    corr_values = corr_values.index[-1]
    label, dtype, value = corr_values.split('~')
    value = np.array([value]).astype(dtype)[0]
    self.param_object.remove_is(label, value)
    return self",len(corr_values) <= 1,100,len(corr_values) == 0,False,61.04735835807847,N/A
"def __init__(self, task, experiment_name, metric=None):
    """"""

        Creates an input model for Scan(). Optimized for being used together
        with Params(). For example:

        p = talos.AutoParams().params
        model = talos.AutoModel(task='binary').model

        talos.Scan(x, y, p, model)

        NOTE: the parameter space from Params() is very large, so use limits
        in or reducers in Scan() accordingly.

        task : string or None
            If 'continuous' then mae is used for metric, if 'binary',
            'multiclass', or 'multilabel', f1score is used. Accuracy is always
            used.
        experiment_name | str | Must be same as in `Scan()`
        metric : None or list
            You can also input a list with one or more custom metrics or names
            of Keras or Talos metrics.
        """"""
    from talos.callbacks.experiment_log import ExperimentLog
    self.task = task
    self.experiment_name = experiment_name
    self.metric = metric
<mask>:
        self.metrics = self._set_metric()
    elif self.metric is not None and isinstance(self.metric, list):
        self.metrics = self.metric + ['acc']
    else:
        print('Either pick task or provide list as input for metric.')
    self.model = self._create_input_model
    self.callback = ExperimentLog",self.task is not None,156,"self.metric is None and isinstance(self.metric, str)",False,6.754312828675707,N/A
"def _set_metric(self):
    """"""Sets the metric for the model based on the experiment type
        or a list of metrics from user.""""""
    import talos as ta
<mask>:
        return [ta.utils.metrics.f1score, 'acc']
    elif self.task == 'continuous':
        return [ta.utils.metrics.mae, 'acc']","self.task in ['binary', 'multiclass', 'multilabel']",35,self.task == 'f1',False,13.13084334918613,N/A
"def _create_input_model(self, x_train, y_train, x_val, y_val, params):
    import wrangle as wr
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dropout, Flatten
    from tensorflow.keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional
    model = Sequential()
<mask>:
        x_train = wr.array_reshape_conv1d(x_train)
        x_val = wr.array_reshape_conv1d(x_val)
    if params['network'] == 'conv1d':
        model.add(Conv1D(params['first_neuron'], x_train.shape[1]))
        model.add(Flatten())
    elif params['network'] == 'lstm':
        model.add(LSTM(params['first_neuron']))
    if params['network'] == 'bidirectional_lstm':
        model.add(Bidirectional(LSTM(params['first_neuron'])))
    elif params['network'] == 'simplernn':
        model.add(SimpleRNN(params['first_neuron']))
    elif params['network'] == 'dense':
        model.add(Dense(params['first_neuron'], input_dim=x_train.shape[1], activation='relu', kernel_initializer=params['kernel_initializer']))
    model.add(Dropout(params['dropout']))
    from talos.model.hidden_layers import hidden_layers
    hidden_layers(model, params, 1)
    from talos.model.output_layer import output_layer
    activation, last_neuron = output_layer(self.task, params['last_activation'], y_train, y_val)
    model.add(Dense(last_neuron, activation=activation, kernel_initializer=params['kernel_initializer']))
    from talos.model.normalizers import lr_normalizer
    optimizer = params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer']))
    model.compile(optimizer=optimizer, loss=params['losses'], metrics=self.metrics)
    out = model.fit(x_train, y_train, batch_size=params['batch_size'], epochs=params['epochs'], verbose=0, callbacks=[self.callback(self.experiment_name, params)], validation_data=(x_val, y_val))
    return (out, model)",params['network'] != 'dense',115,params['network'] == 'conv1d',False,43.47208719449914,N/A
"def __init__(self, params=None, task='binary', replace=True, auto=True, network=True, resample_params=4):
    """"""A facility for generating or appending params dictionary.

        params : dict or None
        task : str
             'binary', 'multi_class', 'multi_label', or 'continuous'
        replace : bool
             Replace current dictionary entries with new ones.
        auto : bool
             Automatically generate or append params dictionary with
             all available parameters.
        network : bool
             Adds several network architectures as parameters. This is to be
             used as an input together with KerasModel(). If False then only
             'dense' will be added.
        resample_params | int or False | The number of values per parameter
        """"""
    self._task = task
    self._replace = replace
    self._network = network
<mask>:
        self.params = {}
    else:
        self.params = params
    if auto:
        self._automated()
    if resample_params is not False:
        self.resample_params(resample_params)",params is None,119,params is None,True,100.00000000000004,N/A
"def _automated(self, shapes='fixed'):
    """"""Automatically generate a comprehensive
        parameter dict to be used in Scan()

        shapes : string
            Either 'fixed' or 'sloped'

        """"""
<mask>:
        self.shapes()
    else:
        self.shapes_slope()
    self.layers()
    self.dropout()
    self.optimizers()
    self.activations()
    self.neurons()
    self.losses()
    self.batch_size()
    self.epochs()
    self.kernel_initializers()
    self.lr()
    if self._network:
        self.networks()
    else:
        self.params['network'] = ['dense']
    self.last_activations()",shapes == 'fixed',44,shapes,False,4.9787068367863965,N/A
"def shapes(self, shapes='auto'):
    """"""Uses triangle, funnel, and brick shapes.""""""
<mask>:
        self._append_params('shapes', ['triangle', 'funnel', 'brick'])
    else:
        self._append_params('shapes', shapes)",shapes == 'auto',17,shapes == 'auto',True,100.00000000000004,N/A
"def optimizers(self, optimizers='auto'):
    """"""If `optimizers='auto'` then optimizers will be picked based on
        automatically. Otherwise input a list with one or
        more optimizers will be used.
        """"""
<mask>:
        self._append_params('optimizer', [Adam, Adagrad, SGD])
    else:
        self._append_params('optimizer', optimizers)",optimizers == 'auto',34,optimizers == 'auto',True,100.00000000000004,N/A
"def activations(self, activations='auto'):
    """"""If `activations='auto'` then activations will be picked based on
        automatically. Otherwise input a list with one or
        more activations will be used.
        """"""
<mask>:
        activations = ['relu', 'elu']
    self._append_params('activation', activations)",activations == 'auto',33,activations == 'auto',True,100.00000000000004,N/A
"def start(self, x, y, **kwargs):
    """"""Start the scan. Note that you can use `Scan()` arguments as you
        would otherwise directly interacting with `Scan()`.

        `x` | array or list of arrays | prediction features
        `y` | array or list of arrays | prediction outcome variable
        `kwargs` | arguments | any `Scan()` argument can be passed here

        """"""
    import talos
    m = talos.autom8.AutoModel(self.task, self.experiment_name).model
    try:
        kwargs['params']
        scan_object = talos.Scan(x, y, model=m, experiment_name=self.experiment_name, **kwargs)
    except KeyError:
        p = talos.autom8.AutoParams(task=self.task)
<mask>:
            p.resample_params(self.max_param_values)
        params = p.params
        scan_object = talos.Scan(x=x, y=y, params=params, model=m, experiment_name=self.experiment_name, **kwargs)
    return scan_object",self.max_param_values is not None,91,self.max_param_values,False,65.14390575310559,N/A
"def test_scan():
    print('\n >>> start Scan()...')
    import talos
    import tensorflow as tf
    from tensorflow.keras.losses import binary_crossentropy
    from tensorflow.keras.optimizers.legacy import Adam
    from tensorflow.keras.activations import relu, elu
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.models import Sequential
    p = {'activation': [relu, elu], 'optimizer': ['Adagrad', Adam], 'losses': ['LogCosh', binary_crossentropy], 'shapes': ['brick', 'funnel', 'triangle'], 'first_neuron': [16], 'hidden_layers': [0, 1, 2, 3], 'dropout': (0.05, 0.35, 0.1), 'epochs': [50]}

    def iris_model(x_train, y_train, x_val, y_val, params):
        model = Sequential()
        model.add(Dense(params['first_neuron'], input_dim=4, activation=params['activation']))
        talos.utils.hidden_layers(model, params, 3)
        model.add(Dense(3, activation='softmax'))
<mask>:
            opt = params['optimizer']
        else:
            opt = params['optimizer']()
        model.compile(optimizer=opt, loss=params['losses'], metrics=['acc', talos.utils.metrics.f1score])
        out = model.fit(x_train, y_train, batch_size=25, epochs=params['epochs'], validation_data=(x_val, y_val), verbose=0)
        return (out, model)
    x, y = talos.templates.datasets.iris()
    p_for_q = {'activation': ['relu', 'elu'], 'optimizer': ['Adagrad', 'Adam'], 'losses': ['LogCosh'], 'shapes': ['brick'], 'first_neuron': [16, 32, 64, 128], 'hidden_layers': [0, 1, 2, 3], 'dropout': [0.2, 0.3, 0.4], 'batch_size': [20, 30, 40, 50], 'epochs': [10]}
    scan_object = talos.Scan(x=x, y=y, params=p_for_q, model=iris_model, experiment_name='test_q', val_split=0.3, random_method='uniform_mersenne', round_limit=15, reduction_method='spearman', reduction_interval=10, reduction_window=9, reduction_threshold=0.01, reduction_metric='val_acc', minimize_loss=False)
    x = x[:50]
    y = y[:50]
    p['epochs'] = [5]
    talos.Scan(x=x, y=y, x_val=x, y_val=y, params=p, model=iris_model, experiment_name='test_iris', fraction_limit=0.05)
    talos.Scan(x=x, y=y, params=p, model=iris_model, experiment_name='test_2', x_val=x, y_val=y, random_method='latin_suduko', seed=3, performance_target=['acc', 0.01, False], round_limit=3, disable_progress_bar=True, print_params=True, clear_session=False)
    talos.Scan(x=x, y=y, params=p, model=iris_model, experiment_name='test_3', x_val=None, y_val=None, val_split=0.3, random_method='sobol', seed=5, performance_target=['val_acc', 0.1, False], fraction_limit=None, time_limit='2099-09-09 09:09', reduction_method='spearman', reduction_interval=2, reduction_window=2, reduction_threshold=0.2, reduction_metric='loss', minimize_loss=True, clear_session=False, boolean_limit=lambda p: p['first_neuron'] * p['hidden_layers'] < 220)
    print('finised Scan() \n')
    print('\n >>> start Scan() object ...')
    scan_object.best_model()
    scan_object.best_model('loss', True)
    scan_object.evaluate_models(x_val=scan_object.x, y_val=scan_object.y, task='multi_label')
    scan_object.evaluate_models(x_val=scan_object.x, y_val=scan_object.y, task='multi_label', n_models=3, metric='val_loss', folds=3, shuffle=False, asc=True)
    print('finised Scan() object \n')
    return scan_object","isinstance(params['optimizer'], str)",247,"isinstance(params['optimizer'], str)",True,100.00000000000004,N/A
"def process_message(self, data):
    """"""
        The Slack Bot's only required method -- checks if the message involves this bot.
        :param data:
        :return:
        """"""
<mask>:
        return
    if len(ONLY_LISTEN) > 0 and data['channel'] not in ONLY_LISTEN:
        return
    command_prefix = data['text'].split(' ')[0].lower()
    if COMMANDS.get(command_prefix):
        process_the_command(data, command_prefix)",data['channel'] in IGNORE_ROOMS,42,not data,False,2.489353418393197,N/A
"def process_the_command(data, command_prefix):
    """"""
    Will perform all command_plugins duties if a command_plugins arrived.

    :param data:
    :param command_prefix:
    :return:
    """"""
    user_data, error = get_user_data(data)
<mask>:
        send_error(data['channel'], 'ERROR: Unable to communicate with the Slack API. Error:\n{}'.format(error))
        return
    if COMMANDS[command_prefix]['user_data_required']:
        COMMANDS[command_prefix]['func'](data, user_data)
    else:
        COMMANDS[command_prefix]['func'](data)",error,41,error,True,100.00000000000004,N/A
"def setup(slackclient):
    """"""
    This is called by the Slack RTM Bot to initialize the plugin.

    This contains code to load all the secrets that are used by all the other services.
    :return:
    """"""
    secrets = get_credentials()
    from . import bot_components
    bot_components.SLACK_CLIENT = slackclient
    print('[-->] Enabling Auth Plugins')
    for name, plugin in AUTH_PLUGINS.items():
        print('\t[ ] Enabling Auth Plugin: {}'.format(name))
        plugin.setup(secrets)
        print('\t[+] Successfully enabled auth plugin ""{}""'.format(name))
    print('[✔] Completed enabling auth plugins plugins.')
    print('[-->] Enabling Command Plugins')
    for name, plugin in COMMAND_PLUGINS.items():
        print('[ ] Enabling Command Plugin: {}'.format(name))
        plugin.setup(secrets)
        for cmd in plugin.commands.values():
<mask>:
                print(""\t[+] Adding command: '{cmd}'"".format(cmd=cmd['command']))
                COMMANDS[cmd['command'].lower()] = cmd
                if cmd.get('help'):
                    HELP_TEXT.append('`{cmd}` - {help}\n'.format(cmd=cmd['command'], help=cmd['help']))
                else:
                    print('\t[!] Not adding help text for hidden command: {}'.format(cmd['command']))
            else:
                print(""\t[/] Skipping disabled command: '{cmd}'"".format(cmd=cmd['command']))
        print('[+] Successfully enabled command plugin ""{}""'.format(name))
    print('[✔] Completed enabling command plugins.')",cmd['enabled'],132,cmd.get('enabled'),False,9.652434877402245,N/A
"def setup(self, secrets, **kwargs):
    for variable, secret in secrets.items():
<mask>:
            domain, host, ikey, skey = secret.split(',')
            self.clients[domain] = Client(ikey, skey, host)
    if not len(self.clients):
        raise NoSecretsProvidedError('Must provide secrets to enable authentication.')",'DUO_' in variable,31,variable == 'client',False,12.44023474812678,N/A
"def authenticate(self, data, user_data, **kwargs):
    domain = user_data['profile']['email'].split('@')[1]
<mask>:
        send_error(data['channel'], '💀 @{}: Duo in this bot is not configured for the domain: `{}`. It needs to be configured for you to run this command.'.format(user_data['name'], domain), markdown=True, thread=data['ts'])
        return False
    send_info(data['channel'], '🎟 @{}: Sending a Duo notification to your device. You must approve!'.format(user_data['name']), markdown=True, ephemeral_user=user_data['id'])
    try:
        result = self._perform_auth(user_data, self.clients[domain])
    except InvalidDuoResponseError as idre:
        send_error(data['channel'], '💀 @{}: There was a problem communicating with Duo. Got this status: {}. Aborting...'.format(user_data['name'], str(idre)), thread=data['ts'], markdown=True)
        return False
    except CantDuoUserError as _:
        send_error(data['channel'], ""💀 @{}: I can't Duo authenticate you. Please consult with your identity team. Aborting..."".format(user_data['name']), thread=data['ts'], markdown=True)
        return False
    except Exception as e:
        send_error(data['channel'], '💀 @{}: I encountered some issue with Duo... Here are the details: ```{}```'.format(user_data['name'], str(e)), thread=data['ts'], markdown=True)
        return False
    if not result:
        send_error(data['channel'], '💀 @{}: Your Duo request was rejected. Aborting...'.format(user_data['name']), markdown=True, thread=data['ts'])
        return False
    send_success(data['channel'], '🎸 @{}: Duo approved! Completing request...'.format(user_data['name']), markdown=True, ephemeral_user=user_data['id'])
    return True",not self.clients.get(domain),156,domain not in self.clients,False,20.82186541080652,N/A
"def _perform_auth(self, user_data, client):
    duo_params = {'username': user_data['profile']['email'], 'factor': 'push', 'device': 'auto'}
    response, data = client.api_call('POST', '/auth/v2/auth', duo_params)
    result = json.loads(data.decode('utf-8'))
<mask>:
        raise InvalidDuoResponseError(response.status)
    if result['stat'] != 'OK':
        raise CantDuoUserError()
    if result['response']['result'] == 'allow':
        return True
    return False",response.status != 200,38,response.status != 200,True,100.00000000000004,N/A
"def extract_url(plugin_obj, url, **kwargs):
<mask>:
        url = url.split('|')[0]
    return url.replace('<', '').replace('>', '')",'|' in url,12,'|' in url,True,100.00000000000004,N/A
"def validate_homepage(plugin_obj, homepage, **kwargs):
    url = extract_url(plugin_obj, homepage)
<mask>:
        if not validators.url(url):
            raise ParseException('homepage', 'Invalid homepage URL was sent in. It must be a well formed URL.')
    return url",url != '',29,url,False,4.9787068367863965,N/A
"@hubcommander_command(name='!SetDescription', usage='!SetDescription <OrgWithRepo> <Repo> <""The description in quotes"">', description=""This will set the repository's description."", required=[dict(name='org', properties=dict(type=str, help='The organization that contains the repo.'), validation_func=lookup_real_org, validation_func_kwargs={}), dict(name='repo', properties=dict(type=str, help='The repository to set the description on.'), validation_func=extract_repo_name, validation_func_kwargs={}), dict(name='description', properties=dict(type=str, help='The description to set in quotes. (Empty quotes clears)'), lowercase=False)], optional=[])
@auth()
@repo_must_exist()
def set_description_command(self, data, user_data, org, repo, description):
    """"""
        Changes a repository description.

        Command is as follows: !setdescription <organization> <repo> <description>
        :param data:
        :param user_data:
        :param org:
        :param repo:
        :param description:
        :return:
        """"""
    send_info(data['channel'], '@{}: Working, Please wait...'.format(user_data['name']), thread=data['ts'])
<mask>:
        return
    if description == '':
        send_success(data['channel'], ""@{}: The {}/{} repository's description field has been cleared."".format(user_data['name'], org, repo), markdown=True, thread=data['ts'])
    else:
        send_success(data['channel'], ""@{}: The {}/{} repository's description has been modified to:\n`{}`."".format(user_data['name'], org, repo, description), markdown=True, thread=data['ts'])","not self.make_repo_edit(data, user_data, repo, org, description=description)",124,not data['description'],False,0.3471019544239176,N/A
"@hubcommander_command(name='!SetHomepage', usage='!SetHomepage <OrgWithRepo> <Repo> <""http://theHomePageUrlInQuotes"" - OR - """" to remove>', description=""This will set the repository's homepage."", required=[dict(name='org', properties=dict(type=str, help='The organization that contains the repo.'), validation_func=lookup_real_org, validation_func_kwargs={}), dict(name='repo', properties=dict(type=str, help='The repository to set the homepage on.'), validation_func=extract_repo_name, validation_func_kwargs={}), dict(name='homepage', properties=dict(type=str, help='The homepage to set in quotes. (Empty quotes clears)'), validation_func=validate_homepage, validation_func_kwargs={})], optional=[])
@auth()
@repo_must_exist()
def set_repo_homepage_command(self, data, user_data, org, repo, homepage):
    """"""
        Changes a repository's homepage.

        Command is as follows: !sethomepage <organization> <repo> <homepage>
        :param data:
        :param user_data:
        :param org:
        :param repo:
        :param homepage:
        :return:
        """"""
    send_info(data['channel'], '@{}: Working, Please wait...'.format(user_data['name']), thread=data['ts'])
<mask>:
        return
    if homepage == '':
        send_success(data['channel'], ""@{}: The {}/{} repository's homepage field has been cleared."".format(user_data['name'], org, repo, homepage), markdown=True, thread=data['ts'])
    else:
        send_success(data['channel'], ""@{}: The {}/{} repository's homepage has been modified to:\n`{}`."".format(user_data['name'], org, repo, homepage), markdown=True, thread=data['ts'])","not self.make_repo_edit(data, user_data, repo, org, homepage=homepage)",129,not data['github'],False,0.3471019544239176,N/A
"@hubcommander_command(name='!CreateRepo', usage='!CreateRepo <OrgToCreateRepoIn> <NewRepoName>', description='This will create a new repository on GitHub.', required=[dict(name='org', properties=dict(type=str, help='The organization to create the repo in.'), validation_func=lookup_real_org, validation_func_kwargs={}), dict(name='repo', properties=dict(type=str, help='The name of the new repo to create.'), lowercase=False, validation_func=extract_repo_name, validation_func_kwargs={})], optional=[])
@auth()
def create_repo_command(self, data, user_data, org, repo):
    """"""
        Creates a new repository (default is private unless the org is public only).

        Command is as follows: !createrepo <organization> <new_repo>
        :param repo:
        :param org:
        :param user_data:
        :param data:
        :return:
        """"""
    send_info(data['channel'], '@{}: Working, Please wait...'.format(user_data['name']), thread=data['ts'])
    try:
        result = self.check_gh_for_existing_repo(repo, org)
<mask>:
            if '{}/{}'.format(org, repo) == result['full_name']:
                send_error(data['channel'], '@{}: This repository already exists in {}!'.format(user_data['name'], org), thread=data['ts'])
                return
    except Exception as e:
        send_error(data['channel'], '@{}: I encountered a problem:\n\n{}'.format(user_data['name'], e), thread=data['ts'])
        return
    try:
        visibility = True if not ORGS[org]['public_only'] else False
        self.create_new_repo(repo, org, visibility)
    except Exception as e:
        send_error(data['channel'], '@{}: I encountered a problem:\n\n{}'.format(user_data['name'], e), thread=data['ts'])
        return
    time.sleep(2)
    try:
        for perm_dict in ORGS[org]['new_repo_teams']:
            self.set_repo_permissions(repo, org, perm_dict['name'], perm_dict['perm'])
    except Exception as e:
        send_error(data['channel'], '@{}: I encountered a problem setting repo permissions for team {team}: \n\n{exc}'.format(user_data['name'], team=perm_dict['name'], exc=e), thread=data['ts'])
        return
    message = '@{}: The new repo: {} has been created in {}.\n'.format(user_data['name'], repo, org)
    message += 'You can access the repo at: https://github.com/{org}/{repo}\n'.format(org=org, repo=repo)
    visibility = 'PRIVATE' if visibility else 'PUBLIC'
    message += 'The repository is {visibility}.\nYou are free to set up the repo as you like.\n'.format(visibility=visibility)
    send_success(data['channel'], message, thread=data['ts'])",result,222,result,True,100.00000000000004,N/A
"@hubcommander_command(name='!SetDefaultBranch', usage='!SetDefaultBranch <OrgThatHasRepo> <Repo> <BranchName>', description='This will set the default branch on a GitHub repo.\n\nPlease Note: GitHub prefers lowercase branch names. You may encounter issues with uppercase letters.', required=[dict(name='org', properties=dict(type=str, help='The organization that contains the repo.'), validation_func=lookup_real_org, validation_func_kwargs={}), dict(name='repo', properties=dict(type=str, help='The name of the repo to set the default on.'), validation_func=extract_repo_name, validation_func_kwargs={}), dict(name='branch', properties=dict(type=str, help='The name of the branch to set as default. (Case-Sensitive)'), lowercase=False)], optional=[])
@auth()
@repo_must_exist()
@branch_must_exist()
def set_default_branch_command(self, data, user_data, org, repo, branch):
    """"""
        Sets the default branch of a repo.

        Command is as follows: !setdefaultbranch <organization> <repo> <branch>
        :param branch:
        :param repo:
        :param org:
        :param user_data:
        :param data:
        :return:
        """"""
    send_info(data['channel'], '@{}: Working, Please wait...'.format(user_data['name']), thread=data['ts'])
<mask>:
        return
    send_success(data['channel'], ""@{}: The {}/{} repository's default branch has been set to: `{}`."".format(user_data['name'], org, repo, branch), markdown=True, thread=data['ts'])","not self.make_repo_edit(data, user_data, repo, org, default_branch=branch)",129,not data['defaultbranch'],False,0.23266939806850076,N/A
"@hubcommander_command(name='!ListPRs', usage='!ListPRs <OrgThatHasRepo> <Repo> <State>', description='This will list pull requests for a repo.', required=[dict(name='org', properties=dict(type=str, help='The organization that contains the repo.'), validation_func=lookup_real_org, validation_func_kwargs={}), dict(name='repo', properties=dict(type=str, help='The name of the repo to list PRs on.'), validation_func=extract_repo_name, validation_func_kwargs={}), dict(name='state', properties=dict(type=str.lower, help='The state of the PR. Must be one of: `{values}`'), choices='permitted_states')], optional=[])
@auth()
@repo_must_exist()
def list_pull_requests_command(self, data, user_data, org, repo, state):
    """"""
        List the Pull Requests for a repo.

        Command is as follows: !listprs <organization> <repo> <state>
        :param state:
        :param repo:
        :param org:
        :param user_data:
        :param data:
        :return:
        """"""
    send_info(data['channel'], '@{}: Working, Please wait...'.format(user_data['name']), thread=data['ts'])
    pull_requests = self.get_repo_prs(data, user_data, repo, org, state)
<mask>:
        if isinstance(pull_requests, list):
            send_info(data['channel'], '@{}: No matching pull requests were found in *{}*.'.format(user_data['name'], repo), thread=data['ts'])
        return
    headers = ['#PR', 'Title', 'Opened by', 'Assignee', 'State']
    rows = []
    for pr in pull_requests:
        assignee = pr['assignee']['login'] if pr['assignee'] is not None else '-'
        rows.append([pr['number'], pr['title'], pr['user']['login'], assignee, pr['state'].title()])
    send_raw(data['channel'], text='Repository: *{}* \n\n```{}```'.format(repo, tabulate(rows, headers=headers, tablefmt='orgtbl')), thread=data['ts'])",not pull_requests,155,pull_requests,False,71.65313105737896,N/A
"def repo_must_exist(org_arg='org'):

    def command_decorator(func):

        def decorated_command(github_plugin, data, user_data, *args, **kwargs):
<mask>:
                repos = [kwargs['repo']]
            else:
                repos = kwargs['repos']
            for repo in repos:
                if not github_plugin.check_if_repo_exists(data, user_data, repo, kwargs[org_arg]):
                    return
            return func(github_plugin, data, user_data, *args, **kwargs)
        return decorated_command
    return command_decorator",kwargs.get('repo'),39,org_arg == 'org',False,0.0,N/A
"def team_must_exist(org_arg='org', team_arg='team'):

    def command_decorator(func):

        def decorated_command(github_plugin, data, user_data, *args, **kwargs):
            team_id = github_plugin.find_team_id_by_name(kwargs[org_arg], kwargs[team_arg])
<mask>:
                send_error(data['channel'], '@{}: The GitHub team: {} does not exist.'.format(user_data['name'], kwargs[team_arg]), thread=data['ts'])
                return
            return func(github_plugin, data, user_data, *args, **kwargs)
        return decorated_command
    return command_decorator",not team_id,38,not team_id,True,100.00000000000004,N/A
"def github_user_exists(user_arg):

    def command_decorator(func):

        def decorated_command(github_plugin, data, user_data, *args, **kwargs):
            try:
                found_user = github_plugin.get_github_user(kwargs[user_arg])
<mask>:
                    send_error(data['channel'], '@{}: The GitHub user: {} does not exist.'.format(user_data['name'], kwargs[user_arg]), thread=data['ts'])
                    return
            except Exception as e:
                send_error(data['channel'], ""@{}: A problem was encountered communicating with GitHub to verify the user's GitHub id. Here are the details:\n{}"".format(user_data['name'], str(e)), thread=data['ts'])
                return
            return func(github_plugin, data, user_data, *args, **kwargs)
        return decorated_command
    return command_decorator",not found_user,63,not found_user,True,100.00000000000004,N/A
"def branch_must_exist(repo_arg='repo', org_arg='org', branch_arg='branch'):
    """"""
    This should be placed AFTER the `@repo_must_exist()` decorator.
    :param repo_arg:
    :param org_arg:
    :param branch_arg:
    :param kwargs:
    :return:
    """"""

    def command_decorator(func):

        def decorated_command(github_plugin, data, user_data, *args, **kwargs):
<mask>:
                send_error(data['channel'], '@{}: This repository does not have the branch: `{}`.'.format(user_data['name'], kwargs[branch_arg]), markdown=True, thread=data['ts'])
                return
            return func(github_plugin, data, user_data, *args, **kwargs)
        return decorated_command
    return command_decorator","not github_plugin.check_for_repo_branch(kwargs[repo_arg], kwargs[org_arg], kwargs[branch_arg])",56,branch_arg in kwargs,False,0.1293634803852345,N/A
"def setup(self, secrets, **kwargs):
    from hubcommander.command_plugins.enabled_plugins import COMMAND_PLUGINS
<mask>:
        self.commands = {}
        print('[X] Travis CI Plugin is not enabling any commands because the GitHub plugin is not enabled.')
        return
    self.org_lookup = {}
    for org in ORGS.items():
        self.org_lookup[org[0].lower()] = (org[0], org[1])
        for alias in org[1]['aliases']:
            self.org_lookup[alias] = (org[0], org[1])
    self.credentials = {'pro': {'user': secrets['TRAVIS_PRO_USER'], 'id': secrets['TRAVIS_PRO_ID'], 'token': secrets['TRAVIS_PRO_TOKEN']}, 'public': {'user': secrets['TRAVIS_PUBLIC_USER'], 'id': secrets['TRAVIS_PUBLIC_ID'], 'token': secrets['TRAVIS_PUBLIC_TOKEN']}}
    for cmd, keys in USER_COMMAND_DICT.items():
        self.commands[cmd].update(keys)",not COMMAND_PLUGINS.get('github'),70,not COMMAND_PLUGINS.get('github'),True,100.00000000000004,N/A
"@hubcommander_command(name='!EnableTravis', usage='!EnableTravis <OrgWithRepo> <Repo> [--public=true]', description='This will enable Travis CI on a GitHub repository.', required=[dict(name='org', properties=dict(type=str, help='The organization that contains the repo.'), validation_func=lookup_real_org, validation_func_kwargs={}), dict(name='repo', properties=dict(type=str, help='The repository to enable Travis CI on.'), validation_func=extract_repo_name, validation_func_kwargs={})], optional=[dict(name='--public', properties=dict(type=str, help='When set to true - attempts to enable Travis CI using the public travis-ci.org'))])
@auth()
def enable_travis_command(self, data, user_data, org, repo, public):
    """"""
        Enables Travis CI on a repository within the organization.

        Command is as follows: !enabletravis <organization> <repo> [--public=true]
        :param public:
        :param repo:
        :param org:
        :param user_data:
        :param data:
        :return:
        """"""
    from hubcommander.command_plugins.enabled_plugins import COMMAND_PLUGINS
    github_plugin = COMMAND_PLUGINS['github']
    send_info(data['channel'], '@{}: Working, Please wait...'.format(user_data['name']), thread=data['ts'])
    try:
        repo_result = github_plugin.check_gh_for_existing_repo(repo, org)
<mask>:
            send_error(data['channel'], '@{}: This repository does not exist in {}!'.format(user_data['name'], org), thread=data['ts'])
            return
    except Exception as e:
        send_error(data['channel'], '@{}: I encountered a problem:\n\n{}'.format(user_data['name'], e), thread=data['ts'])
        return
    which = 'public' if public and public.lower() == 'true' else 'pro'
    try:
        send_info(data['channel'], ':skull: Need to sync Travis CI with GitHub. Please wait...', thread=data['ts'])
        self.sync_with_travis(which)
        send_info(data['channel'], ':guitar: Synced! Going to enable Travis CI on the repo now...', thread=data['ts'])
        travis_data = self.look_for_repo(which, repo_result)
        if not travis_data:
            send_error(data['channel'], ""@{}: Couldn't find the repo in Travis for some reason...\n\n"".format(user_data['name']), thread=data['ts'])
            return
        if travis_data['active']:
            send_success(data['channel'], '@{}: Travis CI is already enabled on {}/{}.\n\n'.format(user_data['name'], org, repo), thread=data['ts'])
            return
        self.enable_travis_on_repo(which, repo_result)
    except Exception as e:
        send_error(data['channel'], '@{}: I encountered a problem communicating with Travis CI:\n\n{}'.format(user_data['name'], e), thread=data['ts'])
        return
    message = '@{}: Travis CI has been enabled on {}/{}.\n\n'.format(user_data['name'], org, repo)
    send_success(data['channel'], message, thread=data['ts'])",not repo_result,240,repo_result is None,False,39.76353643835252,N/A
"def sync_with_travis(self, which):
    """"""
        Syncs Travis CI with GitHub to ensure that it can see all the latest
        :param which:
        :return:
        """"""
    result = requests.post('{base}/user/{userid}/sync'.format(base=TRAVIS_URLS[which], userid=self.credentials[which]['id']), headers=self._make_headers(which))
<mask>:
        raise TravisCIException('Travis CI Status Code: {}'.format(result.status_code))
    time.sleep(2)
    while True:
        response = requests.get('{base}/user/{userid}'.format(base=TRAVIS_URLS[which], userid=self.credentials[which]['id']), headers=self._make_headers(which))
        if response.status_code != 200:
            raise TravisCIException('Sync Status Code: {}'.format(response.status_code))
        result = json.loads(response.text)
        if not result['is_syncing']:
            break
        time.sleep(2)",result.status_code != 200,59,result.status_code != 200,True,100.00000000000004,N/A
"def look_for_repo(self, which, repo_dict):
    """"""
        This will check if a repository is currently seen in Travis CI.
        :param which:
        :param repo_dict:
        :return:
        """"""
    result = requests.get('{base}/repo/{id}'.format(base=TRAVIS_URLS[which], id=repo_dict['full_name'].replace('/', '%2F')), headers=self._make_headers(which))
<mask>:
        return None
    elif result.status_code != 200:
        raise TravisCIException('Repo Lookup Status Code: {}'.format(result.status_code))
    return json.loads(result.text)",result.status_code == 404,44,result.status_code == 404,True,100.00000000000004,N/A
"def enable_travis_on_repo(self, which, repo_dict):
    """"""
        This will enable Travis CI on a specified repository.
        :param which:
        :param repo_dict:
        :return:
        """"""
    result = requests.post('{base}/repo/{repo}/activate'.format(base=TRAVIS_URLS[which], repo=repo_dict['full_name'].replace('/', '%2F')), headers=self._make_headers(which))
<mask>:
        raise TravisCIException('Enable Repo Status Code: {}'.format(result.status_code))",result.status_code != 200,33,result.status_code != 200,True,100.00000000000004,N/A
"def slack_client_side_effect(*args, **kwargs):
<mask>:
        if kwargs['user'] == 'error':
            return {'error': 'Error'}
        return USER_DATA",args[0] == 'users.info',13,kwargs.get('user'),False,4.923026124015933,N/A
"@pytest.mark.parametrize('markdown', [True, False])
def test_send_error(slack_client, markdown):
    from hubcommander.bot_components.slack_comm import send_error
    attachment = {'text': 'ʕ•ᴥ•ʔ', 'color': 'danger'}
<mask>:
        attachment['mrkdwn_in'] = ['text']
    send_error('some_channel', attachment['text'], markdown)
    actually_said('some_channel', [attachment], slack_client)",markdown,26,markdown,True,100.00000000000004,N/A
"@pytest.mark.parametrize('markdown', [True, False])
def test_send_info(slack_client, markdown):
    from hubcommander.bot_components.slack_comm import send_info, WORKING_COLOR
    attachment = {'text': 'ʕ•ᴥ•ʔ', 'color': WORKING_COLOR}
<mask>:
        attachment['mrkdwn_in'] = ['text']
    send_info('some_channel', attachment['text'], markdown)
    actually_said('some_channel', [attachment], slack_client)",markdown,27,markdown,True,100.00000000000004,N/A
"@pytest.mark.parametrize('markdown', [True, False])
def test_send_success(slack_client, markdown):
    from hubcommander.bot_components.slack_comm import send_success
    attachment = {'text': 'ʕ•ᴥ•ʔ', 'color': 'good'}
<mask>:
        attachment['mrkdwn_in'] = ['text']
    send_success('some_channel', attachment['text'], markdown)
    actually_said('some_channel', [attachment], slack_client)",markdown,26,markdown,True,100.00000000000004,N/A
"def say(channel, attachments, text=None, ephemeral_user=None, thread=None):
    """"""
    Sends a message (with attachments) to Slack. Use the send_* methods instead.
    :param channel:
    :param attachments:
    :param text:
    :param ephemeral_user:ID of the user who will receive the ephemeral message
    :param thread:
    :return:
    """"""
    kwargs_to_send = {'channel': channel, 'text': text if text else ' ', 'attachments': json.dumps(attachments), 'as_user': True}
    verb = 'chat.postMessage'
<mask>:
        kwargs_to_send['user'] = ephemeral_user
        verb = 'chat.postEphemeral'
    if thread:
        kwargs_to_send['thread_ts'] = thread
    bot_components.SLACK_CLIENT.api_call(verb, **kwargs_to_send)",ephemeral_user,72,ephemeral_user,True,100.00000000000004,N/A
"def send_error(channel, text, markdown=False, ephemeral_user=None, thread=None):
    """"""
    Sends an ""error"" message to Slack.
    :param channel:
    :param text:
    :param markdown: If True, then look for markdown in the message.
    :param ephemeral_user:ID of the user who will receive the ephemeral message
    :param thread:
    :return:
    """"""
    attachment = {'text': text, 'color': 'danger'}
<mask>:
        attachment['mrkdwn_in'] = ['text']
    say(channel, [attachment], ephemeral_user=ephemeral_user, thread=thread)",markdown,57,markdown,True,100.00000000000004,N/A
"def send_info(channel, text, markdown=False, ephemeral_user=None, thread=None):
    """"""
    Sends an ""info"" message to Slack.
    :param channel:
    :param text:
    :param markdown: If True, then look for markdown in the message.
    :param ephemeral_user:ID of the user who will receive the ephemeral message
    :param thread:
    :return:
    """"""
    attachment = {'text': text, 'color': WORKING_COLOR}
<mask>:
        attachment['mrkdwn_in'] = ['text']
    say(channel, [attachment], ephemeral_user=ephemeral_user, thread=thread)",markdown,57,markdown,True,100.00000000000004,N/A
"def send_success(channel, text, markdown=False, ephemeral_user=None, thread=None):
    """"""
    Sends an ""success"" message to Slack.
    :param channel:
    :param text:
    :param markdown: If True, then look for markdown in the message.
    :param ephemeral_user:ID of the user who will receive the ephemeral message
    :param thread:
    :return:
    """"""
    attachment = {'text': text, 'color': 'good'}
<mask>:
        attachment['mrkdwn_in'] = ['text']
    say(channel, [attachment], ephemeral_user=ephemeral_user, thread=thread)",markdown,57,markdown,True,100.00000000000004,N/A
"def get_user_data(data):
    """"""
    Gets information about the calling user from the Slack API.
    NOTE: Must be called after get_tokens()

    :param data:
    :return:
    """"""
    result = bot_components.SLACK_CLIENT.api_call('users.info', user=data['user'])
<mask>:
        return (None, result['error'])
    else:
        return (result['user'], None)",result.get('error'),35,result['error'],False,11.521590992286539,N/A
"def preformat_args_with_spaces(text, num_quoted):
    """"""
    THIS METHOD IS DEPRECATED! USE THE DECORATORS FOR PARSING!!

    This method will not only strip out the things that need to be stripped out, but it will also
    ensure that double-quoted objects are extracted as independent arguments.

    For example, if the text passed in was:
    !SetDescription Netflix HubCommander ""A Slack bot for GitHub management"", we would want to get back:
    a list that contains: [""netflix"", ""hubcommander"", '""A Slack bot for GitHub management""']

    The `num_quoted` param refers to the number of double-quoted parameters are required for the command.
    For the example above, there is one double-quoted parameter required for the command.

    :param text:
    :param num_quoted:
    :return:
    """"""
    warnings.simplefilter('always', DeprecationWarning)
    warnings.warn(""The function: 'preformat_args_with_spaces' is deprecated. Please use the decorators for argument parsing."", DeprecationWarning)
    working = text.replace('[', '').replace(']', '').replace('{', '').replace('}', '').replace(u'“', '""').replace(u'”', '""').replace(u'‘', ""'"").replace(u'’', ""'"")
<mask>:
        raise SystemExit()
    if working.count('""') % 2 != 0:
        raise SystemExit()
    quotes = working.split('""')[1::2]
    if len(quotes) != num_quoted:
        raise SystemExit()
    working = working.replace('""', '')
    for quote in quotes:
        working = working.replace(quote, '')
    working = working.lower()
    if num_quoted > 1:
        working = working[0:-(num_quoted - 1)]
    space_delimited = working.split(' ')[1:-1]
    return space_delimited + quotes","working.count('""') < 2",188,"working.count('""') % 2 != 0",False,57.60844201603898,N/A
"def extract_repo_name(plugin_obj, reponame, **kwargs):
    """"""
    Reponames can be FQDN's. Slack has an annoying habit of sending over URL's like so:
    <http://www.foo.com|www.foo.com>
    ^^ Need to pull out the URL. In our case, we care only about the label, which is the last part between | and >
    :param plugin_obj: Not used
    :param reponame:
    :return:
    """"""
<mask>:
        return reponame
    split_repo = reponame.split('|')[1]
    return split_repo.replace('>', '')",'|' not in reponame,63,not reponame.startswith('http://www.foo.com'),False,3.1251907639724417,N/A
"def parse_toggles(plugin_obj, toggle, toggle_type='toggle', **kwargs):
    """"""
    Parses typical toggle values, like off, on, enabled, disabled, true, false, etc.
    :param plugin_obj: Not used
    :param toggle_type:
    :param toggle:
    :return:
    """"""
    toggle = toggle.lower()
<mask>:
        return True
    elif toggle in TOGGLE_OFF_VALUES:
        return False
    raise ParseException(toggle_type, 'Acceptable values are: `{on}, {off}`'.format(on=', '.join(TOGGLE_ON_VALUES), off=', '.join(TOGGLE_OFF_VALUES)))",toggle in TOGGLE_ON_VALUES,50,toggle in TOGGLE_ON_VALUES,True,100.00000000000004,N/A
"def format_help_text(data, user_data, **kwargs):
    full_help_text = '@{user}: `{command_name}`: {description}\n\n```{usage}```\n\n{required}{optional}'
    required_args = []
<mask>:
        required_args.append('Required Arguments:')
        for required in kwargs['required']:
            if type(required['name']) is list:
                required_args.append('\t`{name}`\t{help}'.format(name=', '.join(required['name']), help=required['properties']['help']))
            else:
                required_args.append('\t`{name}`\t{help}'.format(name=required['name'], help=required['properties']['help']))
        required_args = '\n'.join(required_args) + '\n\n'
    optional_args = ['Optional Arguments:', '\t`-h, --help`\tShow this help text.']
    if kwargs.get('optional'):
        for optional in kwargs['optional']:
            if type(optional['name']) is list:
                optional_args.append('\t`{name}`\t{help}'.format(name=', '.join(optional['name']), help=optional['properties']['help']))
            else:
                optional_args.append('\t`{name}`\t{help}'.format(name=optional['name'], help=optional['properties']['help']))
    optional_args = '\n'.join(optional_args)
    return full_help_text.format(user=user_data['name'], command_name=kwargs['name'], description=kwargs['description'], usage=kwargs['usage'], required=required_args if required_args else '', optional=optional_args if optional_args else '')",kwargs.get('required'),77,kwargs.get('required'),True,100.00000000000004,N/A
"def perform_additional_verification(plugin_obj, args, **kwargs):
    """"""
    This will run the custom verification functions that you can set for parameters.

    This will also, by default, lowercase all values that arrive. This behavior can be disabled
    via the lowercase=False flag for the argument.
    :param plugin_obj:
    :param args:
    :param kwargs:
    :return:
    """"""
    for at in ARG_TYPE:
<mask>:
            for argument in kwargs[at]:
                real_arg_name = argument['name'].replace('--', '')
                if args.get(real_arg_name):
                    if type(args[real_arg_name]) is str:
                        if argument.get('uppercase', False):
                            args[real_arg_name] = args[real_arg_name].upper()
                        elif argument.get('lowercase', True):
                            args[real_arg_name] = args[real_arg_name].lower()
                        if argument.get('cleanup', True):
                            args[real_arg_name] = args[real_arg_name].replace('<', '').replace('>', '').replace('{', '').replace('}', '').replace('[', '').replace(']', '').replace('&lt;', '').replace('&gt;', '')
                    if argument.get('validation_func'):
                        validation_kwargs = {}
                        if argument.get('validation_func_kwargs'):
                            validation_kwargs = argument['validation_func_kwargs']
                        args[real_arg_name] = argument['validation_func'](plugin_obj, args[real_arg_name], **validation_kwargs)
    return args",kwargs.get(at),110,at in kwargs,False,12.753667906901528,N/A
"def hubcommander_command(**kwargs):

    def command_decorator(func):

        def decorated_command(plugin_obj, data, user_data):
            parser = argparse.ArgumentParser(prog=kwargs['name'], description=kwargs['description'], usage=kwargs['usage'])
            arg_type = ['required', 'optional']
            for at in arg_type:
<mask>:
                    for argument in kwargs[at]:
                        if argument.get('choices'):
                            argument['properties']['choices'] = plugin_obj.commands[kwargs['name']][argument['choices']]
                            argument['properties']['help'] = argument['properties']['help'].format(values=', '.join(plugin_obj.commands[kwargs['name']][argument['choices']]))
                        parser.add_argument(argument['name'], **argument['properties'])
            data['text'] = data['text'].replace(u'“', '""').replace(u'”', '""').replace(u'‘', ""'"").replace(u'’', ""'"")
            split_args = shlex.split(data['text'])[1:]
            try:
                args = vars(parser.parse_args(split_args))
            except SystemExit as _:
                send_info(data['channel'], format_help_text(data, user_data, **kwargs), markdown=True, ephemeral_user=user_data['id'])
                return
            try:
                args = perform_additional_verification(plugin_obj, args, **kwargs)
            except ParseException as pe:
                send_error(data['channel'], pe.format_proper_usage(user_data['name']), markdown=True, ephemeral_user=user_data['id'])
                return
            except Exception as e:
                send_error(data['channel'], 'An exception was encountered while running validation for the input. The exception details are: `{}`'.format(str(e)), markdown=True)
                return
            data['command_name'] = kwargs['name']
            return func(plugin_obj, data, user_data, **args)
        return decorated_command
    return command_decorator",kwargs.get(at),111,at in kwargs,False,12.753667906901528,N/A
"def auth(**kwargs):

    def command_decorator(func):

        def decorated_command(command_plugin, data, user_data, *args, **kwargs):
<mask>:
                if not command_plugin.commands[data['command_name']]['auth']['plugin'].authenticate(data, user_data, *args, **command_plugin.commands[data['command_name']]['auth']['kwargs']):
                    return
            return func(command_plugin, data, user_data, *args, **kwargs)
        return decorated_command
    return command_decorator",command_plugin.commands[data['command_name']].get('auth'),28,data['command_name'] in command_plugin.commands.keys(),False,49.74531436214772,N/A
"def fix_ttf(ttfpath: Path, name: str):
    print(crayons.yellow(f'Generating TTF for {name}...'))
    version = '1.0'
    with SFDPATH.open() as f:
        for line in f.readlines():
<mask>:
                version = line.split()[1]
                break
    with NamedTemporaryFile() as sfd:
        subprocess.run([f""""""fontforge -c 'f = open(""{ttfpath}""); f.os2_version = 4; f.os2_weight_width_slope_only = True; f.save(""{sfd.name}"")'""""""], cwd=BUILD_DIR, shell=True, check=True)
        script = ';\n'.join([f'Open(""{sfd.name}"")', 'SelectWorthOutputting()', 'RemoveOverlap()', 'CorrectDirection()', 'ScaleToEm(2048)', 'RenameGlyphs(""AGL with PUA"")', 'Reencode(""unicode"")', f'SetTTFName(0x409, 3, ""{name}"")', f'SetTTFName(0x409, 5, ""{version}"")', f'SetTTFName(0x409, 8, ""Slavfox"")', f'SetTTFName(0x409, 9, ""Slavfox"")', f'SetTTFName(0x409, 11, ""https://github.com/slavfox/Cozette"")', f""SetTTFName(0x409, 13, LoadStringFromFile({repr(str((REPO_ROOT / 'LICENSE').resolve()))}))"", 'SetTTFName(0x409, 14, ""https://github.com/slavfox/Cozette/blob/master/LICENSE"")', f'Generate(""{name}.dfont"")', f'Generate(""{name}.otf"")', f'Generate(""{name}.ttf"")', f'Generate(""{name}.woff"")', f'Generate(""{name}.woff2"")'])
        with NamedTemporaryFile(mode='w+', suffix='.pe') as f:
            print(f.name)
            f.write(script)
            f.flush()
            f.seek(0)
            subprocess.run([f'fontforge -script {f.name}'], cwd=BUILD_DIR, shell=True, check=True)
    ttfpath.unlink()",line.startswith('Version '),99,line.startswith('Version'),False,45.48019047027906,N/A
"def variant(sfd_path: Path, variant_name: str, variant_source: Path) -> tuple[Path, str]:
    outpath = BUILD_DIR / f'{variant_name}.sfd'
    replacements = {}
    with variant_source.open() as f:
        for line in f:
<mask>:
                replacements[int(codepoint.group(1))] = next(f)
    with outpath.open('w') as f:
        with sfd_path.open() as src:
            replacement = None
            for line in src:
                if replacement is not None:
                    f.write(replacement)
                    replacement = None
                elif line.startswith('FAMILY_NAME'):
                    f.write(line)
                else:
                    f.write(line.replace('Cozette', variant_name))
                if (codepoint := BDF_CODEPOINT_RE.match(line)):
                    if int(codepoint.group(1)) in replacements:
                        replacement = replacements[int(codepoint.group(1))]
    return (outpath, variant_name)",(codepoint := BDF_CODEPOINT_RE.match(line)),74,(codepoint := BDF_CODEPOINT_RE.match(line)),True,100.00000000000004,N/A
"def scan_for_codepoints(dir: Path) -> Dict[int, List[Path]]:
<mask>:
        return {cp: [dir] for cp in scan_file_for_nonascii(dir)}
    non_ascii_codepoints: Dict[int, List[Path]] = {}
    for path in dir.glob('**/*'):
        if path.is_file():
            for cp in scan_file_for_nonascii(path):
                non_ascii_codepoints.setdefault(cp, []).append(path)
    return non_ascii_codepoints",dir.is_file(),33,dir.is_file(),True,100.00000000000004,N/A
"def print_codepoints_for_changelog(codepoints: Dict[int, List[Path]], print_source_files=False, reverse=False) -> None:
    for cp in sorted(codepoints, reverse=reverse):
        ch = chr(cp)
        try:
            name = uniname(ch).strip()
        except ValueError:
            name = ''
<mask>:
            if 'VARIATION SELECTOR' in name:
                continue
            name = ' ' + name
        print(f'**{ch} U+{cp:04X}{name}** {eaw(ch)}', end='')
        if print_source_files:
            print(f"": {' '.join((str(p) for p in codepoints[cp]))}"")
        else:
            print('\n', end='')",name,54,name,True,100.00000000000004,N/A
"def wrap_text(src: str, width=79) -> Sample:
    sample_h = 1
    running_w = 0
    idx = 0
    linebreaks = []
    while idx < len(src):
<mask>:
            linebreaks.append(idx - 1)
            running_w = 0
            sample_h += 1
        if any(((h := f'${color}$') == src[idx:len(h)] for color in default_palette.keys())):
            idx += len(h)
        else:
            running_w += 1 if charwidth(src[idx]) != 'W' else 2
            idx += 1
    for idx in reversed(linebreaks):
        src = src[:idx].rstrip() + '\n' + src[idx:].lstrip()
    return Sample(src, width, sample_h)",running_w - int(src[idx] == ' ') > width,73,running_w == 0,False,6.074116824213336,N/A
"def save_sample(fnt: str, sample: Sample, output_path: Path, fgcolor: str='#abb2bf', bgcolor: str='#282c34', palette: Palette=None):
<mask>:
        palette = default_palette
    colored = color_text(sample.text, palette)
    with tempfile.NamedTemporaryFile('w', delete=False) as f:
        f.write(colored)
        fp = f.name
    cmd = quote(f'tput civis &&cat {fp} && sleep 1 && import -window $WINDOWID {output_path}')
    subprocess.run(['xterm', '-en', 'utf8', '-fg', fgcolor, '-bg', bgcolor, '-fa', fnt, '-geometry', f'{sample.width}x{sample.height - 1}+100+100', '-dc', '-cu', '-cn', '-e', f'bash -c {cmd}'], stdin=subprocess.PIPE, stdout=subprocess.PIPE)
    Path(fp).unlink()",palette is None,67,palette is None,True,100.00000000000004,N/A
"def make_charmap(sfd: Path) -> List[str]:
    text = ['        0 1 2 3 4 5 6 7 8 9 A B C D E F', '       ┌───────────────────────────────']
    codepoints = sfd_codepoints(sfd)
    for i in range(0, codepoints[-1] + 16, 16):
        line = ''
        for j in range(16):
<mask>:
                ch = chr(cp)
            else:
                ch = ' '
            if 768 <= cp < 880 and cp in codepoints:
                line += ' '
            line += ch
            if charwidth(ch) not in 'FW':
                line += ' '
        if (line := line.rstrip()):
            text.append(f'U+{i // 16:04X}_│{line}')
    return text","(cp := (i + j)) in codepoints and (not category(chr(cp)).startswith(('Z', 'Cc', 'Cf')))",88,cp < 880,False,0.0008951104098702025,N/A
"def sfd_codepoints(sfd: Path) -> List[int]:
    codepoints = []
    with sfd.open() as f:
        chars = False
        for line in f:
<mask>:
                if line.startswith('Encoding:'):
                    codepoints.append(int(line.split(maxsplit=2)[1]))
            elif line.startswith('BeginChars'):
                chars = True
    return sorted(codepoints)",chars,30,not chars,False,49.99999999999999,N/A
"def get_codepoints(cozette_sfd: str):
    codepoints = {}
    current_codepoint = None
    for line in cozette_sfd.splitlines():
<mask>:
            codepoints[current_codepoint] = line
            current_codepoint = None
        elif (match := char_regex.match(line)):
            current_codepoint = int(match.group(2))
    return codepoints",current_codepoint,29,current_codepoint,True,100.00000000000004,N/A
"def get_changelog():
    previous_codepoints = get_codepoints(get_last_cozette_sfd())
    with COZETTE_SFD.open() as f:
        current_codepoints = get_codepoints(f.read())
    print(f'Changelog since {get_last_ver()}: {len(current_codepoints)} glyphs found')
    added = set(current_codepoints) - set(previous_codepoints)
    removed = set(previous_codepoints) - set(current_codepoints)
    changed = set()
    for k, v in current_codepoints.items():
<mask>:
            changed.add(k)
    if added:
        print('### Added\n')
        for codepoint in sorted(added):
            print_codepoint(codepoint)
        print('')
    if changed:
        print('### Changed\n')
        for codepoint in sorted(changed):
            print_codepoint(codepoint)
        print('')
    if removed:
        print('### Removed\n')
        for codepoint in sorted(removed):
            print_codepoint(codepoint)
        print('')",k in previous_codepoints and previous_codepoints[k] != v,68,v != previous_codepoints[k],False,34.931613812566766,N/A
"def double_size(src: TextIO, out: TextIO):
    bitmap = False
    for line in src.readlines():
<mask>:
            bitmap = False
        elif bitmap:
            line = line.strip()
            pad = len(line) % 2
            line = line + '0' * pad
            size = len(line) * 2
            binary = bin(int(line.strip(), 16))[2:]
            rescaled = ''.join([x * 2 for x in binary])
            res = hex(int(rescaled, 2))[2:].upper()
            line = '0' * (size - len(res)) + res
            line = (line + '\n') * 2
        elif any([line.startswith(x) for x in scale_lines]):
            words = line.split()
            for i, num in enumerate(words[1:]):
                words[i + 1] = str(int(num) * 2)
            line = ' '.join(words) + '\n'
        elif line.startswith('SIZE '):
            words = line.strip().split()
            words[1] = str(int(words[1]) * 2)
            line = ' '.join(words) + '\n'
        elif line.startswith('FONT '):
            xlfd = line[len('FONT '):].strip().split('-')
            xlfd[7] = f'{int(xlfd[7]) * 2}'
            xlfd[8] = f'{int(xlfd[8]) * 2}'
            xlfd[12] = f'{int(xlfd[12]) * 2}'
            line = 'FONT ' + '-'.join(xlfd) + '\n'
        elif line.startswith('BITMAP'):
            bitmap = True
        if line.startswith('FAMILY_NAME'):
            out.write(line)
            continue
        out.write(line.replace('Cozette', 'CozetteHiDpi'))",line.startswith('ENDCHAR'),156,line.startswith('BITMAP'),False,53.7284965911771,N/A
"def mfcc(signal, sampling_frequency, frame_length=0.02, frame_stride=0.01, num_cepstral=13, num_filters=40, fft_length=512, low_frequency=0, high_frequency=None, dc_elimination=True):
    """"""Compute MFCC features from an audio signal.

    Args:

         signal (array): the audio signal from which to compute features.
             Should be an N x 1 array
         sampling_frequency (int): the sampling frequency of the signal
             we are working with.
         frame_length (float): the length of each frame in seconds.
             Default is 0.020s
         frame_stride (float): the step between successive frames in seconds.
             Default is 0.02s (means no overlap)
         num_filters (int): the number of filters in the filterbank,
             default 40.
         fft_length (int): number of FFT points. Default is 512.
         low_frequency (float): lowest band edge of mel filters.
             In Hz, default is 0.
         high_frequency (float): highest band edge of mel filters.
             In Hz, default is samplerate/2
         num_cepstral (int): Number of cepstral coefficients.
         dc_elimination (bool): hIf the first dc component should
             be eliminated or not.

    Returns:
        array: A numpy array of size (num_frames x num_cepstral) containing mfcc features.
    """"""
    feature, energy = mfe(signal, sampling_frequency=sampling_frequency, frame_length=frame_length, frame_stride=frame_stride, num_filters=num_filters, fft_length=fft_length, low_frequency=low_frequency, high_frequency=high_frequency)
<mask>:
        return np.empty((0, num_cepstral))
    feature = np.log(feature)
    feature = dct(feature, type=2, axis=-1, norm='ortho')[:, :num_cepstral]
    if dc_elimination:
        feature[:, 0] = np.log(energy)
    return feature",len(feature) == 0,186,feature is None,False,7.253154775624655,N/A
"def stack_frames(sig, sampling_frequency, frame_length=0.02, frame_stride=0.02, filter=lambda x: np.ones((x,)), zero_padding=True):
    """"""Frame a signal into overlapping frames.

    Args:
        sig (array): The audio signal to frame of size (N,).
        sampling_frequency (int): The sampling frequency of the signal.
        frame_length (float): The length of the frame in second.
        frame_stride (float): The stride between frames.
        filter (array): The time-domain filter for applying to each frame.
            By default it is one so nothing will be changed.
        zero_padding (bool): If the samples is not a multiple of
            frame_length(number of frames sample), zero padding will
            be done for generating last frame.

    Returns:
            array: Stacked_frames-Array of frames of size (number_of_frames x frame_len).

    """"""
    s = 'Signal dimention should be of the format of (N,) but it is %s instead'
    assert sig.ndim == 1, s % str(sig.shape)
    length_signal = sig.shape[0]
    frame_sample_length = int(np.round(sampling_frequency * frame_length))
    frame_stride = float(np.round(sampling_frequency * frame_stride))
<mask>:
        numframes = int(math.ceil((length_signal - frame_sample_length) / frame_stride))
        print(numframes, length_signal, frame_sample_length, frame_stride)
        len_sig = int(numframes * frame_stride + frame_sample_length)
        additive_zeros = np.zeros((len_sig - length_signal,))
        signal = np.concatenate((sig, additive_zeros))
    else:
        numframes = int(math.floor((length_signal - frame_sample_length) / frame_stride))
        len_sig = int((numframes - 1) * frame_stride + frame_sample_length)
        signal = sig[0:len_sig]
    indices = np.tile(np.arange(0, frame_sample_length), (numframes, 1)) + np.tile(np.arange(0, numframes * frame_stride, frame_stride), (frame_sample_length, 1)).T
    indices = np.array(indices, dtype=np.int32)
    frames = signal[indices]
    window = np.tile(filter(frame_sample_length), (numframes, 1))
    Extracted_Frames = frames * window
    return Extracted_Frames",zero_padding,220,zero_padding,True,100.00000000000004,N/A
"def log_power_spectrum(frames, fft_points=512, normalize=True):
    """"""Log power spectrum of each frame in frames.

    Args:
        frames (array): The frame array in which each row is a frame.
        fft_points (int): The length of FFT. If fft_length is greater than
            frame_len, the frames will be zero-padded.
        normalize (bool): If normalize=True, the log power spectrum
            will be normalized.

    Returns:
           array: The power spectrum - If frames is an
           num_frames x sample_per_frame matrix, output will be
           num_frames x fft_length.
    """"""
    power_spec = power_spectrum(frames, fft_points)
    power_spec[power_spec <= 1e-20] = 1e-20
    log_power_spec = 10 * np.log10(power_spec)
<mask>:
        return log_power_spec - np.max(log_power_spec)
    else:
        return log_power_spec",normalize,96,normalize,True,100.00000000000004,N/A
"def cmvn(vec, variance_normalization=False):
    """""" This function is aimed to perform global cepstral mean and
        variance normalization (CMVN) on input feature vector ""vec"".
        The code assumes that there is one observation per row.

    Args:
        vec (array): input feature matrix
            (size:(num_observation,num_features))
        variance_normalization (bool): If the variance
            normilization should be performed or not.

    Return:
          array: The mean(or mean+variance) normalized feature vector.
    """"""
    eps = 2 ** (-30)
    rows, cols = vec.shape
    norm = np.mean(vec, axis=0)
    norm_vec = np.tile(norm, (rows, 1))
    mean_subtracted = vec - norm_vec
<mask>:
        stdev = np.std(mean_subtracted, axis=0)
        stdev_vec = np.tile(stdev, (rows, 1))
        output = mean_subtracted / (stdev_vec + eps)
    else:
        output = mean_subtracted
    return output",variance_normalization,105,variance_normalization,True,100.00000000000004,N/A
"def cmvnw(vec, win_size=301, variance_normalization=False):
    """""" This function is aimed to perform local cepstral mean and
    variance normalization on a sliding window. The code assumes that
    there is one observation per row.

    Args:
        vec (array): input feature matrix
            (size:(num_observation,num_features))
        win_size (int): The size of sliding window for local normalization.
            Default=301 which is around 3s if 100 Hz rate is
            considered(== 10ms frame stide)
        variance_normalization (bool): If the variance normilization should
            be performed or not.

    Return:
          array: The mean(or mean+variance) normalized feature vector.
    """"""
    eps = 2 ** (-30)
    rows, cols = vec.shape
    assert isinstance(win_size, int), ""Size must be of type 'int'!""
    assert win_size % 2 == 1, 'Windows size must be odd!'
    pad_size = int((win_size - 1) / 2)
    vec_pad = np.lib.pad(vec, ((pad_size, pad_size), (0, 0)), 'symmetric')
    mean_subtracted = np.zeros(np.shape(vec), dtype=np.float32)
    for i in range(rows):
        window = vec_pad[i:i + win_size, :]
        window_mean = np.mean(window, axis=0)
        mean_subtracted[i, :] = vec[i, :] - window_mean
<mask>:
        variance_normalized = np.zeros(np.shape(vec), dtype=np.float32)
        vec_pad_variance = np.lib.pad(mean_subtracted, ((pad_size, pad_size), (0, 0)), 'symmetric')
        for i in range(rows):
            window = vec_pad_variance[i:i + win_size, :]
            window_variance = np.std(window, axis=0)
            variance_normalized[i, :] = mean_subtracted[i, :] / (window_variance + eps)
        output = variance_normalized
    else:
        output = mean_subtracted
    return output",variance_normalization,196,variance_normalization,True,100.00000000000004,N/A
"def test_parse_url():
    file_id = '0B_NiLAzvehC9R2stRmQyM3ZiVjQ'
    urls = [('https://drive.google.com/open?id={}'.format(file_id), (file_id, False), True), ('https://drive.google.com/uc?id={}'.format(file_id), (file_id, True), False), ('https://drive.google.com/file/d/{}/view?usp=sharing'.format(file_id), (file_id, False), True), ('https://drive.google.com/a/jsk.imi.i.u-tokyo.ac.jp/uc?id={}&export=download'.format(file_id), (file_id, True), False)]
    for url, expected, check_warn in urls:
<mask>:
            with pytest.warns(UserWarning):
                assert parse_url(url) == expected
        else:
            assert parse_url(url) == expected",check_warn,41,check_warn,True,100.00000000000004,N/A
"def _test_cli_with_md5(url_or_id, md5, options=None):
    with tempfile.TemporaryDirectory() as d:
        file_path = os.path.join(d, 'file')
        cmd = ['gdown', '--no-cookies', url_or_id, '-O', file_path]
<mask>:
            cmd.extend(options)
        subprocess.call(cmd)
        _assert_filehash(path=file_path, hash=f'md5:{md5}')",options is not None,24,options,False,4.9787068367863965,N/A
"def md5sum(filename, blocksize=None):
    warnings.warn('md5sum is deprecated and will be removed in the future.', FutureWarning)
<mask>:
        blocksize = 65536
    hash = hashlib.md5()
    with open(filename, 'rb') as f:
        for block in iter(lambda: f.read(blocksize), b''):
            hash.update(block)
    return hash.hexdigest()",blocksize is None,35,blocksize is None,True,100.00000000000004,N/A
"def assert_md5sum(filename, md5, quiet=False, blocksize=None):
    warnings.warn('assert_md5sum is deprecated and will be removed in the future.', FutureWarning)
<mask>:
        raise ValueError(f'MD5 must be 32 chars: {md5}')
    md5_actual = md5sum(filename)
    if md5_actual == md5:
        if not quiet:
            print(f'MD5 matches: {filename!r} == {md5!r}', file=sys.stderr)
        return True
    raise AssertionError(f""MD5 doesn't match:\nactual: {md5_actual}\nexpected: {md5}"")","not (isinstance(md5, str) and len(md5) == 32)",48,len(md5) != 32,False,11.717090906676962,N/A
"def cached_download(url=None, path=None, md5=None, quiet=False, postprocess=None, hash: Optional[str]=None, **kwargs):
    """"""Cached download from URL.

    Parameters
    ----------
    url: str
        URL. Google Drive URL is also supported.
    path: str, optional
        Output filename. Default is basename of URL.
    md5: str, optional
        Expected MD5 for specified file. Deprecated in favor of `hash`.
    quiet: bool
        Suppress terminal output. Default is False.
    postprocess: callable, optional
        Function called with filename as postprocess.
    hash: str, optional
        Hash value of file in the format of {algorithm}:{hash_value}
        such as sha256:abcdef.... Supported algorithms: md5, sha1, sha256, sha512.
    kwargs: dict
        Keyword arguments to be passed to `download`.

    Returns
    -------
    path: str
        Output filename.
    """"""
<mask>:
        path = url.replace('/', '-SLASH-').replace(':', '-COLON-').replace('=', '-EQUAL-').replace('?', '-QUESTION-')
        path = osp.join(cache_root, path)
    if md5 is not None and hash is not None:
        raise ValueError('md5 and hash cannot be specified at the same time.')
    if md5 is not None:
        warnings.warn(""md5 is deprecated in favor of hash. Please use hash='md5:xxx...' instead."", FutureWarning)
        hash = f'md5:{md5}'
    del md5
    if osp.exists(path) and (not hash):
        if not quiet:
            print(f'File exists: {path}', file=sys.stderr)
        return path
    elif osp.exists(path) and hash:
        try:
            _assert_filehash(path=path, hash=hash, quiet=quiet)
            return path
        except AssertionError as e:
            print(e, file=sys.stderr)
    lock_path = osp.join(cache_root, '_dl_lock')
    try:
        os.makedirs(osp.dirname(path))
    except OSError:
        pass
    temp_root = tempfile.mkdtemp(dir=cache_root)
    try:
        temp_path = osp.join(temp_root, 'dl')
        log_message_hash = f'Hash: {hash}\n' if hash else ''
        download(url=url, output=temp_path, quiet=quiet, log_messages={'start': f'Cached downloading...\n{log_message_hash}', 'output': f'To: {path}\n'}, **kwargs)
        with filelock.FileLock(lock_path):
            shutil.move(temp_path, path)
    except Exception:
        shutil.rmtree(temp_root)
        raise
    if hash:
        _assert_filehash(path=path, hash=hash, quiet=quiet)
    if postprocess is not None:
        postprocess(path)
    return path",path is None,242,url is not None,False,18.99589214128981,N/A
"def _compute_filehash(path, algorithm):
    BLOCKSIZE = 65536
<mask>:
        raise ValueError(f'Unsupported hash algorithm: {algorithm}. Supported algorithms: {hashlib.algorithms_guaranteed}')
    algorithm_instance = getattr(hashlib, algorithm)()
    with open(path, 'rb') as f:
        for block in iter(lambda: f.read(BLOCKSIZE), b''):
            algorithm_instance.update(block)
    return f'{algorithm}:{algorithm_instance.hexdigest()}'",algorithm not in hashlib.algorithms_guaranteed,33,algorithm not in hashlib.algorithms_guaranteed,True,100.00000000000004,N/A
"def _assert_filehash(path, hash, quiet=False, blocksize=None):
<mask>:
        raise ValueError(f'Invalid hash: {hash}. Hash must be in the format of {{algorithm}}:{{hash_value}}.')
    algorithm = hash.split(':')[0]
    hash_actual = _compute_filehash(path=path, algorithm=algorithm)
    if hash_actual == hash:
        return True
    raise AssertionError(f""File hash doesn't match:\nactual: {hash_actual}\nexpected: {hash}"")",':' not in hash,38,"not hash.split(':')[0] in ['0.5.6.7', '0.5.6.7', '0.5.6.7.8', '0.5.6.7.8', '0.5.6.7.8', '0.5.6.7.8.8', '0.5.6.7.8.8', '0.5.6.7.8.",False,5.4957573647494575,N/A
"def get_url_from_gdrive_confirmation(contents):
    url = ''
    for line in contents.splitlines():
        m = re.search('href=""(\\/uc\\?export=download[^""]+)', line)
<mask>:
            url = 'https://docs.google.com' + m.groups()[0]
            url = url.replace('&amp;', '&')
            break
        soup = bs4.BeautifulSoup(line, features='html.parser')
        form = soup.select_one('#download-form')
        if form is not None:
            url = form['action'].replace('&amp;', '&')
            url_components = urllib.parse.urlsplit(url)
            query_params = urllib.parse.parse_qs(url_components.query)
            for param in form.findChildren('input', attrs={'type': 'hidden'}):
                query_params[param['name']] = param['value']
            query = urllib.parse.urlencode(query_params, doseq=True)
            url = urllib.parse.urlunsplit(url_components._replace(query=query))
            break
        m = re.search('""downloadUrl"":""([^""]+)', line)
        if m:
            url = m.groups()[0]
            url = url.replace('\\u003d', '=')
            url = url.replace('\\u0026', '&')
            break
        m = re.search('<p class=""uc-error-subcaption"">(.*)</p>', line)
        if m:
            error = m.groups()[0]
            raise FileURLRetrievalError(error)
    if not url:
        raise FileURLRetrievalError(""Cannot retrieve the public link of the file. You may need to change the permission to 'Anyone with the link', or have had many accesses. Check FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq."")
    return url",m,128,m,True,100.00000000000004,N/A
"def _get_filename_from_response(response):
    content_disposition = urllib.parse.unquote(response.headers['Content-Disposition'])
    m = re.search(""filename\\*=UTF-8''(.*)"", content_disposition)
<mask>:
        filename = m.groups()[0]
        return filename.replace(osp.sep, '_')
    m = re.search('attachment; filename=""(.*?)""', content_disposition)
    if m:
        filename = m.groups()[0]
        return filename
    return None",m,30,m,True,100.00000000000004,N/A
"def _get_modified_time_from_response(response):
<mask>:
        return None
    raw = response.headers['Last-Modified']
    if raw is None:
        return None
    return email.utils.parsedate_to_datetime(raw)",'Last-Modified' not in response.headers,16,response.status_code != 200,False,11.044795567078939,N/A
"def _get_session(proxy, use_cookies, user_agent, return_cookies_file=False):
    sess = requests.session()
    sess.headers.update({'User-Agent': user_agent})
<mask>:
        sess.proxies = {'http': proxy, 'https': proxy}
        print('Using proxy:', proxy, file=sys.stderr)
    cookies_file = osp.join(home, '.cache/gdown/cookies.txt')
    if use_cookies and osp.exists(cookies_file):
        cookie_jar = MozillaCookieJar(cookies_file)
        cookie_jar.load()
        sess.cookies.update(cookie_jar)
    if return_cookies_file:
        return (sess, cookies_file)
    else:
        return sess",proxy is not None,42,proxy,False,4.9787068367863965,N/A
"def _parse_google_drive_file(url, content):
    """"""Extracts information about the current page file and its children.""""""
    folder_soup = bs4.BeautifulSoup(content, features='html.parser')
    encoded_data = None
    for script in folder_soup.select('script'):
        inner_html = script.decode_contents()
<mask>:
            regex_iter = re.compile(""'((?:[^'\\\\]|\\\\.)*)'"").finditer(inner_html)
            try:
                encoded_data = next(itertools.islice(regex_iter, 1, None)).group(1)
            except StopIteration:
                raise RuntimeError(""Couldn't find the folder encoded JS string"")
            break
    if encoded_data is None:
        raise RuntimeError(""Cannot retrieve the folder information from the link. You may need to change the permission to 'Anyone with the link', or have had many accesses. Check FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq."")
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', category=DeprecationWarning)
        decoded = encoded_data.encode('utf-8').decode('unicode_escape')
    folder_arr = json.loads(decoded)
    folder_contents = [] if folder_arr[0] is None else folder_arr[0]
    sep = ' - '
    splitted = folder_soup.title.contents[0].split(sep)
    if len(splitted) >= 2:
        name = sep.join(splitted[:-1])
    else:
        raise RuntimeError('file/folder name cannot be extracted from: {}'.format(folder_soup.title.contents[0]))
    gdrive_file = _GoogleDriveFile(id=url.split('/')[-1], name=name, type=_GoogleDriveFile.TYPE_FOLDER)
    id_name_type_iter = [(e[0], e[2].encode('raw_unicode_escape').decode('utf-8'), e[3]) for e in folder_contents]
    return (gdrive_file, id_name_type_iter)",'_DRIVE_ivd' in inner_html,142,inner_html,False,13.533528323661276,N/A
"def _download_and_parse_google_drive_link(sess, url, quiet=False, remaining_ok=False, verify=True):
    """"""Get folder structure of Google Drive folder URL.""""""
    return_code = True
    for _ in range(2):
<mask>:
            if '?' in url:
                url += '&hl=en'
            else:
                url += '?hl=en'
        res = sess.get(url, verify=verify)
        if res.status_code != 200:
            return (False, None)
        if is_google_drive_url(url):
            break
        if not is_google_drive_url(res.url):
            break
        url = res.url
    gdrive_file, id_name_type_iter = _parse_google_drive_file(url=url, content=res.text)
    for child_id, child_name, child_type in id_name_type_iter:
        if child_type != _GoogleDriveFile.TYPE_FOLDER:
            if not quiet:
                print('Processing file', child_id, child_name)
            gdrive_file.children.append(_GoogleDriveFile(id=child_id, name=child_name, type=child_type))
            if not return_code:
                return (return_code, None)
            continue
        if not quiet:
            print('Retrieving folder', child_id, child_name)
        return_code, child = _download_and_parse_google_drive_link(sess=sess, url='https://drive.google.com/drive/folders/' + child_id, quiet=quiet, remaining_ok=remaining_ok)
        if not return_code:
            return (return_code, None)
        gdrive_file.children.append(child)
    has_at_least_max_files = len(gdrive_file.children) == MAX_NUMBER_FILES
    if not remaining_ok and has_at_least_max_files:
        message = ' '.join(['The gdrive folder with url: {url}'.format(url=url), 'has more than {max} files,'.format(max=MAX_NUMBER_FILES), ""gdrive can't download more than this limit.""])
        raise FolderContentsMaximumLimitError(message)
    return (return_code, gdrive_file)",is_google_drive_url(url),145,not url.startswith('http'),False,5.630400552901077,N/A
"def _get_directory_structure(gdrive_file, previous_path):
    """"""Converts a Google Drive folder structure into a local directory list.""""""
    directory_structure = []
    for file in gdrive_file.children:
        file.name = file.name.replace(osp.sep, '_')
<mask>:
            directory_structure.append((None, osp.join(previous_path, file.name)))
            for i in _get_directory_structure(file, osp.join(previous_path, file.name)):
                directory_structure.append(i)
        elif not file.children:
            directory_structure.append((file.id, osp.join(previous_path, file.name)))
    return directory_structure",file.is_folder(),44,file.children,False,14.506309551249304,N/A
"def parse_url(url, warning=True):
    """"""Parse URLs especially for Google Drive links.

    file_id: ID of file on Google Drive.
    is_download_link: Flag if it is download link of Google Drive.
    """"""
    parsed = urllib.parse.urlparse(url)
    query = urllib.parse.parse_qs(parsed.query)
    is_gdrive = is_google_drive_url(url=url)
    is_download_link = parsed.path.endswith('/uc')
<mask>:
        return (is_gdrive, is_download_link)
    file_id = None
    if 'id' in query:
        file_ids = query['id']
        if len(file_ids) == 1:
            file_id = file_ids[0]
    else:
        patterns = ['^/file/d/(.*?)/(edit|view)$', '^/file/u/[0-9]+/d/(.*?)/(edit|view)$', '^/document/d/(.*?)/(edit|htmlview|view)$', '^/document/u/[0-9]+/d/(.*?)/(edit|htmlview|view)$', '^/presentation/d/(.*?)/(edit|htmlview|view)$', '^/presentation/u/[0-9]+/d/(.*?)/(edit|htmlview|view)$', '^/spreadsheets/d/(.*?)/(edit|htmlview|view)$', '^/spreadsheets/u/[0-9]+/d/(.*?)/(edit|htmlview|view)$']
        for pattern in patterns:
            match = re.match(pattern, parsed.path)
            if match:
                file_id = match.groups()[0]
                break
    if warning and (not is_download_link):
        warnings.warn('You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: {url}'.format(url='https://drive.google.com/uc?id={}'.format(file_id)))
    return (file_id, is_download_link)",not is_gdrive,122,is_gdrive,False,71.65313105737896,N/A
"def extractall(path, to=None):
    """"""Extract archive file.

    Parameters
    ----------
    path: str
        Path of archive file to be extracted.
    to: str, optional
        Directory to which the archive file will be extracted.
        If None, it will be set to the parent directory of the archive file.
    """"""
<mask>:
        to = osp.dirname(path)
    if path.endswith('.zip'):
        opener, mode = (zipfile.ZipFile, 'r')
    elif path.endswith('.tar'):
        opener, mode = (tarfile.open, 'r')
    elif path.endswith('.tar.gz') or path.endswith('.tgz'):
        opener, mode = (tarfile.open, 'r:gz')
    elif path.endswith('.tar.bz2') or path.endswith('.tbz'):
        opener, mode = (tarfile.open, 'r:bz2')
    else:
        raise ValueError(""Could not extract '%s' as no appropriate extractor is found"" % path)

    def namelist(f):
        if isinstance(f, zipfile.ZipFile):
            return f.namelist()
        return [m.path for m in f.members]

    def filelist(f):
        files = []
        for fname in namelist(f):
            fname = osp.join(to, fname)
            files.append(fname)
        return files
    with opener(path, mode) as f:
        f.extractall(path=to)
    return filelist(f)",to is None,131,to is None,True,100.00000000000004,N/A
"def file_size(argv):
<mask>:
        m = re.match('([0-9]+)(GB|MB|KB|B)', argv)
        if not m:
            raise TypeError
        size, unit = m.groups()
        size = float(size)
        if unit == 'KB':
            size *= 1024
        elif unit == 'MB':
            size *= 1024 ** 2
        elif unit == 'GB':
            size *= 1024 ** 3
        elif unit == 'B':
            pass
        return size",argv is not None,51,argv,False,4.9787068367863965,N/A
"def get_user_starred_by_username(self, username: str, after: str='', topic_stargazer_count_limit: int=0):
    items = []
    result = self.client.execute(QUERY, variable_values={'username': username, 'after': after})
    has_next = result['user']['starredRepositories']['pageInfo']['hasNextPage']
    end_cursor = result['user']['starredRepositories']['pageInfo']['endCursor']
    for repo in result['user']['starredRepositories']['nodes']:
        name = repo['nameWithOwner']
        description = repo['description'] if repo['description'] else ''
        language = repo['languages']['edges'][0]['node']['name'] if repo['languages']['edges'] else ''
        url = repo['url']
        stargazer_count = repo['stargazerCount']
        is_private = repo['isPrivate']
        topics = [tag['topic']['name'] for tag in repo['repositoryTopics']['nodes'] if tag['topic']['stargazerCount'] > topic_stargazer_count_limit]
        items.append(Repository(name, description, language, url, stargazer_count, is_private, topics))
<mask>:
        items.extend(self.get_user_starred_by_username(username, end_cursor, topic_stargazer_count_limit))
    return items",has_next,78,has_next,True,100.00000000000004,N/A
"@click.command()
@click.option('--username', envvar='USER', required=True, help='GitHub username')
@click.option('--token', envvar='GITHUB_TOKEN', required=True, help='GitHub token')
@click.option('--sort', is_flag=True, show_default=True, help='sort by category[language/topic] name alphabetically')
@click.option('--topic', is_flag=True, show_default=True, help='category by topic, default is category by language')
@click.option('--topic_limit', default=500, show_default=True, type=int, help='topic stargazer_count gt number, set bigger to reduce topics number')
@click.option('--repository', default='', show_default=True, help='repository name')
@click.option('--filename', default='README.md', show_default=True, help='file name')
@click.option('--message', default='update awesome-stars, created by starred', show_default=True, help='commit message')
@click.option('--private', is_flag=True, default=False, show_default=True, help='include private repos')
@click.version_option(version=VERSION, prog_name='starred')
def starred(username, token, sort, topic, repository, filename, message, private, topic_limit):
    """"""GitHub starred

    creating your own Awesome List by GitHub stars!

    example:
        starred --username maguowei --token=xxxxxxxx --sort > README.md
    """"""
    gh = GitHubGQL(token)
    try:
        stars = gh.get_user_starred_by_username(username, topic_stargazer_count_limit=topic_limit)
    except Exception as e:
        click.secho(f'Error: {e}', fg='red')
        return
<mask>:
        file = BytesIO()
        sys.stdout = file
    else:
        file = None
    click.echo(desc)
    repo_dict = {}
    for s in stars:
        if s.is_private and (not private):
            continue
        description = html_escape(s.description).replace('\n', '').strip()[:TEXT_LENGTH_LIMIT] if s.description else ''
        if topic:
            for category in s.topics or [DEFAULT_CATEGORY.lower()]:
                if category not in repo_dict:
                    repo_dict[category] = []
                repo_dict[category].append([s.name, s.url, description])
        else:
            category = s.language or DEFAULT_CATEGORY
            if category not in repo_dict:
                repo_dict[category] = []
            repo_dict[category].append([s.name, s.url, description])
    if sort:
        repo_dict = OrderedDict(sorted(repo_dict.items(), key=lambda cate: cate[0]))
    for category in repo_dict.keys():
        data = u'- [{}](#{})'.format(category, '-'.join(category.lower().split()))
        click.echo(data)
    click.echo('')
    for category in repo_dict:
        click.echo('## {} \n'.format(category.replace('#', '# #')))
        for repo in repo_dict[category]:
            data = u'- [{}]({}) - {}'.format(*repo)
            click.echo(data)
        click.echo('')
    click.echo(license_.format(username=username))
    if file:
        gh = GitHub(token=token)
        try:
            rep = gh.repository(username, repository)
            try:
                rep.file_contents(f'/{filename}').update(message, file.getvalue())
            except NotFoundError:
                rep.create_file(filename, message, file.getvalue())
        except NotFoundError:
            rep = gh.create_repository(repository, 'A curated list of my GitHub stars!')
            rep.create_file(filename, 'starred initial commit', file.getvalue())
        click.launch(rep.html_url)",repository,263,not private,False,0.0,N/A
"def remove_dup_items(lst):
    new_lst = []
    for item in lst:
<mask>:
            new_lst.append(item)
    return new_lst",item not in new_lst,13,item not in new_lst,True,100.00000000000004,N/A
"def parse_pinyins(fp):
    pinyin_map = {}
    for line in fp:
        line = line.strip()
<mask>:
            continue
        code, pinyin = line.split('#')[0].split(':')
        pinyin = ','.join([x.strip() for x in pinyin.split() if x.strip()])
        pinyin_map[code.strip()] = pinyin.split(',')
    return pinyin_map",line.startswith('#') or not line,32,not line,False,1.110899653824231,N/A
"def merge(raw_pinyin_map, adjust_pinyin_map, overwrite_pinyin_map):
    new_pinyin_map = {}
    for code, pinyins in raw_pinyin_map.items():
<mask>:
            pinyins = overwrite_pinyin_map[code]
        elif code in adjust_pinyin_map:
            pinyins = adjust_pinyin_map[code] + pinyins
        new_pinyin_map[code] = remove_dup_items(pinyins)
    return new_pinyin_map",code in overwrite_pinyin_map,30,code in overwrite_pinyin_map,True,100.00000000000004,N/A
"def extend_pinyins(old_map, new_map, only_no_exists=False):
    for code, pinyins in new_map.items():
<mask>:
            if code not in old_map:
                old_map[code] = pinyins
        else:
            old_map.setdefault(code, []).extend(pinyins)",only_no_exists,21,only_no_exists,True,100.00000000000004,N/A
"def parse_china_x():
    with open('tools/china-8105-06062014.txt') as fp:
        for line in fp:
            line = line.strip()
<mask>:
                continue
            yield line.split()[0]",line.startswith('#') or not line,17,not line,False,1.110899653824231,N/A
"def diff(kmandarin, zdic, commons):
    for key in commons:
        hanzi = code_to_hanzi(key)
<mask>:
            value = kmandarin[key][0]
            if key in zdic and value != zdic[key][0]:
                yield '{0}: {1}  # {2} -> {3}'.format(key, value, hanzi, zdic[key][0])
            else:
                yield '{0}: {1}  # {2}'.format(key, value, hanzi)
        elif key in zdic:
            value = zdic[key][0]
            yield '{0}: {1}  # {2}'.format(key, value, hanzi)
        else:
            yield '# {0}: {1}  # {2}'.format(key, '<-', hanzi)",key in kmandarin,64,key in kmandarin,True,100.00000000000004,N/A
"def get_han_point(text):
<mask>:
        return ('', '')
    regex = re.compile('(?P<point>[A-Z0-9]+) \\((?P<han>[^\\)]+)\\)')
    result = regex.findall(text)
    return result[0]",not text,15,not text,True,100.00000000000004,N/A
"def point_to_u_point(point):
    point = point.upper()
<mask>:
        point = 'U+' + point
    return point",not point.startswith('U+'),13,point.isdigit(),False,11.415938068117505,N/A
"def gen_pua_data(gbk, unicode_4_1, pinyin_map):
    gbk_point, gbk_han = gbk
    gbk_point = point_to_u_point(gbk_point)
    unicode_4_1_point, unicode_4_1_han = unicode_4_1
    unicode_4_1_point = point_to_u_point(unicode_4_1_point)
    pinyins = ','.join(pinyin_map.get(unicode_4_1_point, []))
    prefix = ''
<mask>:
        prefix = '# '
    return '{prefix}{gbk_point}: {pinyins}  # {gbk_han}  Unihan: {unicode_4_1_point} {unicode_4_1_han}'.format(**locals())",not pinyins,38,"len(pinyin_map.get(unicode_4_1_point, [])) > 1",False,0.0,N/A
"def fetch_info(hanzi):
    url = 'http://www.guoxuedashi.com/zidian/so.php'
    params = {'sokeyzi': hanzi, 'kz': 1, 'submit': ''}
    html = fetch_html(url, params)
    pq = PyQuery(html)
    pq = PyQuery(pq('table.zui td')[1])
    text = pq('tr').text()
    text_alternate = pq(html)('.info_txt2')('em').text()
    pinyin = ''
    pinyin_match = re_pinyin.search(text)
<mask>:
        pinyin = pinyin_match.group('pinyin')
    code = re_code.search(text).group('code')
    alternate = ''
    alternate_match = re_alternate.search(text_alternate)
    if alternate_match is not None:
        alternate = alternate_match.group('alternate')
    return HanziInfo(pinyin, code, alternate)",pinyin_match is not None,61,pinyin_match is not None,True,100.00000000000004,N/A
"def parse_hanzi(hanzi):
    info = fetch_info(hanzi)
<mask>:
        alternate = fetch_info(info.alternate)
    else:
        alternate = ''
    return HanziInfo(info.pinyin, info.code, alternate)",not info.pinyin and info.alternate,17,info.alternate,False,18.887560283756194,N/A
"def main(lines):
    for line in lines:
<mask>:
            code = line.split(':')[0].strip('# ')
            code = code[2:]
            info = parse_hanzi(code)
            pinyin = info.pinyin
            extra = ''
            if not pinyin and info.alternate:
                alternate = info.alternate
                pinyin = alternate.pinyin
                extra = '  => U+{0}'.format(alternate.code)
                if ',' in pinyin:
                    first_pinyin, extra_pinyin = pinyin.split(',', 1)
                    pinyin = first_pinyin
                    extra += '  ?-> ' + extra_pinyin
            if pinyin:
                line = line.strip()
                line = line[2:]
                line = line.replace('<-', pinyin)
                if extra:
                    line += extra
        yield line.strip()",line.startswith('# U+') and '<-' in line,77,':' in line,False,2.815135668143185,N/A
"def add(self, item):
<mask>:
        self._data[item] = 0
    self._data[item] += 1",item not in self._data,10,item not in self._data,True,100.00000000000004,N/A
"def main():
    han_counter = {}
    for hans, pinyin_list in phrases_dict.items():
<mask>:
            continue
        for i, han in enumerate(hans):
            pinyins = pinyin_list[i]
            for pinyin in pinyins:
                if han not in han_counter:
                    han_counter[han] = Counter()
                counter = han_counter[han]
                counter.add(pinyin)
    for han, counter in sorted(han_counter.items(), key=lambda x: ord(x[0])):
        code = han_to_code(han)
        pinyin = ','.join([x[0] for x in counter.most_common()])
        if pinyin in ['xx']:
            continue
        print('{0}: {1}  # {2}'.format(code, pinyin, han))",len(hans) != len(pinyin_list),65,len(hans) < 2,False,18.693159143202898,N/A
"def remove_dup_items(lst):
    new_list = []
    for item in lst:
<mask>:
            new_list.append(item)
    return new_list",item not in new_list,13,item not in new_list,True,100.00000000000004,N/A
"def parse(lines, kind='kHanyuPinyin', ignore_prefix='#') -> str:
    re_line = re_match_pinyin_line(kind)
    re_pinyin = re_kinds_map[kind]
    for line in lines:
        line = line.strip()
<mask>:
            continue
        match = re_line.match(line)
        if match is None:
            continue
        code = match.group('code')
        raw_pinyin = match.group('pinyin')
        raw_pinyins = re_pinyin.findall(raw_pinyin)
        for n, values in enumerate(raw_pinyins):
            value = []
            for v in values:
                value.extend(v.split(','))
            raw_pinyins[n] = value
        pinyins = functools.reduce(operator.add, raw_pinyins)
        pinyins = [x.strip() for x in pinyins if x.strip()]
        pinyins = remove_dup_items(pinyins)
        pinyin = ','.join(pinyins)
        yield (code, pinyin)",line.startswith(ignore_prefix),76,not line or line.startswith('#') or line.startswith('#') or line.startswith('#') or line.startswith('#'),False,6.414921514603761,N/A
"def get_key(key, file_name, dest):
<mask>:
        fn_args = [file_name]
        args = dest.get('key_args')
        if args:
            fn_args.append(args)
        object_key = key(*fn_args)
    elif key == '/':
        object_key = file_name
    else:
        object_key = '%s/%s' % (key.strip('/'), file_name)
    return object_key","hasattr(key, '__call__')",33,key == 'function',False,2.1617886496312457,N/A
"def get_aws_credentials():
    access_key = getattr(settings, 'AWS_ACCESS_KEY_ID', None)
    secret_key = getattr(settings, 'AWS_SECRET_ACCESS_KEY', None)
<mask>:
        return AWSCredentials(None, secret_key, access_key)
    if not SESSION:
        return AWSCredentials(None, None, None)
    creds = SESSION.get_credentials()
    if creds:
        return AWSCredentials(creds.token, creds.secret_key, creds.access_key)
    else:
        return AWSCredentials(None, None, None)",access_key and secret_key,38,not access_key or not secret_key,False,29.84745896009822,N/A
"def get_custom_region_from_s3_dests(self, dest='login-not-required'):
    s3_dest = settings.S3DIRECT_DESTINATIONS.get(dest)
    region = settings.AWS_S3_REGION_NAME
<mask>:
        region = s3_dest['region']
    return region",s3_dest and 'region' in s3_dest,15,s3_dest,False,13.533528323661276,N/A
"@csrf_protect
@require_POST
def get_upload_params(request):
    """"""Authorises user and validates given file properties.""""""
    file_name = request.POST['name']
    file_type = request.POST['type']
    file_size = int(request.POST['size'])
    dest = get_s3direct_destinations().get(request.POST.get('dest', None), None)
<mask>:
        resp = json.dumps({'error': 'File destination does not exist.'})
        return HttpResponseNotFound(resp, content_type='application/json')
    auth = dest.get('auth')
    if auth and (not auth(request.user)):
        resp = json.dumps({'error': 'Permission denied.'})
        return HttpResponseForbidden(resp, content_type='application/json')
    allowed = dest.get('allowed')
    if (allowed and file_type not in allowed) and allowed != '*':
        resp = json.dumps({'error': 'Invalid file type (%s).' % file_type})
        return HttpResponseBadRequest(resp, content_type='application/json')
    cl_range = dest.get('content_length_range')
    if cl_range and (not cl_range[0] <= file_size <= cl_range[1]):
        msg = 'Invalid file size (must be between %s and %s bytes).'
        resp = json.dumps({'error': msg % cl_range})
        return HttpResponseBadRequest(resp, content_type='application/json')
    key = dest.get('key')
    if not key:
        resp = json.dumps({'error': 'Missing destination path.'})
        return HttpResponseServerError(resp, content_type='application/json')
    bucket = dest.get('bucket', getattr(settings, 'AWS_STORAGE_BUCKET_NAME', None))
    if not bucket:
        resp = json.dumps({'error': 'S3 bucket config missing.'})
        return HttpResponseServerError(resp, content_type='application/json')
    region = dest.get('region', getattr(settings, 'AWS_S3_REGION_NAME', None))
    if not region:
        resp = json.dumps({'error': 'S3 region config missing.'})
        return HttpResponseServerError(resp, content_type='application/json')
    endpoint = dest.get('endpoint', getattr(settings, 'AWS_S3_ENDPOINT_URL', None))
    if not endpoint:
        resp = json.dumps({'error': 'S3 endpoint config missing.'})
        return HttpResponseServerError(resp, content_type='application/json')
    aws_credentials = get_aws_credentials()
    if not aws_credentials.secret_key or not aws_credentials.access_key:
        resp = json.dumps({'error': 'AWS credentials config missing.'})
        return HttpResponseServerError(resp, content_type='application/json')
    upload_data = {'object_key': get_key(key, file_name, dest), 'access_key_id': aws_credentials.access_key, 'session_token': aws_credentials.token, 'region': region, 'bucket': bucket, 'endpoint': endpoint, 'acl': dest.get('acl') or 'public-read', 'allow_existence_optimization': dest.get('allow_existence_optimization', False)}
    optional_params = ['content_disposition', 'cache_control', 'server_side_encryption']
    for optional_param in optional_params:
        if optional_param in dest:
            option = dest.get(optional_param)
            if hasattr(option, '__call__'):
                upload_data[optional_param] = option(file_name)
            else:
                upload_data[optional_param] = option
    resp = json.dumps(upload_data)
    return HttpResponse(resp, content_type='application/json')",not dest,258,not dest,True,100.00000000000004,N/A
"@csrf_protect
@require_POST
def generate_aws_v4_signature(request):
    message = unquote(request.POST['to_sign'])
    dest = get_s3direct_destinations().get(unquote(request.POST['dest']))
    signing_date = datetime.strptime(request.POST['datetime'], '%Y%m%dT%H%M%SZ')
    auth = dest.get('auth')
<mask>:
        resp = json.dumps({'error': 'Permission denied.'})
        return HttpResponseForbidden(resp, content_type='application/json')
    region = dest.get('region') or getattr(settings, 'AWS_S3_REGION_NAME', None)
    if not region:
        resp = json.dumps({'error': 'S3 region config missing.'})
        return HttpResponseServerError(resp, content_type='application/json')
    aws_credentials = get_aws_credentials()
    if not aws_credentials.secret_key or not aws_credentials.access_key:
        resp = json.dumps({'error': 'AWS credentials config missing.'})
        return HttpResponseServerError(resp, content_type='application/json')
    signing_key = get_aws_v4_signing_key(aws_credentials.secret_key, signing_date, region, 's3')
    signature = get_aws_v4_signature(signing_key, message)
    resp = json.dumps({'s3ObjKey': signature})
    return HttpResponse(resp, content_type='application/json')",auth and (not auth(request.user)),82,not auth,False,1.110899653824231,N/A
"@pytest.fixture(scope='module', params=cluster_configs, ids=[str(cc) for cc in cluster_configs])
def running_cluster(request):
    """"""
    Return the name of a running Flintrock cluster.
    """"""
    cluster_name = 'running-cluster-' + random_string()
    try:
        launch_cluster(cluster_name=cluster_name, instance_type=request.param.instance_type, spark_version=request.param.spark_version, spark_git_commit=request.param.spark_git_commit)
<mask>:
            stop_cluster(cluster_name)
            start_cluster(cluster_name)
        yield cluster_name
    finally:
        p = subprocess.run(['flintrock', 'destroy', cluster_name, '--assume-yes'])
        assert p.returncode == 0",request.param.restarted,45,request.param.spark_version,False,41.11336169005198,N/A
"def test_code_compiles():
    for path in TEST_PATHS:
<mask>:
            result = compileall.compile_dir(path)
        else:
            result = compileall.compile_file(path)
        assert result == 1",os.path.isdir(path),18,os.path.isdir(path),True,100.00000000000004,N/A
"@pytest.mark.parametrize('spark_version', ['', '3.5.0', 'a28880f3b9c63d86368bcd6cbbaa6a9af7075409'])
def test_templates(dummy_cluster, spark_version):
    template_dir = os.path.join(FLINTROCK_ROOT_DIR, 'flintrock', 'templates')
    for dirpath, dirnames, filenames in os.walk(template_dir):
<mask>:
            for filename in filenames:
                template_path = os.path.join(dirpath, filename)
                mapping = generate_template_mapping(cluster=dummy_cluster, hadoop_version='', spark_version=spark_version, spark_executor_instances=0)
                get_formatted_template(path=template_path, mapping=mapping)",filenames,35,len(filenames) > 0,False,8.116697886877475,N/A
"def option_requires(*, option: str, conditional_value=None, requires_all: list=[], requires_any: list=[], scope: dict):
    """"""
    Raise an exception if an option's requirements are not met.

    The option's requirements are checked only if the option has a ""truthy"" value
    (i.e. it's not a ""falsy"" value like '', None, or False), and if its value is
    equal to conditional_value, if conditional_value is not None.

    requires_all: Every option in this list must be defined.
    requires_any: At least one option in this list must be defined.

    This function looks for values by converting the option names to their
    corresponding variable names (e.g. --option-a becomes option_a) and looking them
    up in the provided scope.
    """"""
    option_value = scope[option_name_to_variable_name(option)]
<mask>:
        if requires_all:
            for required_option in requires_all:
                required_name = option_name_to_variable_name(required_option)
                if required_name not in scope or not scope[required_name]:
                    raise UsageError('Error: Missing option ""{missing_option}"" is required by ""{option}{space}{conditional_value}"".'.format(missing_option=required_option, option=option, space=' ' if conditional_value is not None else '', conditional_value=conditional_value if conditional_value is not None else ''))
        if requires_any:
            for required_option in requires_any:
                required_name = option_name_to_variable_name(required_option)
                if required_name in scope and scope[required_name] is not None:
                    break
            else:
                raise UsageError('Error: ""{option}{space}{conditional_value}"" requires at least one of the following options to be set: {at_least}'.format(option=option, space=' ' if conditional_value is not None else '', conditional_value=conditional_value if conditional_value is not None else '', at_least=', '.join(['""' + ra + '""' for ra in requires_any])))",option_value and (conditional_value is None or option_value == conditional_value),217,option_value is None,False,4.186576731652839,N/A
"def mutually_exclusive(*, options: list, scope: dict):
    """"""
    Raise an exception if more than one of the provided options is specified.

    This function looks for values by converting the option names to their
    corresponding variable names (e.g. --option-a becomes option_a) and looking them
    up in the provided scope.
    """"""
    mutually_exclusive_names = [option_name_to_variable_name(o) for o in options]
    used_options = set()
    for name, value in scope.items():
<mask>:
            used_options.add(name)
    if len(used_options) > 1:
        bad_option1 = used_options.pop()
        bad_option2 = used_options.pop()
        raise UsageError('Error: ""{option1}"" and ""{option2}"" are mutually exclusive.\n  {option1}: {value1}\n  {option2}: {value2}'.format(option1=variable_name_to_option_name(bad_option1), value1=scope[bad_option1], option2=variable_name_to_option_name(bad_option2), value2=scope[bad_option2]))",name in mutually_exclusive_names and scope[name],90,name not in mutually_exclusive_names,False,42.88819424803536,N/A
"def configure_log(debug: bool):
    root_logger = logging.getLogger('flintrock')
    handler = logging.StreamHandler(sys.stdout)
    handler.setLevel(logging.DEBUG)
<mask>:
        root_logger.setLevel(logging.DEBUG)
        handler.setFormatter(logging.Formatter('%(asctime)s - flintrock.%(module)-9s - %(levelname)-5s - %(message)s'))
    else:
        root_logger.setLevel(logging.INFO)
        handler.setFormatter(logging.Formatter('%(message)s'))
    root_logger.addHandler(handler)",debug,23,debug,True,100.00000000000004,N/A
"def build_hdfs_download_url(ctx, param, value):
    hdfs_version = ctx.params['hdfs_version']
<mask>:
        logger.warning('Hadoop download source appears to point to a file, not a directory. Flintrock will not try to determine the correct file to download based on the Hadoop version.')
        hdfs_download_url = value
    else:
        hdfs_download_url = value.rstrip('/') + '/hadoop-{v}.tar.gz'
    return hdfs_download_url.format(v=hdfs_version)",value.endswith('.gz') or value.endswith('.tgz'),47,not value.is_dir(),False,4.264163893764324,N/A
"def build_spark_download_url(ctx, param, value):
    spark_version = ctx.params['spark_version']
    hadoop_version = ctx.params['hdfs_version']
    hadoop_build_version = spark_hadoop_build_version(hadoop_version)
<mask>:
        spark_version_tuple = tuple(map(int, spark_version.split('.')))
        if spark_version_tuple >= (3, 3, 0):
            hadoop_build_version = hadoop_build_version.split('.')[0]
    if value.endswith('.gz') or value.endswith('.tgz'):
        logger.warning('Spark download source appears to point to a file, not a directory. Flintrock will not try to determine the correct file to download based on the Spark and Hadoop versions.')
        spark_download_url = value
    else:
        spark_download_url = value.rstrip('/') + '/spark-{v}-bin-{hv}.tgz'
    return spark_download_url.format(v=spark_version, hv=hadoop_build_version)",spark_version,73,hadoop_build_version,False,21.3643503198117,N/A
"def __init__(self, ip_protocol, from_port, to_port, src_group=None, cidr_ip=None):
<mask>:
        raise ValueError('src_group and cidr_ip are mutually exclusive. Specify one or the other. See: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html#EC2.SecurityGroup.authorize_ingress')
    if not src_group and (not cidr_ip):
        raise ValueError('One of src_group or cidr_ip must be specified.')
    self.ip_protocol = ip_protocol
    self.from_port = from_port
    self.to_port = to_port
    self.src_group = src_group if src_group else ''
    self.cidr_ip = cidr_ip if cidr_ip else ''",src_group and cidr_ip,60,src_group and cidr_ip,True,100.00000000000004,N/A
"def __init__(self, *, spark_executor_instances: int, version: str=None, hadoop_version: str, download_source: str=None, git_commit: str=None, git_repository: str=None):
    assert bool(version) ^ bool(git_commit)
<mask>:
        assert git_repository
    self.spark_executor_instances = spark_executor_instances
    self.version = version
    self.hadoop_version = hadoop_version
    self.download_source = download_source
    self.git_commit = git_commit
    self.git_repository = git_repository
    self.manifest = {'version': version, 'spark_executor_instances': spark_executor_instances, 'hadoop_version': hadoop_version, 'download_source': download_source, 'git_commit': git_commit, 'git_repository': git_repository}",git_commit,54,git_repository is not None,False,16.233395773754953,N/A
"def install(self, ssh_client: paramiko.client.SSHClient, cluster: FlintrockCluster):
    logger.info('[{h}] Installing Spark...'.format(h=ssh_client.get_transport().getpeername()[0]))
<mask>:
        with ssh_client.open_sftp() as sftp:
            sftp.put(localpath=os.path.join(SCRIPTS_DIR, 'download-package.py'), remotepath='/tmp/download-package.py')
        logger.debug('[{h}] Downloading Spark from: {s}'.format(h=ssh_client.get_transport().getpeername()[0], s=self.download_source))
        ssh_check_output(client=ssh_client, command='\n                    python /tmp/download-package.py ""{download_source}"" ""spark""\n                '.format(download_source=self.download_source.format(v=self.version)))
    else:
        ssh_check_output(client=ssh_client, command='\n                    set -e\n                    sudo yum install -y git\n                    sudo yum install -y java-devel\n                    ')
        logger.debug('[{h}] Cloning Spark at {c} from: {s}'.format(h=ssh_client.get_transport().getpeername()[0], c=self.git_commit, s=self.git_repository))
        ssh_check_output(client=ssh_client, command='\n                    set -e\n                    git clone {repo} spark\n                    cd spark\n                    git reset --hard {commit}\n                    if [ -e ""make-distribution.sh"" ]; then\n                        ./make-distribution.sh -Phadoop-{hadoop_short_version}\n                    else\n                        ./dev/make-distribution.sh -Phadoop-{hadoop_short_version}\n                    fi\n                '.format(repo=shlex.quote(self.git_repository), commit=shlex.quote(self.git_commit), hadoop_short_version=spark_hadoop_build_version(self.hadoop_version)))
    ssh_check_output(client=ssh_client, command='\n                set -e\n                for f in $(find spark/bin -type f -executable -not -name \'*.cmd\'); do\n                    sudo ln -s ""$(pwd)/$f"" ""/usr/local/bin/$(basename $f)""\n                done\n                echo ""export SPARK_HOME=\'$(pwd)/spark\'"" >> .bashrc\n            ')",self.version,113,self.download_source,False,21.3643503198117,N/A
"def get_subprocess_env() -> dict:
    """"""
    Get the environment we want to use when making subprocess calls.
    This takes care of details that affect subprocess calls made from
    PyInstaller-packaged versions of Flintrock.

    For more information see: https://github.com/pyinstaller/pyinstaller/blob/develop/doc/runtime-information.rst#ld_library_path--libpath-considerations
    """"""
    env = dict(os.environ)
<mask>:
        env['LD_LIBRARY_PATH'] = env.get('LD_LIBRARY_PATH_ORIG', '')
    return env",FROZEN,47,'LD_LIBRARY_PATH_ORIG' in env,False,0.0,N/A
"def spark_hadoop_build_version(hadoop_version: str) -> str:
    """"""
    Given a Hadoop version, determine the Hadoop build of Spark to use.
    """"""
    hadoop_version = tuple(map(int, hadoop_version.split('.')))
<mask>:
        return 'hadoop2.6'
    elif (2, 7) <= hadoop_version < (3, 0):
        return 'hadoop2.7'
    elif (3, 0) <= hadoop_version:
        return 'hadoop3.2'","hadoop_version < (2, 7)",43,"(2, 6) <= hadoop_version < (2, 7)",False,51.2949710782752,N/A
"def load_manifest(self, *, user: str, identity_file: str):
    """"""
        Load a cluster's manifest from the master. This will populate information
        about installed services and configured storage.

        Providers shouldn't need to override this method.
        """"""
<mask>:
        return
    master_ssh_client = get_ssh_client(user=user, host=self.master_ip, identity_file=identity_file, wait=True, print_status=False)
    with master_ssh_client:
        manifest_raw = ssh_check_output(client=master_ssh_client, command='\n                    cat ""$HOME/.flintrock-manifest.json""\n                ')
        ephemeral_dirs_raw = ssh_check_output(client=master_ssh_client, command='\n                    shopt -s nullglob\n                    for f in /media/ephemeral*; do\n                        echo ""$f""\n                    done\n                ')
    manifest = json.loads(manifest_raw)
    self.ssh_key_pair = SSHKeyPair(public=manifest['ssh_key_pair']['public'], private=manifest['ssh_key_pair']['private'])
    self.java_version = manifest['java_version']
    services = []
    for [service_name, manifest] in manifest['services']:
        service = globals()[service_name](**manifest)
        services.append(service)
    self.services = services
    storage_dirs = StorageDirs(root='/media/root', ephemeral=sorted(ephemeral_dirs_raw.splitlines()), persistent=None)
    self.storage_dirs = storage_dirs",not self.master_ip,100,self.is_master(),False,15.619699684601283,N/A
"def run_command(self, *, master_only: bool, user: str, identity_file: str, command: tuple):
    """"""
        Run a shell command on each node of an existing cluster.

        If master_only is True, then run the comand on the master only.
        """"""
<mask>:
        target_hosts = [self.master_ip]
    else:
        target_hosts = [self.master_ip] + self.slave_ips
    partial_func = functools.partial(run_command_node, user=user, identity_file=identity_file, command=command)
    hosts = target_hosts
    run_against_hosts(partial_func=partial_func, hosts=hosts)",master_only,57,master_only,True,100.00000000000004,N/A
"def copy_file(self, *, master_only: bool, user: str, identity_file: str, local_path: str, remote_path: str):
    """"""
        Copy a file to each node of an existing cluster.

        If master_only is True, then copy the file to the master only.
        """"""
<mask>:
        target_hosts = [self.master_ip]
    else:
        target_hosts = [self.master_ip] + self.slave_ips
    partial_func = functools.partial(copy_file_node, user=user, identity_file=identity_file, local_path=local_path, remote_path=remote_path)
    hosts = target_hosts
    run_against_hosts(partial_func=partial_func, hosts=hosts)",master_only,59,master_only,True,100.00000000000004,N/A
"def get_installed_java_version(client: paramiko.client.SSHClient):
    """"""
    :return: the major version (5,6,7,8...) of the currently installed Java or None if not installed
    """"""
    possible_cmds = ['$JAVA_HOME/bin/java -version', 'java -version']
    for command in possible_cmds:
        try:
            output = ssh_check_output(client=client, command=command)
            tokens = output.split()
<mask>:
                version_parts = tokens[2].strip('""').split('.')
                if len(version_parts) >= 2:
                    if version_parts[0] == '1':
                        return int(version_parts[1])
                    else:
                        return int(version_parts[0])
        except SSHError:
            pass
    return None",len(tokens) >= 3,60,"tokens[1].strip('""').split('.')[1] == 'java'",False,2.224972978585352,N/A
"def ensure_java(client: paramiko.client.SSHClient, java_version: int):
    """"""
    Ensures that Java is available on the machine and that it has a
    version of at least java_version.

    The specified version of Java will be installed if it does not
    exist or the existing version has a major version lower than java_version.

    :param client:
    :param java_version:
        minimum version of Java required
    :return:
    """"""
    host = client.get_transport().getpeername()[0]
    installed_java_version = get_installed_java_version(client)
<mask>:
        logger.info('Java {j} is already installed, skipping Java install'.format(j=installed_java_version))
        return
    if installed_java_version and installed_java_version > java_version:
        logger.warning('\n            Existing Java {j} installation is newer than the configured version {java_version}.\n            Your applications will be executed with Java {j}.\n            Please choose a different AMI if this does not work for you.\n            '.format(j=installed_java_version, java_version=java_version))
        return
    if installed_java_version and installed_java_version < java_version:
        logger.info('\n                Existing Java {j} will be upgraded to Adoptium OpenJDK {java_version}\n                '.format(j=installed_java_version, java_version=java_version))
    logger.info('[{h}] Installing Adoptium OpenJDK Java {j}...'.format(h=host, j=java_version))
    install_adoptium_repo(client)
    java_package = 'temurin-{j}-jdk'.format(j=java_version)
    ssh_check_output(client=client, command='\n            set -e\n\n            # Install Java first to protect packages that depend on Java from being removed.\n            sudo yum install -q -y {jp}\n\n            # Remove any older versions of Java to force the default Java to the requested version.\n            # We don\'t use /etc/alternatives because it does not seem to update links in /usr/lib/jvm correctly,\n            # and we don\'t just rely on JAVA_HOME because some programs use java directly in the PATH.\n            sudo yum remove -y java-1.6.0-openjdk java-1.7.0-openjdk\n\n            sudo sh -c ""echo export JAVA_HOME=/usr/lib/jvm/{jp} >> /etc/environment""\n            source /etc/environment\n        '.format(jp=java_package))",installed_java_version == java_version,236,installed_java_version,False,36.78794411714425,N/A
"@property
def instances(self):
<mask>:
        return [self.master_instance] + self.slave_instances
    else:
        return self.slave_instances",self.master_instance,11,self.master_instance,True,100.00000000000004,N/A
"@property
@functools.lru_cache()
def private_network(self) -> bool:
    ec2 = boto3.resource(service_name='ec2', region_name=self.region)
<mask>:
        reference_instance = self.master_instance
    else:
        reference_instance = self.slave_instances[0]
    return not ec2.Subnet(reference_instance.subnet_id).map_public_ip_on_launch",self.master_instance,21,self.master_instance,True,100.00000000000004,N/A
"@property
def slave_ips(self):
<mask>:
        return [i.private_ip_address for i in self.slave_instances]
    else:
        return [i.public_ip_address for i in self.slave_instances]",self.private_network,17,self.private,False,51.341711903259224,N/A
"def get_ssh_client(*, user: str, host: str, identity_file: str, wait: bool=False, print_status: bool=None) -> paramiko.client.SSHClient:
    """"""
    Get an SSH client for the provided host, waiting as necessary for SSH to become
    available.
    """"""
<mask>:
        print_status = wait
    client = paramiko.client.SSHClient()
    client.load_system_host_keys()
    client.set_missing_host_key_policy(paramiko.client.AutoAddPolicy())
    if wait:
        tries = 100
    else:
        tries = 3
    while tries > 0:
        try:
            tries -= 1
            client.connect(username=user, hostname=host, key_filename=identity_file, look_for_keys=False, timeout=3)
            if print_status:
                logger.info('[{h}] SSH online.'.format(h=host))
            break
        except socket.timeout as e:
            logger.debug('[{h}] SSH timeout.'.format(h=host))
            time.sleep(5)
        except paramiko.ssh_exception.NoValidConnectionsError as e:
            if any((error.errno != errno.ECONNREFUSED for error in e.errors.values())):
                raise
            logger.debug('[{h}] SSH exception: {e}'.format(h=host, e=e))
            time.sleep(5)
        except paramiko.ssh_exception.AuthenticationException as e:
            logger.debug('[{h}] SSH AuthenticationException.'.format(h=host))
            time.sleep(5)
        except paramiko.ssh_exception.SSHException as e:
            raise SSHError(host=host, message='SSH protocol error. Possible causes include using the wrong key file or username.') from e
    else:
        raise SSHError(host=host, message='Could not connect via SSH.')
    return client",print_status is None,135,print_status is None,True,100.00000000000004,N/A
"def ssh_check_output(client: paramiko.client.SSHClient, command: str, timeout_seconds: int=None):
    """"""
    Run a command via the provided SSH client and return the output captured
    on stdout.

    Raise an exception if the command returns a non-zero code.
    """"""
    stdin, stdout, stderr = client.exec_command(command, get_pty=True, timeout=timeout_seconds)
    stdout_output = stdout.read().decode('utf8').rstrip('\n')
    stderr_output = stderr.read().decode('utf8').rstrip('\n')
    exit_status = stdout.channel.recv_exit_status()
<mask>:
        raise SSHError(host=client.get_transport().getpeername()[0], message=stdout_output + stderr_output)
    return stdout_output",exit_status,58,exit_status != 0,False,30.213753973567677,N/A
"def unmount_devices(devices):
    """"""
    Unmount the provided devices.
    """"""
    with open('/proc/mounts') as m:
        mounts = [Mount(*line.split()) for line in m.read().splitlines()]
    for mount in mounts:
<mask>:
            subprocess.check_output(['sudo', 'umount', mount.device_name])",mount.device_name in [d.kname for d in devices],27,mount.device_name in devices,False,28.06734040412015,N/A
"def format_devices(devices):
    """"""
    Create an ext4 filesystem on the provided devices.
    """"""
    format_processes = []
    for device in devices:
        p = subprocess.Popen(['sudo', 'mkfs.ext4', '-F', '-E', 'lazy_itable_init=0,lazy_journal_init=0', device.kname], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        format_processes.append(p)
    for p in format_processes:
        stdout_raw, stderr_raw = p.communicate()
        stdout, stderr = (stdout_raw.decode('utf-8'), stderr_raw.decode('utf-8'))
        return_code = p.returncode
<mask>:
            raise Exception('Format process returned non-zero exit code: {code}\n{error}'.format(code=return_code, error=stderr))",return_code != 0,56,return_code != 0,True,100.00000000000004,N/A
"def display_images(self):
    for i in reversed(range(self.grid_layout.count())):
        widget = self.grid_layout.itemAt(i).widget()
<mask>:
            self.grid_layout.removeWidget(widget)
            widget.deleteLater()
    images = self.get_images_to_display()
    for i, image_path in enumerate(images):
        row = i // self.columns
        column = i % self.columns
        label = QLabel()
        pixmap = QPixmap(image_path)
        label.setPixmap(pixmap.scaledToWidth(200, Qt.SmoothTransformation))
        checkbox = QCheckBox('Image ' + str(i + 1))
        self.grid_layout.addWidget(label, row, column * 2)
        self.grid_layout.addWidget(checkbox, row, column * 2 + 1)
    self.grid_layout.setHorizontalSpacing(10)
    self.grid_layout.setVerticalSpacing(10)",widget is not None,60,widget,False,4.9787068367863965,N/A
"def start_scan(self):
<mask>:
        self.start_scan_btn.setEnabled(False)
        self.start_scan_btn.setText('Scanning...')
        self.start_scan_btn.repaint()
        self.scanner_thread = threading.Thread(target=scanning_process, args=(self.settings_index['folders_to_scan'], self.settings_index.get('folders_to_ignore', []))).start()",not self.scanner_thread or not self.scanner_thread.is_alive(),12,"self.settings_index.get('folders_to_scan', [])",False,5.750551296762977,N/A
"def add_folder(self):
    folder_path = QFileDialog.getExistingDirectory(self, 'Select Folder for Scan')
<mask>:
        folders_to_scan = list(set(self.settings_index.get('folders_to_scan', []) + [folder_path]))
        self.settings_index['folders_to_scan'] = folders_to_scan
        self.folders_to_scan_list_widget.clear()
        self.folders_to_scan_list_widget.addItem(folder_path)",folder_path,21,folder_path,True,100.00000000000004,N/A
"def ignore_folder(self):
    folder_path = QFileDialog.getExistingDirectory(self, 'Select Folder to Ignore')
<mask>:
        folders_to_ignore = list(set(self.settings_index.get('folders_to_ignore', []) + [folder_path]))
        self.settings_index['folders_to_ignore'] = folders_to_ignore
        self.folders_to_ignore_list_widget.clear()
        self.folders_to_ignore_list_widget.addItem(folder_path)",folder_path,21,folder_path,True,100.00000000000004,N/A
"def scanning_process(list_of_dirs, list_of_ignored_dirs):
    from nudenet import NudeDetector
    from liteindex import DefinedIndex
    from glob import iglob
    import os
    import hashlib
    images_index = DefinedIndex('images_index', schema={'image_path': 'string', 'image_size': 'number', 'image_hash': 'string', 'FEMALE_GENITALIA_COVERED': 'number', 'FACE_FEMALE': 'number', 'BUTTOCKS_EXPOSED': 'number', 'FEMALE_BREAST_EXPOSED': 'number', 'FEMALE_GENITALIA_EXPOSED': 'number', 'MALE_BREAST_EXPOSED': 'number', 'ANUS_EXPOSED': 'number', 'FEET_EXPOSED': 'number', 'BELLY_COVERED': 'number', 'FEET_COVERED': 'number', 'ARMPITS_COVERED': 'number', 'ARMPITS_EXPOSED': 'number', 'FACE_MALE': 'number', 'BELLY_EXPOSED': 'number', 'MALE_GENITALIA_EXPOSED': 'number', 'ANUS_COVERED': 'number', 'FEMALE_BREAST_COVERED': 'number', 'BUTTOCKS_COVERED': 'number'}, db_path='images_index.db')
    detector = NudeDetector()
    for dir in list_of_dirs:
        for file in iglob(os.path.join(dir, '**', '*.*'), recursive=True):
<mask>:
                if os.path.splitext(file)[1].lower() in ['.jpg', '.jpeg', '.png']:
                    if not any((ignored_dir in file for ignored_dir in list_of_ignored_dirs)):
                        preds = detector.detect(file)
                        data = {p['class']: float(p['score']) for p in preds}
                        data['image_path'] = file
                        data['image_size'] = os.path.getsize(file)
                        data['image_hash'] = hashlib.sha256(open(file, 'rb').read()).hexdigest()
                        images_index.update({data['image_hash']: data})",os.path.isfile(file),118,file.endswith('.nudenet'),False,8.25791079503452,N/A
"def _read_image(image_path, target_size=320):
<mask>:
        mat = cv2.imread(image_path)
    elif isinstance(image_path, np.ndarray):
        mat = image_path
    elif isinstance(image_path, bytes):
        mat = cv2.imdecode(np.frombuffer(image_path, np.uint8), -1)
    elif isinstance(image_path, _io.BufferedReader):
        mat = cv2.imdecode(np.frombuffer(image_path.read(), np.uint8), -1)
    else:
        raise ValueError('please make sure the image_path is str or np.ndarray or bytes')
    image_original_width, image_original_height = (mat.shape[1], mat.shape[0])
    mat_c3 = cv2.cvtColor(mat, cv2.COLOR_RGBA2BGR)
    max_size = max(mat_c3.shape[:2])
    x_pad = max_size - mat_c3.shape[1]
    x_ratio = max_size / mat_c3.shape[1]
    y_pad = max_size - mat_c3.shape[0]
    y_ratio = max_size / mat_c3.shape[0]
    mat_pad = cv2.copyMakeBorder(mat_c3, 0, y_pad, 0, x_pad, cv2.BORDER_CONSTANT)
    input_blob = cv2.dnn.blobFromImage(mat_pad, 1 / 255.0, (target_size, target_size), (0, 0, 0), swapRB=True, crop=False)
    return (input_blob, x_ratio, y_ratio, x_pad, y_pad, image_original_width, image_original_height)","isinstance(image_path, str)",103,"isinstance(image_path, str)",True,100.00000000000004,N/A
"def _postprocess(output, x_pad, y_pad, x_ratio, y_ratio, image_original_width, image_original_height, model_width, model_height):
    outputs = np.transpose(np.squeeze(output[0]))
    rows = outputs.shape[0]
    boxes = []
    scores = []
    class_ids = []
    for i in range(rows):
        classes_scores = outputs[i][4:]
        max_score = np.amax(classes_scores)
<mask>:
            class_id = np.argmax(classes_scores)
            x, y, w, h = outputs[i][0:4]
            x = x - w / 2
            y = y - h / 2
            x = x * (image_original_width + x_pad) / model_width
            y = y * (image_original_height + y_pad) / model_height
            w = w * (image_original_width + x_pad) / model_width
            h = h * (image_original_height + y_pad) / model_height
            x = x
            y = y
            x = max(0, min(x, image_original_width))
            y = max(0, min(y, image_original_height))
            w = min(w, image_original_width - x)
            h = min(h, image_original_height - y)
            class_ids.append(class_id)
            scores.append(max_score)
            boxes.append([x, y, w, h])
    indices = cv2.dnn.NMSBoxes(boxes, scores, 0.25, 0.45)
    detections = []
    for i in indices:
        box = boxes[i]
        score = scores[i]
        class_id = class_ids[i]
        x, y, w, h = box
        detections.append({'class': __labels[class_id], 'score': float(score), 'box': [int(x), int(y), int(w), int(h)]})
    return detections",max_score >= 0.2,168,len(classes_scores) > 4,False,6.567274736060395,N/A
"def censor(self, image_path, classes=[], output_path=None):
    detections = self.detect(image_path)
<mask>:
        detections = [detection for detection in detections if detection['class'] in classes]
    img = cv2.imread(image_path)
    for detection in detections:
        box = detection['box']
        x, y, w, h = (box[0], box[1], box[2], box[3])
        img[y:y + h, x:x + w] = (0, 0, 0)
    if not output_path:
        image_path, ext = os.path.splitext(image_path)
        output_path = f'{image_path}_censored{ext}'
    cv2.imwrite(output_path, img)
    return output_path",classes,63,classes,True,100.00000000000004,N/A
"def get_mem_stats(fields: List[str]) -> Dict[str, int]:
    stats = {}
    with open('/proc/meminfo') as meminfo:
        for line in meminfo:
            items = line.split()
            key = items[0][:-1]
<mask>:
                fields.remove(key)
                stats[key] = int(items[1]) * 1024
            if not fields:
                break
    assert len(fields) == 0
    return stats",items[2] == 'kB' and key in fields,40,key in fields,False,6.948345122280157,N/A
"def __init__(self):
    os.environ['NCPU'] = str(NCPU)
    os.environ['RAM_SIZE'] = str(RAM_SIZE)
    self.config = {}
<mask>:
        try:
            self.config.update(Config.parse_config(DEF_CONFIG))
        except OSError:
            error(f'Error loading {DEF_CONFIG}')
    if os.path.isfile(ETC_CONFIG):
        try:
            self.config.update(Config.parse_config(ETC_CONFIG))
        except OSError:
            warn(f'Could not load {DEF_CONFIG}')
    config_files = {}
    for path in [VEN_SYSD, RUN_SYSD, ETC_SYSD]:
        path += '/swap.conf.d'
        for file_path in glob.glob(f'{path}/*.conf'):
            if not os.access(file_path, os.R_OK) or os.path.isdir(file_path):
                if os.path.isfile(file_path):
                    warn(f'Permission denied reading: {file_path}')
                continue
            config_files[os.path.basename(file_path)] = file_path
            debug(f'Found {file_path}')
    debug(f'Selected configuration artifacts: {list(config_files.values())}')
    config_files = dict(sorted(config_files.items()))
    for config_file in config_files.values():
        info(f'Load: {config_file}')
        self.config.update(Config.parse_config(config_file))",os.path.isfile(DEF_CONFIG),77,os.path.isfile(DEF_CONFIG),True,100.00000000000004,N/A
"def get(self, key: str, as_type: Type=str) -> as_type:
<mask>:
        return self.config[key].lower() in ['yes', 'y', '1', 'true']
    return as_type(self.config[key])",as_type is bool,18,"isinstance(key, str)",False,0.0,N/A
"@staticmethod
def parse_config(file: str) -> Dict[str, str]:
    config = {}
    lines = None
    with open(file) as f:
        lines = f.read().splitlines()
    for line in lines:
        line = line.strip()
<mask>:
            continue
        key, value = line.split('=', 1)
        config[key] = subprocess.run([f'echo {value}'], shell=True, check=True, text=True, stdout=subprocess.PIPE).stdout.rstrip()
    return config",line.startswith('#') or '=' not in line,44,not line,False,0.10630920484560723,N/A
"def __init__(self, config: Config, sem: sysv_ipc.Semaphore):
    self.assign_config(config)
    self.sem = sem
<mask>:
        warn(f'swapfc_frequency must be in range of 1..86400: {self.swapfc_frequency} - set to 1')
        self.swapfc_frequency = 1
    self.polling_rate = self.swapfc_frequency
    systemd.daemon.notify('STATUS=Monitoring memory status...')
    makedirs(os.path.dirname(self.swapfc_path))
    self.fs_type, subvolume = self.get_fs_type()
    if self.fs_type == 'btrfs':
        if not subvolume:
            if os.path.exists(self.swapfc_path):
                warn('swapFC: swapfc_path is an existing btrfs inode but not a subvolume, removing...')
                if os.path.isdir(self.swapfc_path):
                    shutil.rmtree(self.swapfc_path)
                else:
                    os.remove(self.swapfc_path)
            subprocess.run(['btrfs', 'subvolume', 'create', self.swapfc_path], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    else:
        makedirs(self.swapfc_path)
    self.chunk_size = int(subprocess.run(['numfmt', '--to=none', '--from=iec', self.swapfc_chunk_size], check=True, text=True, stdout=subprocess.PIPE).stdout)
    self.block_size = os.statvfs(self.swapfc_path).f_bsize
    if self.fs_type == 'btrfs':
        if KMAJOR >= 5:
            self.swapfc_nocow = True
        else:
            self.swapfc_force_use_loop = True
    if not 1 <= self.swapfc_max_count <= 32:
        warn('swapfc_max_count must be in range 1..32, reset to 1')
        self.swapfc_max_count = 1
    makedirs(f'{WORK_DIR}/swapfc')
    self.allocated = 0
    for _ in range(self.swapfc_min_count):
        self.create_swapfile()",not 1 <= self.swapfc_frequency <= 24 * 60 * 60,127,not 1 <= self.swapfc_frequency <= 86400,False,64.74591278836637,N/A
"def copy_configs(wpa_enabled_choice):
    os.system('mkdir /usr/lib/raspiwifi')
    os.system('mkdir /etc/raspiwifi')
    os.system('cp -a libs/* /usr/lib/raspiwifi/')
    os.system('mv /etc/wpa_supplicant/wpa_supplicant.conf /etc/wpa_supplicant/wpa_supplicant.conf.original')
    os.system('rm -f ./tmp/*')
    os.system('mv /etc/dnsmasq.conf /etc/dnsmasq.conf.original')
    os.system('cp /usr/lib/raspiwifi/reset_device/static_files/dnsmasq.conf /etc/')
<mask>:
        os.system('cp /usr/lib/raspiwifi/reset_device/static_files/hostapd.conf.wpa /etc/hostapd/hostapd.conf')
    else:
        os.system('cp /usr/lib/raspiwifi/reset_device/static_files/hostapd.conf.nowpa /etc/hostapd/hostapd.conf')
    os.system('mv /etc/dhcpcd.conf /etc/dhcpcd.conf.original')
    os.system('cp /usr/lib/raspiwifi/reset_device/static_files/dhcpcd.conf /etc/')
    os.system('mkdir /etc/cron.raspiwifi')
    os.system('cp /usr/lib/raspiwifi/reset_device/static_files/aphost_bootstrapper /etc/cron.raspiwifi')
    os.system('chmod +x /etc/cron.raspiwifi/aphost_bootstrapper')
    os.system('echo ""# RaspiWiFi Startup"" >> /etc/crontab')
    os.system('echo ""@reboot root run-parts /etc/cron.raspiwifi/"" >> /etc/crontab')
    os.system('mv /usr/lib/raspiwifi/reset_device/static_files/raspiwifi.conf /etc/raspiwifi')
    os.system('touch /etc/raspiwifi/host_mode')",wpa_enabled_choice.lower() == 'y',62,wpa_enabled_choice,False,24.659696394160658,N/A
"def update_main_config_file(entered_ssid, auto_config_choice, auto_config_delay, ssl_enabled_choice, server_port_choice, wpa_enabled_choice, wpa_entered_key):
<mask>:
        os.system(""sed -i 's/RaspiWiFi Setup/"" + entered_ssid + ""/' /etc/raspiwifi/raspiwifi.conf"")
    if wpa_enabled_choice.lower() == 'y':
        os.system(""sed -i 's/wpa_enabled=0/wpa_enabled=1/' /etc/raspiwifi/raspiwifi.conf"")
        os.system(""sed -i 's/wpa_key=0/wpa_key="" + wpa_entered_key + ""/' /etc/raspiwifi/raspiwifi.conf"")
    if auto_config_choice.lower() == 'y':
        os.system(""sed -i 's/auto_config=0/auto_config=1/' /etc/raspiwifi/raspiwifi.conf"")
    if auto_config_delay != '':
        os.system(""sed -i 's/auto_config_delay=300/auto_config_delay="" + auto_config_delay + ""/' /etc/raspiwifi/raspiwifi.conf"")
    if ssl_enabled_choice.lower() == 'y':
        os.system(""sed -i 's/ssl_enabled=0/ssl_enabled=1/' /etc/raspiwifi/raspiwifi.conf"")
    if server_port_choice != '':
        os.system(""sed -i 's/server_port=80/server_port="" + server_port_choice + ""/' /etc/raspiwifi/raspiwifi.conf"")",entered_ssid != '',74,entered_ssid != '',True,100.00000000000004,N/A
"def wpa_check_activate(wpa_enabled, wpa_key):
    wpa_active = False
    reboot_required = False
    with open('/etc/hostapd/hostapd.conf') as hostapd_conf:
        for line in hostapd_conf:
<mask>:
                wpa_active = True
    if wpa_enabled == '1' and wpa_active == False:
        reboot_required = True
        os.system('cp /usr/lib/raspiwifi/reset_device/static_files/hostapd.conf.wpa /etc/hostapd/hostapd.conf')
    if wpa_enabled == '1':
        with fileinput.FileInput('/etc/hostapd/hostapd.conf', inplace=True) as hostapd_conf:
            for line in hostapd_conf:
                if 'wpa_passphrase' in line:
                    if 'wpa_passphrase=' + wpa_key not in line:
                        print('wpa_passphrase=' + wpa_key)
                        os.system('reboot')
                    else:
                        print(line, end='')
                else:
                    print(line, end='')
    if wpa_enabled == '0' and wpa_active == True:
        reboot_required = True
        os.system('cp /usr/lib/raspiwifi/reset_device/static_files/hostapd.conf.nowpa /etc/hostapd/hostapd.conf')
    return reboot_required",'wpa_passphrase' in line,85,'wpa_enabled' in line,False,30.213753973567677,N/A
"def update_ssid(ssid_prefix, serial_last_four):
    reboot_required = False
    ssid_correct = False
    with open('/etc/hostapd/hostapd.conf') as hostapd_conf:
        for line in hostapd_conf:
<mask>:
                ssid_correct = True
    if ssid_correct == False:
        with fileinput.FileInput('/etc/hostapd/hostapd.conf', inplace=True) as file:
            for line in file:
                if 'ssid=' in line:
                    line_array = line.split('=')
                    line_array[1] = ssid_prefix + ' ' + serial_last_four
                    print(line_array[0] + '=' + line_array[1])
                else:
                    print(line, end='')
        reboot_required = True
    return reboot_required",ssid_prefix in line,62,'ssid=' in line,False,21.3643503198117,N/A
"def is_wifi_active():
    iwconfig_out = subprocess.check_output(['iwconfig']).decode('utf-8')
    wifi_active = True
<mask>:
        wifi_active = False
    return wifi_active",'Access Point: Not-Associated' in iwconfig_out,14,'wifi_active' in iwconfig_out,False,35.64026463354184,N/A
"def reset_to_host_mode():
<mask>:
        os.system('aplay /usr/lib/raspiwifi/reset_device/button_chime.wav')
        os.system('rm -f /etc/wpa_supplicant/wpa_supplicant.conf')
        os.system('rm -f /home/pi/Projects/RaspiWifi/tmp/*')
        os.system('rm /etc/cron.raspiwifi/apclient_bootstrapper')
        os.system('cp /usr/lib/raspiwifi/reset_device/static_files/aphost_bootstrapper /etc/cron.raspiwifi/')
        os.system('chmod +x /etc/cron.raspiwifi/aphost_bootstrapper')
        os.system('mv /etc/dhcpcd.conf /etc/dhcpcd.conf.original')
        os.system('cp /usr/lib/raspiwifi/reset_device/static_files/dhcpcd.conf /etc/')
        os.system('mv /etc/dnsmasq.conf /etc/dnsmasq.conf.original')
        os.system('cp /usr/lib/raspiwifi/reset_device/static_files/dnsmasq.conf /etc/')
        os.system('cp /usr/lib/raspiwifi/reset_device/static_files/dhcpcd.conf /etc/')
        os.system('touch /etc/raspiwifi/host_mode')
    os.system('reboot')",not os.path.isfile('/etc/raspiwifi/host_mode'),37,os.name == 'nt',False,2.5953911943191175,N/A
"@app.route('/save_wpa_credentials', methods=['GET', 'POST'])
def save_wpa_credentials():
    config_hash = config_file_hash()
    wpa_enabled = request.form.get('wpa_enabled')
    wpa_key = request.form['wpa_key']
<mask>:
        update_wpa(1, wpa_key)
    else:
        update_wpa(0, wpa_key)

    def sleep_and_reboot_for_wpa():
        time.sleep(2)
        os.system('reboot')
    t = Thread(target=sleep_and_reboot_for_wpa)
    t.start()
    config_hash = config_file_hash()
    return render_template('save_wpa_credentials.html', wpa_enabled=config_hash['wpa_enabled'], wpa_key=config_hash['wpa_key'])",str(wpa_enabled) == '1',35,wpa_enabled,False,13.533528323661276,N/A
"def scan_wifi_networks():
    iwlist_raw = subprocess.Popen(['iwlist', 'scan'], stdout=subprocess.PIPE)
    ap_list, err = iwlist_raw.communicate()
    ap_array = []
    for line in ap_list.decode('utf-8').rsplit('\n'):
<mask>:
            ap_ssid = line[27:-1]
            if ap_ssid != '':
                ap_array.append(ap_ssid)
    return ap_array",'ESSID' in line,29,line.startswith('wifi:'),False,5.522397783539471,N/A
"def create_wpa_supplicant(ssid, wifi_key):
    temp_conf_file = open('wpa_supplicant.conf.tmp', 'w')
    temp_conf_file.write('ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev\n')
    temp_conf_file.write('update_config=1\n')
    temp_conf_file.write('\n')
    temp_conf_file.write('network={\n')
    temp_conf_file.write('\tssid=""' + ssid + '""\n')
<mask>:
        temp_conf_file.write('\tkey_mgmt=NONE\n')
    else:
        temp_conf_file.write('\tpsk=""' + wifi_key + '""\n')
    temp_conf_file.write('\t}')
    temp_conf_file.close
    os.system('mv wpa_supplicant.conf.tmp /etc/wpa_supplicant/wpa_supplicant.conf')",wifi_key == '',30,wifi_key == 'NONE',False,75.98356856515926,N/A
"def update_wpa(wpa_enabled, wpa_key):
    with fileinput.FileInput('/etc/raspiwifi/raspiwifi.conf', inplace=True) as raspiwifi_conf:
        for line in raspiwifi_conf:
<mask>:
                line_array = line.split('=')
                line_array[1] = wpa_enabled
                print(line_array[0] + '=' + str(line_array[1]))
            if 'wpa_key=' in line:
                line_array = line.split('=')
                line_array[1] = wpa_key
                print(line_array[0] + '=' + line_array[1])
            if 'wpa_enabled=' not in line and 'wpa_key=' not in line:
                print(line, end='')",'wpa_enabled=' in line,51,'wpa_enabled=' in line,True,100.00000000000004,N/A
"@contextlib.contextmanager
def closed_tempfile(suffix, text=None, dir_name=None, prefix=None):
    file_name = None
    try:
<mask>:
            dir_name = tempfile.mkdtemp(suffix=dir_name)
        with tempfile.NamedTemporaryFile('w+t', suffix=suffix, prefix=prefix, delete=False, dir=dir_name) as test_file:
            file_name = test_file.name
            if text:
                test_file.write(text)
                test_file.flush()
        yield file_name
    finally:
        if dir_name:
            shutil.rmtree(dir_name, ignore_errors=True)
        elif file_name:
            os.remove(file_name)",dir_name,39,dir_name,True,100.00000000000004,N/A
"@contextlib.contextmanager
def assert_produces_warning(expected_warning=Warning, filter_level='always', clear=None, check_stacklevel=True):
    """"""
    Context manager for running code that expects to raise (or not raise)
    warnings.  Checks that code raises the expected warning and only the
    expected warning. Pass ``False`` or ``None`` to check that it does *not*
    raise a warning. Defaults to ``exception.Warning``, baseclass of all
    Warnings. (basically a wrapper around ``warnings.catch_warnings``).
    >>> import warnings
    >>> with assert_produces_warning():
    ...     warnings.warn(UserWarning())
    ...
    >>> with assert_produces_warning(False):
    ...     warnings.warn(RuntimeWarning())
    ...
    Traceback (most recent call last):
        ...
    AssertionError: Caused unexpected warning(s): ['RuntimeWarning'].
    >>> with assert_produces_warning(UserWarning):
    ...     warnings.warn(RuntimeWarning())
    Traceback (most recent call last):
        ...
    AssertionError: Did not see expected warning of class 'UserWarning'.
    ..warn:: This is *not* thread-safe.
    """"""
    with warnings.catch_warnings(record=True) as w:
<mask>:
            if not is_list_like(clear):
                clear = [clear]
            for m in clear:
                try:
                    m.__warningregistry__.clear()
                except Exception as e:
                    print(str(e))
        saw_warning = False
        warnings.simplefilter(filter_level)
        yield w
        extra_warnings = []
        for actual_warning in w:
            if expected_warning and issubclass(actual_warning.category, expected_warning):
                saw_warning = True
                if check_stacklevel and issubclass(actual_warning.category, (FutureWarning, DeprecationWarning)):
                    from inspect import getframeinfo, stack
                    caller = getframeinfo(stack()[2][0])
                    msg = ('Warning not set with correct stacklevel. ' + 'File where warning is raised: {0} != {1}. ' + 'Warning message: {2}').format(actual_warning.filename, caller.filename, actual_warning.message)
                    assert actual_warning.filename == caller.filename, msg
            else:
                extra_warnings.append(actual_warning.category.__name__)
        if expected_warning:
            assert saw_warning, 'Did not see expected warning of class %r.' % expected_warning.__name__
        assert not extra_warnings, 'Caused unexpected warning(s): %r.' % extra_warnings",clear is not None,221,clear,False,4.9787068367863965,N/A
"def setUp(self):
<mask>:
        if 'TRAVIS_BUILD_DIR' in os.environ:
            os.environ['HOME'] = os.environ['TRAVIS_BUILD_DIR']
            print('Using TRAVIS_BUILD_DIR as HOME')
        else:
            os.environ['HOME'] = str(os.getcwd())
            print('Using current dir as HOME')
    print(os.environ['HOME'])",'HOME' not in os.environ,24,'HOME' not in os.environ,True,100.00000000000004,N/A
"def test_conversion_with_data_files(self):
    test_data_dir = os.path.join(os.path.dirname(__file__), 'test_data')
    test_docx_file = os.path.join(test_data_dir, 'test.docx')
<mask>:
        os.remove(test_docx_file)
    result = pypandoc.convert_file(os.path.join(test_data_dir, 'index.html'), to='docx', format='html', outputfile=test_docx_file, sandbox=True)
    print(result)",os.path.exists(test_docx_file),21,os.path.exists(test_docx_file),True,100.00000000000004,N/A
"def test_convert_with_custom_writer(self):
    version = pypandoc.get_pandoc_version()
    major = int(version.split('.')[0])
<mask>:
        return
    lua_file_content = self.create_sample_lua()
    with closed_tempfile('.md', text='# title\n') as file_name:
        with closed_tempfile('.lua', text=lua_file_content, dir_name='foo-bar+baz') as lua_file_name:
            expected = f'<h1 id=""title"">title</h1>{os.linesep}'
            received = pypandoc.convert_file(file_name, lua_file_name)
            self.assertEqualExceptForNewlineEnd(expected, received)",major == 3,35,major != 2,False,18.99589214128981,N/A
"def _check_log_handler():
<mask>:
        return
    ch = logging.StreamHandler()
    ch.setLevel(logging.DEBUG)
    formatter = logging.Formatter('[%(levelname)s] %(message)s')
    ch.setFormatter(formatter)
    logger.addHandler(ch)",logger.handlers,14,logger.handlers is None,False,39.76353643835252,N/A
"def convert_text(source: typing.Union[str, bytes], to: str, format: str, extra_args: Iterable=(), encoding: str='utf-8', outputfile: Union[None, str, Path]=None, filters: Union[Iterable, None]=None, verify_format: bool=True, sandbox: bool=False, cworkdir: Union[str, None]=None) -> str:
    """"""Converts given `source` from `format` to `to`.

    :param source: Unicode string or bytes (see encoding)

    :param str to: format into which the input should be converted;
        can be one of `pypandoc.get_pandoc_formats()[1]`

    :param str format: the format of the inputs;
        can be one of `pypandoc.get_pandoc_formats()[1]`

    :param list extra_args: extra arguments (list of strings) to be passed to pandoc
        (Default value = ())

    :param str encoding: the encoding of the input bytes (Default value = 'utf-8')

    :param str outputfile: output will be written to outputfile
        or the converted content returned if None.
        The output filename can be specified as a string or pathlib.Path object.
        (Default value = None)

    :param list filters: pandoc filters e.g. filters=['pandoc-citeproc']

    :param bool verify_format: Verify from and to format before converting.
        Should only be set False when confident of the formats
        and performance is an issue. (Default value = True)

    :param bool sandbox: Run pandoc in pandocs own sandbox mode, limiting IO operations
        in readers and writers to reading the files specified on the command line.
        Anyone using pandoc on untrusted user input should use this option.
        Note: This only does something, on pandoc >= 2.15 (Default value = False)

    :param str cworkdir: set the current working directory (Default value = None)

    :returns: converted string or an empty string if an outputfile was given
    :rtype: str

    :raises RuntimeError:
        if any of the inputs are not valid of if pandoc fails with an error
    :raises OSError:
        if pandoc is not found; make sure it has been installed
        and is available at path.
    """"""
<mask>:
        source = source.decode(encoding, errors='ignore')
    return _convert_input(source, format, 'string', to, extra_args=extra_args, outputfile=outputfile, filters=filters, verify_format=verify_format, sandbox=sandbox, cworkdir=cworkdir)","isinstance(source, bytes)",298,"not isinstance(source, str)",False,43.47208719449914,N/A
"def _identify_path(source) -> bool:
<mask>:
        for single_source in source:
            if not _identify_path(single_source):
                return False
        return True
    is_path = False
    try:
        is_path = os.path.exists(source)
    except UnicodeEncodeError:
        is_path = os.path.exists(source.encode('utf-8'))
    except:
        pass
    if not is_path:
        try:
            is_path = len(glob.glob(source)) >= 1
        except UnicodeEncodeError:
            is_path = len(glob.glob(source.encode('utf-8'))) >= 1
        except:
            pass
    if not is_path:
        try:
            result = urllib.parse.urlparse(source)
            if result.scheme in ['http', 'https']:
                is_path = True
            elif result.scheme and result.netloc and result.path:
                is_path = True
            elif result.scheme == 'file' and result.path:
                is_path = os.path.exists(url2path(source))
        except AttributeError:
            pass
    return is_path","isinstance(source, list)",86,"isinstance(source, list)",True,100.00000000000004,N/A
"def _is_network_path(source):
    try:
        result = urllib.parse.urlparse(source)
<mask>:
            return True
        elif result.scheme and result.netloc and result.path:
            return True
        elif result.scheme == 'file' and result.path:
            return os.path.exists(url2path(source))
    except AttributeError:
        pass
    return False","result.scheme in ['http', 'https']",30,result.scheme and result.netloc,False,18.370727471078336,N/A
"def normalize_format(fmt):
    formats = {'dbk': 'docbook', 'md': 'markdown', 'tex': 'latex'}
    fmt = formats.get(fmt, fmt)
<mask>:
        fmt = 'rst' + fmt[4:]
    return fmt",fmt[:4] == 'rest',22,fmt.startswith('rst'),False,5.815868174415823,N/A
"def _handle_linux(filename, targetfolder):
    logger.info(f'Unpacking {filename} to tempfolder...')
    tempfolder = tempfile.mkdtemp()
    cur_wd = os.getcwd()
    filename = os.path.abspath(filename)
    try:
        os.chdir(tempfolder)
        cmd = ['ar', 'x', filename]
        subprocess.check_call(cmd)
        files = os.listdir('.')
        archive_name = next((x for x in files if x.startswith('data.tar')))
        cmd = ['tar', 'xf', archive_name]
        subprocess.check_call(cmd)
        exe = 'pandoc'
        src = os.path.join(tempfolder, 'usr', 'bin', exe)
        dst = os.path.join(targetfolder, exe)
        logger.info(f'Copying {exe} to {targetfolder} ...')
        shutil.copyfile(src, dst)
        _make_executable(dst)
        exe = 'pandoc-citeproc'
        src = os.path.join(tempfolder, 'usr', 'bin', exe)
        dst = os.path.join(targetfolder, exe)
<mask>:
            logger.info(f'Copying {exe} to {targetfolder} ...')
            shutil.copyfile(src, dst)
            _make_executable(dst)
        src = os.path.join(tempfolder, 'usr', 'share', 'doc', 'pandoc', 'copyright')
        dst = os.path.join(targetfolder, 'copyright.pandoc')
        logger.info(f'Copying copyright to {targetfolder} ...')
        shutil.copyfile(src, dst)
    finally:
        os.chdir(cur_wd)
        shutil.rmtree(tempfolder)",os.path.exists(src),107,"exe in ['pandoc-citeproc', 'copyright']",False,0.0,N/A
"def _handle_darwin(filename, targetfolder):
    logger.info(f'Unpacking {filename} to tempfolder...')
    tempfolder = tempfile.mkdtemp()
    pkgutilfolder = os.path.join(tempfolder, 'tmp')
    cmd = ['pkgutil', '--expand', filename, pkgutilfolder]
    subprocess.check_call(cmd)
    cmd = ['tar', 'xvf', os.path.join(pkgutilfolder, 'pandoc.pkg', 'Payload'), '-C', pkgutilfolder]
    subprocess.check_call(cmd)
    exe = 'pandoc'
    src = os.path.join(pkgutilfolder, 'usr', 'local', 'bin', exe)
    dst = os.path.join(targetfolder, exe)
    logger.info(f'Copying {exe} to {targetfolder} ...')
    shutil.copyfile(src, dst)
    _make_executable(dst)
    exe = 'pandoc-citeproc'
    src = os.path.join(pkgutilfolder, 'usr', 'local', 'bin', exe)
    dst = os.path.join(targetfolder, exe)
<mask>:
        logger.info(f'Copying {exe} to {targetfolder} ...')
        shutil.copyfile(src, dst)
        _make_executable(dst)
    shutil.rmtree(tempfolder)
    logger.info('Done.')",os.path.exists(src),78,not os.path.exists(dst),False,61.04735835807847,N/A
"def _handle_win32(filename, targetfolder):
    logger.info(f'Unpacking {filename} to tempfolder...')
    tempfolder = tempfile.mkdtemp()
    cmd = ['msiexec', '/a', filename, '/qb', 'TARGETDIR=%s' % tempfolder]
    subprocess.check_call(cmd)
    exe = 'pandoc.exe'
    src = os.path.join(tempfolder, 'Pandoc', exe)
    dst = os.path.join(targetfolder, exe)
    logger.info(f'Copying {exe} to {targetfolder} ...')
    shutil.copyfile(src, dst)
    exe = 'pandoc-citeproc.exe'
    src = os.path.join(tempfolder, 'Pandoc', exe)
    dst = os.path.join(targetfolder, exe)
<mask>:
        logger.info(f'Copying {exe} to {targetfolder} ...')
        shutil.copyfile(src, dst)
    exe = 'COPYRIGHT.txt'
    src = os.path.join(tempfolder, 'Pandoc', exe)
    dst = os.path.join(targetfolder, exe)
    logger.info(f'Copying {exe} to {targetfolder} ...')
    shutil.copyfile(src, dst)
    shutil.rmtree(tempfolder)
    logger.info('Done.')",os.path.exists(src),80,os.name == 'nt',False,11.631736348831648,N/A
"def download_pandoc(url: Union[str, None]=None, targetfolder: Union[str, None]=None, version: str='latest', delete_installer: bool=False, download_folder: Union[str, None]=None) -> None:
    """"""Download and unpack pandoc

    Downloads prebuild binaries for pandoc from `url` and unpacks it into
    `targetfolder`.

    :param str url: URL for the to be downloaded pandoc binary distribution for
        the platform under which this python runs. If no `url` is give, uses
        the latest available release at the time pypandoc was released.

    :param str targetfolder: directory, where the binaries should be installed
        to. If no `targetfolder` is given, uses a platform specific user
        location: `~/bin` on Linux, `~/Applications/pandoc` on Mac OS X, and
        `~\\AppData\\Local\\Pandoc` on Windows.

    :param str download_folder: Directory where the installer should download files
        before unpacking to the target folder. If no `download_folder` is given,
        uses the current directory. example: `/tmp/`, `/tmp`
    """"""
    _check_log_handler()
    pf = sys.platform
<mask>:
        if pf.startswith('linux'):
            pf = 'linux'
            arch = platform.architecture()[0]
            if arch != '64bit':
                raise RuntimeError(f'Linux pandoc is only compiled for 64bit. Got arch={arch}.')
        pandoc_urls, _ = _get_pandoc_urls(version)
        if pf not in pandoc_urls:
            raise RuntimeError(""Can't handle your platform (only Linux, Mac OS X, Windows)."")
        url = pandoc_urls[pf]
    filename = url.split('/')[-1]
    if download_folder is not None:
        if download_folder.endswith('/'):
            download_folder = download_folder[:-1]
        filename = os.path.join(os.path.expanduser(download_folder), filename)
    if os.path.isfile(filename):
        logger.info(f'Using already downloaded file {filename}')
    else:
        logger.info(f'Downloading pandoc from {url} ...')
        response = urllib.request.urlopen(url)
        with open(filename, 'wb') as out_file:
            shutil.copyfileobj(response, out_file)
    if targetfolder is None:
        targetfolder = DEFAULT_TARGET_FOLDER[pf]
    targetfolder = os.path.expanduser(targetfolder)
    try:
        os.makedirs(targetfolder)
    except OSError:
        pass
    unpack = globals().get('_handle_' + pf)
    assert unpack is not None, ""Can't handle download, only Linux, Windows and OS X are supported.""
    unpack(filename, targetfolder)
    if delete_installer:
        os.remove(filename)",url is None,262,url is not None,False,35.35533905932737,N/A
"def __init__(self, dtype=None, *args, **kwargs):
<mask>:
        dtype = keras.backend.floatx()
    self.dtype = dtype
    super(Cast, self).__init__(*args, **kwargs)",dtype is None,15,dtype is None,True,100.00000000000004,N/A
"def create_generator(args):
    transform_generator = random_transform_generator(flip_x_chance=0.5)
<mask>:
        from ..preprocessing.coco import CocoGenerator
        generator = CocoGenerator(args.coco_path, args.coco_set, transform_generator=transform_generator, config=args.config)
    elif args.dataset_type == 'csv':
        from ..preprocessing.csv_generator import CSVGenerator
        generator = CSVGenerator(args.annotations, args.classes, transform_generator=transform_generator, config=args.config)
    else:
        raise ValueError('Invalid data type received: {}'.format(args.dataset_type))
    return generator",args.dataset_type == 'coco',39,args.dataset_type == 'coco',True,100.00000000000004,N/A
"def run(generator, args, anchor_params):
    for i in range(generator.size()):
        image = generator.load_image(i)
        annotations = generator.load_annotations(i)
<mask>:
            image, annotations = generator.random_transform_group_entry(image, annotations)
        if args.resize:
            image, image_scale = generator.resize_image(image)
            annotations['bboxes'] *= image_scale
            for m in range(len(annotations['masks'])):
                annotations['masks'][m], _ = generator.resize_image(annotations['masks'][m])
        anchors = anchors_for_shape(image.shape, anchor_params=anchor_params)
        positive_indices, _, max_indices = compute_gt_annotations(anchors, annotations['bboxes'])
        if args.anchors:
            anchors = generator.generate_anchors(image.shape)
            positive_indices, _, max_indices = compute_gt_annotations(anchors, annotations['bboxes'])
            draw_boxes(image, anchors[positive_indices], (255, 255, 0), thickness=1)
        if args.annotations:
            draw_annotations(image, annotations, color=(0, 0, 255), label_to_name=generator.label_to_name)
            draw_boxes(image, annotations['bboxes'][max_indices[positive_indices], :], (0, 255, 0))
        if args.masks:
            for m in range(len(annotations['masks'])):
                box = annotations['bboxes'][m].astype(int)
                mask = annotations['masks'][m][box[1]:box[3], box[0]:box[2]]
                draw_mask(image, box, mask, annotations['labels'][m].astype(int))
                caption = '{}'.format(generator.label_to_name(annotations['labels'][m]))
                draw_caption(image, box, caption)
        cv2.imshow('Image', image)
        if cv2.waitKey() == ord('q'):
            return False
    return True",args.random_transform,111,args.random,False,51.341711903259224,N/A
"def main(args=None):
<mask>:
        args = sys.argv[1:]
    args = parse_args(args)
    generator = create_generator(args)
    if args.config:
        args.config = read_config_file(args.config)
    anchor_params = None
    if args.config and 'anchor_parameters' in args.config:
        anchor_params = parse_anchor_parameters(args.config)
    cv2.namedWindow('Image', cv2.WINDOW_NORMAL)
    if args.loop:
        while run(generator, args, anchor_params):
            pass
    else:
        run(generator, args, anchor_params)",args is None,42,args is None,True,100.00000000000004,N/A
"def model_with_weights(model, weights, skip_mismatch):
<mask>:
        model.load_weights(weights, by_name=True, skip_mismatch=skip_mismatch)
    return model",weights is not None,10,weights is not None,True,100.00000000000004,N/A
"def create_callbacks(model, training_model, prediction_model, validation_generator, args, create_evaluation=Evaluate):
    callbacks = []
<mask>:
        os.makedirs(args.snapshot_path, exist_ok=True)
        checkpoint = keras.callbacks.ModelCheckpoint(os.path.join(args.snapshot_path, '{backbone}_{dataset_type}_{{epoch:02d}}.h5'.format(backbone=args.backbone, dataset_type=args.dataset_type)), verbose=1)
        checkpoint = RedirectModel(checkpoint, model)
        callbacks.append(checkpoint)
    tensorboard_callback = None
    if args.tensorboard_dir:
        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=args.tensorboard_dir, histogram_freq=0, batch_size=args.batch_size, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)
        callbacks.append(tensorboard_callback)
    if args.evaluation and validation_generator:
        if args.dataset_type == 'coco':
            from ..callbacks.coco import CocoEval
            evaluation = CocoEval(validation_generator)
        elif create_evaluation:
            evaluation = create_evaluation(validation_generator, tensorboard=tensorboard_callback, weighted_average=args.weighted_average)
        else:
            evaluation = Evaluate(validation_generator, tensorboard=tensorboard_callback, weighted_average=args.weighted_average)
        evaluation = RedirectModel(evaluation, prediction_model)
        callbacks.append(evaluation)
    callbacks.append(keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=2, verbose=1, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0))
    return callbacks",args.snapshots,84,args.snapshot_path,False,21.3643503198117,N/A
"def create_generators(args):
    transform_generator = random_transform_generator(flip_x_chance=0.5)
<mask>:
        from ..preprocessing.coco import CocoGenerator
        train_generator = CocoGenerator(args.coco_path, 'train2017', transform_generator=transform_generator, batch_size=args.batch_size, config=args.config)
        validation_generator = None
        if args.evaluation:
            validation_generator = CocoGenerator(args.coco_path, 'val2017', batch_size=args.batch_size, config=args.config)
    elif args.dataset_type == 'csv':
        from ..preprocessing.csv_generator import CSVGenerator
        train_generator = CSVGenerator(args.annotations, args.classes, transform_generator=transform_generator, batch_size=args.batch_size, config=args.config)
        if args.val_annotations:
            validation_generator = CSVGenerator(args.val_annotations, args.classes, batch_size=args.batch_size, config=args.config)
        else:
            validation_generator = None
    else:
        raise ValueError('Invalid data type received: {}'.format(args.dataset_type))
    return (train_generator, validation_generator)",args.dataset_type == 'coco',65,args.dataset_type == 'coco',True,100.00000000000004,N/A
"def main(args=None):
<mask>:
        args = sys.argv[1:]
    args = parse_args(args)
    check_keras_version()
    backbone = models.backbone(args.backbone)
    if args.gpu:
        setup_gpu(args.gpu)
    if args.config:
        args.config = read_config_file(args.config)
    train_generator, validation_generator = create_generators(args)
    if args.snapshot is not None:
        print('Loading model, this may take a second...')
        model = models.load_model(args.snapshot, backbone_name=args.backbone)
        training_model = model
        prediction_model = model
    else:
        weights = args.weights
        if weights is None and args.imagenet_weights:
            weights = backbone.download_imagenet()
        anchor_params = None
        if args.config and 'anchor_parameters' in args.config:
            anchor_params = parse_anchor_parameters(args.config)
        print('Creating model, this may take a second...')
        model, training_model, prediction_model = create_models(backbone_retinanet=backbone.maskrcnn, num_classes=train_generator.num_classes(), weights=weights, freeze_backbone=args.freeze_backbone, class_specific_filter=args.class_specific_filter, anchor_params=anchor_params)
    print(model.summary())
    callbacks = create_callbacks(model, training_model, prediction_model, validation_generator, args)
    if args.workers > 0:
        use_multiprocessing = True
    else:
        use_multiprocessing = False
    training_model.fit_generator(generator=train_generator, steps_per_epoch=args.steps, epochs=args.epochs, verbose=1, callbacks=callbacks, workers=args.workers, use_multiprocessing=use_multiprocessing, max_queue_size=args.max_queue_size)",args is None,116,args is None,True,100.00000000000004,N/A
"def create_generator(args):
<mask>:
        from ..preprocessing.coco import CocoGenerator
        validation_generator = CocoGenerator(args.coco_path, 'val2017', image_min_side=args.image_min_side, image_max_side=args.image_max_side, config=args.config)
    elif args.dataset_type == 'csv':
        validation_generator = CSVGenerator(args.annotations, args.classes, image_min_side=args.image_min_side, image_max_side=args.image_max_side, config=args.config)
    else:
        raise ValueError('Invalid data type received: {}'.format(args.dataset_type))
    return validation_generator",args.dataset_type == 'coco',34,args.dataset_type == 'coco',True,100.00000000000004,N/A
"def main(args=None):
<mask>:
        args = sys.argv[1:]
    args = parse_args(args)
    check_keras_version()
    if args.gpu:
        setup_gpu(args.gpu)
    if args.config:
        args.config = read_config_file(args.config)
    if args.save_path is not None and (not os.path.exists(args.save_path)):
        os.makedirs(args.save_path)
    generator = create_generator(args)
    print('Loading model, this may take a second...')
    model = models.load_model(args.model, backbone_name=args.backbone)
    if args.dataset_type == 'coco':
        from ..utils.coco_eval import evaluate_coco
        evaluate_coco(generator, model, args.score_threshold)
    else:
        average_precisions = evaluate(generator, model, iou_threshold=args.iou_threshold, score_threshold=args.score_threshold, max_detections=args.max_detections, binarize_threshold=args.binarize_threshold, save_path=args.save_path)
        total_instances = []
        precisions = []
        for label, (average_precision, num_annotations) in average_precisions.items():
            print('{:.0f} instances of class'.format(num_annotations), generator.label_to_name(label), 'with average precision: {:.4f}'.format(average_precision))
            total_instances.append(num_annotations)
            precisions.append(average_precision)
        if args.weighted_average:
            print('mAP: {:.4f}'.format(sum([a * b for a, b in zip(total_instances, precisions)]) / sum(total_instances)))
        else:
            print('mAP: {:.4f}'.format(sum(precisions) / sum((x > 0 for x in total_instances))))",args is None,110,args is None,True,100.00000000000004,N/A
"def on_epoch_end(self, epoch, logs):
    average_precisions = evaluate(self.generator, self.model, iou_threshold=self.iou_threshold, score_threshold=self.score_threshold, max_detections=self.max_detections, save_path=self.save_path)
    total_instances = []
    precisions = []
    for label, (average_precision, num_annotations) in average_precisions.items():
<mask>:
            print('{:.0f} instances of class'.format(num_annotations), self.generator.label_to_name(label), 'with average precision: {:.4f}'.format(average_precision))
        total_instances.append(num_annotations)
        precisions.append(average_precision)
    if self.weighted_average:
        mean_ap = sum([a * b for a, b in zip(total_instances, precisions)]) / sum(total_instances)
    else:
        mean_ap = sum(precisions) / sum((x > 0 for x in total_instances))
    if self.tensorboard is not None and self.tensorboard.writer is not None:
        import tensorflow as tf
        summary = tf.Summary()
        summary_value = summary.value.add()
        summary_value.simple_value = mean_ap
        summary_value.tag = 'mAP'
        self.tensorboard.writer.add_summary(summary, epoch)
    if self.verbose == 1:
        print('mAP: {:.4f}'.format(mean_ap))
    logs['mAP'] = mean_ap",self.verbose == 1,100,self.verbose == 1,True,100.00000000000004,N/A
"def draw_mask(image, box, mask, label=None, color=None, binarize_threshold=0.5):
    """""" Draws a mask in a given box.

    Args
        image              : Three dimensional image to draw on.
        box                : Vector of at least 4 values (x1, y1, x2, y2) representing a box in the image.
        mask               : A 2D float mask which will be reshaped to the size of the box, binarized and drawn over the image.
        color              : Color to draw the mask with. If the box has 5 values, the last value is assumed to be the label and used to construct a default color.
        binarize_threshold : Threshold used for binarizing the mask.
    Returns
        indices            : List of indices representing a mask.
    """"""
<mask>:
        color = label_color(label)
    if color is None:
        color = (0, 255, 0)
    mask = mask.astype(np.float32)
    mask = cv2.resize(mask, (box[2] - box[0], box[3] - box[1]))
    mask = (mask > binarize_threshold).astype(np.uint8)
    mask_image = np.zeros((image.shape[0], image.shape[1]), np.uint8)
    mask_image[box[1]:box[3], box[0]:box[2]] = mask
    mask = mask_image
    border = mask - cv2.erode(mask, np.ones((5, 5), np.uint8), iterations=1)
    mask = (np.stack([mask] * 3, axis=2) * color).astype(np.uint8)
    border = (np.stack([border] * 3, axis=2) * (255, 255, 255)).astype(np.uint8)
    indices = np.where(mask != [0, 0, 0])
    image[indices[0], indices[1], :] = 0.5 * image[indices[0], indices[1], :] + 0.5 * mask[indices[0], indices[1], :]
    border_indices = np.where(border != [0, 0, 0])
    image[border_indices[0], border_indices[1], :] = 0.2 * image[border_indices[0], border_indices[1], :] + 0.8 * border[border_indices[0], border_indices[1], :]
    return indices",label is not None,227,label is not None,True,100.00000000000004,N/A
"def draw_masks(image, boxes, masks, labels=None, color=None, binarize_threshold=0.5):
    """""" Draws a list of masks given a list of boxes.

    Args
        image              : Three dimensional image to draw on.
        boxes              : Matrix of shape (N, >=4) (at least 4 values: (x1, y1, x2, y2)) representing boxes in the image.
        masks              : Matrix of shape (N, H, W) of N masks of shape (H, W) which will be reshaped to the size of the corresponding box, binarized and drawn over the image.
        labels             : Optional list of labels, used to color the masks with. If provided, color is ignored.
        color              : Color or to draw the masks with.
        binarize_threshold : Threshold used for binarizing the masks.
    Returns
        indices            : List of lists of indices ; each list of indices represents a mask.
    """"""
<mask>:
        labels = [None for _ in range(boxes.shape[0])]
    indices = []
    for box, mask, label in zip(boxes, masks, labels):
        indices.append(draw_mask(image, box, mask, label=label, color=color, binarize_threshold=binarize_threshold))
    return indices",labels is None,157,labels is None,True,100.00000000000004,N/A
"def evaluate_coco(generator, model, threshold=0.05):
    results = []
    image_ids = []
    for index in range(generator.size()):
        image = generator.load_image(index)
        image_shape = image.shape
        image = generator.preprocess_image(image)
        image, scale = generator.resize_image(image)
        outputs = model.predict_on_batch(np.expand_dims(image, axis=0))
        boxes = outputs[-4]
        scores = outputs[-3]
        labels = outputs[-2]
        masks = outputs[-1]
        boxes /= scale
        boxes[..., 2] -= boxes[..., 0]
        boxes[..., 3] -= boxes[..., 1]
        for box, score, label, mask in zip(boxes[0], scores[0], labels[0], masks[0]):
<mask>:
                break
            b = box.astype(int)
            mask = mask.astype(np.float32)
            mask = cv2.resize(mask[:, :, label], (b[2], b[3]))
            mask = (mask > 0.5).astype(np.uint8)
            segmentation = np.zeros((image_shape[0], image_shape[1]), dtype=np.uint8)
            segmentation[b[1]:b[1] + b[3], b[0]:b[0] + b[2]] = mask
            segmentation = mask_utils.encode(np.asfortranarray(segmentation))
            image_result = {'image_id': generator.image_ids[index], 'category_id': generator.label_to_coco_label(label), 'score': float(score), 'bbox': box.tolist(), 'segmentation': segmentation}
            if not isinstance(image_result['segmentation']['counts'], str):
                image_result['segmentation']['counts'] = image_result['segmentation']['counts'].decode()
            results.append(image_result)
        image_ids.append(generator.image_ids[index])
        print('{}/{}'.format(index, generator.size()), end='\r')
    if not len(results):
        return
    json.dump(results, open('{}_segm_results.json'.format(generator.set_name), 'w'), indent=4)
    json.dump(image_ids, open('{}_processed_image_ids.json'.format(generator.set_name), 'w'), indent=4)
    coco_true = generator.coco
    coco_pred = coco_true.loadRes('{}_segm_results.json'.format(generator.set_name))
    coco_eval = COCOeval(coco_true, coco_pred, 'segm')
    coco_eval.params.imgIds = image_ids
    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()",score < threshold,155,score < threshold,True,100.00000000000004,N/A
"def _get_detections(generator, model, score_threshold=0.05, max_detections=100, save_path=None):
    """""" Get the detections from the model using the generator.

    The result is a list of lists such that the size is:
        all_detections[num_images][num_classes] = detections[num_detections, 4 + num_classes]

    # Arguments
        generator       : The generator used to run images through the model.
        model           : The model to run on the images.
        score_threshold : The score confidence threshold to use.
        max_detections  : The maximum number of detections to use per image.
        save_path       : The path to save the images with visualized detections to.
    # Returns
        A list of lists containing the detections for each image in the generator.
    """"""
    all_detections = [[None for i in range(generator.num_classes())] for j in range(generator.size())]
    all_masks = [[None for i in range(generator.num_classes())] for j in range(generator.size())]
    for i in range(generator.size()):
        raw_image = generator.load_image(i)
        image = generator.preprocess_image(raw_image.copy())
        image, scale = generator.resize_image(image)
        outputs = model.predict_on_batch(np.expand_dims(image, axis=0))
        boxes = outputs[-4]
        scores = outputs[-3]
        labels = outputs[-2]
        masks = outputs[-1]
        boxes /= scale
        indices = np.where(scores[0, :] > score_threshold)[0]
        scores = scores[0][indices]
        scores_sort = np.argsort(-scores)[:max_detections]
        image_boxes = boxes[0, indices[scores_sort], :]
        image_scores = scores[scores_sort]
        image_labels = labels[0, indices[scores_sort]]
        image_masks = masks[0, indices[scores_sort], :, :, image_labels]
        image_detections = np.concatenate([image_boxes, np.expand_dims(image_scores, axis=1), np.expand_dims(image_labels, axis=1)], axis=1)
<mask>:
            draw_detections(raw_image, image_boxes, image_scores, image_labels, score_threshold=score_threshold, label_to_name=generator.label_to_name)
            draw_masks(raw_image, image_boxes.astype(int), image_masks, labels=image_labels)
            cv2.imwrite(os.path.join(save_path, '{}.png'.format(i)), raw_image)
        for label in range(generator.num_classes()):
            all_detections[i][label] = image_detections[image_detections[:, -1] == label, :-1]
            all_masks[i][label] = image_masks[image_detections[:, -1] == label, ...]
        print('{}/{}'.format(i + 1, generator.size()), end='\r')
    return (all_detections, all_masks)",save_path is not None,237,save_path,False,36.78794411714425,N/A
"def evaluate(generator, model, iou_threshold=0.5, score_threshold=0.05, max_detections=100, binarize_threshold=0.5, save_path=None):
    """""" Evaluate a given dataset using a given model.

    # Arguments
        generator          : The generator that represents the dataset to evaluate.
        model              : The model to evaluate.
        iou_threshold      : The threshold used to consider when a detection is positive or negative.
        score_threshold    : The score confidence threshold to use for detections.
        max_detections     : The maximum number of detections to use per image.
        binarize_threshold : Threshold to binarize the masks with.
        save_path          : The path to save images with visualized detections to.
    # Returns
        A dict mapping class names to mAP scores.
    """"""
    all_detections, all_masks = _get_detections(generator, model, score_threshold=score_threshold, max_detections=max_detections, save_path=save_path)
    all_annotations, all_gt_masks = _get_annotations(generator)
    average_precisions = {}
    for label in range(generator.num_classes()):
        false_positives = np.zeros((0,))
        true_positives = np.zeros((0,))
        scores = np.zeros((0,))
        num_annotations = 0.0
        for i in range(generator.size()):
            detections = all_detections[i][label]
            masks = all_masks[i][label]
            annotations = all_annotations[i][label]
            gt_masks = all_gt_masks[i][label]
            num_annotations += annotations.shape[0]
            detected_annotations = []
            for d, mask in zip(detections, masks):
                box = d[:4].astype(int)
                scores = np.append(scores, d[4])
<mask>:
                    false_positives = np.append(false_positives, 1)
                    true_positives = np.append(true_positives, 0)
                    continue
                mask = cv2.resize(mask, (box[2] - box[0], box[3] - box[1]))
                mask = (mask > binarize_threshold).astype(np.uint8)
                mask_image = np.zeros_like(gt_masks[0])
                mask_image[box[1]:box[3], box[0]:box[2]] = mask
                mask = mask_image
                overlaps = compute_overlap(np.expand_dims(mask, axis=0), gt_masks)
                assigned_annotation = np.argmax(overlaps, axis=1)
                max_overlap = overlaps[0, assigned_annotation]
                if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:
                    false_positives = np.append(false_positives, 0)
                    true_positives = np.append(true_positives, 1)
                    detected_annotations.append(assigned_annotation)
                else:
                    false_positives = np.append(false_positives, 1)
                    true_positives = np.append(true_positives, 0)
        if num_annotations == 0:
            average_precisions[label] = (0, 0)
            continue
        indices = np.argsort(-scores)
        false_positives = false_positives[indices]
        true_positives = true_positives[indices]
        false_positives = np.cumsum(false_positives)
        true_positives = np.cumsum(true_positives)
        recall = true_positives / num_annotations
        precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)
        average_precision = _compute_ap(recall, precision)
        average_precisions[label] = (average_precision, num_annotations)
    return average_precisions",annotations.shape[0] == 0,287,box.sum() > 0,False,5.868924818816531,N/A
"def backbone(backbone_name):
    """""" Returns a backbone object for the given backbone_name.
    """"""
<mask>:
        from .resnet import ResNetBackbone as b
    else:
        raise NotImplementedError(""Backbone class for  '{}' not implemented."".format(backbone_name))
    return b(backbone_name)",'resnet' in backbone_name,29,"backbone_name in ('resnet', 'resnet_backbone')",False,15.851165692617148,N/A
"def resnet_maskrcnn(num_classes, backbone='resnet50', inputs=None, modifier=None, mask_dtype=keras.backend.floatx(), **kwargs):
<mask>:
        inputs = keras.layers.Input(shape=(None, None, 3), name='image')
    if backbone == 'resnet50':
        resnet = keras_resnet.models.ResNet50(inputs, include_top=False, freeze_bn=True)
    elif backbone == 'resnet101':
        resnet = keras_resnet.models.ResNet101(inputs, include_top=False, freeze_bn=True)
    elif backbone == 'resnet152':
        resnet = keras_resnet.models.ResNet152(inputs, include_top=False, freeze_bn=True)
    if modifier:
        resnet = modifier(resnet)
    model = retinanet.retinanet_mask(inputs=inputs, num_classes=num_classes, backbone_layers=resnet.outputs[1:], mask_dtype=mask_dtype, **kwargs)
    return model",inputs is None,55,inputs is None,True,100.00000000000004,N/A
"def default_mask_model(num_classes, pyramid_feature_size=256, mask_feature_size=256, roi_size=(14, 14), mask_size=(28, 28), name='mask_submodel', mask_dtype=keras.backend.floatx(), retinanet_dtype=keras.backend.floatx()):
    options = {'kernel_size': 3, 'strides': 1, 'padding': 'same', 'kernel_initializer': keras.initializers.normal(mean=0.0, stddev=0.01, seed=None), 'bias_initializer': 'zeros', 'activation': 'relu'}
    inputs = keras.layers.Input(shape=(None, roi_size[0], roi_size[1], pyramid_feature_size))
    outputs = inputs
<mask>:
        outputs = keras.layers.TimeDistributed(Cast(dtype=mask_dtype), name='cast_masks')(outputs)
    for i in range(4):
        outputs = keras.layers.TimeDistributed(keras.layers.Conv2D(filters=mask_feature_size, **options), name='roi_mask_{}'.format(i))(outputs)
    outputs = keras.layers.TimeDistributed(Upsample(mask_size), name='roi_mask_upsample')(outputs)
    outputs = keras.layers.TimeDistributed(keras.layers.Conv2D(filters=mask_feature_size, **options), name='roi_mask_features')(outputs)
    outputs = keras.layers.TimeDistributed(keras.layers.Conv2D(filters=num_classes, kernel_size=1, activation='sigmoid'), name='roi_mask')(outputs)
    if mask_dtype != retinanet_dtype:
        outputs = keras.layers.TimeDistributed(Cast(dtype=retinanet_dtype), name='recast_masks')(outputs)
    return keras.models.Model(inputs=inputs, outputs=outputs, name=name)",mask_dtype != retinanet_dtype,77,mask_dtype != 'cast',False,54.44460596606694,N/A
"def load_annotations(self, image_index):
    image_info = self.coco.loadImgs(self.image_ids[image_index])[0]
    annotations_ids = self.coco.getAnnIds(imgIds=self.image_ids[image_index], iscrowd=False)
    annotations = {'labels': np.empty((0,)), 'bboxes': np.empty((0, 4)), 'masks': []}
<mask>:
        return annotations
    coco_annotations = self.coco.loadAnns(annotations_ids)
    for idx, a in enumerate(coco_annotations):
        if 'segmentation' not in a:
            raise ValueError(""Expected 'segmentation' key in annotation, got: {}"".format(a))
        if a['bbox'][2] < 1 or a['bbox'][3] < 1:
            continue
        annotations['labels'] = np.concatenate([annotations['labels'], [self.coco_label_to_label(a['category_id'])]], axis=0)
        annotations['bboxes'] = np.concatenate([annotations['bboxes'], [[a['bbox'][0], a['bbox'][1], a['bbox'][0] + a['bbox'][2], a['bbox'][1] + a['bbox'][3]]]], axis=0)
        mask = np.zeros((image_info['height'], image_info['width'], 1), dtype=np.uint8)
        for seg in a['segmentation']:
            points = np.array(seg).reshape((len(seg) // 2, 2)).astype(int)
            cv2.fillPoly(mask, [points.astype(int)], (1,))
        annotations['masks'].append(mask.astype(float))
    return annotations",len(annotations_ids) == 0,91,len(annotations_ids) == 0,True,100.00000000000004,N/A
"def __init__(self, transform_generator=None, batch_size=1, group_method='ratio', shuffle_groups=True, image_min_side=800, image_max_side=1333, transform_parameters=None, compute_shapes=guess_shapes, compute_anchor_targets=anchor_targets_bbox, config=None):
    self.transform_generator = transform_generator
    self.batch_size = int(batch_size)
    self.group_method = group_method
    self.shuffle_groups = shuffle_groups
    self.image_min_side = image_min_side
    self.image_max_side = image_max_side
    self.transform_parameters = transform_parameters or TransformParameters()
    self.compute_shapes = compute_shapes
    self.compute_anchor_targets = compute_anchor_targets
    self.config = config
    self.group_images()
<mask>:
        self.on_epoch_end()",self.shuffle_groups,47,self.shuffle_groups,True,100.00000000000004,N/A
"def filter_annotations(self, image_group, annotations_group, group):
    """""" Filter annotations by removing those that are outside of the image bounds or whose width/height < 0.
        """"""
    for index, (image, annotations) in enumerate(zip(image_group, annotations_group)):
        invalid_indices = np.where((annotations['bboxes'][:, 2] <= annotations['bboxes'][:, 0]) | (annotations['bboxes'][:, 3] <= annotations['bboxes'][:, 1]) | (annotations['bboxes'][:, 0] < 0) | (annotations['bboxes'][:, 1] < 0) | (annotations['bboxes'][:, 2] > image.shape[1]) | (annotations['bboxes'][:, 3] > image.shape[0]))[0]
<mask>:
            warnings.warn('Image with id {} (shape {}) contains the following invalid boxes: {}.'.format(group[index], image.shape, annotations['bboxes'][invalid_indices, :]))
            for k in annotations_group[index].keys():
                if type(annotations_group[index][k]) == list:
                    for i in invalid_indices[::-1]:
                        del annotations_group[index][k][i]
                else:
                    annotations_group[index][k] = np.delete(annotations[k], invalid_indices, axis=0)
    return (image_group, annotations_group)",len(invalid_indices),103,len(invalid_indices) > 0,False,68.037493331712,N/A
"def random_transform_group_entry(self, image, annotations, transform=None):
    """""" Randomly transforms image and annotation.
        """"""
<mask>:
        if transform is None:
            transform = adjust_transform_for_image(next(self.transform_generator), image, self.transform_parameters.relative_translation)
        image = apply_transform(transform, image, self.transform_parameters)
        for i, mask in enumerate(annotations['masks']):
            annotations['masks'][i] = apply_transform(transform, mask, self.transform_parameters)
            annotations['masks'][i] = np.expand_dims(annotations['masks'][i], axis=2)
        annotations['bboxes'] = annotations['bboxes'].copy()
        for index in range(annotations['bboxes'].shape[0]):
            annotations['bboxes'][index, :] = transform_aabb(transform, annotations['bboxes'][index, :])
    return (image, annotations)",transform or self.transform_generator,57,self.random,False,14.506309551249304,N/A
"def group_images(self):
    order = list(range(self.size()))
<mask>:
        random.shuffle(order)
    elif self.group_method == 'ratio':
        order.sort(key=lambda x: self.image_aspect_ratio(x))
    self.groups = [[order[x % len(order)] for x in range(i, i + self.batch_size)] for i in range(0, len(order), self.batch_size)]",self.group_method == 'random',32,self.group_method == 'shuffle',False,84.08964152537145,N/A
"def generate_anchors(self, image_shape):
    anchor_params = None
<mask>:
        anchor_params = parse_anchor_parameters(self.config)
    return anchors_for_shape(image_shape, anchor_params=anchor_params, shapes_callback=self.compute_shapes)",self.config and 'anchor_parameters' in self.config,14,self.config.get('anchor_parameters'),False,24.70315512339778,N/A
"def _read_classes(csv_reader):
    result = {}
    for line, row in enumerate(csv_reader):
        try:
            class_name, class_id = row
        except ValueError:
            raise_from(ValueError(""line {}: format should be 'class_name,class_id'"".format(line)), None)
        class_id = _parse(class_id, int, 'line {}: malformed class ID: {{}}'.format(line))
<mask>:
            raise ValueError(""line {}: duplicate class name: '{}'"".format(line, class_name))
        result[class_name] = class_id
    return result",class_name in result,48,class_name in result,True,100.00000000000004,N/A
"def _read_annotations(csv_reader, classes):
    result = {}
    for line, row in enumerate(csv_reader):
        try:
            img_file, x1, y1, x2, y2, class_name, mask_path = row
        except ValueError:
            raise_from(ValueError(""line {}: format should be 'img_file,x1,y1,x2,y2,class_name,mask_path' or 'img_file,,,,,'"".format(line)), None)
<mask>:
            result[img_file] = []
        if (x1, y1, x2, y2, class_name) == ('', '', '', '', ''):
            continue
        x1 = _parse(x1, int, 'line {}: malformed x1: {{}}'.format(line))
        y1 = _parse(y1, int, 'line {}: malformed y1: {{}}'.format(line))
        x2 = _parse(x2, int, 'line {}: malformed x2: {{}}'.format(line))
        y2 = _parse(y2, int, 'line {}: malformed y2: {{}}'.format(line))
        if x2 <= x1:
            raise ValueError('line {}: x2 ({}) must be higher than x1 ({})'.format(line, x2, x1))
        if y2 <= y1:
            raise ValueError('line {}: y2 ({}) must be higher than y1 ({})'.format(line, y2, y1))
        if class_name not in classes:
            raise ValueError(""line {}: unknown class name: '{}' (classes: {})"".format(line, class_name, classes))
        result[img_file].append({'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'class': class_name, 'mask_path': mask_path})
    return result",img_file not in result,149,img_file not in result,True,100.00000000000004,N/A
"def _open_for_csv(path):
    """"""
    Open a file with flags suitable for csv.reader.

    This is different for python2 it means with mode 'rb',
    for python3 this means 'r' with ""universal newlines"".
    """"""
<mask>:
        return open(path, 'rb')
    else:
        return open(path, 'r', newline='')",sys.version_info[0] < 3,39,sys.version_info[0] == 2,False,67.86502681586727,N/A
"def __init__(self, csv_data_file, csv_class_file, base_dir=None, **kwargs):
    self.image_names = []
    self.image_data = {}
    self.base_dir = base_dir
<mask>:
        self.base_dir = os.path.dirname(csv_data_file)
    try:
        with _open_for_csv(csv_class_file) as file:
            self.classes = _read_classes(csv.reader(file, delimiter=','))
    except ValueError as e:
        raise_from(ValueError('invalid CSV class file: {}: {}'.format(csv_class_file, e)), None)
    self.labels = {}
    for key, value in self.classes.items():
        self.labels[value] = key
    try:
        with _open_for_csv(csv_data_file) as file:
            self.image_data = _read_annotations(csv.reader(file, delimiter=','), self.classes)
    except ValueError as e:
        raise_from(ValueError('invalid CSV annotations file: {}: {}'.format(csv_data_file, e)), None)
    self.image_names = list(self.image_data.keys())
    super(CSVGenerator, self).__init__(**kwargs)",self.base_dir is None,78,self.base_dir is None,True,100.00000000000004,N/A
"def test_main(ascii_file, binary_file, tmpdir, speedups):
    original_argv = sys.argv[:]
    args_pre = ['stl']
    args_post = [str(tmpdir.join('output.stl'))]
<mask>:
        args_pre.append('-s')
    try:
        sys.argv[:] = [*args_pre, ascii_file, *args_post]
        main.main()
        sys.argv[:] = [*args_pre, '-r', ascii_file, *args_post]
        main.main()
        sys.argv[:] = [*args_pre, '-a', binary_file, *args_post]
        main.main()
        sys.argv[:] = [*args_pre, '-b', ascii_file, *args_post]
        main.main()
    finally:
        sys.argv[:] = original_argv",not speedups,48,speedups,False,36.78794411714425,N/A
"def test_corrupt_ascii_file(tmpdir, speedups):
    tmp_file = tmpdir.join('tmp.stl')
    with tmp_file.open('w+') as fh:
        fh.write(_STL_FILE)
        fh.seek(40)
        print('####\n' * 100, file=fh)
        fh.seek(0)
<mask>:
            with pytest.raises(AssertionError):
                mesh.Mesh.from_file(str(tmp_file), fh=fh, speedups=speedups)
    with tmp_file.open('w+') as fh:
        fh.write(_STL_FILE)
        fh.seek(40)
        print(' ' * 100, file=fh)
        fh.seek(80)
        fh.write(struct.pack('<i', 10).decode('utf-8'))
        fh.seek(0)
        with pytest.raises(AssertionError):
            mesh.Mesh.from_file(str(tmp_file), fh=fh, speedups=speedups)",speedups and sys.version_info.major != 2,43,sys.version_info[0] < 3,False,32.16094349518416,N/A
"def _test_conversion(from_, to, mode, speedups):
    from_ = py.path.local(from_)
    to = py.path.local(to)
    for name in from_.listdir():
        source_file = from_.join(name)
        expected_file = to.join(name)
<mask>:
            continue
        mesh = stl.StlMesh(source_file, speedups=speedups)
        with open(str(expected_file), 'rb') as expected_fh:
            expected = expected_fh.read()
            if mode is stl.BINARY:
                expected = expected[80:]
            with tempfile.TemporaryFile() as dest_fh:
                mesh.save(name, dest_fh, mode)
                dest_fh.seek(0)
                dest = dest_fh.read()
                if mode is stl.BINARY:
                    dest = dest[80:]
                assert dest.strip() == expected.strip()",not expected_file.exists(),64,not os.path.exists(str(expected_file)),False,19.104081613647544,N/A
"def _test(tmpdir, speedups, mode, use_filehandle=True):
    filename = TESTS_PATH / 'stl_binary' / 'rear_case.stl'
<mask>:
        with open(filename, 'rb') as fh:
            mesh.Mesh.from_file(filename, fh=fh, speedups=speedups, mode=mode)
        with open(filename, 'rb') as fh:
            fh = io.BytesIO(fh.read())
            mesh.Mesh.from_file(filename, fh=fh, speedups=speedups, mode=mode)
    else:
        mesh.Mesh.from_file(filename, speedups=speedups, mode=mode)",use_filehandle,38,use_filehandle,True,100.00000000000004,N/A
"def to_array(array, round):
    __tracebackhide__ = True
<mask>:
        array = np.array(array)
    if round:
        array = array.round(round)
    return array","not isinstance(array, np.ndarray)",17,"isinstance(array, np.ndarray)",False,88.24969025845958,N/A
"def array_equals(left, right, round=6):
    __tracebackhide__ = True
    left = to_array(left, round)
    right = to_array(right, round)
    message = f'Arrays are unequal:\n{left}\n{right}'
<mask>:
        message += '\nDifference:\n%s' % (left - right)
    assert (left == right).all(), message",left.size == right.size,33,round == 6,False,11.752701606523267,N/A
"@pytest.mark.parametrize('filename', ('Star.stl', 'StarWithEmptyHeader.stl'))
def test_mass_properties_for_star(binary_ascii_path, filename, speedups):
    """"""
    Checks the results of method get_mass_properties() on
    STL ASCII and binary files Star.stl and
    STL binary file StarWithEmptyHeader.stl (with no header)
    One checks the results obtained with stl
    with the ones obtained with meshlab
    """"""
    filename = binary_ascii_path / filename
<mask>:
        pytest.skip('STL file does not exist')
    mesh = stl.StlMesh(str(filename), speedups=speedups)
    volume, cog, inertia = mesh.get_mass_properties()
    assert close([volume], [1.416599])
    assert close(cog, [1.29904, 0.170197, 1.499999])
    assert close(inertia, [[+0.509549, +0.0, -0.0], [+0.0, +0.991236, +0.0], [-0.0, +0.0, +0.50955]])",not filename.exists(),82,not os.path.exists(filename),False,21.10534063187263,N/A
"def test_chinese_name(tmpdir, speedups):
    name = 'Test Chinese name 月球'
    _stl_file = f'\n    solid {name}\n      facet normal -0.014565 0.073223 -0.002897\n        outer loop\n          vertex 0.399344 0.461940 1.044090\n          vertex 0.500000 0.500000 1.500000\n          vertex 0.576120 0.500000 1.117320\n        endloop\n      endfacet\n    endsolid\n    '.lstrip()
    tmp_file = tmpdir.join('tmp.stl')
    with tmp_file.open('wb+') as fh:
        fh.write(b(_stl_file))
        fh.seek(0)
        test_mesh = mesh.Mesh.from_file(str(tmp_file), fh=fh, speedups=speedups)
<mask>:
            assert test_mesh.name.lower() == b(name).lower()
        else:
            assert test_mesh.name == b(name)",speedups,61,sys.version_info[0] == 2,False,0.0,N/A
"def test_long_name(tmpdir, speedups):
    name = 'Just Some Very Long Name which will not fit within the standard'
    name += name
    _stl_file = f'\n    solid {name}\n      facet normal -0.014565 0.073223 -0.002897\n        outer loop\n          vertex 0.399344 0.461940 1.044090\n          vertex 0.500000 0.500000 1.500000\n          vertex 0.576120 0.500000 1.117320\n        endloop\n      endfacet\n    endsolid\n    '.lstrip()
    tmp_file = tmpdir.join('tmp.stl')
    with tmp_file.open('wb+') as fh:
        fh.write(b(_stl_file))
        fh.seek(0)
        test_mesh = mesh.Mesh.from_file(str(tmp_file), fh=fh, speedups=speedups)
<mask>:
            assert test_mesh.name.lower() == b(name).lower()
        else:
            assert test_mesh.name == b(name)",speedups,72,sys.version_info[0] == 2,False,0.0,N/A
"@pytest.mark.skipif(sys.platform.startswith('win'), reason='Only makes sense on Unix')
def test_locale_restore(speedups):
<mask>:
        pytest.skip('Only makes sense with speedups')
    old_locale = locale.nl_langinfo(locale.CODESET)
    filename = FILES_PATH / 'bwb.stl'
    mesh.Mesh.from_file(filename, speedups=speedups)
    new_locale = locale.nl_langinfo(locale.CODESET)
    assert old_locale == new_locale",not speedups,31,speedups,False,36.78794411714425,N/A
"@pytest.mark.skipif(sys.platform.startswith('win'), reason='Only makes sense on Unix')
def test_use_with_qt_with_custom_locale_decimal_delimeter(speedups):
<mask>:
        pytest.skip('Only makes sense with speedups')
    venv = os.environ.get('VIRTUAL_ENV', '')
    if sys.version_info[:2] == (3, 6) and venv.startswith('/home/travis/'):
        pytest.skip('PySide2/PyQt5 tests are broken on Travis Python 3.6')
    try:
        from PySide2 import QtWidgets
    except ImportError:
        try:
            from PyQt5 import QtWidgets
        except ImportError:
            warnings.warn('Unable to import PySide2/PyQt5, skipping locale tests', ImportWarning, stacklevel=1)
            pytest.skip('PySide2/PyQt5 missing')
    assert QtWidgets
    dir_path = os.path.dirname(os.path.realpath(__file__))
    script_path = os.path.join(dir_path, 'qt-lc_numeric-reproducer')
    env = os.environ.copy()
    env['LC_NUMERIC'] = 'cs_CZ.utf-8'
    prefix = tuple()
    if sys.platform.startswith('linux'):
        prefix = ('xvfb-run', '-a')
    p = subprocess.Popen((*prefix, sys.executable, script_path), env=env, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    out, err = p.communicate()
    sys.stdout.write(out)
    sys.stderr.write(err)
    assert 'File too large' not in out
    assert 'File too large' not in err
    assert p.returncode == 0",not speedups,115,speedups,False,36.78794411714425,N/A
"def b(s, encoding='ascii', errors='replace'):
<mask>:
        return bytes(s, encoding, errors)
    else:
        return s","isinstance(s, str)",12,"isinstance(s, bytes_types)",False,36.55552228545123,N/A
"def _get_name(args):
    names = [args.name, getattr(args.outfile, 'name', None), getattr(args.infile, 'name', None), 'numpy-stl-%06d' % random.randint(0, 1000000)]
    for name in names:
<mask>:
            continue
        elif name.startswith('<'):
            continue
        elif '\\AppData\\Local\\Temp' in name:
            continue
        else:
            return name
    return None","not isinstance(name, str)",34,name.startswith('_'),False,7.267884212102741,N/A
"def main():
    parser = _get_parser('Convert STL files from ascii to binary and back')
    parser.add_argument('-a', '--ascii', action='store_true', help='Write ASCII file (default is binary)')
    parser.add_argument('-b', '--binary', action='store_true', help='Force binary file (for TTYs)')
    args = parser.parse_args()
    name = _get_name(args)
    stl_file = stl.StlMesh(filename=name, fh=args.infile, calculate_normals=False, remove_empty_areas=args.remove_empty_areas, speedups=not args.disable_speedups)
<mask>:
        mode = stl.BINARY
    elif args.ascii:
        mode = stl.ASCII
    else:
        mode = stl.AUTOMATIC
    stl_file.save(name, args.outfile, mode=mode, update_normals=not args.use_file_normals)",args.binary,62,args.binary,True,100.00000000000004,N/A
"@classmethod
def map(cls, value):
<mask>:
        value = cls.SINGLE
    elif value and value in cls:
        pass
    else:
        value = cls.NONE
    return value",value is True,21,value and value in cls,False,10.682175159905848,N/A
"def logged(class_):
    logger_name = logger.Logged._Logged__get_name(__name__, class_.__name__)
    class_.logger = logging.getLogger(logger_name)
    for key in dir(logger.Logged):
<mask>:
            setattr(class_, key, getattr(class_, key))
    return class_",not key.startswith('__'),20,key.startswith('_'),False,65.48907866815301,N/A
"def _get_or_update(key):

    def _get(self):
        attr = f'_{key}'
<mask>:
            getattr(self, f'update_{key}')()
        return getattr(self, attr)
    return _get","not hasattr(self, attr)",15,"not hasattr(self, attr)",True,100.00000000000004,N/A
"def __init__(self, data, calculate_normals=True, remove_empty_areas=False, remove_duplicate_polygons=RemoveDuplicates.NONE, name='', speedups=True, **kwargs):
    super().__init__(**kwargs)
    self.speedups = speedups
<mask>:
        data = self.remove_empty_areas(data)
    if RemoveDuplicates.map(remove_duplicate_polygons).value:
        data = self.remove_duplicate_polygons(data, remove_duplicate_polygons)
    self.name = name
    self.data = data
    if calculate_normals:
        self.update_normals()",remove_empty_areas,32,remove_empty_areas,True,100.00000000000004,N/A
"@classmethod
def remove_duplicate_polygons(cls, data, value=RemoveDuplicates.SINGLE):
    value = RemoveDuplicates.map(value)
    polygons = data['vectors'].sum(axis=1)
    idx = np.lexsort(polygons.T)
    diff = np.any(polygons[idx[1:]] != polygons[idx[:-1]], axis=1)
<mask>:
        return data[np.sort(idx[np.concatenate(([True], diff))])]
    elif value is RemoveDuplicates.ALL:
        diff_a = np.concatenate(([True], diff))
        diff_b = np.concatenate((diff, [True]))
        diff = np.concatenate((diff, [False]))
        filtered_data = data[np.sort(idx[diff_a & diff_b])]
        if len(filtered_data) <= len(data) / 2:
            return data[np.sort(idx[diff_a])]
        else:
            return data[np.sort(idx[diff])]
    else:
        return data",value is RemoveDuplicates.SINGLE,59,value is RemoveDuplicates.SINGLE,True,100.00000000000004,N/A
"@classmethod
def load(cls, fh, mode=AUTOMATIC, speedups=True):
    """"""Load Mesh from STL file

        Automatically detects binary versus ascii STL files.

        :param file fh: The file handle to open
        :param int mode: Automatically detect the filetype or force binary
        """"""
    header = fh.read(HEADER_SIZE)
<mask>:
        return None
    if isinstance(header, str):
        header = b(header)
    if mode is AUTOMATIC:
        if header.lstrip().lower().startswith(b'solid'):
            try:
                name, data = cls._load_ascii(fh, header, speedups=speedups)
            except RuntimeError as exception:
                recoverable, e = exception.args
                if recoverable:
                    name, data = cls._load_binary(fh, header, check_size=False)
                else:
                    fh.seek(HEADER_SIZE)
                    name, data = cls._load_binary(fh, header, check_size=True)
        else:
            name, data = cls._load_binary(fh, header)
    elif mode is ASCII:
        name, data = cls._load_ascii(fh, header, speedups=speedups)
    else:
        name, data = cls._load_binary(fh, header)
    return (name, data)",not header,111,not header,True,100.00000000000004,N/A
"@classmethod
def _load_binary(cls, fh, header, check_size=False):
    count_data = fh.read(COUNT_SIZE)
<mask>:
        count = 0
    else:
        count, = struct.unpack('<i', b(count_data))
    assert count < MAX_COUNT, 'File too large, got %d triangles which exceeds the maximum of %d' % (count, MAX_COUNT)
    if check_size:
        try:
            fh.seek(0, os.SEEK_END)
            raw_size = fh.tell() - HEADER_SIZE - COUNT_SIZE
            expected_count = int(raw_size / cls.dtype.itemsize)
            assert expected_count == count, 'Expected %d vectors but header indicates %d' % (expected_count, count)
            fh.seek(HEADER_SIZE + COUNT_SIZE)
        except OSError:
            pass
    name = header.strip()
    try:
        return (name, np.fromfile(fh, dtype=cls.dtype, count=count))
    except io.UnsupportedOperation:
        data = np.frombuffer(fh.read(), dtype=cls.dtype, count=count)
        return (name, data.copy())",len(count_data) != COUNT_SIZE,93,count_data == '',False,14.110009442520557,N/A
"@classmethod
def _ascii_reader(cls, fh, header):
<mask>:
        recoverable = [True]
    else:
        recoverable = [False]
        header += b(fh.read(BUFFER_SIZE))
    lines = b(header).split(b'\n')

    def get(prefix=''):
        prefix = b(prefix).lower()
        if lines:
            raw_line = lines.pop(0)
        else:
            raise RuntimeError(recoverable[0], 'Unable to find more lines')
        if not lines:
            recoverable[0] = False
            lines[:] = b(fh.read(BUFFER_SIZE)).split(b'\n')
            raw_line += lines.pop(0)
        raw_line = raw_line.strip()
        line = raw_line.lower()
        if line == b(''):
            return get(prefix)
        if prefix:
            if line.startswith(prefix):
                values = line.replace(prefix, b(''), 1).strip().split()
            elif line.startswith((b('endsolid'), b('end solid'))):
                size_unprocessedlines = sum((len(line) + 1 for line in lines)) - 1
                if size_unprocessedlines > 0:
                    position = fh.tell()
                    fh.seek(position - size_unprocessedlines)
                raise StopIteration()
            else:
                raise RuntimeError(recoverable[0], f'{line!r} should start with {prefix!r}')
            if len(values) == 3:
                return [float(v) for v in values]
            else:
                raise RuntimeError(recoverable[0], f'Incorrect value {line!r}')
        else:
            return b(raw_line)
    line = get()
    if not lines:
        raise RuntimeError(recoverable[0], 'No lines found, impossible to read')
    yield line[5:].strip()
    while True:
        try:
            normals = get('facet normal')
            assert get().lower() == b('outer loop')
            v0 = get('vertex')
            v1 = get('vertex')
            v2 = get('vertex')
            assert get().lower() == b('endloop')
            assert get().lower() == b('endfacet')
            attrs = 0
            yield (normals, (v0, v1, v2), attrs)
        except AssertionError as e:
            raise RuntimeError(recoverable[0], e) from e
        except StopIteration:
            return",b'\n' in header,190,header.startswith(b('true')),False,4.767707020457095,N/A
"@classmethod
def _load_ascii(cls, fh, header, speedups=True):
    try:
        fh.fileno()
    except io.UnsupportedOperation:
        speedups = False
<mask>:
        return _speedups.ascii_read(fh, header)
    else:
        iterator = cls._ascii_reader(fh, header)
        name = next(iterator)
        return (name, np.fromiter(iterator, dtype=cls.dtype))",_speedups and speedups,29,speedups,False,4.9787068367863965,N/A
"def save(self, filename, fh=None, mode=AUTOMATIC, update_normals=True):
    """"""Save the STL to a (binary) file

        If mode is :py:data:`AUTOMATIC` an :py:data:`ASCII` file will be
        written if the output is a TTY and a :py:data:`BINARY` file otherwise.

        :param str filename: The file to load
        :param file fh: The file handle to open
        :param int mode: The mode to write, default is :py:data:`AUTOMATIC`.
        :param bool update_normals: Whether to update the normals
        """"""
    assert filename, 'Filename is required for the STL headers'
<mask>:
        self.update_normals()
    if mode is AUTOMATIC:
        if fh:
            try:
                if os.isatty(fh.fileno()):
                    write = self._write_ascii
                else:
                    write = self._write_binary
            except OSError:
                write = self._write_binary
        else:
            write = self._write_binary
    elif mode is BINARY:
        write = self._write_binary
    elif mode is ASCII:
        write = self._write_ascii
    else:
        raise ValueError(f'Mode {mode!r} is invalid')
    if isinstance(fh, io.TextIOBase):
        raise TypeError('File handles should be in binary mode - even when writing an ASCII STL.')
    name = self.name
    if not name:
        name = os.path.split(filename)[-1]
    try:
        if fh:
            write(fh, name)
        else:
            with open(filename, 'wb') as fh:
                write(fh, name)
    except OSError:
        pass",update_normals,167,update_normals,True,100.00000000000004,N/A
"def run(self):
<mask>:
        with open('ethereum_requirements.txt') as f:
            install_reqs = f.readlines()
            eth_reqs = [str(ir) for ir in install_reqs]
            reqs.extend(eth_reqs)
    else:
        with open('bitcoin_requirements.txt') as f:
            install_reqs = f.readlines()
            btc_reqs = [str(ir) for ir in install_reqs]
            reqs.extend(btc_reqs)
    install(reqs)",self.blockchain == 'ethereum',35,self.ethereum,False,20.24518585186855,N/A
"def get_config():
    global config
<mask>:
        config = cert_issuer.config.get_config()
    return config",config == None,10,config is None,False,24.840753130578644,N/A
"def get_config():
    configure_logger()
    p = configargparse.getArgumentParser(default_config_files=[os.path.join(PATH, 'conf.ini'), '/etc/cert-issuer/conf.ini'])
    add_arguments(p)
    parsed_config, _ = p.parse_known_args()
<mask>:
        logging.warning('Your app is configured to skip the wifi check when the USB is plugged in. Read the documentation to ensure this is what you want, since this is less secure')
    parsed_config.chain = Chain.parse_from_chain(parsed_config.chain)
    if parsed_config.chain.blockchain_type != BlockchainType.bitcoin and parsed_config.chain.blockchain_type != BlockchainType.ethereum and (parsed_config.chain.blockchain_type != BlockchainType.mock):
        raise UnknownChainError(parsed_config.chain.name)
    logging.info('This run will try to issue on the %s chain', parsed_config.chain.name)
    if parsed_config.chain.blockchain_type == BlockchainType.bitcoin:
        bitcoin_chain_for_python_bitcoinlib = parsed_config.chain
        if parsed_config.chain == Chain.bitcoin_regtest:
            bitcoin_chain_for_python_bitcoinlib = Chain.bitcoin_regtest
        bitcoin.SelectParams(chain_to_bitcoin_network(bitcoin_chain_for_python_bitcoinlib))
    global CONFIG
    CONFIG = parsed_config
    return parsed_config",not parsed_config.safe_mode,94,parsed_config.chain is None,False,35.64026463354184,N/A
"def main(app_config):
    chain = app_config.chain
<mask>:
        from cert_issuer.blockchain_handlers import ethereum
        certificate_batch_handler, transaction_handler, connector = ethereum.instantiate_blockchain_handlers(app_config)
    else:
        from cert_issuer.blockchain_handlers import bitcoin
        certificate_batch_handler, transaction_handler, connector = bitcoin.instantiate_blockchain_handlers(app_config)
    return issue(app_config, certificate_batch_handler, transaction_handler)",chain.is_ethereum_type(),29,chain == 'ethereum',False,4.576506607182439,N/A
"def to_source_id(txid, chain):
<mask>:
        return txid
    else:
        return 'This has not been issued on a blockchain and is for testing only'",chain.is_bitcoin_type() or chain.is_ethereum_type(),21,chain == 'chain',False,0.3756625385528342,N/A
"def add_proof(self, certificate_json, merkle_proof, app_config=None):
<mask>:
        if not isinstance(certificate_json['proof'], list):
            initial_proof = certificate_json['proof']
            certificate_json['proof'] = [initial_proof]
        if self.is_multiple_proof_config_chained(app_config):
            self.add_chained_proof(certificate_json, merkle_proof)
        else:
            certificate_json['proof'].append(merkle_proof)
    else:
        certificate_json['proof'] = merkle_proof
    self.update_context_for_proof(certificate_json)
    return certificate_json",'proof' in certificate_json,29,app_config is not None,False,8.116697886877475,N/A
"def is_multiple_proof_config_chained(self, app_config):
<mask>:
        return True
    return app_config.multiple_proofs == 'chained'",app_config is None,10,app_config.multiple_proofs is None,False,22.31618068926665,N/A
"def update_context_for_proof(self, certificate_json):
    context = certificate_json['@context']
<mask>:
        context.append(self.contextUrls.data_integrity_proof_v2())
    if array_intersect(self.contextUrls.v3_all(), context):
        for v3_context in self.contextUrls.v3_all():
            if v3_context in context:
                index = context.index(v3_context)
                del context[index]
        context.append(self.contextUrls.v3_1_canonical())",self.contextUrls.data_integrity_proof_v2() not in context,25,"array_intersect(self.contextUrls.data_integrity_proof_v2(), context)",False,63.139223413648885,N/A
"@staticmethod
def preload_contexts():
<mask>:
        return
    for url, path in zip(CONFIG.context_urls, CONFIG.context_file_paths):
        with open(os.path.join(os.getcwd(), path)) as context_file:
            context_data = json.load(context_file)
            extend_preloaded_context(url, context_data)",CONFIG is None or CONFIG.context_urls is None or CONFIG.context_file_paths is None,21,not CONFIG.context_urls or not CONFIG.context_file_paths,False,44.28354515428595,N/A
"def start(self):
<mask>:
        check_internet_off(self.path_to_secret)
    else:
        logging.warning('app is configured to skip the wifi check when the USB is plugged in. Read the documentation to ensure this is what you want, since this is less secure')
    self.wif = import_key(self.path_to_secret)",self.safe_mode,37,self.is_internet_off(),False,10.552670315936318,N/A
"def stop(self):
    self.wif = None
<mask>:
        check_internet_on(self.path_to_secret)
    else:
        logging.warning('app is configured to skip the wifi check when the USB is plugged in. Read the documentation to ensure this is what you want, since this is less secure')",self.safe_mode,37,self.path_to_secret,False,14.535768424205482,N/A
"def check_internet_off(secrets_file_path):
    """"""If internet off and USB plugged in, returns true. Else, continues to wait...""""""
    while True:
<mask>:
            break
        else:
            print('Turn off your internet and plug in your USB to continue...')
            time.sleep(10)
    return True",internet_on() is False and os.path.exists(secrets_file_path),34,os.path.isfile(secrets_file_path),False,37.73045521602795,N/A
"def check_internet_on(secrets_file_path):
    """"""If internet on and USB unplugged, returns true. Else, continues to wait...""""""
    while True:
<mask>:
            break
        else:
            print('Turn on your internet and unplug your USB to continue...')
            time.sleep(10)
    return True",internet_on() is True and (not os.path.exists(secrets_file_path)),32,check_internet_on(secrets_file_path),False,28.692623076011028,N/A
"def __init__(self, uid, unsigned_certs_dir, signed_certs_dir, blockcerts_dir, final_blockcerts_dir, file_extension=JSON_EXT):
    self.uid = uid
    self.unsigned_cert_file_name = os.path.join(unsigned_certs_dir, uid + file_extension)
<mask>:
        self.signed_cert_file_name = os.path.join(signed_certs_dir, uid + file_extension)
    self.blockchain_cert_file_name = os.path.join(blockcerts_dir, uid + file_extension)
    self.final_blockchain_cert_file_name = os.path.join(final_blockcerts_dir, uid + file_extension)",signed_certs_dir,36,signed_certs_dir,True,100.00000000000004,N/A
"def prepare_issuance_batch(unsigned_certs_dir, signed_certs_dir, blockchain_certs_dir, work_dir, file_extension=JSON_EXT):
    """"""
    Prepares file system for issuing a batch of certificates. Copies inputs to work_dir, and ensures
    that all output dirs required for processing the batch exist.
    :param unsigned_certs_dir: input certificates
    :param signed_certs_dir: output dir
    :param blockchain_certs_dir: output dir
    :param work_dir: work dir
    :return:
    """"""
    os.makedirs(work_dir, exist_ok=True)
    os.makedirs(blockchain_certs_dir, exist_ok=True)
    os.makedirs(signed_certs_dir, exist_ok=True)
    for item in os.listdir(work_dir):
        file_path = os.path.join(work_dir, item)
<mask>:
            shutil.rmtree(file_path)
    unsigned_certs_work_dir = os.path.join(work_dir, UNSIGNED_CERTIFICATES_DIR)
    signed_certs_work_dir = os.path.join(work_dir, SIGNED_CERTIFICATES_DIR)
    blockchain_certs_work_dir = os.path.join(work_dir, BLOCKCHAIN_CERTIFICATES_DIR)
    shutil.copytree(unsigned_certs_dir, unsigned_certs_work_dir)
    os.makedirs(signed_certs_work_dir, exist_ok=True)
    os.makedirs(blockchain_certs_work_dir, exist_ok=True)
    cert_info = collections.OrderedDict()
    input_file_pattern = str(os.path.join(unsigned_certs_work_dir, '*' + file_extension))
    matches = glob2.iglob(input_file_pattern, with_matches=True)
    if not matches:
        logging.warning('No certificates to process')
        raise NoCertificatesFoundError('No certificates to process')
    for filename, (uid,) in sorted(matches):
        certificate_metadata = CertificateMetadata(uid=uid, unsigned_certs_dir=unsigned_certs_work_dir, signed_certs_dir=signed_certs_work_dir, blockcerts_dir=blockchain_certs_work_dir, final_blockcerts_dir=blockchain_certs_dir, file_extension=file_extension)
        cert_info[uid] = certificate_metadata
    logging.info('Processing %d certificates', len(cert_info))
    return cert_info",os.path.isdir(file_path),131,os.path.exists(file_path),False,65.80370064762461,N/A
"def to_pycoin_chain(chain):
<mask>:
        return 'XTN'
    elif chain == Chain.bitcoin_mainnet:
        return 'BTC'
    else:
        raise UnknownChainError(chain.name)",chain == Chain.bitcoin_regtest or chain == Chain.bitcoin_testnet,14,chain == Chain.xtnnet,False,12.148233648426672,N/A
"def tx_to_blink(chain, tx_id):
    blink = 'blink:'
<mask>:
        blink += 'btc:regtest:'
    elif chain == Chain.bitcoin_testnet:
        blink += 'btc:testnet:'
    elif chain == Chain.bitcoin_mainnet:
        blink += 'btc:mainnet:'
    elif chain == Chain.ethereum_ropsten:
        blink += 'eth:ropsten:'
    elif chain == Chain.ethereum_goerli:
        blink += 'eth:goerli:'
    elif chain == Chain.ethereum_sepolia:
        blink += 'eth:sepolia:'
    elif chain == Chain.ethereum_mainnet:
        blink += 'eth:mainnet:'
    elif chain == Chain.mockchain:
        blink += 'mocknet:'
    else:
        raise UnknownChainError(chain.name)
    return blink + tx_id",chain == Chain.bitcoin_regtest,66,chain == Chain.bitcoin_regtest,True,100.00000000000004,N/A
"def _process_directories(self, config):
    unsigned_certs_dir = config.unsigned_certificates_dir
    signed_certs_dir = config.signed_certificates_dir
    blockchain_certificates_dir = config.blockchain_certificates_dir
    work_dir = config.work_dir
    certificates_metadata = helpers.prepare_issuance_batch(unsigned_certs_dir, signed_certs_dir, blockchain_certificates_dir, work_dir)
    num_certificates = len(certificates_metadata)
<mask>:
        return None
    logging.info('Processing %d certificates under work path=%s', num_certificates, work_dir)
    self.set_certificates_in_batch(certificates_metadata)",num_certificates < 1,36,num_certificates == 0,False,30.213753973567677,N/A
"def broadcast_tx(self, tx):
    hextx = to_hex(tx)
    broadcast_url = self.base_url + '/txs/push'
<mask>:
        broadcast_url += '?token=' + self.api_token
    response = requests.post(broadcast_url, json={'tx': hextx})
    if int(response.status_code) == 201:
        tx_id = response.json().get('tx', None)
        tx_hash = tx_id.get('hash', None)
        return tx_hash
    logging.error('Error broadcasting the transaction through the Blockcypher API. Error msg: %s', response.text)
    raise BroadcastError(response.text)",self.api_token,50,self.api_token,True,100.00000000000004,N/A
"def spendables_for_address(self, address):
    """"""
        Return a list of Spendable objects for the
        given bitcoin address.
        """"""
    logging.info('trying to get spendables from blockcypher')
    spendables = []
    url_append = '?unspentOnly=true&includeScript=true'
<mask>:
        url_append += '&token=' + self.api_token
    url = self.base_url + '/addrs/' + address + url_append
    response = requests.get(url)
    if int(response.status_code) == 200:
        for txn in response.json().get('txrefs', []):
            coin_value = txn.get('value')
            script = h2b(txn.get('script'))
            previous_hash = h2b_rev(txn.get('tx_hash'))
            previous_index = txn.get('tx_output_n')
            spendables.append(Spendable(coin_value, script, previous_hash, previous_index))
    return spendables",self.api_token,73,self.api_token,True,100.00000000000004,N/A
"def broadcast_tx(self, tx):
    hextx = to_hex(tx)
    broadcast_url = self.base_url + '/tx'
    response = requests.post(broadcast_url, data=hextx)
<mask>:
        tx_id = response.text
        return tx_id
    logging.error('Error broadcasting the transaction through the Blockstream API. Error msg: %s', response.text)
    raise BroadcastError(response.text)",int(response.status_code) == 200,35,response.status_code == 200,False,47.52203774792177,N/A
"def get_unspent_outputs(self, address):
    """"""
        Get unspent outputs at the address
        :param address:
        :return:
        """"""
    logging.debug('get_unspent_outputs for address=%s', address)
    spendables = self.spendables_for_address(bitcoin_address=address)
<mask>:
        return sorted(spendables, key=lambda x: hash(x.coin_value))
    return None",spendables,29,spendables,True,100.00000000000004,N/A
"def get_balance(self, address):
    """"""
        Get balance available to spend at the address
        :param address:
        :return:
        """"""
    spendables = self.get_unspent_outputs(address)
<mask>:
        logging.warning('address %s has a balance of 0', address)
        return 0
    balance = sum((s.coin_value for s in spendables))
    return balance",not spendables,39,not spendables,True,100.00000000000004,N/A
"def initialize_signer(app_config):
    path_to_secret = os.path.join(app_config.usb_name, app_config.key_file)
<mask>:
        signer = BitcoinSigner(bitcoin_chain=app_config.chain)
    elif app_config.chain.is_mock_type():
        signer = None
    else:
        raise UnknownChainError(app_config.chain)
    secret_manager = FileSecretManager(signer=signer, path_to_secret=path_to_secret, safe_mode=app_config.safe_mode, issuing_address=app_config.issuing_address)
    return secret_manager",app_config.chain.is_bitcoin_type(),26,app_config.chain.is_chain_type(),False,76.11606003349888,N/A
"def instantiate_blockchain_handlers(app_config, file_mode=True):
    issuing_address = app_config.issuing_address
    chain = app_config.chain
    secret_manager = initialize_signer(app_config)
<mask>:
        certificate_batch_handler = CertificateBatchHandler(secret_manager=secret_manager, certificate_handler=CertificateV3Handler(app_config), merkle_tree=MerkleTreeGenerator(), config=app_config)
    else:
        certificate_batch_handler = CertificateBatchWebHandler(secret_manager=secret_manager, certificate_handler=CertificateWebV3Handler(app_config), merkle_tree=MerkleTreeGenerator(), config=app_config)
    if chain.is_mock_type():
        transaction_handler = MockTransactionHandler()
        connector = MockServiceProviderConnector()
    else:
        cost_constants = BitcoinTransactionCostConstants(app_config.tx_fee, app_config.dust_threshold, app_config.satoshi_per_byte)
        connector = BitcoinServiceProviderConnector(chain, app_config.bitcoind)
        transaction_handler = BitcoinTransactionHandler(connector, cost_constants, secret_manager, issuing_address=issuing_address)
    return (certificate_batch_handler, transaction_handler, connector)",file_mode,54,file_mode,True,100.00000000000004,N/A
"def ensure_balance(self):
    balance = self.connector.get_balance(self.issuing_address)
    transaction_cost = self.transaction_creator.estimate_cost_for_certificate_batch(self.tx_cost_constants)
    logging.info('Total cost will be %d satoshis', transaction_cost)
<mask>:
        error_message = 'Please add {} satoshis to the address {}'.format(transaction_cost - balance, self.issuing_address)
        logging.error(error_message)
        raise InsufficientFundsError(error_message)",transaction_cost > balance,32,transaction_cost - balance < self.satoshis_threshold,False,14.991106946711685,N/A
"def create_transaction(self, op_return_bytes):
<mask>:
        inputs = self.prepared_inputs
    else:
        spendables = self.connector.get_unspent_outputs(self.issuing_address)
        if not spendables:
            error_message = 'No money to spend at address {}'.format(self.issuing_address)
            logging.error(error_message)
            raise InsufficientFundsError(error_message)
        cost = self.transaction_creator.estimate_cost_for_certificate_batch(self.tx_cost_constants)
        current_total = 0
        inputs = []
        random.shuffle(spendables)
        for s in spendables:
            inputs.append(s)
            current_total += s.coin_value
            if current_total > cost:
                break
    tx = self.transaction_creator.create_transaction(self.tx_cost_constants, self.issuing_address, inputs, op_return_bytes)
    hex_tx = b2h(tx.serialize())
    logging.info('Unsigned hextx=%s', hex_tx)
    prepared_tx = tx_utils.prepare_tx_for_signing(hex_tx, inputs)
    return prepared_tx",self.prepared_inputs,67,self.issuing_address and self.prepared_inputs,False,35.08439695638686,N/A
"def create_trx(op_return_val, issuing_transaction_fee, issuing_address, tx_outs, tx_inputs):
    """"""

    :param op_return_val:
    :param issuing_transaction_fee:
    :param issuing_address:
    :param tx_outs:
    :param tx_input:
    :return:
    """"""
    cert_out = CMutableTxOut(0, CScript([OP_RETURN, op_return_val]))
    tx_ins = []
    value_in = 0
    for tx_input in tx_inputs:
        tx_ins.append(CTxIn(COutPoint(tx_input.tx_hash, tx_input.tx_out_index)))
        value_in += tx_input.coin_value
    amount = value_in - issuing_transaction_fee
<mask>:
        change_out = create_transaction_output(issuing_address, amount)
        tx_outs = tx_outs + [change_out]
    tx_outs = tx_outs + [cert_out]
    transaction = CMutableTransaction(tx_ins, tx_outs)
    return transaction",amount > 0,65,issuing_address and amount,False,10.682175159905848,N/A
"def verify_transaction(signed_hextx, op_return_value):
    """"""
    Verify OP_RETURN field in transaction
    :param signed_hextx:
    :param op_return_value:
    :return:
    """"""
    logging.info('verifying op_return value for transaction')
    op_return_hash = signed_hextx[-72:-8]
    result = op_return_value == op_return_hash
<mask>:
        error_message = 'There was a problem verifying the transaction'
        raise UnverifiedTransactionError(error_message)
    logging.info('verified OP_RETURN')",not result,42,not result,True,100.00000000000004,N/A
"def sign_transaction(self, wif, transaction_to_sign):
    netcode = to_pycoin_chain(self.bitcoin_chain)
    network = network_for_netcode(netcode)
    key = network.parse.wif(wif)
    secret_exponent = key.secret_exponent()
    lookup = build_hash160_lookup([secret_exponent])
    signed_transaction = transaction_to_sign.sign(lookup)
    for input in signed_transaction.txs_in:
<mask>:
            logging.error('Unable to sign transaction. hextx=%s', signed_transaction.as_hex())
            raise UnableToSignTxError('Unable to sign transaction')
    return signed_transaction",len(input.script) == 0,40,input.hextx != input.hextx,False,10.786826322527466,N/A
"def verify_signature(uid, signed_cert_file_name, issuing_address):
    """"""
    Verify the certificate signature matches the expected. Double-check the uid field in the certificate and use
    VerifyMessage to confirm that the signature in the certificate matches the issuing_address.

    Raises error is verification fails.

    Raises UnverifiedSignatureError if signature is invalid

    :param uid:
    :param signed_cert_file_name:
    :param issuing_address:
    :return:
    """"""
    logging.info('verifying signature for certificate with uid=%s:', uid)
    with open(signed_cert_file_name) as in_file:
        signed_cert = in_file.read()
        signed_cert_json = json.loads(signed_cert)
        to_verify = uid
        signature = signed_cert_json['signature']
        verified = verify_message(issuing_address, to_verify, signature)
<mask>:
            error_message = 'There was a problem with the signature for certificate uid={}'.format(uid)
            raise UnverifiedSignatureError(error_message)
        logging.info('verified signature')",not verified,97,not verified,True,100.00000000000004,N/A
"def __init__(self, ethereum_chain, app_config, local_node=False):
    self.ethereum_chain = ethereum_chain
    self.local_node = local_node
    self.connectors = {}
    eth_provider_list = []
<mask>:
        self.ethereum_rpc_url = app_config.ethereum_rpc_url
        eth_provider_list.append(EthereumRPCProvider(self.ethereum_rpc_url))
    etherscan_api_token = None
    if hasattr(app_config, 'api_token'):
        logging.warning('The api_token config property is deprecated in favor of the etherscan_api_token property.  It still works, but please switch over soon.')
        etherscan_api_token = app_config.etherscan_api_token
    if hasattr(app_config, 'etherscan_api_token'):
        etherscan_api_token = app_config.etherscan_api_token
    eth_provider_list.append(EtherscanBroadcaster('https://api.etherscan.io/api', etherscan_api_token))
    self.connectors[Chain.ethereum_mainnet] = eth_provider_list
    rop_provider_list = []
    if hasattr(app_config, 'ropsten_rpc_url'):
        self.ropsten_rpc_url = app_config.ropsten_rpc_url
        rop_provider_list.append(EthereumRPCProvider(self.ropsten_rpc_url))
    rop_provider_list.append(EtherscanBroadcaster('https://api-ropsten.etherscan.io/api', etherscan_api_token))
    self.connectors[Chain.ethereum_ropsten] = rop_provider_list
    goe_provider_list = []
    if hasattr(app_config, 'goerli_rpc_url'):
        self.goerli_rpc_url = app_config.goerli_rpc_url
        goe_provider_list.append(EthereumRPCProvider(self.goerli_rpc_url))
    goe_provider_list.append(EtherscanBroadcaster('https://api-goerli.etherscan.io/api', etherscan_api_token))
    self.connectors[Chain.ethereum_goerli] = goe_provider_list
    sep_provider_list = []
    if hasattr(app_config, 'sepolia_rpc_url'):
        self.sepolia_rpc_url = app_config.sepolia_rpc_url
        sep_provider_list.append(EthereumRPCProvider(self.sepolia_rpc_url))
    sep_provider_list.append(EtherscanBroadcaster('https://api-sepolia.etherscan.io/api', etherscan_api_token))
    self.connectors[Chain.ethereum_sepolia] = sep_provider_list","hasattr(app_config, 'ethereum_rpc_url')",107,"hasattr(app_config, 'ethereum_rpc_url')",True,100.00000000000004,N/A
"def broadcast_tx(self, tx):
    last_exception = None
    final_tx_id = None
    for attempt_number in range(0, MAX_BROADCAST_ATTEMPTS):
        for m in self.get_providers_for_chain(self.ethereum_chain, self.local_node):
            try:
                logging.debug('m=%s', m)
                txid = m.broadcast_tx(tx)
<mask>:
                    logging.info('Broadcasting succeeded with method_provider=%s, txid=%s', str(m), txid)
                    if final_tx_id and final_tx_id != txid:
                        logging.error('This should never happen; fail and investigate if it does. Got conflicting tx_ids=%s and %s. Hextx=%s', final_tx_id, txid, tx.as_hex())
                        raise Exception('Got conflicting tx_ids.')
                    final_tx_id = txid
                return txid
            except Exception as e:
                logging.warning('Caught exception trying provider %s. Trying another. Exception=%s', str(m), e)
                last_exception = e
        if final_tx_id:
            return final_tx_id
        else:
            logging.warning('Broadcasting failed. Waiting before retrying. This is attempt number %d', attempt_number)
            time.sleep(BROADCAST_RETRY_INTERVAL)
    logging.error('Failed broadcasting through all providers')
    logging.error(last_exception, exc_info=True)
    raise BroadcastError(last_exception)",txid,110,txid,True,100.00000000000004,N/A
"def broadcast_tx(self, tx):
    tx_hex = tx
    broadcast_url = self.base_url + '?module=proxy&action=eth_sendRawTransaction'
<mask>:
        broadcast_url += '&apikey=%s' % self.api_token
    response = self.send_request('POST', broadcast_url, {'hex': tx_hex})
    if 'error' in response.json():
        logging.error('Etherscan returned an error: %s', response.json()['error'])
        raise BroadcastError(response.json()['error'])
    if int(response.status_code) == 200:
        if response.json().get('message', None) == 'NOTOK':
            raise BroadcastError(response.json().get('result', None))
        tx_id = response.json().get('result', None)
        logging.info('Transaction ID obtained from broadcast through Etherscan: %s', tx_id)
        return tx_id
    logging.error('Error broadcasting the transaction through the Etherscan API. Error msg: %s', response.text)
    raise BroadcastError(response.text)",self.api_token,76,self.api_token,True,100.00000000000004,N/A
"def get_balance(self, address):
    """"""
        returns the balance in wei
        with some inspiration from PyWallet
        """"""
    broadcast_url = self.base_url + '?module=account&action=balance'
    broadcast_url += '&address=%s' % address
    broadcast_url += '&tag=pending'
<mask>:
        broadcast_url += '&apikey=%s' % self.api_token
    response = self.send_request('GET', broadcast_url)
    if int(response.status_code) == 200:
        if response.json().get('message', None) == 'NOTOK':
            raise BroadcastError(response.json().get('result', None))
        balance = int(response.json().get('result', None))
        logging.info('Balance check succeeded: %s', response.json())
        return balance
    raise BroadcastError(response.text)",self.api_token,63,self.api_token,True,100.00000000000004,N/A
"def gas_price(self):
    """"""
        returns the gas price in wei
        """"""
    api_url = self.base_url + '?module=proxy&action=eth_gasPrice'
<mask>:
        api_url += '&apikey=%s' % self.api_token
    response = self.send_request('GET', api_url)
    if int(response.status_code) == 200:
        gas = int(response.json().get('result', None), 0)
        logging.info('Gas price: %s', response.json())
        return gas
    raise BroadcastError(response.text)",self.api_token,42,self.api_token,True,100.00000000000004,N/A
"def __init__(self, max_priority_fee_per_gas, recommended_gas_price, recommended_gas_limit):
    self.max_priority_fee_per_gas = max_priority_fee_per_gas
    self.recommended_gas_price = recommended_gas_price
    self.recommended_gas_limit = recommended_gas_limit
    logging.info('Set cost constants to recommended_gas_price=%f Gwei, recommended_gas_limit=%d gas', self.recommended_gas_price / ONE_BILLION, self.recommended_gas_limit)
<mask>:
        logging.info('and max_priority_fee_per_gas=%f Gwei', self.max_priority_fee_per_gas / ONE_BILLION)",self.max_priority_fee_per_gas,33,self.max_priority_fee_per_gas > 0,False,82.4236750264605,N/A
"def initialize_signer(app_config):
    path_to_secret = os.path.join(app_config.usb_name, app_config.key_file)
<mask>:
        signer = EthereumSigner(ethereum_chain=app_config.chain)
    elif app_config.is_mock_type():
        signer = None
    else:
        raise UnknownChainError(app_config.chain)
    secret_manager = FileSecretManager(signer=signer, path_to_secret=path_to_secret, safe_mode=app_config.safe_mode, issuing_address=app_config.issuing_address)
    return secret_manager",app_config.chain.is_ethereum_type(),26,app_config.is_ethereum(),False,40.96777877652662,N/A
"def instantiate_blockchain_handlers(app_config, file_mode=True):
    issuing_address = app_config.issuing_address
    chain = app_config.chain
    secret_manager = initialize_signer(app_config)
    certificate_batch_handler = (CertificateBatchHandler if file_mode else CertificateBatchWebHandler)(secret_manager=secret_manager, certificate_handler=(CertificateV3Handler if file_mode else CertificateWebV3Handler)(app_config), merkle_tree=MerkleTreeGenerator(), config=app_config)
<mask>:
        transaction_handler = MockTransactionHandler()
    elif chain.is_ethereum_type():
        nonce = app_config.nonce
        connector = EthereumServiceProviderConnector(chain, app_config)
        if app_config.gas_price_dynamic:
            gas_price = connector.gas_price()
        else:
            gas_price = app_config.gas_price
        cost_constants = EthereumTransactionCostConstants(app_config.max_priority_fee_per_gas, gas_price, app_config.gas_limit)
        transaction_handler = EthereumTransactionHandler(connector, nonce, cost_constants, secret_manager, issuing_address=issuing_address)
    return (certificate_batch_handler, transaction_handler, connector)",chain.is_mock_type(),64,chain.is_mock_type(),True,100.00000000000004,N/A
"def ensure_balance(self):
    self.balance = self.connector.get_balance(self.issuing_address)
    transaction_cost = self.tx_cost_constants.get_recommended_max_cost()
    logging.info('Total cost will be no more than %d wei', transaction_cost)
<mask>:
        error_message = 'Please add {} wei to the address {}'.format(transaction_cost - self.balance, self.issuing_address)
        logging.error(error_message)
        raise InsufficientFundsError(error_message)",transaction_cost > self.balance,35,self.balance > transaction_cost,False,42.72870063962342,N/A
"def create_transaction(self, blockchain_bytes):
<mask>:
        nonce = self.nonce or self.connector.get_address_nonce(self.issuing_address)
        logging.info('NONCE IS %d', nonce)
        toaddress = Web3.to_checksum_address('0xdeaddeaddeaddeaddeaddeaddeaddeaddeaddead')
        prepared_tx = self.transaction_creator.create_transaction(self.tx_cost_constants, self.issuing_address, nonce, toaddress, blockchain_bytes)
        return prepared_tx
    else:
        raise InsufficientFundsError('Not sufficient ether to spend at: %s', self.issuing_address)",self.balance,35,self.is_spendable(self.issuing_address),False,6.772997136689072,N/A
"def create_ethereum_trx(nonce, to_address, blockchain_bytes, max_priority_fee_per_gas, gasprice, gaslimit):
    value = 0
    tx = dict(nonce=nonce, gas=gaslimit, to=to_address, value=value, data=blockchain_bytes)
<mask>:
        tx['maxFeePerGas'] = gasprice
        tx['maxPriorityFeePerGas'] = max_priority_fee_per_gas
    else:
        tx['gasPrice'] = gasprice
    return tx",max_priority_fee_per_gas,30,max_priority_fee_per_gas,True,100.00000000000004,N/A
"def verify_eth_transaction(signed_hextx, eth_data_field):
    """"""
    Verify ethDataField field in transaction
    :param signed_hextx:
    :param eth_data_field:
    :return:
    """"""
    logging.info('verifying ethDataField value for transaction')
    ethdata_hash = []
    for s in signed_hextx.split('80a0'):
        ethdata_hash.append(s)
    ethdata_hash = ethdata_hash[1][:64]
    result = eth_data_field == ethdata_hash
<mask>:
        error_message = 'There was a problem verifying the transaction'
        raise UnverifiedTransactionError(error_message)
    logging.info('verified ethDataField')",not result,50,result,False,36.78794411714425,N/A
"def __init__(self, ethereum_chain):
    self.ethereum_chain = ethereum_chain
<mask>:
        self.netcode = 1
    elif ethereum_chain.external_display_value == 'ethereumRopsten':
        self.netcode = 3
    elif ethereum_chain.external_display_value == 'ethereumGoerli':
        self.netcode = 5
    elif ethereum_chain.external_display_value == 'ethereumSepolia':
        self.netcode = 11155111
    else:
        self.netcode = None",ethereum_chain.external_display_value == 'ethereumMainnet',35,ethereum_chain.external_display_value == 'ethereumSoccer',False,90.36020036098445,N/A
"def sign_transaction(self, wif, transaction_to_sign):
<mask>:
        try:
            transaction_to_sign['chainId'] = self.netcode
            raw_tx = web3.Account.sign_transaction(transaction_to_sign, wif)['raw_transaction']
            raw_tx_hex = to_hex(raw_tx)
            return raw_tx_hex
        except Exception as msg:
            logging.error('error occurred when ETH signing transaction: %s', msg)
            return {'error': True, 'message': msg}
    else:
        raise UnableToSignTxError('""sign_transaction()"" expects a dict representing an unsigned transaction with fields such as ""gas"", ""to"", ""data"", etc. run ""$ python cert_issuer -h"" for more information on transaction configuration.')","isinstance(transaction_to_sign, dict)",64,"isinstance(transaction_to_sign, dict)",True,100.00000000000004,N/A
"def get_creation_time(self, issuance_timezone):
<mask>:
        return datetime.now(timezone.utc).replace(microsecond=0, tzinfo=None).isoformat() + 'Z'
    else:
        return datetime.now().astimezone().replace(microsecond=0).isoformat()",issuance_timezone == 'UTC',12,issuance_timezone,False,36.78794411714425,N/A
"def validate_metadata_structure(metadata):
<mask>:
        try:
            json_object = copy(metadata)
            del json_object['schema']
            validate(instance=json_object, schema=metadata['schema'])
        except Exception as e:
            print(e)
            raise Exception('Certificate.metadata object does not match its provided schema')
    else:
        logging.warning('\n            The metadata object provided with the certificate does not include a `schema` property.\n            Not defining such property will result in errors in the rendering of the metadata property in the UI projects. \n            ')
    if 'displayOrder' not in metadata:
        logging.warning('\n           The metadata object provided with the certificate does not include a `displayOrder` property.\n           Not defining such property will result in errors in the rendering of the metadata property in the UI projects. \n           ')
        return
    else:
        verify_display_order_properties(metadata)",'schema' in metadata,104,'schema' in metadata,True,100.00000000000004,N/A
"def verify_display_order_properties(metadata):
    display_order = metadata['displayOrder']
    checked_groups = []
    for item in display_order:
        path = item.split('.')
        group = path[0]
<mask>:
            if group not in checked_groups:
                logging.warning('`metadata.displayOrder` property references a group named: \x1b[1m%s\x1b[0m which does not exist in metadata object.', group)
                checked_groups.append(group)
        else:
            property = path[1]
            if property not in metadata[group]:
                logging.warning('`metadata.displayOrder` property references a property named: \x1b[1m%s\x1b[0m which does not exist in group: \x1b[1m%s\x1b[0m.', property, group)
            else:
                verify_title_is_set(property, group, metadata)
    pass",group not in metadata,70,len(path) == 2,False,0.0,N/A
"def verify_title_is_set(property, group, metadata):
<mask>:
        return
    schema = metadata['schema']
    if 'title' not in schema['properties'][group]['properties'][property]:
        logging.warning('No title has been defined for property: \x1b[1m{0}\x1b[0m in group: \x1b[1m{1}\x1b[0m.\n            Title should be defined under path `schema.properties.{1}.properties.{0}.title`'.format(property, group))
    pass",'schema' not in metadata,34,property not in metadata['properties'],False,24.446151121745054,N/A
"@abstractmethod
def validate_certificate(self, certificate_metadata):
    validate_type(certificate_metadata['type'])
    validate_context(certificate_metadata['@context'], certificate_metadata['type'])
<mask>:
        validate_metadata_structure(json.loads(certificate_metadata['metadata']))
    if certificate_metadata['type'][0] == 'VerifiableCredential':
        verify_credential(certificate_metadata)
    if certificate_metadata['type'][0] == 'VerifiablePresentation':
        verify_presentation(certificate_metadata)
    pass",'metadata' in certificate_metadata,20,'metadata' in certificate_metadata,True,100.00000000000004,N/A
"def validate_type(certificate_type):
    compulsory_types = ['VerifiableCredential', 'VerifiablePresentation']
<mask>:
        raise ValueError('`type` property must be an array')
    contains_compulsory_types = list(set(compulsory_types) & set(certificate_type))
    if len(certificate_type) == 0 or len(contains_compulsory_types) == 0:
        raise ValueError('`type` property must be an array with at least `VerifiableCredential` or `VerifiablePresentation` value')
    pass","not isinstance(certificate_type, list)",42,"not isinstance(certificate_type, list)",True,100.00000000000004,N/A
"def validate_context(context, type):
    ContextUrlsInstance = ContextUrls()
    vc_context_url = [ContextUrlsInstance.verifiable_credential_v1(), ContextUrlsInstance.verifiable_credential_v2()]
    blockcerts_valid_context_url = ContextUrlsInstance.v3_all()
<mask>:
        raise ValueError('`@context` property must be an array')
    if context[0] not in vc_context_url:
        raise ValueError('First @context declared must be one of {}, was given {}'.format(vc_context_url, context[0]))
    if is_V1_verifiable_credential(context) and is_V2_verifiable_credential(context):
        raise ValueError('Cannot have both v1 and v2 Verifiable Credentials contexts defined in the context array')
    if len(type) > 1 and len(context) == 1:
        raise ValueError('A more specific type: {}, was detected, yet no context seems provided for that type'.format(type[1]))
    if context[-1] not in blockcerts_valid_context_url:
        logging.warning(""\n           Last `@context` is not blockcerts' context. It is not a critical issue but some issues have come up at times\n           because of some properties of a different context overwriting blockcerts' taxonomy. Check this property\n           again in case of verification issue.\n           "")
    pass","not isinstance(context, list)",130,"not isinstance(context, list)",True,100.00000000000004,N/A
"def validate_credential_subject(credential_subject, credential_schema):
<mask>:
        credential_schema = [credential_schema]
    if not isinstance(credential_subject, list):
        credential_subject = [credential_subject]
    for schema in credential_schema:
        schema_url = schema['id']
        local_filename, headers = urlretrieve(schema_url)
        with open(local_filename) as f:
            schema = json.load(f)
            for subject in credential_subject:
                jsonschema_validate(subject, schema)
    pass","not isinstance(credential_schema, list)",39,"not isinstance(credential_schema, list)",True,100.00000000000004,N/A
"def validate_issuer(certificate_issuer):
    has_error = False
<mask>:
        has_error = True
    if isinstance(certificate_issuer, dict) and (not is_valid_url(certificate_issuer['id'])):
        has_error = True
    if isinstance(certificate_issuer, list):
        has_error = True
    if has_error:
        raise ValueError('`issuer` property must be a URL string or an object with an `id` property containing a URL string')
    pass","isinstance(certificate_issuer, str) and (not is_valid_url(certificate_issuer))",46,"isinstance(certificate_issuer, str)",False,17.37739434504452,N/A
"def get_recursive_expansion_permitted(self) -> bool:
    """"""
        Defined at serializer level or based on RECURSIVE_EXPANSION_PERMITTED setting
        """"""
<mask>:
        return self.recursive_expansion_permitted
    else:
        return RECURSIVE_EXPANSION_PERMITTED",self.recursive_expansion_permitted is not None,21,self.recursive_expansion_permitted,False,65.14390575310559,N/A
"def to_representation(self, instance):
<mask>:
        self.apply_flex_fields(self.fields, self._flex_options_rep_only)
        self._flex_fields_rep_applied = True
    return super().to_representation(instance)",not self._flex_fields_rep_applied,11,not self._flex_fields_rep_applied,True,100.00000000000004,N/A
"def _make_expanded_field_serializer(self, name, nested_expand, nested_fields, nested_omit):
    """"""
        Returns an instance of the dynamically created nested serializer.
        """"""
    field_options = self._expandable_fields[name]
<mask>:
        serializer_class = field_options[0]
        settings = copy.deepcopy(field_options[1]) if len(field_options) > 1 else {}
    else:
        serializer_class = field_options
        settings = {}
    if type(serializer_class) == str:
        serializer_class = self._get_serializer_class_from_lazy_string(serializer_class)
    if issubclass(serializer_class, serializers.Serializer):
        settings['context'] = self.context
    if issubclass(serializer_class, FlexFieldsSerializerMixin):
        settings['parent'] = self
        if name in nested_expand:
            settings[EXPAND_PARAM] = nested_expand[name]
        if name in nested_fields:
            settings[FIELDS_PARAM] = nested_fields[name]
        if name in nested_omit:
            settings[OMIT_PARAM] = nested_omit[name]
    return serializer_class(**settings)","isinstance(field_options, tuple)",82,"isinstance(field_options, list)",False,70.71067811865478,N/A
"def _get_serializer_class_from_lazy_string(self, full_lazy_path: str):
    path_parts = full_lazy_path.split('.')
    class_name = path_parts.pop()
    path = '.'.join(path_parts)
    serializer_class, error = self._import_serializer_class(path, class_name)
<mask>:
        serializer_class, error = self._import_serializer_class(path + '.serializers', class_name)
    if serializer_class:
        return serializer_class
    raise Exception(error)",error and (not path.endswith('.serializers')),32,not serializer_class and 'serializers' in path,False,3.8902180856807296,N/A
"def _get_fields_names_to_remove(self, current_fields: List[str], omit_fields: List[str], sparse_fields: List[str], next_level_omits: List[str]) -> List[str]:
    """"""
        Remove fields that are found in omit list, and if sparse names
        are passed, remove any fields not found in that list.
        """"""
    sparse = len(sparse_fields) > 0
    to_remove = []
<mask>:
        return to_remove
    for field_name in current_fields:
        should_exist = self._should_field_exist(field_name, omit_fields, sparse_fields, next_level_omits)
        if not should_exist:
            to_remove.append(field_name)
    return to_remove",not sparse and len(omit_fields) == 0,63,not sparse,False,0.673794699908547,N/A
"def is_expanded(request, field: str) -> bool:
    """""" Examines request object to return boolean of whether
        passed field is expanded.
    """"""
    expand_value = request.query_params.get(EXPAND_PARAM)
    expand_fields = []
<mask>:
        for f in expand_value.split(','):
            expand_fields.extend([_ for _ in f.split('.')])
    return any((field for field in expand_fields if field in WILDCARD_VALUES)) or field in expand_fields",expand_value,50,expand_value,True,100.00000000000004,N/A
"def is_included(request, field: str) -> bool:
    """""" Examines request object to return boolean of whether
        passed field has been excluded, either because `fields` is
        set, and it is not among them, or because `omit` is set and
        it is among them.
    """"""
    sparse_value = request.query_params.get(FIELDS_PARAM)
    omit_value = request.query_params.get(OMIT_PARAM)
    sparse_fields, omit_fields = ([], [])
<mask>:
        for f in sparse_value.split(','):
            sparse_fields.extend([_ for _ in f.split('.')])
    if omit_value:
        for f in omit_value.split(','):
            omit_fields.extend([_ for _ in f.split('.')])
    if len(sparse_fields) > 0 and field not in sparse_fields:
        return False
    if len(omit_fields) > 0 and field in omit_fields:
        return False
    return True",sparse_value,97,sparse_value,True,100.00000000000004,N/A
"def split_levels(fields):
    """"""
        Convert dot-notation such as ['a', 'a.b', 'a.d', 'c'] into
        current-level fields ['a', 'c'] and next-level fields
        {'a': ['b', 'd']}.
    """"""
    first_level_fields = []
    next_level_fields = {}
<mask>:
        return (first_level_fields, next_level_fields)
    assert isinstance(fields, Iterable), '`fields` must be iterable (e.g. list, tuple, or generator)'
    if isinstance(fields, str):
        fields = [a.strip() for a in fields.split(',') if a.strip()]
    for e in fields:
        if '.' in e:
            first_level, next_level = e.split('.', 1)
            first_level_fields.append(first_level)
            next_level_fields.setdefault(first_level, []).append(next_level)
        else:
            first_level_fields.append(e)
    first_level_fields = list(set(first_level_fields))
    return (first_level_fields, next_level_fields)",not fields,81,not fields,True,100.00000000000004,N/A
"@staticmethod
def _get_expandable_fields(serializer_class: FlexFieldsModelSerializer) -> list:
    expandable_fields = list(getattr(serializer_class.Meta, 'expandable_fields').items())
    expand_list = []
    while expandable_fields:
        key, cls = expandable_fields.pop()
        cls = cls[0] if hasattr(cls, '__iter__') else cls
        expand_list.append(key)
<mask>:
            next_layer = getattr(cls.Meta, 'expandable_fields')
            expandable_fields.extend([(f'{key}.{k}', cls) for k, cls in list(next_layer.items())])
    return expand_list","hasattr(cls, 'Meta') and issubclass(cls, FlexFieldsSerializerMixin) and hasattr(cls.Meta, 'expandable_fields')",42,"getattr(cls.Meta, 'expandable_fields', None)",False,23.853726289570925,N/A
"def get_schema_fields(self, view):
    assert coreapi is not None, 'coreapi must be installed to use `get_schema_fields()`'
    assert coreschema is not None, 'coreschema must be installed to use `get_schema_fields()`'
    serializer_class = view.get_serializer_class()
<mask>:
        return []
    fields = self._get_fields(serializer_class)
    expandable_fields_joined = ','.join(self._get_expandable_fields(serializer_class))
    return [coreapi.Field(name=FIELDS_PARAM, required=False, location='query', schema=coreschema.String(title='Selected fields', description='Specify required fields by comma'), example=(fields or 'field1,field2,nested.field') + ',' + WILDCARD_VALUES_JOINED), coreapi.Field(name=OMIT_PARAM, required=False, location='query', schema=coreschema.String(title='Omitted fields', description='Specify omitted fields by comma'), example=(fields or 'field1,field2,nested.field') + ',' + WILDCARD_VALUES_JOINED), coreapi.Field(name=EXPAND_PARAM, required=False, location='query', schema=coreschema.String(title='Expanded fields', description='Specify expanded fields by comma'), example=(expandable_fields_joined or 'field1,field2,nested.field') + ',' + WILDCARD_VALUES_JOINED)]","not issubclass(serializer_class, FlexFieldsSerializerMixin)",91,serializer_class is None,False,17.86690863748233,N/A
"def get_schema_operation_parameters(self, view):
    serializer_class = view.get_serializer_class()
<mask>:
        return []
    fields = self._get_fields(serializer_class)
    expandable_fields = self._get_expandable_fields(serializer_class)
    expandable_fields.extend(WILDCARD_VALUES)
    parameters = [{'name': FIELDS_PARAM, 'required': False, 'in': 'query', 'description': 'Specify required fields by comma', 'schema': {'title': 'Selected fields', 'type': 'string'}, 'example': (fields or 'field1,field2,nested.field') + ',' + WILDCARD_VALUES_JOINED}, {'name': OMIT_PARAM, 'required': False, 'in': 'query', 'description': 'Specify omitted fields by comma', 'schema': {'title': 'Omitted fields', 'type': 'string'}, 'example': (fields or 'field1,field2,nested.field') + ',' + WILDCARD_VALUES_JOINED}, {'name': EXPAND_PARAM, 'required': False, 'in': 'query', 'description': 'Select fields to expand', 'style': 'form', 'explode': False, 'schema': {'title': 'Expanded fields', 'type': 'array', 'items': {'type': 'string', 'enum': expandable_fields}}}]
    return parameters","not issubclass(serializer_class, FlexFieldsSerializerMixin)",98,not serializer_class,False,18.306026428729766,N/A
"def filter_queryset(self, request: Request, queryset: QuerySet, view: GenericViewSet):
<mask>:
        return queryset
    auto_remove_fields_from_query = getattr(view, 'auto_remove_fields_from_query', True)
    auto_select_related_on_query = getattr(view, 'auto_select_related_on_query', True)
    required_query_fields = list(getattr(view, 'required_query_fields', []))
    serializer = view.get_serializer(context=view.get_serializer_context())
    serializer.apply_flex_fields(serializer.fields, serializer._flex_options_rep_only)
    serializer._flex_fields_rep_applied = True
    model_fields = []
    nested_model_fields = []
    for field in serializer.fields.values():
        model_field = self._get_field(field.source, queryset.model)
        if model_field:
            model_fields.append(model_field)
            if field.field_name in serializer.expanded_fields or (model_field.is_relation and (not model_field.many_to_one)) or (model_field.is_relation and model_field.many_to_one and (not model_field.concrete)):
                nested_model_fields.append(model_field)
    if auto_remove_fields_from_query:
        queryset = queryset.only(*required_query_fields + [model_field.name for model_field in model_fields if not model_field.is_relation or (model_field.many_to_one and model_field.concrete)])
    if auto_select_related_on_query and nested_model_fields:
        queryset = queryset.select_related(*(model_field.name for model_field in nested_model_fields if model_field.is_relation and model_field.many_to_one and model_field.concrete))
        queryset = queryset.prefetch_related(*(model_field.name for model_field in nested_model_fields if model_field.is_relation and (not model_field.many_to_one) or (model_field.is_relation and model_field.many_to_one and (not model_field.concrete))))
    return queryset","not issubclass(view.get_serializer_class(), FlexFieldsSerializerMixin) or request.method != 'GET'",124,not request.user.is_superuser,False,2.4133890647162253,N/A
"def get_serializer_context(self):
    default_context = super(FlexFieldsMixin, self).get_serializer_context()
<mask>:
        default_context['permitted_expands'] = self.permit_list_expands
    return default_context","hasattr(self, 'action') and self.action == 'list'",12,self.permit_list_expands,False,5.57394613616858,N/A
"def __init__(self, query_params=None, method='GET'):
<mask>:
        query_params = {}
    self.query_params = query_params
    self.method = method",query_params is None,14,query_params is None,True,100.00000000000004,N/A
"def __init__(self, query_params=None, method='GET'):
<mask>:
        query_params = MultiValueDict()
    self.query_params = query_params
    self.method = method",query_params is None,14,query_params is None,True,100.00000000000004,N/A
"def get_diet(self, obj):
<mask>:
        return 'homemade lasanga'
    return 'pet food'",obj.name == 'Garfield',10,obj.id == 0,False,22.957488466614336,N/A
"def convert_old_data(model):
    try:
<mask>:
            can_be_verified = True
            if 'need_appops' in model.keys():
                if can_be_verified and model['need_appops']:
                    can_be_verified = False
                model.pop('need_appops')
            if 'feature_affected' in model.keys():
                if can_be_verified and model['feature_affected']:
                    can_be_verified = False
                model.pop('feature_affected')
            if can_be_verified and model['recommended']:
                can_be_verified = False
            model['verified'] = can_be_verified
        if 'reason' in model.keys():
            if not 'overwrite_default' in model['reason'].keys() and len(model['reason'].keys()) > 1:
                model['reason']['overwrite_default'] = False
            if 'default' in model['reason'].keys() and (not 'en' in model['reason'].keys()):
                model['reason']['en'] = model['reason']['default']
                model['reason'].pop('default')
            model['description'] = model['reason']
            model.pop('reason')
        if 'observers' in model.keys():
            for observer in model['observers']:
                if 'userId' in observer.keys():
                    observer.pop('userId')
        return model
    except (RuntimeError, KeyError) as e:
        print(""Error when converting data: '{0}'\n"".format(model['package']))
        raise e",not 'verified' in model.keys(),101,'verified' in model.keys(),False,86.6877899750182,N/A
"def login_github(arg):
<mask>:
        username = arg[:arg.index('+')]
        passwd = arg[arg.index('+') + 1:]
        return Github(username, passwd)
    else:
        return Github(arg)",'+' in arg,17,'+' in arg,True,100.00000000000004,N/A
"def is_issue_need_discussion(issue):
    for label in issue.labels:
<mask>:
            return True
    return False",'need discussion' in label.name,11,label.startswith('#') and label.endswith('#'),False,4.5739135561238005,N/A
"def main():
    description_msg = 'Storage Redirect rules helper (GitHub: {0})'.format(GITHUB_URL)
    version_msg = 'Storage Redirect rules helper {0}\nGitHub: {1}'.format(VERSION, GITHUB_URL)
    opt_parser = OptionParser(usage=USAGE, description=description_msg, version=version_msg)
    opt_parser.add_option('--convert', metavar='ORIGIN_RULES_PATH', help='convert old version configs to latest version.')
    opt_parser.add_option('-2', '--merge', action='store_true', dest='merge', default=False, help='merge converted configs to current (latest) rules')
    opt_parser.add_option('-3', '--make-verified-list', metavar='RULES_PATH', help='make verified apps list from current repo (Use rules path ([git repo]/rules))')
    opt_parser.add_option('-i', '--input', metavar='INPUT_PATH', help='configs input path (--merge: Use converted configs (output) path)')
    opt_parser.add_option('-o', '--output', metavar='OUTPUT', help='configs output path (--merge: Use target rules path ([git repo]/rules))')
    opt_parser.add_option('--merge-verified-list', action='store_true', dest='merge_verified_list', default=False, help='merge output verified apps (when you edited verified_apps.json maually, you will need this.) Only use --make-verfied-list can add this arg.')
    opt_parser.add_option('-1', '--download-from-issues', action='store_true', dest='download_issues', default=False, help='Download rules from open issues (only auto rules supported currently) to a directory')
    opt_parser.add_option('-g', '--login-github', metavar=""'ACCESS_TOKEN' or 'USERNAME+PASSWD'"", help='Login github by access token or password when operations need request github api.')
    opt_parser.add_option('-4', '--close-issues-if-existing', metavar='LOCAL_RULES_PATH', help='Close rules issues if existing. Local repo rules path required.')
    opt_parser.add_option('-5', '--add-ids-for-observers', metavar='LOCAL_RULES_PATH', help='Add ids for observers in rules.')
    options, _ = opt_parser.parse_args()
<mask>:
        convert(options.convert)
    elif options.merge:
        merge(input=options.input, output=options.output)
    elif options.make_verified_list:
        make_verfied_list(options.make_verified_list)
        if options.merge_verified_list:
            merge_verified_list(options.make_verified_list)
    elif options.download_issues:
        if not options.login_github:
            print(""Error: you need to login github with '-g' or '--login-github'. Use '--help' or read README.md to learn more"")
        else:
            download_issues(login_github(options.login_github))
    elif options.close_issues_if_existing:
        if not options.login_github:
            print(""Error: you need to login github with '-g' or '--login-github'. Use '--help' or read README.md to learn more"")
        else:
            close_existing_rules_issues(login_github(options.login_github), options.close_issues_if_existing)
    elif options.add_ids_for_observers:
        add_ids_for_observers(options.add_ids_for_observers)
    else:
        opt_parser.print_help()",options.convert,239,args.download_issues,False,10.682175159905848,N/A
"def convert(input):
    rules_path = input
    rules = list_filter(is_app_rules, list_map(lambda item: rules_path + os.sep + item, os.listdir(rules_path)))
    print('Found rules count: %d' % len(rules))
    output_path = input + os.sep + 'output'
<mask>:
        os.remove(output_path)
    if not os.path.exists(output_path):
        os.mkdir(output_path)
    print('Output to ' + output_path)
    for rule in rules:
        with codecs.open(rule, mode='r', encoding='utf-8') as f:
            model = json.loads(f.read(), object_pairs_hook=OrderedDict)
            data_converter.convert_old_data(model)
            with codecs.open(output_path + os.sep + model['package'] + '.json', mode='w', encoding='utf-8') as out:
                out.write(json.dumps(model, indent=2, ensure_ascii=False))
                out.close()
            f.close()
    print('Finished converting.')",os.path.isfile(output_path),74,os.path.exists(output_path),False,65.80370064762461,N/A
"def merge(input, output):
    filepath_to_package_name = lambda filepath: filepath[filepath.rindex(os.sep) + 1:filepath.rindex('.json')]
    input_rules = list_filter(is_app_rules, list_map(lambda item: input + os.sep + item, os.listdir(input)))
    existing_packs = list_map(filepath_to_package_name, list_filter(is_app_rules, list_map(lambda item: output + os.sep + 'apps' + os.sep + item, os.listdir(output + os.sep + 'apps'))))
    skipped_file = 0
    finished_file = 0
    for input_rule in input_rules:
        package_name = filepath_to_package_name(input_rule)
<mask>:
            skipped_file += 1
        else:
            shutil.copy(input_rule, output + os.sep + 'apps' + os.sep + package_name + '.json')
            finished_file += 1
    print('Finished merging. Skipped %d files. Copied %d files.' % (skipped_file, finished_file))",package_name in existing_packs,85,package_name in existing_packs,True,100.00000000000004,N/A
"def make_verfied_list(path):
    rules = list_filter(is_app_rules, list_map(lambda item: path + os.sep + 'apps' + os.sep + item, os.listdir(path + os.sep + 'apps')))
    verified_apps = []
    for rule in rules:
        with codecs.open(rule, mode='r', encoding='utf-8') as f:
            print(rule)
            model = json.loads(f.read())
<mask>:
                verified_apps.append({'package_name': model['package']})
            f.close()
    print('Found verified apps count: %d' % len(verified_apps))
    with codecs.open(path + os.sep + 'verified_apps.output.json', mode='w', encoding='utf-8') as out:
        print('Output to ' + path + os.sep + 'verified_apps.output.json')
        out.write(json.dumps(verified_apps, indent=2, ensure_ascii=False))
        out.close()
        print('Finished making list.')",'verified' in model.keys() and model['verified'],75,'package' in model,False,2.739887961440494,N/A
"def merge_verified_list(path):
    app_list = []
    with codecs.open(path + os.sep + 'verified_apps.json', mode='r', encoding='utf-8') as f:
        app_list = list_map(lambda item: item['package_name'], json.loads(f.read()))
        f.close()
    print('Origin data count: %d' % len(app_list))
    with codecs.open(path + os.sep + 'verified_apps.output.json', mode='r', encoding='utf-8') as f:
        added_count = 0
        for new_item in list_map(lambda item: item['package_name'], json.loads(f.read())):
<mask>:
                app_list.append(new_item)
                added_count += 1
        f.close()
    print('Added %d items from verified_apps.output.json. Now it will be deleted.' % added_count)
    os.remove(path + os.sep + 'verified_apps.output.json')
    app_list.sort()
    with codecs.open(path + os.sep + 'verified_apps.json', mode='w', encoding='utf-8') as out:
        out.write(json.dumps(list_map(lambda package_name: {'package_name': package_name}, app_list), indent=2, ensure_ascii=False))
        out.close()
    print('Finished merge verified_apps.json')",not new_item in app_list,93,new_item['package_name'] not in app_list,False,28.917849332325716,N/A
"def test_conversion(self) -> None:
    for input_filename, expected_output_filename in (('ansicolor.txt', 'ansicolor.html'), ('ansicolor_eix.txt', 'ansicolor_eix.html')):
        with open(join(_here, input_filename), 'rb') as input:
            test_data = ''.join(read_to_unicode(input))
        with open(join(_here, expected_output_filename), 'rb') as output:
            expected_data = [e.rstrip('\n') for e in read_to_unicode(output)]
        html = Ansi2HTMLConverter().convert(test_data, ensure_trailing_newline=True).split('\n')
<mask>:
            html = html[:-1]
        assert '\n'.join(html) == '\n'.join(expected_data)",html and html[-1] == '',46,html[-1] == '\n',False,58.73949094699213,N/A
"def color_component(x: int) -> int:
    """"""
    Implements the 6x6x6 color cube values of 8bit mode described at
    https://en.wikipedia.org/wiki/ANSI_escape_code#8-bit
    """"""
<mask>:
        return 0
    return 55 + 40 * x",x == 0,28,x < 1,False,19.716118825581447,N/A
"def intensify(color: str, dark_bg: bool, amount: int=64) -> str:
<mask>:
        amount = -amount
    rgb = tuple((max(0, min(255, amount + int(color[i:i + 2], 16))) for i in (1, 3, 5)))
    return '#%.2x%.2x%.2x' % rgb",not dark_bg,33,dark_bg,False,71.65313105737896,N/A
"def get_styles(dark_bg: bool=True, line_wrap: bool=True, scheme: str='ansi2html') -> List[Rule]:
    css = [Rule('.ansi2html-content', white_space=('pre', 'pre-wrap')[line_wrap], word_wrap='break-word', display='inline'), Rule('.body_foreground', color=('#000000', '#AAAAAA')[dark_bg]), Rule('.body_background', background_color=('#AAAAAA', '#000000')[dark_bg]), Rule('.inv_foreground', color=('#000000', '#AAAAAA')[not dark_bg]), Rule('.inv_background', background_color=('#AAAAAA', '#000000')[not dark_bg]), Rule('.ansi1', font_weight='bold'), Rule('.ansi2', font_weight='lighter'), Rule('.ansi3', font_style='italic'), Rule('.ansi4', text_decoration='underline'), Rule('.ansi5', text_decoration='blink'), Rule('.ansi6', text_decoration='blink'), Rule('.ansi8', visibility='hidden'), Rule('.ansi9', text_decoration='line-through')]
    try:
        pal = SCHEME[scheme]
    except KeyError as e:
        raise ValueError(f'Unsupported color scheme {scheme!r}') from e
<mask>:
        raise RuntimeError(f'Color scheme {scheme!r} specifies fewer than 16 colors. 16 colors are required.')
    for _index in range(8):
        css.append(Rule('.ansi3%s' % _index, color=pal[_index]))
        css.append(Rule('.inv3%s' % _index, background_color=pal[_index]))
    for _index in range(8):
        css.append(Rule('.ansi4%s' % _index, background_color=pal[_index]))
        css.append(Rule('.inv4%s' % _index, color=pal[_index]))
    for _index in range(8):
        css.append(Rule('.ansi9%s' % _index, color=pal[_index + 8]))
        css.append(Rule('.inv9%s' % _index, background_color=pal[_index + 8]))
    for _index in range(8):
        css.append(Rule('.ansi10%s' % _index, background_color=pal[_index + 8]))
        css.append(Rule('.inv10%s' % _index, color=pal[_index + 8]))
    for _index in range(len(pal)):
        css.append(Rule('.ansi38-%s' % _index, color=pal[_index]))
        css.append(Rule('.inv38-%s' % _index, background_color=pal[_index]))
    for _index in range(len(pal)):
        css.append(Rule('.ansi48-%s' % _index, background_color=pal[_index]))
        css.append(Rule('.inv48-%s' % _index, color=pal[_index]))
    for green in range(0, 6):
        for red in range(0, 6):
            for blue in range(0, 6):
                css.append(Rule('.ansi38-%s' % index(red, green, blue), color=color(red, green, blue)))
                css.append(Rule('.inv38-%s' % index(red, green, blue), background=color(red, green, blue)))
                css.append(Rule('.ansi48-%s' % index(red, green, blue), background=color(red, green, blue)))
                css.append(Rule('.inv48-%s' % index(red, green, blue), color=color(red, green, blue)))
    for grey in range(0, 24):
        css.append(Rule('.ansi38-%s' % index2(grey), color=level(grey)))
        css.append(Rule('.inv38-%s' % index2(grey), background=level(grey)))
        css.append(Rule('.ansi48-%s' % index2(grey), background=level(grey)))
        css.append(Rule('.inv48-%s' % index2(grey), color=level(grey)))
    css.extend(truecolor_rules)
    return css",len(pal) < 16,226,len(pal) > 16,False,53.7284965911771,N/A
"def add_truecolor_style_rule(is_foreground: bool, ansi_code: int, r: int, g: int, b: int, parameter: str) -> None:
    rule_name = '.ansi{}-{}'.format(ansi_code, parameter)
    color = '#{:02X}{:02X}{:02X}'.format(r, g, b)
<mask>:
        rule = Rule(rule_name, color=color)
    else:
        rule = Rule(rule_name, background_color=color)
    truecolor_rules.append(rule)",is_foreground,35,is_foreground,True,100.00000000000004,N/A
"def adjust(self, ansi_code: int, parameter: Optional[str]=None) -> None:
<mask>:
        self.intensity = ansi_code
    elif ansi_code in (ANSI_STYLE_ITALIC, ANSI_STYLE_NORMAL):
        self.style = ansi_code
    elif ansi_code in (ANSI_BLINK_SLOW, ANSI_BLINK_FAST, ANSI_BLINK_OFF):
        self.blink = ansi_code
    elif ansi_code in (ANSI_UNDERLINE_ON, ANSI_UNDERLINE_OFF):
        self.underline = ansi_code
    elif ansi_code in (ANSI_CROSSED_OUT_ON, ANSI_CROSSED_OUT_OFF):
        self.crossedout = ansi_code
    elif ansi_code in (ANSI_VISIBILITY_ON, ANSI_VISIBILITY_OFF):
        self.visibility = ansi_code
    elif ANSI_FOREGROUND_CUSTOM_MIN <= ansi_code <= ANSI_FOREGROUND_CUSTOM_MAX:
        self.foreground = (ansi_code, None)
    elif ANSI_FOREGROUND_HIGH_INTENSITY_MIN <= ansi_code <= ANSI_FOREGROUND_HIGH_INTENSITY_MAX:
        self.foreground = (ansi_code, None)
    elif ansi_code == ANSI_FOREGROUND:
        self.foreground = (ansi_code, parameter)
    elif ansi_code == ANSI_FOREGROUND_DEFAULT:
        self.foreground = (ansi_code, None)
    elif ANSI_BACKGROUND_CUSTOM_MIN <= ansi_code <= ANSI_BACKGROUND_CUSTOM_MAX:
        self.background = (ansi_code, None)
    elif ANSI_BACKGROUND_HIGH_INTENSITY_MIN <= ansi_code <= ANSI_BACKGROUND_HIGH_INTENSITY_MAX:
        self.background = (ansi_code, None)
    elif ansi_code == ANSI_BACKGROUND:
        self.background = (ansi_code, parameter)
    elif ansi_code == ANSI_BACKGROUND_DEFAULT:
        self.background = (ansi_code, None)
    elif ansi_code in (ANSI_NEGATIVE_ON, ANSI_NEGATIVE_OFF):
        self.negative = ansi_code","ansi_code in (ANSI_INTENSITY_INCREASED, ANSI_INTENSITY_REDUCED, ANSI_INTENSITY_NORMAL)",133,"ansi_code in (ANSI_INTENSITY_OFF, ANSI_INTENSITY_ON)",False,49.786680163326096,N/A
"def adjust_truecolor(self, ansi_code: int, r: int, g: int, b: int) -> None:
    parameter = '{:03d}{:03d}{:03d}'.format(r, g, b)
    is_foreground = ansi_code == ANSI_FOREGROUND
    add_truecolor_style_rule(is_foreground, ansi_code, r, g, b, parameter)
<mask>:
        self.foreground = (ansi_code, parameter)
    else:
        self.background = (ansi_code, parameter)",is_foreground,38,is_foreground,True,100.00000000000004,N/A
"def to_css_classes(self) -> List[str]:
    css_classes: List[str] = []

    def append_unless_default(output: List[str], value: int, default: int) -> None:
<mask>:
            css_class = 'ansi%d' % value
            output.append(css_class)

    def append_color_unless_default(output: List[str], color: Tuple[int, Optional[str]], default: int, negative: bool, neg_css_class: str) -> None:
        value, parameter = color
        if value != default:
            prefix = 'inv' if negative else 'ansi'
            css_class_index = str(value) if parameter is None else '%d-%s' % (value, parameter)
            output.append(prefix + css_class_index)
        elif negative:
            output.append(neg_css_class)
    append_unless_default(css_classes, self.intensity, ANSI_INTENSITY_NORMAL)
    append_unless_default(css_classes, self.style, ANSI_STYLE_NORMAL)
    append_unless_default(css_classes, self.blink, ANSI_BLINK_OFF)
    append_unless_default(css_classes, self.underline, ANSI_UNDERLINE_OFF)
    append_unless_default(css_classes, self.crossedout, ANSI_CROSSED_OUT_OFF)
    append_unless_default(css_classes, self.visibility, ANSI_VISIBILITY_ON)
    flip_fore_and_background = self.negative == ANSI_NEGATIVE_ON
    append_color_unless_default(css_classes, self.foreground, ANSI_FOREGROUND_DEFAULT, flip_fore_and_background, 'inv_background')
    append_color_unless_default(css_classes, self.background, ANSI_BACKGROUND_DEFAULT, flip_fore_and_background, 'inv_foreground')
    return css_classes",value != default,106,value != default,True,100.00000000000004,N/A
"def _needs_extra_newline(text: str) -> bool:
<mask>:
        return False
    return True",not text or text.endswith('\n'),10,text.endswith('\n'),False,68.72892787909726,N/A
"def __init__(self, latex: bool=False, inline: bool=False, dark_bg: bool=True, line_wrap: bool=True, font_size: str='normal', linkify: bool=False, escaped: bool=True, markup_lines: bool=False, output_encoding: str='utf-8', scheme: str='ansi2html', title: str='') -> None:
    self.latex = latex
    self.inline = inline
    self.dark_bg = dark_bg
    self.line_wrap = line_wrap
    self.font_size = font_size
    self.linkify = linkify
    self.escaped = escaped
    self.markup_lines = markup_lines
    self.output_encoding = output_encoding
    self.scheme = scheme
    self.title = title
    self._attrs: Attributes
    self.hyperref = False
<mask>:
        self.styles = dict([(item.klass.strip('.'), item) for item in get_styles(self.dark_bg, self.line_wrap, self.scheme)])
    self.vt100_box_codes_prog = re.compile('\x1b\\(([B0])')
    self.ansi_codes_prog = re.compile('\x1b\\[([\\d;:]*)([a-zA-z])')
    self.url_matcher = re.compile(""(((((https?|ftps?|gopher|telnet|nntp)://)|(mailto:|news:))(%[0-9A-Fa-f]{2}|[-()_.!~*\\';/?#:@&=+$,A-Za-z0-9])+)([).!\\';/?:,][\\s])?)"")
    self.osc_link_re = re.compile('\x1b\\]8;;(.*?)\x07(.*?)\x1b\\]8;;\x07')",inline,87,self.inline,False,27.516060407455225,N/A
"def __init__(self, window=None, element=None, subscription=None):
    """"""Attach event handler to dom::element or sciter::window.""""""
    super().__init__()
    self.subscription = subscription if subscription is not None else EventHandler.DEFAULT_EVENTS
    self.element = None
    self._attached_to_window = None
    self._attached_to_element = None
    self._dispatcher = dict()
    self._executor = None
    self.set_dispatch_options()
<mask>:
        self.attach(window, element, subscription)",window or element,43,window is not None,False,15.97357760615681,N/A
"def attach(self, window=None, element=None, subscription=None):
    """"""Attach event handler to dom::element or sciter::window.""""""
    assert window or element
    self.subscription = subscription if subscription is not None else EventHandler.DEFAULT_EVENTS
    self._event_handler_proc = sciter.capi.scdef.ElementEventProc(self._element_proc)
    tag = id(self)
<mask>:
        self._attached_to_window = window
        ok = _api.SciterWindowAttachEventHandler(window, self._event_handler_proc, tag, self.subscription)
        if ok != SCDOM_RESULT.SCDOM_OK:
            raise sciter.SciterError('Could not attach to window')
    elif element:
        self._attached_to_element = element
        ok = _api.SciterAttachEventHandler(element, self._event_handler_proc, tag)
        if ok != SCDOM_RESULT.SCDOM_OK:
            raise sciter.SciterError('Could not attach to element')
    pass",window,73,window,True,100.00000000000004,N/A
"def detach(self):
    """"""Detach event handler from dom::element or sciter::window.""""""
    tag = id(self)
<mask>:
        self._executor.shutdown()
        self._executor = None
    if self._attached_to_window:
        ok = _api.SciterWindowDetachEventHandler(self._attached_to_window, self._event_handler_proc, tag)
        if ok != SCDOM_RESULT.SCDOM_OK:
            raise sciter.SciterError('Could not detach from window')
        self._attached_to_window = None
    elif self._attached_to_element:
        ok = _api.SciterDetachEventHandler(self._attached_to_element, self._event_handler_proc, tag)
        if ok != SCDOM_RESULT.SCDOM_OK:
            raise sciter.SciterError('Could not attach from element')
        self._attached_to_element = None
    pass",self._executor is not None,58,self._executor,False,47.23665527410149,N/A
"def _dispatcher_update(self, force=False):
<mask>:
        return
    if not force and (not self._dispatcher['runtime']):
        return
    required = self._dispatcher['require']
    handlers = {}
    for name in dir(self):
        member = getattr(self, name, None)
        attr = getattr(member, '_from_sciter', False)
        fnname = attr if isinstance(attr, str) else name
        if attr or not required:
            handlers[fnname] = member
    self._dispatcher['handlers'] = handlers
    pass",not self._dispatcher['enabled'],52,not self._dispatcher,False,54.88116360940266,N/A
"def _on_script_call(self, f):
    self._dispatcher_update()
    fname = f.name.decode('utf-8')
    fn = self._dispatcher['handlers'].get(fname)
    call_raw = self._dispatcher['static']
    rv = None
    value_args = None
<mask>:
        try:
            value_args = [sciter.Value(f.argv[i]) for i in range(f.argc)]
            rv = self.on_script_call(fname, value_args)
        except Exception as e:
            rv = self.script_exception_handler(fname, e)
    if rv is None and fn:
        cfg = getattr(fn, '_sciter_cfg', {})
        skip_exception = not cfg.get('safe', True)
        if cfg.get('threading') and cfg.get('promise'):
            raise sciter.SciterError(""Don't mix `threading` and `promise` in @script"")
        try:
            if cfg.get('convert'):
                args = sciter.Value.unpack_from(f.argv, f.argc)
            else:
                args = [sciter.Value(f.argv[i]) for i in range(f.argc)]
            if cfg.get('threading'):

                def on_thread_done(fut):
                    if fut.cancelled():
                        return
                    exc = fut.exception()
                    if exc is None:
                        fut.result()
                    else:
                        self.script_exception_handler(fname, exc)
                    pass
                if self._executor is None:
                    from concurrent.futures import ThreadPoolExecutor
                    self._executor = ThreadPoolExecutor()
                fut = self._executor.submit(fn, *args)
                fut.add_done_callback(on_thread_done)
                return True
            elif cfg.get('promise'):
                jsargs = args[0]
                jsresolve = args[1]
                jsreject = args[2]

                def on_task_done(fut):
                    if fut.cancelled():
                        return
                    exc = fut.exception()
                    if exc is None:
                        rv = fut.result()
                        jsresolve(rv)
                    else:
                        exc = self.script_exception_handler(fname, exc)
                        if skip_exception:
                            jsresolve(str(exc))
                        else:
                            jsreject(str(exc))
                    pass
                if self._executor is None:
                    from concurrent.futures import ThreadPoolExecutor
                    self._executor = ThreadPoolExecutor()
                fut = self._executor.submit(fn, *jsargs)
                fut.add_done_callback(on_task_done)
                return True
            rv = fn(*args)
        except Exception as e:
            exc = self.script_exception_handler(fname, e)
            rv = str(exc) if skip_exception else exc
    if not fn and call_raw == True:
        try:
            value_args = [sciter.Value(f.argv[i]) for i in range(f.argc)]
            rv = self.on_script_call(fname, value_args)
        except Exception as e:
            rv = self.script_exception_handler(fname, e)
    if fn or rv is not None:
        sciter.Value.pack_to(f.result, rv)
        return True
    return False",call_raw == 'always',234,fn is None,False,0.0,N/A
"def __init__(self, ismain=False, ispopup=False, ischild=False, resizeable=True, parent=None, uni_theme=False, debug=True, pos=None, size=None, subscription=None):
    """"""Create a new window and setup the sciter and dom callbacks.""""""
    super().__init__()
    from sciter.capi.scdef import SCITER_CREATE_WINDOW_FLAGS
    flags = SCITER_CREATE_WINDOW_FLAGS.SW_CONTROLS
<mask>:
        flags = flags | SCITER_CREATE_WINDOW_FLAGS.SW_RESIZEABLE
    if ismain:
        flags = flags | SCITER_CREATE_WINDOW_FLAGS.SW_MAIN | SCITER_CREATE_WINDOW_FLAGS.SW_TITLEBAR
    elif ispopup:
        flags = flags | SCITER_CREATE_WINDOW_FLAGS.SW_POPUP
    elif ischild:
        flags = flags | SCITER_CREATE_WINDOW_FLAGS.SW_CHILD
    if uni_theme:
        _api.SciterSetOption(None, sciter.capi.scdef.SCITER_RT_OPTIONS.SCITER_SET_UX_THEMING, True)
    if debug:
        flags = flags | SCITER_CREATE_WINDOW_FLAGS.SW_ENABLE_DEBUG
    self.setup_debug(debug_windows=debug, debug_output=True)
    self.window_flags = flags
    self._title_changed = False
    rect = sciter.capi.sctypes.RECT()
    if pos is not None:
        rect.left = pos[0]
        rect.top = pos[1]
        if size is None:
            raise ValueError('`size` is required if `pos` is provided!')
    if size is not None:
        rect.right = rect.left + size[0]
        rect.bottom = rect.top + size[1]
    if not pos and (not size):
        rect = None
    self.hwnd = self._create(flags, rect=rect, parent=parent)
    if not self.hwnd:
        raise sciter.SciterError('Could not create window')
    self.setup_callback(self.hwnd)
    self.attach(window=self.hwnd, subscription=subscription)
    pass",resizeable,147,resizeable,True,100.00000000000004,N/A
"def run_app(self, show=True):
    """"""Show window and run the main app message loop until window been closed.""""""
<mask>:
        self.expand()
    ret = super().run_app()
    return ret",show,23,show,True,100.00000000000004,N/A
"def _document_ready(self, target):
<mask>:
        return
    root = sciter.Element(target)
    title = root.find_first('html > head > title')
    if title:
        self.set_title(title.get_text())
    pass",self._title_changed,19,not target,False,0.0,N/A
"def setup_callback(self, hwnd):
    """"""Set callback for sciter engine events.""""""
<mask>:
        raise ValueError('Invalid window handle provided.')
    self.hwnd = hwnd
    self.root = self.get_root()
    self._sciter_handler_proc = SciterHostCallback(self.handle_notification)
    _api.SciterSetCallback(hwnd, self._sciter_handler_proc, ctypes.c_void_p(0))
    pass",not hwnd,28,hwnd is None or hwnd == 0,False,5.522397783539471,N/A
"def setup_debug(self, hwnd=None, debug_windows=True, debug_output=True):
    """"""Setup debug output function for specific window or globally.""""""
    ok = _api.SciterSetOption(hwnd, SCITER_RT_OPTIONS.SCITER_SET_DEBUG_MODE, debug_windows)
<mask>:
        raise sciter.SciterError('Could not set debug mode')
    self._sciter_debug_proc = DEBUG_OUTPUT_PROC(self.on_debug_output)
    if debug_output:
        _api.SciterSetupDebugOutput(hwnd, None, self._sciter_debug_proc)
    else:
        _api.SciterSetupDebugOutput(hwnd, None, DEBUG_OUTPUT_PROC(0))
    pass",not ok,39,not ok,True,100.00000000000004,N/A
"def set_option(self, option, value):
    """"""Set various sciter engine options, see the SCITER_RT_OPTIONS.""""""
    hwnd = self.hwnd
<mask>:
        hwnd = None
    ok = _api.SciterSetOption(hwnd, option, value)
    if not ok:
        raise sciter.SciterError('Could not set option ' + str(option) + '=' + str(value))
    return self","option in (SCITER_RT_OPTIONS.SCITER_SET_GPU_BLACKLIST, SCITER_RT_OPTIONS.SCITER_SET_GFX_LAYER, SCITER_RT_OPTIONS.SCITER_SET_UX_THEMING)",41,hwnd is None,False,0.0,N/A
"def set_home_url(self, url: str):
    """"""Set sciter window home url.""""""
    ok = _api.SciterSetHomeURL(self.hwnd, url)
<mask>:
        raise sciter.SciterError('Could not set home url ' + str(url))
    return self",not ok,25,not ok,True,100.00000000000004,N/A
"def set_media_type(self, media_type: str):
    """"""Set media type of this sciter instance.""""""
    ok = _api.SciterSetMediaType(self.hwnd, media_type)
<mask>:
        raise sciter.SciterError('Could not set media type ' + str(media_type))
    return self",not ok,27,not ok,True,100.00000000000004,N/A
"def set_option(option, value):
    """"""Set various sciter engine global options, see the SCITER_RT_OPTIONS.""""""
    ok = api.SciterSetOption(None, option, value)
<mask>:
        raise SciterError('Could not set option ' + str(option) + '=' + str(value))
    return True",not ok,32,not ok,True,100.00000000000004,N/A
"def runtime_features(file_io=True, socket_io=True, allow_eval=True, allow_sysinfo=True):
    """"""Set runtime features that have been disabled by default since 4.2.5.0""""""
    from .capi.scdef import SCRIPT_RUNTIME_FEATURES
    flags = 0
<mask>:
        flags += SCRIPT_RUNTIME_FEATURES.ALLOW_FILE_IO
    if socket_io:
        flags += SCRIPT_RUNTIME_FEATURES.ALLOW_SOCKET_IO
    if allow_eval:
        flags += SCRIPT_RUNTIME_FEATURES.ALLOW_EVAL
    if allow_sysinfo:
        flags += SCRIPT_RUNTIME_FEATURES.ALLOW_SYSINFO
    return set_option(SCITER_RT_OPTIONS.SCITER_SET_SCRIPT_RUNTIME_FEATURES, flags)",file_io,45,file_io,True,100.00000000000004,N/A
"def script(name=None, convert=True, safe=True, threading=False, promise=False):
    """"""Annotation decorator for the functions that called from script.""""""
<mask>:
        raise SciterError(""Don't mix `threading` and `promise` in @script"")

    def decorator(func):
        attr = True if name is None else name
        func._from_sciter = attr
        func._sciter_cfg = dict(name=name, convert=convert, safe=safe, threading=threading, promise=promise)
        return func
    if name is None or isinstance(name, str):
        return decorator
    func = name
    name = None
    return decorator(func)",threading and promise,64,threading and promise,True,100.00000000000004,N/A
"def async_script(name=None, convert=True, safe=True):
    """"""Annotation decorator for async functions that called from script.""""""

    def decorator(func):
        attr = True if name is None else name
        func._from_sciter = attr
        func._sciter_cfg = dict(name=name, convert=convert, safe=safe, threading=False, promise=True)
        return func
<mask>:
        return decorator
    func = name
    name = None
    return decorator(func)","name is None or isinstance(name, str)",47,func._from_sciter,False,0.0,N/A
"def _create(self, flags, rect, parent):
<mask>:
        ctypes.windll.ole32.OleInitialize(None)
        WindowsWindow._initialized = True
    if rect is None:
        rect = sciter.capi.sctypes.RECT()
    self._msg_delegate = sciter.capi.scdef.SciterWindowDelegate(self._on_msg_delegate)
    return _api.SciterCreateWindow(flags, ctypes.byref(rect), self._msg_delegate, None, parent)",not WindowsWindow._initialized,26,not WindowsWindow._initialized,True,100.00000000000004,N/A
"def _on_msg_delegate(self, hwnd, msg, wparam, lparam, pparam, phandled):
    rv = self.on_message(hwnd, msg, wparam, lparam)
<mask>:
        phandled.contents = 1
        return rv
    return 0",rv is not None,22,rv,False,4.9787068367863965,N/A
"def _window(self, hwnd=None):
<mask>:
        hwnd = self.hwnd
    wnd = self.objc(hwnd, 'window')
    return wnd",hwnd is None,13,hwnd is None,True,100.00000000000004,N/A
"def collapse(self, hide=False):
    """"""Minimize or hide window.""""""
    wnd = self._window()
<mask>:
        self.objc(wnd, 'orderOut:', None)
    else:
        self.objc(wnd, 'performMiniaturize:', self.hwnd)
    return self",hide,20,hide,True,100.00000000000004,N/A
"def expand(self, maximize=False):
    """"""Show or maximize window.""""""
    wnd = self._window()
<mask>:
        self.objc(self.nsApp, 'activateIgnoringOtherApps:', True)
    self.objc(wnd, 'makeKeyAndOrderFront:', None)
    if maximize:
        self.objc(wnd, 'performZoom:', None)
    return self",self.window_flags & sciter.capi.scdef.SCITER_CREATE_WINDOW_FLAGS.SW_TITLEBAR,24,self.nsApp,False,0.07003573710847102,N/A
"def __init__(self, hv_code, script=None):
    """""".""""""
    msg = 'Incompatible type' if hv_code == 2 else 'Bad parameter'
<mask>:
        msg = msg + ' at ' + script
    super().__init__(msg)",script,27,script,True,100.00000000000004,N/A
"@classmethod
def parse(cls, json: str, how=VALUE_STRING_CVT_TYPE.CVT_JSON_LITERAL, throw=True):
    """"""Parse json string into value.""""""
    rv = value()
    ok = _api.ValueFromString(rv, json, len(json), how)
<mask>:
        raise sciter.value.ValueError(VALUE_RESULT.HV_BAD_PARAMETER, 'value.parse')
    return rv",ok != 0 and throw,27,not ok,False,6.7667641618306344,N/A
"def __init__(self, val=None):
    """"""Return a new sciter value wrapped object.""""""
    super().__init__()
    self.data = SCITER_VALUE()
    self.ptr = ctypes.pointer(self.data)
    self._as_parameter_ = self.ptr
    _api.ValueInit(self.ptr)
<mask>:
        self.set_value(val)
    pass",val is not None,24,val is not None,True,100.00000000000004,N/A
"def __repr__(self):
    """"""Machine-like value visualization.""""""
    t = VALUE_TYPE(self.data.t)
    tname = _value_type_names.get(self.data.t, hex(self.data.t))
<mask>:
        return '<%s>' % tname
    if self.data.u != 0:
        subtypes = _value_subtypes.get(t)
        if subtypes:
            tname = tname + ':' + subtypes.get(self.data.u, hex(self.data.u))
    return '<%s: %s>' % (tname, str(self))","t in (VALUE_TYPE.T_UNDEFINED, VALUE_TYPE.T_NULL)",40,tname,False,0.0,N/A
"def __bytes__(self):
    """"""Value to bytes conversion.""""""
<mask>:
        raise TypeError(repr(self))
    p = ctypes.c_char_p()
    n = ctypes.c_uint32()
    ok = _api.ValueBinaryData(self, byref(p), byref(n))
    self._throw_if(ok)
    return p.value",not self.is_bytes(),23,"not isinstance(self, int)",False,8.051153633013374,N/A
"def __init__(self, code, context=None):
    """""".""""""
    name = SCDOM_RESULT(code).name
    msg = name.replace('SCDOM_', '').replace('_', ' ').lower()
<mask>:
        msg = msg + ' at ' + context
    super().__init__(msg)",context is not None,25,context,False,4.9787068367863965,N/A
"@classmethod
def create(cls, text, kind=NODE_TYPE.NT_TEXT):
    """"""Make text or comment node with specified text.""""""
<mask>:
        text = ''
    rv = HNODE()
    if kind == NODE_TYPE.NT_TEXT:
        ok = _api.SciterCreateTextNode(text, len(text), ctypes.byref(rv))
    elif kind == NODE_TYPE.NT_COMMENT:
        ok = _api.SciterCreateCommentNode(text, len(text), ctypes.byref(rv))
    Node._throw_if(ok)
    return Node(rv)",text is None,41,text is None,True,100.00000000000004,N/A
"def __init__(self, node=None):
    """"""Construct Node object from HNODE or HELEMENT.""""""
    super().__init__()
    self.h = None
<mask>:
        if isinstance(node, (HNODE, HELEMENT)):
            self._use(node)
        elif isinstance(node, (Node, Element)):
            self._use(node.h)
        else:
            raise TypeError('Unknown type of Node argument')
    pass",node is not None,33,node is not None,True,100.00000000000004,N/A
"def __eq__(self, other):
    """"""Test equality with another HNODE or Node object.""""""
<mask>:
        return self.h == other
    elif isinstance(other, Node):
        return self.h == other.h
    else:
        return NotImplemented
    pass","isinstance(other, HNODE)",27,"isinstance(other, HNODE)",True,100.00000000000004,N/A
"def __getitem__(self, key):
    """"""Get node child at specified index.""""""
<mask>:
        raise TypeError
    count = len(self)
    key = count + key if key < 0 else key
    if key < 0 or key >= count:
        raise IndexError
    p = HNODE()
    ok = _api.SciterNodeNthChild(self, key, ctypes.byref(p))
    self._throw_if(ok)
    return Node(p)","not isinstance(key, int)",47,"not isinstance(key, int)",True,100.00000000000004,N/A
"def utf16tostr(addr, size=-1):
    """"""Read UTF-16 string from memory and encode as python string.""""""
<mask>:
        return None
    cb = size if size > 0 else 32
    bstr = ctypes.string_at(addr, cb)
    if size >= 0:
        return bstr.decode('utf-16le')
    chunks = []
    while True:
        found = cb
        for i in range(0, cb, 2):
            c = bstr[i]
            if c == 0:
                found = i
                break
            pass
        assert found % 2 == 0, 'truncated string with len ' + str(found)
        chunks.append(bstr[0:found].decode('utf-16le'))
        if found != cb:
            break
        addr = addr + cb
        bstr = ctypes.string_at(addr, cb)
        continue
    return ''.join(chunks)",addr is None,92,addr == 0,False,15.97357760615681,N/A
"@classmethod
def from_param(cls, obj):
<mask>:
        obj = obj.encode('utf-16le') + b'\x00'
    return super(c_utf16_p, cls).from_param(obj)","isinstance(obj, str)",13,"isinstance(obj, bytes)",False,53.7284965911771,N/A
"def __init__(self, string_type: str):
    """"""Construct callback by one of 'char', 'wchar' or 'byte' string type.""""""
    self.text = None
<mask>:
        self.cb = LPCSTR_RECEIVER(self._a2s)
    elif string_type == 'byte':
        self.cb = LPCBYTE_RECEIVER(self._b2s)
    elif string_type == 'wchar':
        self.cb = LPCWSTR_RECEIVER(self._w2s)
    else:
        raise ValueError(""Unknown callback type. Use one of 'char', 'byte' or 'wchar'."")
    self._as_parameter_ = self.cb
    pass",string_type == 'char',52,string_type == 'char',True,100.00000000000004,N/A
"def SciterAPI():
    """"""Bind Sciter API.""""""
<mask>:
        return SciterAPI._api
    import sys
    import ctypes
    scdll = None
    errors = []
    if SCITER_WIN:
        import ctypes.util
        try:
            dll = ctypes.util.find_library(SCITER_DLL_NAME)
            if not dll:
                dll = SCITER_DLL_NAME
            scdll = ctypes.WinDLL(dll)
        except OSError as e:
            errors.append(""'%s': %s"" % (dll, str(e)))
            try:
                dllname = 'sciter64.dll' if sys.maxsize > 2 ** 32 else 'sciter32.dll'
                dll = ctypes.util.find_library(dllname)
                if not dll:
                    dll = dllname
                scdll = ctypes.WinDLL(dll)
            except OSError as e:
                errors.append(""'%s': %s"" % (dll, str(e)))
    else:

        def find_sciter(dllname):
            import ctypes.util
            dllfile = dllname + SCITER_DLL_EXT
            dllpath = ctypes.util.find_library(dllname)
            if not dllpath:

                def find_in_path(dllname, envname):
                    import os
                    if envname in os.environ:
                        for directory in os.environ[envname].split(os.pathsep):
                            fname = os.path.join(directory, dllname)
                            if os.path.isfile(fname):
                                return fname
                    return None
                dllpath = find_in_path(dllfile, 'DYLD_LIBRARY_PATH' if SCITER_OSX else 'LD_LIBRARY_PATH')
                if not dllpath:
                    dllpath = find_in_path(dllfile, 'PATH')
            if not dllpath:
                dllpath = dllfile
            try:
                RTLD_LAZY = 1
                return ctypes.CDLL(dllpath, ctypes.RTLD_LOCAL | RTLD_LAZY)
            except OSError as e:
                errors.append(str(e))
                return None
        scdll = find_sciter(SCITER_DLL_NAME)
        if SCITER_LNX and scdll is None:
            import sys
            scdll = find_sciter('libsciter-gtk-64' if sys.maxsize > 2 ** 32 else 'libsciter-gtk-32')
    if not scdll:
        raise ImportError(SCITER_LOAD_ERROR + '\n' + '\n'.join(errors))
    scdll.SciterAPI.restype = POINTER(ISciterAPI)
    SciterAPI._api = scdll.SciterAPI().contents
    return SciterAPI._api","hasattr(SciterAPI, '_api')",192,SciterAPI._api is not None,False,6.770186228657864,N/A
"def test_09bool(self):
    items = [None, False, True, 0, 1, 0.0, 1.0, u'', u'3', (), (1, 2), [], [3, 4], {}, {'5': 5, '6': 6}]
    for item in items:
        with self.subTest(val=item):
            xval = value(item)
<mask>:
                self.assertTrue(xval)
            else:
                self.assertFalse(xval)
    pass",item,38,xval is True,False,0.0,N/A
"def test_13parse(self):
    items = ['', 'null', '1', '""2""', '2.0', 'true', '[3, 4]', '{""5"": 5, ""6"": 6, seven: ""seven""}']
    for item in items:
        with self.subTest(val=item):
            xval = value.parse(item)
<mask>:
                self.assertTrue(xval)
            else:
                self.assertFalse(xval)
    with self.assertRaises(sciter.value.ValueError):
        item = '{item: '
        xval = value.parse(item)
    pass",xval,41,item == 'true',False,0.0,N/A
"def test_19explicit(self):
    xval = value.null()
    self.assertTrue(xval.is_null())
    self.assertFalse(xval)
    xval = value.symbol('hello')
    self.assertTrue(xval.is_symbol())
    self.assertTrue(xval.is_string())
    self.assertEqual(xval.get_value(), 'hello')
    xval = value.secure_string('secure')
    self.assertTrue(xval.is_string())
    self.assertEqual(xval.get_type(with_unit=True), (VALUE_TYPE.T_STRING, 2))
    self.assertEqual(xval.get_value(), 'secure')
    xval = value(TypeError('error'))
    self.assertTrue(xval.is_error_string())
    self.assertEqual(xval.get_value(), 'error')
<mask>:
        xval = value.color(65535)
        self.assertTrue(xval.is_color())
        self.assertEqual(xval.get_value(), 65535)
        xval = value.duration(12.5)
        self.assertTrue(xval.is_duration())
        self.assertEqual(xval.get_value(), 12.5)
        xval = value.angle(1.0)
        self.assertTrue(xval.is_angle())
        self.assertEqual(xval.get_value(), 1.0)
        pass
    pass",sciter.version_num() > 67109120,50,"hasattr(value, 'color')",False,5.854497694024015,N/A
"def on_event(self, source, target, code, phase, reason):
    he = sciter.Element(source)
<mask>:
        print('native button clicked!')
        return True
    pass",code == sciter.event.BEHAVIOR_EVENTS.BUTTON_CLICK and phase == sciter.event.PHASE_MASK.SINKING and he.test('#native'),17,he.is_mouse_clicked(),False,0.6247933859065515,N/A
"def on_sciter_callback(pld, param):
    """"""Sciter notifications callback.""""""
    ld = pld.contents
<mask>:
        return on_load_data(cast(pld, POINTER(SCN_LOAD_DATA)).contents)
    elif ld.code == SciterNotification.SC_ATTACH_BEHAVIOR:
        return on_create_behavior(cast(pld, POINTER(SCN_ATTACH_BEHAVIOR)).contents)
    return 0",ld.code == SciterNotification.SC_LOAD_DATA,22,ld.code == SciterNotification.SC_LOAD_DATA,True,100.00000000000004,N/A
"def on_wnd_message(hWnd, Msg, wParam, lParam):
    """"""WindowProc Function.""""""
    handled = BOOL(0)
    lr = sapi.SciterProcND(hWnd, Msg, wParam, lParam, byref(handled))
<mask>:
        return lr
    if Msg == WM_DESTROY:
        windll.user32.PostQuitMessage(0)
        return 0
    try:
        return windll.user32.DefWindowProcW(hWnd, Msg, wParam, lParam)
    except:
        import traceback
        etype, evalue, estack = sys.exc_info()
        print('WndProc exception: %X, 0x%04X, 0x%X, 0x%X' % (hWnd, Msg, wParam, lParam))
        traceback.print_exception(etype, evalue, estack)
    return 0",handled,57,not handled,False,49.99999999999999,N/A
"def main():
    clsname = sapi.SciterClassName()
    sciter.runtime_features(allow_sysinfo=True)
    title = u'Win32 Sciter'
    clsname = u'PySciter'
    windll.user32.DefWindowProcW.argtypes = [HWND, c_uint, WPARAM, LPARAM]
    windll.user32.DefWindowProcW.restype = LRESULT
    WndProc = WNDPROCTYPE(on_wnd_message)
    wndClass = WNDCLASSEX()
    wndClass.cbSize = sizeof(WNDCLASSEX)
    wndClass.style = CS_HREDRAW | CS_VREDRAW
    wndClass.lpfnWndProc = WndProc
    wndClass.cbClsExtra = 0
    wndClass.cbWndExtra = 0
    wndClass.hInstance = windll.kernel32.GetModuleHandleW(0)
    wndClass.hIcon = 0
    wndClass.hCursor = windll.user32.LoadCursorW(0, IDC_ARROW)
    wndClass.hBrush = windll.gdi32.GetStockObject(WHITE_BRUSH)
    wndClass.lpszMenuName = 0
    wndClass.lpszClassName = clsname
    wndClass.hIconSm = 0
<mask>:
        err = windll.kernel32.GetLastError()
        print('Failed to register window: ', err)
        exit(0)
    hWnd = windll.user32.CreateWindowExW(0, clsname, title, WS_OVERLAPPEDWINDOW, CW_USEDEFAULT, CW_USEDEFAULT, 800, 600, 0, 0, 0, 0)
    if not hWnd:
        err = windll.kernel32.GetLastError()
        print('Failed to create window: ', err)
        exit(0)
    scproc = SciterHostCallback(on_sciter_callback)
    sapi.SciterSetCallback(hWnd, scproc, None)
    url = u'examples/minimal.htm'
    sapi.SciterLoadFile(hWnd, url)
    windll.user32.ShowWindow(hWnd, SW_SHOW)
    windll.user32.UpdateWindow(hWnd)
    msg = MSG()
    lpmsg = pointer(msg)
    print('Entering message loop')
    while windll.user32.GetMessageW(lpmsg, 0, 0, 0) != 0:
        windll.user32.TranslateMessage(lpmsg)
        windll.user32.DispatchMessageW(lpmsg)
    print('Quit.')",not windll.user32.RegisterClassExW(byref(wndClass)),138,not wndClass.registerWindowW(0),False,4.807478402503058,N/A
"def find_version(*file_paths):
    version_file = read(*file_paths)
    version_match = re.search('^_version_ = [\'\\""]([^\'\\""]*)[\'\\""]', version_file, re.M)
<mask>:
        return version_match.group(1)
    raise RuntimeError('Unable to find version string.')",version_match,21,version_match,True,100.00000000000004,N/A
"def __init__(self, obj, styler_obj: Optional[Styler]=None, columns: Optional[List[str]]=None):
    from_another_styleframe = False
    from_pandas_dataframe = False
<mask>:
        raise TypeError('styler_obj must be {}, got {} instead.'.format(Styler.__name__, type(styler_obj).__name__))
    if isinstance(obj, (pd.DataFrame, np.ndarray)):
        from_pandas_dataframe = True
        if isinstance(obj, np.ndarray):
            obj = pd.DataFrame(obj)
            if columns:
                obj = obj.rename(columns=dict(zip(obj.columns, columns)))
        if obj.empty:
            self.data_df = deepcopy(obj)
        else:
            self.data_df = obj.map(lambda x: Container(x, deepcopy(styler_obj)) if not isinstance(x, Container) else x)
    elif isinstance(obj, pd.Series):
        self.data_df = obj.apply(lambda x: Container(x, deepcopy(styler_obj)) if not isinstance(x, Container) else x)
    elif isinstance(obj, (dict, list)):
        self.data_df = pd.DataFrame(obj).map(lambda x: Container(x, deepcopy(styler_obj)) if not isinstance(x, Container) else x)
    elif isinstance(obj, StyleFrame):
        self.data_df = deepcopy(obj.data_df)
        from_another_styleframe = True
    else:
        raise TypeError(""{} __init__ doesn't support {}"".format(type(self).__name__, type(obj).__name__))
    self.data_df.columns = [Container(col, deepcopy(styler_obj)) if not isinstance(col, Container) else deepcopy(col) for col in self.data_df.columns]
    self.data_df.index = [Container(index, deepcopy(styler_obj)) if not isinstance(index, Container) else deepcopy(index) for index in self.data_df.index]
    if from_pandas_dataframe:
        self.data_df.index.name = obj.index.name
    self._columns_width = obj._columns_width if from_another_styleframe else OrderedDict()
    self._rows_height = obj._rows_height if from_another_styleframe else OrderedDict()
    self._has_custom_headers_style = obj._has_custom_headers_style if from_another_styleframe else False
    self._cond_formatting: List[ColorScaleConditionalFormatRule] = []
    self._default_style = styler_obj or Styler()
    self._index_header_style = obj._index_header_style if from_another_styleframe else self._default_style
    self._known_attrs = {'at': self.data_df.at, 'loc': self.data_df.loc, 'iloc': self.data_df.iloc, 'groupby': self.data_df.groupby, 'index': self.data_df.index, 'fillna': self.data_df.fillna, 'applymap': self.data_df.map, 'map': self.data_df.map}","styler_obj and (not isinstance(styler_obj, Styler))",196,"not isinstance(styler_obj,Styler)",False,51.341711903259224,N/A
"def __getitem__(self, item):
<mask>:
        return self.data_df.__getitem__(item).index
    if isinstance(item, list):
        return StyleFrame(self.data_df.__getitem__(item))
    return Series(self.data_df.__getitem__(item))","isinstance(item, pd.Series)",13,"isinstance(item, int)",False,38.49815007763549,N/A
"def __setitem__(self, key, value):
<mask>:
        self.data_df.__setitem__(Container(key), list(map(Container, value)))
    else:
        self.data_df.__setitem__(Container(key), Container(value))","isinstance(value, (Iterable, pd.Series))",11,"isinstance(value, list)",False,19.765609300943975,N/A
"def __getattr__(self, attr):
<mask>:
        return self.data_df[attr]
    try:
        return self._known_attrs[attr]
    except KeyError:
        raise AttributeError(""'{}' object has no attribute '{}'"".format(type(self).__name__, attr))",attr in self.data_df.columns,19,attr in self.data_df,False,75.14772930752862,N/A
"def _get_column_as_letter(self, sheet: Worksheet, column_to_convert, startcol: int=0) -> str:
    col = column_to_convert.value if isinstance(column_to_convert, Container) else column_to_convert
<mask>:
        raise TypeError('column must be an index, column letter or column name')
    column_as_letter = None
    if col in self.data_df.columns:
        column_index = self.data_df.columns.get_loc(col) + startcol + 1
        column_as_letter = cell.get_column_letter(column_index)
    elif isinstance(col, int) and col >= 1:
        column_as_letter = cell.get_column_letter(startcol + col)
    elif isinstance(col, str) and col <= get_column_letter(sheet.max_column):
        column_as_letter = col
    if column_as_letter is None or cell.column_index_from_string(column_as_letter) > sheet.max_column:
        raise IndexError('column: %s is out of columns range.' % column_to_convert)
    return column_as_letter","not isinstance(col, (int, str))",88,"not isinstance(col, int) and (not isinstance(col, str))",False,37.709297891717654,N/A
"def __init__(self, bg_color: Optional[str]=None, bold: bool=False, font: str=utils.fonts.arial, font_size: Union[int, float]=12.0, font_color: Optional[str]=None, number_format: str=utils.number_formats.general, protection: bool=False, underline: Optional[str]=None, border_type: Union[str, Set[str], Dict[str, str]]=utils.borders.thin, horizontal_alignment: str=utils.horizontal_alignments.center, vertical_alignment: str=utils.vertical_alignments.center, wrap_text: bool=True, shrink_to_fit: bool=True, fill_pattern_type: str=utils.fill_pattern_types.solid, indent: Union[int, float]=0.0, comment_author: Optional[str]=None, comment_text: Optional[str]=None, text_rotation: int=0, date_format: str=utils.number_formats.date, time_format: str=utils.number_formats.time_24_hours, date_time_format: str=utils.number_formats.date_time, strikethrough: bool=False, italic: bool=False):

    def get_color_from_string(color_str: str, default_color: Optional[str]=None) -> str:
<mask>:
            color_str = color_str[1:]
        if not utils.is_hex_color_string(hex_string=color_str):
            color_str = utils.colors.get(color_str, default_color)
        return color_str
    self.bold = bold
    self.font = font
    self.font_size = font_size
    self.number_format = number_format
    self.protection = protection
    self.underline = underline
    self.horizontal_alignment = horizontal_alignment
    self.vertical_alignment = vertical_alignment
    self.bg_color = get_color_from_string(bg_color, default_color=utils.colors.white)
    self.font_color = get_color_from_string(font_color, default_color=utils.colors.black)
    self.shrink_to_fit = shrink_to_fit
    self.wrap_text = wrap_text
    self.indent = indent
    self.comment_author = comment_author
    self.comment_text = comment_text
    self.text_rotation = text_rotation
    self.date_format = date_format
    self.time_format = time_format
    self.date_time_format = date_time_format
    self.strikethrough = strikethrough
    self.italic = italic
    self.fill_pattern_type = fill_pattern_type
    if isinstance(border_type, set):
        self.border_type = {border_location: utils.borders.thin for border_location in border_type}
    elif isinstance(border_type, dict):
        self.border_type = border_type
    else:
        self.border_type = {utils.border_locations.top: border_type, utils.border_locations.right: border_type, utils.border_locations.bottom: border_type, utils.border_locations.left: border_type}
    if border_type == utils.borders.default_grid:
        if bg_color is not None or fill_pattern_type != utils.fill_pattern_types.solid:
            raise ValueError(f'`bg_color`or `fill_pattern_type` conflict with border_type={utils.borders.default_grid}')
        self.border_type = None
        self.fill_pattern_type = None",color_str and color_str.startswith('#'),194,color_str.startswith('#'),False,67.03200460356396,N/A
"def __add__(self, other):
    default = Styler().__dict__
    d = dict(self.__dict__)
    for k, v in other.__dict__.items():
<mask>:
            d[k] = v
    return Styler(**d)",v != default[k],20,v not in default,False,8.9730240870212,N/A
"def to_openpyxl_style(self):
    try:
        openpyxl_style = self.cache[self]
    except KeyError:
<mask>:
            side = Side(border_style=self.border_type, color=utils.colors.black)
            border = Border(left=side, right=side, top=side, bottom=side)
        else:
            border = Border(**{border_location: Side(border_style=border_type, color=utils.colors.black) for border_location, border_type in self.border_type.items()})
        openpyxl_style = self.cache[self] = NamedStyle(name=str(hash(self)), font=Font(name=self.font, size=self.font_size, color=OpenPyColor(self.font_color), bold=self.bold, underline=self.underline, strikethrough=self.strikethrough, italic=self.italic), fill=PatternFill(patternType=self.fill_pattern_type, fgColor=self.bg_color), alignment=Alignment(horizontal=self.horizontal_alignment, vertical=self.vertical_alignment, wrap_text=self.wrap_text, shrink_to_fit=self.shrink_to_fit, indent=self.indent, text_rotation=self.text_rotation), border=border, number_format=self.number_format, protection=Protection(locked=self.protection))
    return openpyxl_style","isinstance(self.border_type, str)",55,self.border_type is not None,False,40.26190971287384,N/A
"@classmethod
def from_openpyxl_style(cls, openpyxl_style: Cell, theme_colors: List[str], openpyxl_comment: Optional[Comment]=None):

    def _calc_new_hex_from_theme_hex_and_tint(theme_hex, color_tint):
<mask>:
            theme_hex = '#' + theme_hex
        color_obj = Color(theme_hex)
        color_obj.luminance = _calc_lum_from_tint(color_tint, color_obj.luminance)
        return color_obj.hex_l[1:]

    def _calc_lum_from_tint(color_tint: Optional[float], current_lum: float) -> float:
        """"""""
            Based on https://ciintelligence.blogspot.co.il/2012/02/converting-excel-theme-color-and-tint.html
            """"""
        if color_tint is None:
            return current_lum
        current_lum *= 255
        if color_tint < 0:
            return current_lum * (1.0 + color_tint) / 255
        return (current_lum * (1.0 - color_tint) + (255 - 255 * (1.0 - color_tint))) / 255
    bg_color = openpyxl_style.fill.fgColor.rgb
    if not isinstance(bg_color, str):
        try:
            bg_color = theme_colors[openpyxl_style.fill.fgColor.theme]
        except (AttributeError, IndexError, TypeError):
            bg_color = utils.colors.white[:6]
        tint = openpyxl_style.fill.fgColor.tint
        bg_color = _calc_new_hex_from_theme_hex_and_tint(bg_color, tint)
    bold = openpyxl_style.font.bold
    strikethrough = openpyxl_style.font.strikethrough
    italic = openpyxl_style.font.italic
    font = openpyxl_style.font.name
    font_size = openpyxl_style.font.size
    try:
        font_color = openpyxl_style.font.color.rgb
    except AttributeError:
        font_color = utils.colors.black
    if not isinstance(font_color, str):
        try:
            font_color = theme_colors[openpyxl_style.font.color.theme]
        except (AttributeError, IndexError, TypeError):
            font_color = utils.colors.black[:6]
        tint = openpyxl_style.font.color.tint
        font_color = _calc_new_hex_from_theme_hex_and_tint(font_color, tint)
    number_format = openpyxl_style.number_format
    protection = openpyxl_style.protection.locked
    underline = openpyxl_style.font.underline
    horizontal_alignment = openpyxl_style.alignment.horizontal
    vertical_alignment = openpyxl_style.alignment.vertical
    wrap_text = openpyxl_style.alignment.wrap_text or False
    shrink_to_fit = openpyxl_style.alignment.shrink_to_fit
    fill_pattern_type = openpyxl_style.fill.patternType
    indent = openpyxl_style.alignment.indent
    text_rotation = openpyxl_style.alignment.text_rotation
    border_type = {utils.border_locations.top: getattr(openpyxl_style.border.top, 'border_style', None), utils.border_locations.right: getattr(openpyxl_style.border.right, 'border_style', None), utils.border_locations.bottom: getattr(openpyxl_style.border.bottom, 'border_style', None), utils.border_locations.left: getattr(openpyxl_style.border.left, 'border_style', None)}
    if openpyxl_comment:
        comment_author = openpyxl_comment.author
        comment_text = openpyxl_comment.text
    else:
        comment_author = None
        comment_text = None
    return cls(bg_color, bold, font, font_size, font_color, number_format, protection, underline, border_type, horizontal_alignment, vertical_alignment, wrap_text, shrink_to_fit, fill_pattern_type, indent, comment_author, comment_text, text_rotation, strikethrough=strikethrough, italic=italic)",not theme_hex.startswith('#'),233,not theme_hex.startswith('#'),True,100.00000000000004,N/A
"def __init__(self, value, styler=None):
    self.value = value
<mask>:
        if isinstance(self.value, pd_timestamp):
            self.style = Styler(number_format=utils.number_formats.default_date_time_format)
        elif isinstance(self.value, dt.date):
            self.style = Styler(number_format=utils.number_formats.default_date_format)
        elif isinstance(self.value, dt.time):
            self.style = Styler(number_format=utils.number_formats.default_time_format)
        else:
            self.style = Styler()
    else:
        self.style = styler",styler is None,34,styler is None,True,100.00000000000004,N/A
"def __eq__(self, other):
<mask>:
        return other.value == self.value
    return other == self.value","isinstance(other, self.__class__)",12,"isinstance(other, Value)",False,19.765609300943975,N/A
"def __ne__(self, other):
<mask>:
        return other.value != self.value
    return other != self.value","isinstance(other, self.__class__)",12,"isinstance(other, self.__class__)",True,100.00000000000004,N/A
"def __gt__(self, other):
<mask>:
        return other.value < self.value
    return other < self.value","isinstance(other, self.__class__)",12,"isinstance(other, Value)",False,19.765609300943975,N/A
"def __ge__(self, other):
<mask>:
        return other.value <= self.value
    return other <= self.value","isinstance(other, self.__class__)",12,"isinstance(other, Value)",False,19.765609300943975,N/A
"def deprecated_kwargs(deprecated_kwargs):

    def wrapper(func):

        @wraps(func)
        def inner(*args, **kwargs):
            for deprecated_kwarg in deprecated_kwargs:
<mask>:
                    new_kwarg = funcs_to_deprecated_kwargs[func.__name__][deprecated_kwarg]
                    warnings.warn('{} kwarg is deprecated, use {} instead'.format(deprecated_kwarg, new_kwarg), DeprecationWarning, stacklevel=2)
            return func(*args, **kwargs)
        return inner
    return wrapper",deprecated_kwarg in kwargs,33,func.__name__ in funcs_to_deprecated_kwargs[func.__name__],False,3.7419436034576044,N/A
"def export_and_get_default_sheet(self, save=False):
    self.sf.to_excel(excel_writer=self.ew, right_to_left=True, columns_to_hide=self.sf.columns[0], row_to_add_filters=0, columns_and_rows_to_freeze='A2', allow_protection=True)
<mask>:
        self.ew.close()
    return self.ew.sheets['Sheet1']",save,13,save,True,100.00000000000004,N/A
"def _load_from_json(self):
<mask>:
        sheets = json.loads(self.input_json)
    elif self.input_path:
        with open(self.input_path) as f:
            sheets = json.load(f)
    else:
        raise TypeError('Neither --json nor --json_path were provided.')
    try:
        jsonschema.validate(sheets, commandline_json_schema)
    except jsonschema.ValidationError as validation_error:
        raise ValueError(validation_error)
    for sheet in sheets:
        self._load_sheet(sheet)",self.input_json,37,self.input_json,True,100.00000000000004,N/A
"def _load_sheet(self, sheet):
    sheet_name = sheet['sheet_name']
    default_cell_style = sheet.get('default_styles', {}).get('cells')
    data = defaultdict(list)
    for col in sheet['columns']:
        col_name = col['col_name']
        col_width = col.get('width')
<mask>:
            self.col_names_to_width[sheet_name][col_name] = col_width
        for cell in col['cells']:
            provided_style = cell.get('style') or col.get('style') or default_cell_style or {}
            unrecognized_styler_kwargs = set(provided_style.keys()) - styler_kwargs
            if unrecognized_styler_kwargs:
                raise TypeError('Styler dict {} contains unexpected argument: {}.\nExpected arguments: {}'.format(provided_style, unrecognized_styler_kwargs, styler_kwargs))
            else:
                data[col_name].append(Container(cell['value'], Styler(**provided_style)))
    sf = StyleFrame(pd.DataFrame(data=data))
    self._apply_headers_style(sf, sheet)
    self._apply_cols_and_rows_dimensions(sf, sheet)
    sf.to_excel(excel_writer=self.excel_writer, sheet_name=sheet_name, **sheet.get('extra_features', {}))
    setattr(self, '{}_sf'.format(sheet_name), sf)",col_width,76,col_width is not None,False,30.213753973567677,N/A
"def _apply_headers_style(self, sf, sheet):
    default_headers_style = sheet.get('default_styles', {}).get('headers')
<mask>:
        sf.apply_headers_style(styler_obj=Styler(**default_headers_style))",default_headers_style,10,default_headers_style,True,100.00000000000004,N/A
"def _apply_cols_and_rows_dimensions(self, sf, sheet):
    sf.set_column_width_dict(self.col_names_to_width[sheet['sheet_name']])
    row_heights = sheet.get('row_heights')
<mask>:
        sf.set_row_height_dict(row_heights)",row_heights,10,row_heights,True,100.00000000000004,N/A
"def get_cli_args():
    parser = argparse.ArgumentParser('Command-line interface for StyleFrame library')
    group = parser.add_mutually_exclusive_group()
    group.add_argument('-v', '--version', action='store_true', default=False, help='print versions of the Python interpreter, openpyxl, pandas and StyleFrame then quit')
    group.add_argument('--json_path', '--json-path', help='path to json file which defines the Excel file')
    group.add_argument('--json', help='json string which defines the Excel file')
    group.add_argument('--show-schema', action='store_true', help='Print the JSON schema used for validation and exit', default=False)
    group.add_argument('--test', help='execute tests', action='store_true')
    parser.add_argument('--output_path', '--output-path', help='path of output Excel file, defaults to output.xlsx', default='output.xlsx')
    cli_args = parser.parse_args()
<mask>:
        parser.error('Either --json_path or --json are required when not using -v or --show-schema')
    return cli_args","not any((cli_args.version, cli_args.show_schema, cli_args.test)) and (not any((cli_args.json_path, cli_args.json)))",92,not cli_args.version or not cli_args.json,False,4.427353417297051,N/A
"def test_implementations_have_required_methods(self):
    for image_class in ImageFile.__subclasses__():
<mask>:
            continue
        with self.subTest(image_class):
            self.assertTrue(hasattr(image_class, 'mime_type'))
            self.assertTrue(hasattr(image_class, 'format_name'))",image_class == BrokenImageFileImplementation,14,image_class.name != 'ImageFile',False,22.089591134157878,N/A
"def test_crop(self):
    """"""
        Test cropping SVGs.

        This finds, recursively, all of the relevant SVG files in the
        tests/images/svg/originals directory. For each file it finds, it:
        - opens the file;
        - performs a crop based on the filename; and
        - compares the attributes on the cropped SVG in memory to the expected
          cropped version stored in the corresponding tests/images/svg/results
          subdirectory.

        An SVG named `some-name-crop-original.svg' will be cropped to the
        dimensions of its original viewport.

        A specific bounding rect for a crop can be specified in the file name
        like this:

            <some descriptive prefix>-crop-<left>_<top>_<right>_<bottom>.svg

        For example, the file `my-svg-crop-0_10_20_30.svg' will be cropped to
        the rect (left=0, top=10, right=20, bottom=30).

        The cropping test files are organised into subdirectories by their
        `preserveAspectRatio' value. To save duplication, files with specific
        crop directives are symlinks to their corresponding 'original' crop
        file.
        """"""

    def get_original_crop_rect(svg_image):
        return (0, 0, svg_image.image.width, svg_image.image.height)
    base_dir = Path('tests/images/svg')
    original_file_paths = (Path(x) for x in glob.iglob(str(base_dir / 'originals/**/*-crop-*.svg'), recursive=True))
    for original_file_path in original_file_paths:
        with open(original_file_path, 'rb') as f:
            original_file = Image.open(f)
        original = SvgImage.open(original_file)
        directive = original_file_path.stem.split('-')[-1]
<mask>:
            rect = get_original_crop_rect(original)
        else:
            rect = tuple((int(x) for x in directive.split('_')))
        cropped = original.crop(rect)
        result_file_path = str(original_file_path).replace('originals', 'results')
        with open(result_file_path, 'rb') as f:
            expected_file = Image.open(f)
        expected = SvgImage.open(expected_file)
        with self.subTest(original=str(original_file_path), expected=str(result_file_path)):
            self.assertEqual(expected.image.view_box, cropped.image.view_box)
            self.assertEqual(expected.image.width, cropped.image.width)
            self.assertEqual(expected.image.height, cropped.image.height)",directive == 'original',213,directive == 'some-name-crop-original.svg',False,30.213753973567677,N/A
"@unittest.skipIf(no_avif_support, 'ImageMagick was built without AVIF support')
def test_save_avif_lossless(self):
    original_image = self.image.image
    lossless_file = self.image.save_as_avif(io.BytesIO(), lossless=True)
    lossless_image = WandImage.open(lossless_file).image
    magick_version = WAND_VERSION.MAGICK_VERSION_INFO
<mask>:
        _, result_metric = original_image.compare(lossless_image, metric='root_mean_square')
        self.assertTrue(result_metric <= 0.02)
    else:
        identical = True
        for x in range(original_image.width):
            for y in range(original_image.height):
                original_pixel = original_image[x, y]
                if original_pixel.alpha == 0.0:
                    continue
                if original_pixel != lossless_image[x, y]:
                    break
        self.assertTrue(identical)","magick_version >= (7, 1)",59,magick_version == 'AVIF',False,16.669006580554246,N/A
"@unittest.skipIf(no_webp_support, 'ImageMagick was built without WebP support')
def test_save_webp_lossless(self):
    original_image = self.image.image
    new_f = io.BytesIO()
    lossless_file = self.image.save_as_webp(new_f, lossless=True)
    lossless_image = WandImage.open(lossless_file).image
    magick_version = WAND_VERSION.MAGICK_VERSION_INFO
<mask>:
        _, result_metric = original_image.compare(lossless_image, metric='root_mean_square')
        self.assertTrue(result_metric <= 0.001)
    else:
        identical = True
        for x in range(original_image.width):
            for y in range(original_image.height):
                original_pixel = original_image[x, y]
                if original_pixel.alpha == 0.0:
                    continue
                if original_pixel != lossless_image[x, y]:
                    break
        self.assertTrue(identical)","magick_version >= (7, 1)",62,magick_version == '1.0',False,16.669006580554246,N/A
"def register_converter(self, from_image_class, to_image_class, func, cost=None):
    self._registered_converters[from_image_class, to_image_class] = func
<mask>:
        self._registered_converter_costs[from_image_class, to_image_class] = cost",cost is not None,15,cost is not None,True,100.00000000000004,N/A
"def register_image_class(self, image_class):
    self._registered_image_classes.add(image_class)
    try:
        image_class.check()
    except Exception as e:
        self._unavailable_image_classes[image_class] = e
    for attr in dir(image_class):
        val = getattr(image_class, attr)
<mask>:
            self.register_operation(image_class, val.__name__, val)
        elif hasattr(val, '_willow_converter_to'):
            self.register_converter(image_class, val._willow_converter_to[0], val, cost=val._willow_converter_to[1])
        elif hasattr(val, '_willow_converter_from'):
            for converter_from, cost in val._willow_converter_from:
                self.register_converter(converter_from, image_class, val, cost=cost)","hasattr(val, '_willow_operation')",44,"hasattr(val, '__name__')",False,44.833867003844595,N/A
"def register_optimizer(self, optimizer_class: 'OptimizerBase'):
    """"""Registers an optimizer class.""""""
    try:
        from django.conf import settings
        enabled_optimizers = getattr(settings, 'WILLOW_OPTIMIZERS', False)
    except ImportError:
        import os
        enabled_optimizers = os.environ.get('WILLOW_OPTIMIZERS', False)
<mask>:
        return
    if isinstance(enabled_optimizers, str):
        if enabled_optimizers.lower() == 'false':
            return
        elif enabled_optimizers.lower() == 'true':
            enabled_optimizers = True
        else:
            enabled_optimizers = enabled_optimizers.split(',')
    if enabled_optimizers is True:
        add_optimizer = True
    else:
        add_optimizer = optimizer_class.library_name in enabled_optimizers
    if add_optimizer and optimizer_class.check_library() and (optimizer_class not in self._registered_optimizers):
        self._registered_optimizers.append(optimizer_class)",not enabled_optimizers,70,not enabled_optimizers,True,100.00000000000004,N/A
"def operation_exists(self, operation_name):
    for image_class_operations in self._registered_operations.values():
<mask>:
            return True
    return False",operation_name in image_class_operations,12,image_class_operations['name'] == operation_name,False,38.67706276352344,N/A
"def get_image_classes(self, with_operation=None, available=None):
    image_classes = self._registered_image_classes.copy()
<mask>:
        image_classes = set(filter(lambda image_class: image_class in self._registered_operations and with_operation in self._registered_operations[image_class], image_classes))
        if not image_classes:
            raise UnrecognisedOperationError(f""Could not find image class with the '{with_operation}' operation"")
    if available:
        available_image_classes = image_classes - set(self._unavailable_image_classes.keys())
        if not available_image_classes:
            raise UnavailableOperationError('\n'.join([f""The operation '{with_operation}' is available in the following image classes but they all raised errors:""] + ['{image_class_name}: {error_message}'.format(image_class_name=image_class.__name__, error_message=str(self._unavailable_image_classes.get(image_class, 'Unknown error'))) for image_class in image_classes]))
        return available_image_classes
    else:
        return image_classes",with_operation,74,with_operation,True,100.00000000000004,N/A
"def __eq__(self, other):
<mask>:
        return False
    return self.scale_x == other.scale_x and self.scale_y == other.scale_y and (self.translate_x == other.translate_x) and (self.translate_y == other.translate_y)","not isinstance(other, self.__class__)",22,"not isinstance(other, Vector3)",False,27.30664777474173,N/A
"def get_viewport_to_user_space_transform(svg: 'SvgImage') -> ViewportToUserSpaceTransform:
    view_box = svg.image.view_box
    preserve_aspect_ratio = svg.image.preserve_aspect_ratio.split()
    try:
        align, meet_or_slice = preserve_aspect_ratio
    except ValueError:
        align = preserve_aspect_ratio[0]
        meet_or_slice = None
    scale_x = svg.image.width / view_box.width
    scale_y = svg.image.height / view_box.height
<mask>:
        x_position = 'min'
        y_position = 'min'
    else:
        x_position = align[1:4].lower()
        y_position = align[5:].lower()
        choose_coefficient = max if meet_or_slice == 'slice' else min
        scale_x = scale_y = choose_coefficient(scale_x, scale_y)
    translate_x = view_box.min_x * scale_x
    translate_y = view_box.min_y * scale_y
    if x_position == 'mid':
        translate_x -= (svg.image.width - view_box.width * scale_x) / 2
    elif x_position == 'max':
        translate_x -= svg.image.width - view_box.width * scale_x
    if y_position == 'mid':
        translate_y -= (svg.image.height - view_box.height * scale_y) / 2
    elif y_position == 'max':
        translate_y -= svg.image.height - view_box.height * scale_y
    return ViewportToUserSpaceTransform(scale_x, scale_y, translate_x, translate_y)",align == 'none',126,align[0] == 'center',False,14.535768424205482,N/A
"def __init__(self, dom: ElementTree, dpi=96, font_size_px=16):
    self.dom = dom
    self.dpi = dpi
    self.font_size_px = font_size_px
    self.view_box = self._get_view_box()
    self.preserve_aspect_ratio = self._get_preserve_aspect_ratio()
    width, width_unit = self._get_width()
    height, height_unit = self._get_height()
<mask>:
        width = height
        width_unit = height_unit
    elif height is None:
        height = width
        height_unit = width_unit
    elif width_unit == '%':
        width = height
        width_unit = height_unit
    elif height_unit == '%':
        height = width
        height_unit = width_unit
    if width is None and height is None or (width_unit == '%' and height_unit == '%'):
        if self.view_box is not None:
            self.width = self.view_box.width
            self.height = self.view_box.height
        else:
            self.width = 300
            self.height = 150
    else:
        self.width = self._convert_to_px(width, width_unit)
        self.height = self._convert_to_px(height, height_unit)
    if self.view_box is None:
        self.view_box = ViewBox(0, 0, self.width, self.height)",width is None,119,width is None,True,100.00000000000004,N/A
"def _get_preserve_aspect_ratio(self):
    value = self.root.get('preserveAspectRatio', '').strip()
<mask>:
        return 'xMidYMid meet'
    if not self.PRESERVE_ASPECT_RATIO_RE.match(value):
        raise InvalidSvgAttribute(f""Unable to parse preserveAspectRatio value '{value}'"")
    return value",value == '',22,value == '',True,100.00000000000004,N/A
"def _get_width(self):
    attr_value = self.root.get('width')
<mask>:
        return self._parse_size(attr_value)
    return (None, None)",attr_value,11,attr_value,True,100.00000000000004,N/A
"@staticmethod
def converter_from(from_class, cost=None):

    def wrapper(func):
<mask>:
            func._willow_converter_from = []
        if isinstance(from_class, list):
            func._willow_converter_from.extend([(sc, cost) for sc in from_class])
        else:
            func._willow_converter_from.append((from_class, cost))
        return func
    return wrapper","not hasattr(func, '_willow_converter_from')",26,"not hasattr(func, '_willow_converter_from')",True,100.00000000000004,N/A
"@classmethod
def open(cls, f):
    image_format = filetype.guess_extension(f)
<mask>:
        image_format = 'svg'
    initial_class = INITIAL_IMAGE_CLASSES.get(image_format)
    if not initial_class:
        if image_format:
            raise UnrecognisedImageFormatError(f'Cannot load {image_format} images ({INITIAL_IMAGE_CLASSES!r})')
        else:
            raise UnrecognisedImageFormatError('Unknown image format')
    return initial_class(f)",image_format is None and cls.maybe_xml(f),32,image_format == 'svg',False,7.964259079165106,N/A
"@classmethod
def maybe_xml(cls, f):
    f.seek(0)
    pattern = re.compile(b'^\\s*<')
    for line in f:
<mask>:
            f.seek(0)
            return True
    f.seek(0)
    return False",pattern.match(line),19,pattern.search(line),False,37.99178428257963,N/A
"def save(self, image_format, output, apply_optimizers=True) -> Optional['ImageFile']:
<mask>:
        raise ValueError(f'Unknown image format: {image_format}')
    operation_name = 'save_as_' + image_format
    return getattr(self, operation_name)(output, apply_optimizers=apply_optimizers)","image_format not in ['jpeg', 'png', 'gif', 'bmp', 'tiff', 'webp', 'svg', 'heic', 'avif', 'ico']",22,image_format not in self.formats,False,5.448856954638708,N/A
"def optimize(self, image_file, image_format: str):
    """"""
        Runs all available optimizers for the given image format on the given image file.

        If the passed image file is a SpooledTemporaryFile or just bytes, we are converting it to a
        NamedTemporaryFile to guarantee we can access the file so the optimizers to work on it.
        If we get a string, we assume it's a path to a file, and will attempt to load it from
        the file system.
        """"""
    optimizers = registry.get_optimizers_for_format(image_format)
<mask>:
        return
    named_file_created = False
    try:
        if isinstance(image_file, (SpooledTemporaryFile, BytesIO)):
            with NamedTemporaryFile(delete=False) as named_file:
                named_file_created = True
                image_file.seek(0)
                copyfileobj(image_file, named_file)
                file_path = named_file.name
        elif hasattr(image_file, 'name'):
            file_path = image_file.name
        elif isinstance(image_file, str):
            file_path = image_file
        elif isinstance(image_file, bytes):
            with NamedTemporaryFile(delete=False) as named_file:
                named_file.write(image_file)
                file_path = named_file.name
                named_file_created = True
        else:
            raise TypeError(f'Cannot optimise {type(image_file)}. It must be a readable object, or a path to a file')
        for optimizer in optimizers:
            optimizer.process(file_path)
        if hasattr(image_file, 'seek'):
            image_file.seek(0)
            with open(file_path, 'rb') as f:
                copyfileobj(f, image_file)
        if hasattr(image_file, 'truncate'):
            image_file.truncate()
    finally:
        if named_file_created:
            os.unlink(file_path)",not optimizers,169,not optimizers,True,100.00000000000004,N/A
"@Image.operation
def resize(self, size):
<mask>:
        if self.has_alpha():
            image = self.image.convert('RGBA')
        else:
            image = self.image.convert('RGB')
    else:
        image = self.image
    return PillowImage(image.resize(size, _PIL_Image().Resampling.LANCZOS))","self.image.mode in ['1', 'P']",21,self.is_rgb,False,6.434818657591886,N/A
"@Image.operation
def crop(self, rect):
    left, top, right, bottom = rect
    width, height = self.image.size
<mask>:
        raise BadImageOperationError(f'Invalid crop dimensions: {rect!r}')
    clamped_rect = (max(0, left), max(0, top), min(right, width), min(bottom, height))
    return PillowImage(self.image.crop(clamped_rect))",left >= right or left >= width or right <= 0 or (top >= bottom) or (top >= height) or (bottom <= 0),32,width != 2 or height != 2,False,0.39666470380128127,N/A
"@Image.operation
def rotate(self, angle):
    """"""
        Accept a multiple of 90 to pass to the underlying Pillow function
        to rotate the image.
        """"""
    Image = _PIL_Image()
    ORIENTATION_TO_TRANSPOSE = {90: Image.Transpose.ROTATE_90, 180: Image.Transpose.ROTATE_180, 270: Image.Transpose.ROTATE_270}
    modulo_angle = angle % 360
<mask>:
        return self
    transpose_code = ORIENTATION_TO_TRANSPOSE.get(modulo_angle)
    if not transpose_code:
        raise UnsupportedRotation('Sorry - we only support right angle rotations - i.e. multiples of 90 degrees')
    rotated = self.image.transpose(transpose_code)
    return PillowImage(rotated)",not modulo_angle,67,modulo_angle == 90,False,30.213753973567677,N/A
"@Image.operation
def set_background_color_rgb(self, color):
<mask>:
        return self
    if not isinstance(color, (tuple, list)) or not len(color) == 3:
        raise TypeError(""the 'color' argument must be a 3-element tuple or list"")
    image = self.image.convert('RGBA')
    new_image = _PIL_Image().new('RGBA', self.image.size, (color[0], color[1], color[2], 255))
    if hasattr(new_image, 'alpha_composite'):
        new_image.alpha_composite(image)
    else:
        new_image = _PIL_Image().alpha_composite(new_image, image)
    return PillowImage(new_image.convert('RGB'))",not self.has_alpha(),50,color is None,False,0.0,N/A
"@Image.operation
def transform_colorspace_to_srgb(self, rendering_intent=0):
    """"""
        Transforms the color of the image to fit inside sRGB color gamut using the
        embedded ICC profile. The resulting image will always be in RGB(A) mode
        and will have a small generic sRGB ICC profile embedded.

        If the image does not have an ICC profile this operation is a no-op.
        Images without a profile are commonly assumed to be in sRGB color space
        already.

        :param rendering_intent: Controls how out-of-gamut colors and handled.
        Defaults to 0 (perceptual) because this is what Pillow defaults to.
        :return: PillowImage in RGB mode
        :raises: PIL.ImageCms.PyCMSError

        Further reading:
            * https://pillow.readthedocs.io/en/stable/reference/ImageCms.html#PIL.ImageCms.profileToProfile
            * https://www.permajet.com/blog/rendering-intents-explained/
        """"""
    icc_profile = self.get_icc_profile()
<mask>:
        return self
    ImageCms = _PIL_ImageCms()
    icc_profile = ImageCms.ImageCmsProfile(BytesIO(icc_profile))
    output_mode = 'RGBA' if self.has_alpha() else 'RGB'
    image = ImageCms.profileToProfile(self.image, icc_profile, ImageCms.createProfile('sRGB'), renderingIntent=rendering_intent, outputMode=output_mode)
    return PillowImage(image)",icc_profile is None,130,icc_profile is None,True,100.00000000000004,N/A
"@Image.operation
def detect_features(self):
    """"""
        Find interesting features of an image suitable for cropping to.
        """"""
    numpy = _numpy()
    cv2 = _cv2()
    points = cv2.goodFeaturesToTrack(self.image, 20, 0.04, 1.0)
<mask>:
        return []
    else:
        points = numpy.reshape(points, (-1, 2))
        return points.tolist()",points is None,38,len(points) == 0,False,6.567274736060395,N/A
"def _find_cascade(self, cascade_filename):
    """"""
        Find the requested OpenCV cascade file.  If a relative path was provided, check local cascades directory.
        """"""
<mask>:
        cascade_filename = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data/cascades', cascade_filename)
    return cascade_filename",not os.path.isabs(cascade_filename),29,not os.path.isabs(cascade_filename),True,100.00000000000004,N/A
"@Image.operation
def crop(self, rect):
    left, top, right, bottom = rect
    width, height = self.image.size
<mask>:
        raise BadImageOperationError(f'Invalid crop dimensions: {rect!r}')
    clone = self._clone()
    clone.image.crop(left=max(0, left), top=max(0, top), right=min(right, width), bottom=min(bottom, height))
    return clone",left >= right or left >= width or right <= 0 or (top >= bottom) or (top >= height) or (bottom <= 0),33,width != 2 or height != 3,False,0.39666470380128127,N/A
"@Image.operation
def rotate(self, angle):
    not_a_multiple_of_90 = angle % 90
<mask>:
        raise UnsupportedRotation('Sorry - we only support right angle rotations - i.e. multiples of 90 degrees')
    clone = self.image.clone()
    clone.rotate(angle)
    return WandImage(clone)",not_a_multiple_of_90,31,not_a_multiple_of_90,True,100.00000000000004,N/A
"@Image.operation
def set_background_color_rgb(self, color):
<mask>:
        return self
    if not isinstance(color, (tuple, list)) or not len(color) == 3:
        raise TypeError(""the 'color' argument must be a 3-element tuple or list"")
    clone = self._clone()
    clone.image.background_color = _wand_color().Color('rgb({}, {}, {})'.format(*color))
    clone.image.alpha_channel = 'remove'
    if clone.image.alpha_channel:
        clone.image.alpha_channel = False
    return clone",not self.has_alpha(),46,color is None,False,0.0,N/A
"@Image.operation
def save_as_jpeg(self, f, quality: int=85, progressive: bool=False, apply_optimizers: bool=True, **kwargs):
    """"""
        Save the image as a JPEG file.

        :param f: the file or file-like object to save to
        :param quality: the image quality
        :param progressive: whether to save as progressive JPEG file.
        :param apply_optimizers: controls whether to run any configured optimizer libraries
        :return: JPEGImageFile
        """"""
    with self.image.convert('pjpeg' if progressive else 'jpeg') as converted:
        converted.compression_quality = quality
        icc_profile = self.get_icc_profile()
<mask>:
            converted.profiles['icc'] = icc_profile
        exif_data = self.get_exif_data()
        if exif_data is not None:
            converted.profiles['exif'] = exif_data
        converted.save(file=f)
    if apply_optimizers:
        self.optimize(f, 'jpeg')
    return JPEGImageFile(f)",icc_profile is not None,92,icc_profile is not None,True,100.00000000000004,N/A
"@Image.operation
def save_as_png(self, f, apply_optimizers: bool=True, **kwargs):
    """"""
        Save the image as a PNG file.

        :param f: the file or file-like object to save to
        :param apply_optimizers: controls whether to run any configured optimizer libraries
        :return: PNGImageFile
        """"""
    with self.image.convert('png') as converted:
        exif_data = self.get_exif_data()
<mask>:
            converted.profiles['exif'] = exif_data
        converted.save(file=f)
    if apply_optimizers:
        self.optimize(f, 'png')
    return PNGImageFile(f)",exif_data is not None,56,exif_data,False,36.78794411714425,N/A
"def __init__(self, fname=None, fix_mpl=False):
<mask>:
        fname = os.path.join(CONFIG['svg.file_path'], fname)
        svg = _transform.fromfile(fname)
        if fix_mpl:
            w, h = svg.get_size()
            svg.set_size((w.replace('pt', ''), h.replace('pt', '')))
        super(SVG, self).__init__(svg.getroot().root)
        if svg.width.endswith('%'):
            self._width = None
        else:
            self._width = Unit(svg.width).to('px')
        if svg.height.endswith('%'):
            self._height = None
        else:
            self._height = Unit(svg.height).to('px')",fname,42,fname,True,100.00000000000004,N/A
"@property
def width(self):
    """"""Get width of the svg file in px""""""
<mask>:
        return self._width.value",self._width,14,self._width is not None,False,41.11336169005198,N/A
"@property
def height(self):
    """"""Get height of the svg file in px""""""
<mask>:
        return self._height.value",self._height,14,self._height is not None,False,41.11336169005198,N/A
"def __init__(self, text, x=None, y=None, **kwargs):
    params = {'size': CONFIG['text.size'], 'weight': CONFIG['text.weight'], 'font': CONFIG['text.font']}
<mask>:
        x, y = CONFIG['text.position']
    params.update(kwargs)
    element = _transform.TextElement(x, y, text, **params)
    Element.__init__(self, element.root)",x is None or y is None,28,x is None and y is None,False,41.11336169005196,N/A
"def tile(self, ncols, nrows):
    """"""Automatically tile the panels of the figure.

        This will re-arranged all elements of the figure (first in the
        hierarchy) so that they will uniformly cover the figure area.

        Parameters
        ----------
        ncols, nrows : type
            The number of columns and rows to arrange the elements into.


        Notes
        -----
        ncols * nrows must be larger or equal to number of
        elements, otherwise some elements will go outside the figure borders.
        """"""
    dx = (self.width / ncols).to('px').value
    dy = (self.height / nrows).to('px').value
    ix, iy = (0, 0)
    for el in self:
        el.move(dx * ix, dy * iy)
        ix += 1
<mask>:
            ix = 0
            iy += 1
        if iy > nrows:
            break
    return self",ix >= ncols,115,ix > ncols,False,45.13864405503391,N/A
"def _transform(self, element, transform_list):
    rows = 0
<mask>:
        return element
    n_elements = len(transform_list)
    rows = n_elements % self.nrows
    cols = int(n_elements / self.nrows)
    if self.col_width is None:
        self.col_width = transform_list[0]['width']
    if self.row_height is None:
        self.row_height = transform_list[0]['height']
    for i in range(rows):
        element = GroupElement([element])
        element.moveto(0, self.row_height)
    for i in range(cols):
        element = GroupElement([element])
        element.moveto(self.col_width, 0)
    return element",not transform_list,57,self.nrows == 0,False,0.0,N/A
"def moveto(self, x, y, scale_x=1, scale_y=None):
    """"""Move and scale element.

        Parameters
        ----------
        x, y : float
             displacement in x and y coordinates in user units ('px').
        scale_x : float
             x-direction scaling factor. To scale down scale_x < 1,  scale up scale_x > 1.
        scale_y : (optional) float
             y-direction scaling factor. To scale down scale_y < 1,  scale up scale_y > 1.
             If set to default (None), then scale_y=scale_x.
        """"""
<mask>:
        scale_y = scale_x
    self.root.set('transform', 'translate(%s, %s) scale(%s %s) %s' % (x, y, scale_x, scale_y, self.root.get('transform') or ''))",scale_y is None,87,scale_y is None,True,100.00000000000004,N/A
"def skew(self, x=0, y=0):
    """"""Skew the element by x and y degrees
        Convenience function which calls skew_x and skew_y

        Parameters
        ----------
        x,y : float, float
            skew angle in degrees (default 0)

            If an x/y angle is given as zero degrees, that transformation is omitted.
        """"""
<mask>:
        self.skew_x(x)
    if y != 0:
        self.skew_y(y)
    return self",x != 0,54,x != 0,True,100.00000000000004,N/A
"def __init__(self, element_list, attrib=None):
    new_group = etree.Element(SVG + 'g', attrib=attrib)
    for e in element_list:
<mask>:
            new_group.append(e.root)
        else:
            new_group.append(e)
    self.root = new_group","isinstance(e, FigureElement)",21,"isinstance(e, SVGElement)",False,53.7284965911771,N/A
"def __init__(self, width=None, height=None):
    self.root = etree.Element(SVG + 'svg', nsmap=NSMAP)
    self.root.set('version', '1.1')
    self._width = 0
    self._height = 0
<mask>:
        self.width = width
    if height:
        self.height = height",width,27,width,True,100.00000000000004,N/A
"@width.setter
def width(self, value):
<mask>:
        value = Unit(value)
    self._width = value.value
    self.root.set('width', str(value))
    self.root.set('viewBox', '0 0 %s %s' % (self._width, self._height))","not isinstance(value, Unit)",21,"isinstance(value, Unit)",False,84.64817248906144,N/A
"def run(self):
    options_text = self.__dataObject.toString()
    logging.debug('Active options: %s%s' % (options_text[:1], options_text[172:]))
    scribus_file = self.__dataObject.getScribusSourceFile()
<mask>:
        self.__dataObject.setOutputFileName(os.path.split(os.path.splitext(scribus_file)[0])[1] + '__single')
    logging.info('Parsing Scribus SLA template file %s' % scribus_file)
    try:
        tree = ET.parse(scribus_file)
    except IOError as exception:
        logging.error('Scribus SLA template file not found: %s' % scribus_file)
        raise
    root = tree.getroot()
    version = root.get('Version')
    logging.debug('Scribus SLA template file version is %s' % version)
    if self.__dataObject.getSaveSettings():
        serial = self.__dataObject.toString()
        logging.debug('Saving current ScribusGenerator settings in your source file.')
        document = root.find('DOCUMENT')
        storage_element = document.find('./JAVA[@NAME=""' + CONST.STORAGE_NAME + '""]')
        if storage_element is None:
            color_element = document.find('./COLOR[1]')
            script_position = list(document).index(color_element)
            logging.debug('Creating new storage element in SLA template at position %s' % script_position)
            storage_element = ET.Element('JAVA', {'NAME': CONST.STORAGE_NAME})
            document.insert(script_position, storage_element)
        storage_element.set('SCRIPT', serial)
        tree.write(scribus_file)
    data = self.parse_data()
    output_filenames = self.generate_templates(root, data)
    if self.__dataObject.getOutputFormat() == CONST.FORMAT_PDF:
        for output_name in output_filenames:
            sla_output_file = self.build_file_path(self.__dataObject.getOutputDirectory(), output_name, CONST.FILE_EXTENSION_SCRIBUS)
            pdf_output_file = self.build_file_path(self.__dataObject.getOutputDirectory(), output_name, CONST.FILE_EXTENSION_PDF)
            self.export_pdf(sla_output_file, pdf_output_file)
            logging.info('PDF file created: %s' % pdf_output_file)
    if not self.__dataObject.getOutputFormat() == CONST.FORMAT_SLA and self.__dataObject.getKeepGeneratedScribusFiles() == CONST.FALSE:
        for output_name in output_filenames:
            sla_output_file = self.build_file_path(self.__dataObject.getOutputDirectory(), output_name, CONST.FILE_EXTENSION_SCRIBUS)
            os.remove(sla_output_file)
    return 1",self.__dataObject.getSingleOutput() and self.__dataObject.getOutputFileName() is CONST.EMPTY,168,not self.__dataObject.getOutputFileName(),False,23.98584296985825,N/A
"def parse_data(self):
    data_file = self.__dataObject.getDataSourceFile()
<mask>:
        logging.error('Data file not found: %s' % data_file)
        raise
    logging.debug('Parsing data file %s' % data_file)
    data = []
    extension = os.path.splitext(data_file)[1]
    if extension == '.json':
        data = self.load_json(data_file)
    if extension == '.csv':
        data = self.load_csv(data_file)
        logging.debug(data)
        if len(data) < 1:
            logging.error('Data file %s has only one line or is empty. ' + 'At least a header line and a line of data is needed. Halting.' % data_file)
            return -1
        first_item = 1
        first_row = self.__dataObject.getFirstRow()
        if first_row != CONST.EMPTY:
            try:
                new_first_item_value = int(first_row)
                first_item = max(new_first_item_value, 1)
            except:
                logging.warning('Could not parse value of ""first row"" as an integer, ' + 'using default value instead.')
        last_item = len(data)
        last_row = self.__dataObject.getLastRow()
        if last_row != CONST.EMPTY:
            try:
                new_last_item_value = int(last_row)
                last_item = min(new_last_item_value, last_item)
            except:
                logging.warning('Could not parse value of ""last row"" as an integer, ' + 'using default value instead.')
        if first_item != 1 or last_item != len(data):
            logging.debug('Custom data range is: %s - %s' % (first_item, last_item))
            data = data[first_item - 1:last_item]
        else:
            logging.debug('Full data range will be used.')
    return data",not os.path.exists(data_file),176,not os.path.isfile(data_file),False,70.16879391277372,N/A
"def generate_templates(self, root, data: list) -> list:
    merge_mode = self.__dataObject.getSingleOutput()
    data_count = len(data)
    root_string = ET.tostring(root, encoding=self.__dataObject.getCsvEncoding(), method='xml').decode()
    records_in_document = 1 + root_string.count(CONST.NEXT_RECORD)
    logging.info('Source document consumes %s data record(s) from %s.' % (records_in_document, data_count))
    template_element = self.overwrite_with_sg_attributes(root)
    pages_count = page_height = vertical_gap = groups_count = objects_count = 0
    self.headers = list(data[0].keys())
    logging.info('Variables from data file(s): %s' % self.headers)
    output_files = []
    index_current = 1
    index_first_of_batch = 1
    buffer = []
    output = ''
    for item in data:
        buffer.append(item)
<mask>:
            logging.debug('Substituting buffer, with index_current being %s and index_first_of_batch %s' % (index_current, index_first_of_batch))
            output = self.substitute_data(self.headers, self.encode_scribus_xml(buffer), ET.tostring(template_element, method='xml').decode().split('\n'), CONST.KEEP_TAB_LINEBREAK, index_first_of_batch=index_first_of_batch)
            if merge_mode:
                if index_current == min(records_in_document, data_count):
                    logging.debug('Generating reference content from buffer at #%s' % index_current)
                    scribus_element = ET.fromstring(output)
                    document_element = scribus_element.find('DOCUMENT')
                    pages_count = int(document_element.get('ANZPAGES'))
                    page_height = float(document_element.get('PAGEHEIGHT'))
                    vertical_gap = float(document_element.get('GapVertical'))
                    groups_count = int(document_element.get('GROUPC'))
                    objects_count = len(scribus_element.findall('.//PAGEOBJECT'))
                    version = str(scribus_element.get('Version'))
                    logging.debug('Current template has #%s page objects' % objects_count)
                    document_element.set('ANZPAGES', str(math.ceil(pages_count * data_count // records_in_document)))
                    document_element.set('DOCCONTRIB', document_element.get('DOCCONTRIB') + CONST.CONTRIB_TEXT)
                logging.debug('Merging content from buffer up to entry index_current #%s' % index_current)
                shifted_elements = self.shift_pages_and_objects(ET.fromstring(output).find('DOCUMENT'), pages_count, page_height, vertical_gap, index_current - 1, records_in_document, groups_count, objects_count, version)
                if index_current > records_in_document:
                    document_element.extend(shifted_elements)
            else:
                output_file = self.create_output_file(index_current, self.__dataObject.getOutputFileName(), item, len(str(data_count)))
                self.write_sla_file(ET.fromstring(output), output_file)
                output_files.append(output_file)
            buffer = []
            index_first_of_batch = index_current + 1
        index_current += 1
    if merge_mode:
        var_names_dic = dict(list(zip(self.headers, self.headers)))
        output_file = self.create_output_file(index_current - 1, self.__dataObject.getOutputFileName(), var_names_dic, len(str(data_count)))
        self.write_sla_file(scribus_element, output_file)
        output_files.append(output_file)
    return output_files",index_current % records_in_document == 0 or index_current == data_count,226,index_current != 0,False,3.118555560555009,N/A
"def overwrite_with_sg_attributes(self, root):
    for page_object in root.findall('.//ItemAttribute[@Parameter=""SGAttribute""]/../..'):
        for sga in page_object.findall('.//ItemAttribute[@Parameter=""SGAttribute""]'):
            reference = sga.get('RelationshipTo')
<mask>:
                reference = '.'
            elif reference.startswith('/'):
                reference = '.' + reference
            attribute = sga.get('Name')
            value = sga.get('Value')
            try:
                targets = page_object.findall(reference)
                if targets:
                    for target in targets:
                        logging.debug('Overwriting value of %s in %s with ""%s""' % (attribute, target.tag, value))
                        target.set(attribute, value)
                else:
                    logging.error('Target ""%s"" could be parsed but designated no node. ' + 'Check it out as it is probably not what you expected to replace %s.' % (reference, attribute))
            except SyntaxError:
                logging.error('XPATH expression ""%s"" could not be parsed ' + 'by ElementTree to overwrite %s. Skipping.' % (reference, attribute))
    return root",reference == '',106,reference == '',True,100.00000000000004,N/A
"def substitute_data(self, var_names: list, data: list, template: list, keep_tabs_lf=0, clean=CONST.CLEAN_UNUSED_EMPTY_VARS, index_first_of_batch=0):
    result = ''
    index = 0
    replacements_outdated = 1
    for line in template:
<mask>:
            result = result + line
            continue
        if replacements_outdated:
            if index < len(data):
                replaced_strings = data[index]
            else:
                replaced_strings = [''] * len(var_names)
            replacements = dict(list(zip(['%VAR_' + n + '%' for n in var_names], replaced_strings)))
            replacements['%VAR_' + CONST.OUTPUTCOUNT_VAR + '%'] = str(index + index_first_of_batch)
            logging.debug('Replacements updated: %s' % replacements)
            replacements_outdated = 0
        if CONST.NEXT_RECORD in line:
            index += 1
            replacements_outdated = 1
        logging.debug('replacing VARS_* in %s' % line[:50].strip())
        line = self.multiple_replace(line, replacements)
        if clean:
            if CONST.REMOVE_CLEANED_ELEMENT_PREFIX:
                line, count = re.subn('\\s*[,;-]*\\s*%VAR_\\w*%\\s*', '', line)
            else:
                line, count = re.subn('\\s*%VAR_\\w*%\\s*', '', line)
            if count > 0:
                logging.debug('cleaned %d empty variable(s)' % count)
            line, count = re.subn('\\s*%s\\w*\\s*' % CONST.NEXT_RECORD, '', line)
        if keep_tabs_lf == 1 and re.search('[\t\n]+', line, flags=re.MULTILINE):
            matches = re.search('(<ITEXT.* CH="")([^""]+)("".*/>)', line, flags=re.MULTILINE | re.DOTALL)
            if matches:
                matches_start = matches.group(1)
                matches_stop = matches.group(3)
                line = re.sub('([\t\n]+)', matches_stop + '\\g<1>' + matches_start, line, flags=re.MULTILINE)
                line = re.sub('\t', '<tab />', line)
                line = re.sub('\n', '<breakline />', line, flags=re.MULTILINE)
                logging.debug('Converted tabs and linebreaks in line: %s' % line)
            else:
                logging.warning('Could not convert tabs and linebreaks in this line, ' + 'kindly report this to the developers: %s' % line)
        result = result + line
    return result","re.search('%VAR_|' + CONST.NEXT_RECORD, line) == None or re.search('\\s*<COLOR\\s+', line) != None",214,result,False,0.0,N/A
"def ife(test, if_result, else_result):
    """""" Utility if-then-else syntactic sugar
    """"""
<mask>:
        return if_result
    return else_result",test,15,not test,False,49.99999999999999,N/A
"def __init__(self, root):
    self.__dataSourceFileEntryVariable = StringVar()
    self.__scribusSourceFileEntryVariable = StringVar()
    self.__dataSeparatorEntryVariable = StringVar()
    self.__dataSeparatorEntryVariable.set(CONST.CSV_SEP)
    self.__dataEncodingEntryVariable = StringVar()
    self.__dataEncodingEntryVariable.set(CONST.CSV_ENCODING)
    self.__outputDirectoryEntryVariable = StringVar()
    self.__outputFileNameEntryVariable = StringVar()
    self.__outputFormatList = [CONST.FORMAT_PDF, CONST.FORMAT_SLA]
    self.__selectedOutputFormat = StringVar()
    self.__selectedOutputFormat.set(CONST.FORMAT_PDF)
    self.__keepGeneratedScribusFilesCheckboxVariable = IntVar()
    self.__mergeOutputCheckboxVariable = IntVar()
    self.__saveCheckboxVariable = IntVar()
    self.__fromVariable = StringVar()
    self.__fromVariable.set(CONST.EMPTY)
    self.__toVariable = StringVar()
    self.__toVariable.set(CONST.EMPTY)
    self.__closeDialogVariable = IntVar()
    self.__root = root
<mask>:
        doc = scribus.getDocName()
        self.__scribusSourceFileEntryVariable.set(doc)
        self.__outputDirectoryEntryVariable.set(os.path.split(doc)[0])
        self.__dataSourceFileEntryVariable.set(os.path.splitext(doc)[0] + '.csv')",scribus.haveDoc(),63,scribus,False,1.8315638888734187,N/A
"def dataSourceFileEntryVariableHandler(self):
    result = tkinter.filedialog.askopenfilename(title='Choose...', defaultextension='.csv', filetypes=[('CSV - comma separated values', '*.csv *.CSV'), ('TSV - tab separated values', '*.tsv *.TSV'), ('TXT - text', '*.txt *.TXT'), ('all', '*.*')], initialdir=os.path.dirname(self.__dataSourceFileEntryVariable.get()))
<mask>:
        self.__dataSourceFileEntryVariable.set(result)",result,30,result,True,100.00000000000004,N/A
"def scribusSourceFileEntryVariableHandler(self):
    result = tkinter.filedialog.askopenfilename(title='Choose...', defaultextension='.sla', filetypes=[('SLA', '*.sla *.SLA')], initialdir=os.path.dirname(self.__scribusSourceFileEntryVariable.get()))
<mask>:
        self.__scribusSourceFileEntryVariable.set(result)",result,12,result,True,100.00000000000004,N/A
"def allValuesSet(self):
    result = 0
<mask>:
        result = 1
    return result",self.__scribusSourceFileEntryVariable.get() != CONST.EMPTY and self.__dataSourceFileEntryVariable.get() != CONST.EMPTY and (self.__outputDirectoryEntryVariable.get() != CONST.EMPTY) and (len(self.__dataEncodingEntryVariable.get()) >= 4) and (len(self.__dataSeparatorEntryVariable.get()) == 1),11,self.valuesSet,False,2.0146281273554202e-10,N/A
"def __init__(self):
    self.__dataSourceFileEntryVariable = ''
    self.__scribusSourceFileEntryVariable = ''
    self.__dataSeparatorEntryVariable = CONST.CSV_SEP
    self.__dataEncodingEntryVariable = CONST.CSV_ENCODING
    self.__outputDirectoryEntryVariable = ''
    self.__outputFileNameEntryVariable = ''
    self.__outputFormatList = [CONST.FORMAT_PDF, CONST.FORMAT_SLA]
    self.__selectedOutputFormat = CONST.FORMAT_PDF
    self.__keepGeneratedScribusFilesVariable = 0
    self.__mergeOutputVariable = 0
    self.__saveSettingsVariable = 0
    self.__fromVariable = CONST.EMPTY
    self.__toVariable = CONST.EMPTY
    self.__closeDialogVariable = 0
<mask>:
        doc = scribus.getDocName()
        self.__scribusSourceFileEntryVariable = doc
        self.__outputDirectoryEntryVariable = os.path.split(doc)[0]
        self.__dataSourceFileEntryVariable = os.path.splitext(doc)[0] + '.csv'
    else:
        doc = ''",scribus.haveDoc(),64,scribus.isScribusEnabled(),False,30.213753973567677,N/A
"def buttonOkHandler(self):
<mask>:
        dataObject = self.createGeneratorDataObject()
        generator = ScribusGenerator(dataObject)
        try:
            generator.run()
            if dataObject.getCloseDialog():
                scribus.messageBox('Scribus Generator', 'Done. Generated files are in\n' + dataObject.getOutputDirectory(), scribus.ICON_NONE, scribus.BUTTON_OK)
                sys.exit()
            else:
                result = scribus.messageBox('Scribus Generator', 'Done. Generated files are in:\n' + dataObject.getOutputDirectory() + '\nClick OK to Generate more data-driven documents.', scribus.ICON_NONE, scribus.BUTTON_OK, scribus.BUTTON_CANCEL)
                if result == scribus.BUTTON_OK:
                    main_wrapper(sys.argv)
                else:
                    sys.exit()
        except IOError as e:
            scribus.messageBox('File Not Found', 'Could not find some input file, please verify your Scribus and Data file settings:\n\n' + str(e), scribus.ICON_WARNING, scribus.BUTTON_OK)
        except ValueError as e:
            scribus.messageBox('Variable Error', 'Could likely not replace a variable with its value,\nplease check your Data File and Data Separator settings:\n\n' + str(e), scribus.ICON_WARNING, scribus.BUTTON_OK)
        except IndexError as e:
            scribus.messageBox('Variable Error', 'Could not find the value for one variable.\nplease check your Data File and Data Separator settings.\n\n' + str(e), scribus.ICON_WARNING, scribus.BUTTON_OK)
        except Exception:
            scribus.messageBox('Error Scribus Generator', 'Something went wrong.\n\nRead the log file for more (in your home directory).' + traceback.format_exc(), scribus.ICON_WARNING, scribus.BUTTON_OK)
    else:
        scribus.messageBox('Validation Failed', 'Please check if all settings have been set correctly!', scribus.ICON_WARNING, scribus.BUTTON_OK)",CONST.TRUE == self.allValuesSet(),168,self.dataObject,False,5.33657305117355,N/A
"def scribusLoadSettingsHandler(self):
    slaFile = GeneratorControl.getScribusSourceFileEntryVariable(self)
<mask>:
        scribus.fileDialog('Choose a file', 'Set a valid Scribus *.sla input file prior to loading its settings')
        return
    dataObject = GeneratorDataObject(scribusSourceFile=slaFile)
    generator = ScribusGenerator(dataObject)
    saved = generator.get_saved_settings()
    if saved:
        dataObject.loadFromString(saved)
        GeneratorControl.setDataSourceFileEntryVariable(self, dataObject.getDataSourceFile())
        GeneratorControl.setDataSeparatorEntryVariable(self, dataObject.getCsvSeparator())
        GeneratorControl.setDataEncodingEntryVariable(self, dataObject.getCsvEncoding())
        GeneratorControl.setOutputDirectoryEntryVariable(self, dataObject.getOutputDirectory())
        GeneratorControl.setOutputFileNameEntryVariable(self, dataObject.getOutputFileName())
        GeneratorControl.setSelectedOutputFormat(self, dataObject.getOutputFormat())
        GeneratorControl.setKeepGeneratedScribusFilesVariable(self, dataObject.getKeepGeneratedScribusFiles())
        GeneratorControl.setMergeOutputVariable(self, dataObject.getSingleOutput())
        GeneratorControl.setFromVariable(self, dataObject.getFirstRow())
        GeneratorControl.setToVariable(self, dataObject.getLastRow())
        GeneratorControl.setCloseDialogVariable(self, dataObject.getCloseDialog())
    else:
        scribus.messageBox('No Settings', 'Input Scribus file contains no former saved settings.', scribus.ICON_WARNING, scribus.BUTTON_OK | BUTTON_DEFAULT)",slaFile is CONST.EMPTY,71,not slaFile,False,11.15650800742149,N/A
"@Vows.assertion
def to_have_a_docstring(topic):
    """"""Custom assertion.  Raises a AssertionError if `topic` has no
    docstring.

    """"""
<mask>:
        raise AssertionError('Expected topic({0}) to have a docstring', topic)","not hasattr(topic, '__doc__')",23,not topic.endswith('.docstring'),False,4.571221768074353,N/A
"def __init__(self, args, kw):
    self.args = args
    self.kw = kw
    self.error = None
<mask>:
        self.error = self.args","len(self.args) >= 1 and isinstance(self.args[0], Exception)",17,len(self.args) > 0,False,17.120323028183236,N/A
"def __getitem__(self, attr):
<mask>:
        return self.args[attr]
    if attr in self.kw:
        return self.kw[attr]
    raise AttributeError",type(attr) is int,14,attr in self.args,False,8.745825313180626,N/A
"def __getattr__(self, attr):
<mask>:
        return self.kw[attr]
    if hasattr(self, attr):
        return self.attr
    raise AttributeError",attr in self.kw,13,attr in self.kw,True,100.00000000000004,N/A
"def _get_first_available_topic(self, index=-1):
<mask>:
        if index > -1 and isinstance(self.topic_value, (list, set, tuple)):
            return self.topic_value[index]
        else:
            return self.topic_value
    elif self.parent:
        return self.parent._get_first_available_topic(index)
    return None",self.topic_value,24,self.topic_value,True,100.00000000000004,N/A
"@staticmethod
def batch(ctx_class):
    """"""Class decorator.  Use on subclasses of `Vows.Context`.

        Test batches in PyVows are the largest unit of tests. The convention
        is to have one test batch per file, and have the batch’s class match
        the file name.

        """"""
    suite = ctx_class.__module__.replace('.', os.path.sep)
    suite = os.path.abspath(suite)
    suite += '.py'
<mask>:
        Vows.suites[suite] = set()
    Vows.suites[suite].add(ctx_class)
    return _batch(ctx_class)",suite not in Vows.suites,57,suite not in Vows.suites,True,100.00000000000004,N/A
"def _get_topic_times(self, contexts=None):
    """"""Returns a dict describing how long testing took for
        each topic in `contexts`.

        """"""
    topic_times = []
<mask>:
        contexts = self.contexts
    for context in contexts:
        topic_times.append({'context': context['name'], 'path': context['filename'], 'elapsed': context['topic_elapsed']})
        ctx_topic_times = self._get_topic_times(context['contexts'])
        topic_times.extend(ctx_topic_times)
    return topic_times",contexts is None,40,contexts is None,True,100.00000000000004,N/A
"def run(path, pattern, verbosity, show_progress, exclusion_patterns=None, inclusion_patterns=None, capture_output=False):
    from pyvows.core import Vows
<mask>:
        Vows.exclude(exclusion_patterns)
    if inclusion_patterns:
        Vows.include(inclusion_patterns)
    Vows.collect(path, pattern)
    on_success = show_progress and VowsDefaultReporter.on_vow_success or None
    on_error = show_progress and VowsDefaultReporter.on_vow_error or None
    result = Vows.run(on_success, on_error, capture_output)
    return result",exclusion_patterns,40,exclusion_patterns,True,100.00000000000004,N/A
"def main():
    """"""PyVows' runtime implementation.
    """"""
    from pyvows.reporting import VowsDefaultReporter
    arguments = Parser().parse_args()
<mask>:
        from pyvows.utils import template
        template()
        sys.exit()
    path, pattern = (arguments.path, arguments.pattern)
    if path and isfile(path):
        path, pattern = split(path)
    if not path:
        path = os.curdir
    if arguments.no_color:
        for color_name, value in inspect.getmembers(Fore):
            if not color_name.startswith('_'):
                setattr(Fore, color_name, '')
    if arguments.cover and COVERAGE_AVAILABLE:
        cov = coverage(source=arguments.cover_package, omit=arguments.cover_omit)
        cov.erase()
        cov.start()
    verbosity = len(arguments.verbosity) if arguments.verbosity else 2
    result = run(path, pattern, verbosity, arguments.progress, exclusion_patterns=arguments.exclude, inclusion_patterns=arguments.include, capture_output=arguments.capture_output)
    reporter = VowsDefaultReporter(result, verbosity)
    reporter.pretty_print()
    if arguments.profile:
        reporter.print_profile(arguments.profile_threshold)
    if result.successful and arguments.cover:
        if not COVERAGE_AVAILABLE:
            print()
            print(yellow('WARNING: Cover disabled because coverage could not be found.'))
            print(yellow('Make sure it is installed and accessible.'))
            print()
        else:
            cov.stop()
            xml = ''
            try:
                with tempfile.NamedTemporaryFile() as tmp:
                    cov.xml_report(outfile=tmp.name)
                    tmp.seek(0)
                    xml = tmp.read()
            except Exception:
                err = sys.exc_info()[1]
                print('Could not run coverage. Error: %s' % err)
            if xml:
                if arguments.cover_report:
                    with open(arguments.cover_report, 'wb') as report:
                        report.write(xml)
                arguments.cover_threshold /= 100.0
                reporter.print_coverage(xml, arguments.cover_threshold)
    if arguments.xunit_output:
        xunit = XUnitReporter(result)
        xunit.write_report(arguments.xunit_file)
    sys.exit(result.errored_tests)",arguments.template,161,arguments.no_template,False,23.643540225079384,N/A
"def locate(pattern, root=os.curdir, recursive=True):
    """"""Recursively locates test files when `pyvows` is run from the
    command line.

    """"""
    root_path = os.path.abspath(root)
<mask>:
        return_files = []
        for path, dirs, files in os.walk(root_path):
            for filename in fnmatch.filter(files, pattern):
                return_files.append(os.path.join(path, filename))
        return return_files
    else:
        return glob.glob(os.path.join(root_path, pattern))",recursive,43,recursive,True,100.00000000000004,N/A
"def __init__(self, *args):
<mask>:
        raise TypeError('VowsInternalError must be instantiated with a string as the first argument')
    if not len(args) >= 2:
        raise IndexError('VowsInternalError must receive at least 2 arguments')
    self.raw_msg = args[0]
    self.args = args[1:]","not isinstance(args[0], str)",35,"not isinstance(args, str)",False,38.734798110329905,N/A
"def skip_if(condition, reason):
    """"""Topic or vow or context decorator.  Causes a topic or vow to be skipped if `condition` is True

    This is equivilent to `if condition: raise SkipTest(reason)`
    """"""
    from pyvows.core import Vows

    def real_decorator(topic_or_vow_or_context):
<mask>:
            return topic_or_vow_or_context
        if type(topic_or_vow_or_context) == type(Vows.Context):

            class klass_wrapper(topic_or_vow_or_context):

                def topic(self):
                    raise SkipTest(reason)
            klass_wrapper.__name__ = topic_or_vow_or_context.__name__
            return klass_wrapper
        else:

            def wrapper(*args, **kwargs):
                raise SkipTest(reason)
            wrapper.__name__ = topic_or_vow_or_context.__name__
            return wrapper
    return real_decorator",not condition,67,condition,False,36.78794411714425,N/A
"def print_profile(self, threshold):
    """"""Prints the 10 slowest topics that took longer than `threshold`
        to test.
        """"""
    'Prints the 10 slowest topics that took longer than\n        `threshold` to test.\n\n        '
    MAX_PATH_SIZE = 40
    topics = self.result.get_worst_topics(number=10, threshold=threshold)
<mask>:
        print(self.header('Slowest Topics'))
        table_header = yellow('  {0}'.format(dim('#')))
        table_header += yellow('  Elapsed     Context File Path                         ')
        table_header += yellow('  Context Name')
        print(table_header)
        for index, topic in enumerate(topics):
            name = self.under_split(topic['context'])
            name = self.camel_split(name)
            topic['path'] = os.path.realpath(topic['path'])
            topic['path'] = '{0!s}'.format(topic['path'])
            topic['path'] = os.path.relpath(topic['path'], os.path.abspath(os.curdir))
            data = {'number': '{number:#2}'.format(number=index + 1), 'time': '{time:.05f}s'.format(time=topic['elapsed']), 'path': '{path:<{width}}'.format(path=topic['path'][-MAX_PATH_SIZE:], width=MAX_PATH_SIZE), 'name': '{name}'.format(name=name)}
            for k, v in data.items():
                if k == 'number':
                    colorized = blue
                if k == 'time':
                    colorized = green
                if k == 'path':
                    colorized = lambda x: dim(white(x))
                if k == 'name':
                    colorized = green
                data[k] = colorized(v)
            print(' {number}  {time}{0}{path}{0}{name}'.format(4 * ' ', **data))
        print()",topics,137,topics,True,100.00000000000004,N/A
"def get_uncovered_lines(self, uncovered_lines, max_num=3):
    """"""Searches for untested lines of code.  Returns a string
        listing the line numbers.

        If the number of uncovered lines is greater than `max_num`, this will
        only explicitly list the first `max_num` uncovered lines, followed
        by ' and ## more' (where '##' is the total number of additional
        uncovered lines.

        """"""
<mask>:
        template_str = []
        for i in range(max_num):
            line_num = uncovered_lines[i]
            template_str.append(line_num)
            if i is not max_num - 1:
                template_str.append(', ')
        template_str.append(', and {num_more_uncovered:d} more'.format(num_more_uncovered=len(uncovered_lines) - max_num))
        return yellow(''.join(template_str))
    return yellow(', '.join(uncovered_lines))",len(uncovered_lines) > max_num,86,max_num > 0,False,15.719010513286515,N/A
"def print_coverage(self, xml, cover_threshold):
    """"""Prints code coverage statistics for your tests.""""""
    print(self.header('Code Coverage'))
    root = self.parse_coverage_xml(xml)
    klasses = sorted(root['classes'], key=lambda klass: klass['line_rate'])
    max_length = max([len(klass['name']) for klass in root['classes']])
    max_coverage = 0
    for klass in klasses:
        coverage = klass['line_rate']
<mask>:
            cover_character = VowsReporter.BROKEN
        else:
            cover_character = VowsReporter.HONORED
        if 100.0 < max_coverage < coverage:
            max_coverage = coverage
            if max_coverage == 100.0:
                print()
        coverage = coverage
        progress = int(coverage * PROGRESS_SIZE)
        offset = None
        if coverage == 0.0:
            offset = 2
        elif 0.0 < coverage < 0.1:
            offset = 1
        else:
            offset = 0
        if coverage == 0.0 and (not klass['uncovered_lines']):
            continue
        print(self.format_class_coverage(cover_character=cover_character, klass=klass['name'], space1=' ' * (max_length - len(klass['name'])), progress=progress, coverage=coverage, space2=' ' * (PROGRESS_SIZE - progress + offset), lines=self.get_uncovered_lines(klass['uncovered_lines']), cover_threshold=cover_threshold))
    print()
    total_coverage = root['overall']
    cover_character = VowsReporter.HONORED if total_coverage >= cover_threshold else VowsReporter.BROKEN
    progress = int(total_coverage * PROGRESS_SIZE)
    print(self.format_overall_coverage(cover_character, max_length, progress, total_coverage))
    print()",coverage < cover_threshold,143,coverage == 0.0,False,12.44023474812678,N/A
"def format_class_coverage(self, cover_character, klass, space1, progress, coverage, space2, lines, cover_threshold):
    """"""Accepts coverage data for a class and returns a formatted string (intended for
        humans).
        """"""
    klass = klass.lstrip('.')
    klass = blue(klass)
    MET_THRESHOLD = coverage >= cover_threshold
    coverage = '{prefix}{coverage:.1%}'.format(prefix=' ' if coverage > 0.0 else '', coverage=coverage)
<mask>:
        coverage = bold(coverage)
    coverage = white(coverage)
    return ' {0} {klass}{space1}\t{progress}{coverage}{space2} {lines}'.format(cover_character, klass=klass, space1=space1, progress=dim('•' * progress), coverage=coverage, space2=space2, lines=lines)",MET_THRESHOLD,67, MET_THRESHOLD,True,100.00000000000004,N/A
"@property
def status_symbol(self):
    """"""Returns the symbol indicating whether all tests passed.""""""
<mask>:
        return VowsReporter.HONORED
    else:
        return VowsReporter.BROKEN",self.result.successful,17,self.is_honoidal,False,21.3643503198117,N/A
"def pretty_print(self, file=sys.stdout):
    """"""Prints PyVows test results.""""""
    print(self.header('Vows Results'), file=file)
<mask>:
        summary = '{indent}{broken} No vows found! » 0 honored • 0 broken • 0 skipped (0.0s)'.format(indent=self.TAB * self.indent, broken=VowsReporter.BROKEN)
        print(summary, file=file)
        return
    if self.verbosity >= V_VERBOSE or self.result.errored_tests:
        print(file=file)
    for context in self.result.contexts:
        self.print_context(context['name'], context, file=file)
    summary = '{0}{1} OK » {honored:d} honored • {broken:d} broken • {skipped:d} skipped ({time:.6f}s)'.format(self.TAB * self.indent, self.status_symbol, honored=self.result.successful_tests, broken=self.result.errored_tests, skipped=self.result.skipped_tests, time=self.result.elapsed_time)
    print(summary, file=file)
    print(file=file)",not self.result.contexts,71,not self.result,False,60.653065971263366,N/A
"def print_context(self, name, context, file=sys.stdout):
    self.indent += 1
<mask>:
        contextName = StringIO()
        self.humanized_print(name, file=contextName)
        contextName = contextName.getvalue().replace('\n', '')
        if context.get('skip', None):
            contextName += ' (SKIPPED: {0})'.format(str(context['skip']))
        print(contextName, file=file)

    def _print_successful_test():
        honored = ensure_encoded(VowsReporter.HONORED)
        topic = ensure_encoded(test['topic'])
        name = ensure_encoded(test['name'])
        if self.verbosity == V_VERBOSE:
            self.humanized_print('{0} {1}'.format(honored, name), file=file)
        elif self.verbosity >= V_EXTRA_VERBOSE:
            if test['enumerated']:
                self.humanized_print('{0} {1} - {2}'.format(honored, topic, name), file=file)
            else:
                self.humanized_print('{0} {1}'.format(honored, name), file=file)

    def _print_skipped_test():
        if self.verbosity >= V_VERBOSE:
            message = StringIO()
            self.humanized_print('{0} {1}'.format(VowsReporter.SKIPPED, test['name']), file=message)
            message = message.getvalue().replace('\n', '')
            if test['skip'] != context['skip']:
                message = '{0} (SKIPPED: {1})'.format(message, str(test['skip']))
            print(message, file=file)

    def _print_failed_test():
        ctx = test['context_instance']

        def _print_traceback():
            self.indent += 2
            traceback_args = (test['error']['type'], test['error']['value'], test['error']['traceback'])
            self.print_traceback(*traceback_args, file=file)
            if 'file' in test:
                file_msg = 'found in {test[file]} at line {test[lineno]}'.format(test=test)
                print('\n', self.indent_msg(red(file_msg)), '\n', file=file)
            self.indent -= 2
        self.humanized_print('{0} {test}'.format(VowsReporter.BROKEN, test=test['name']), file=file)
        if ctx.generated_topic:
            value = yellow(test['topic'])
            self.humanized_print('', file=file)
            self.humanized_print('\tTopic value:', file=file)
            self.humanized_print('\t{value}'.format(value=value), file=file)
            self.humanized_print('\n' * 2, file=file)
        _print_traceback()
    if context.get('error', None):
        e = context['error']
        print('\n', self.indent_msg(blue('Error in {0!s}:'.format(e.source))), file=file)
        self.print_traceback(*e.exc_info, file=file)
    else:
        for test in context['tests']:
            if VowsResult.test_is_successful(test):
                _print_successful_test()
            elif test['skip']:
                _print_skipped_test()
            else:
                _print_failed_test()
    for context in context['contexts']:
        self.print_context(context['name'], context, file=file)
    self.indent -= 1",self.verbosity >= V_VERBOSE or not self.result.eval_context(context),187,self.verbosity >= V_VERBOSE,False,22.31301601484299,N/A
"def ensure_encoded(thing, encoding='utf-8'):
    """"""Ensures proper encoding for unicode characters.

    Currently used only for characters `✓` and `✗`.

    """"""
<mask>:
        return thing
    else:
        return thing.encode(encoding)","isinstance(thing, bytes) or not isinstance(thing, str)",24,"isinstance(thing, unicode)",False,14.16267793669486,N/A
"def format_traceback(self, traceback_list):
    """"""Adds the current level of indentation to a traceback (so it matches
        the current context's indentation).

        """"""

    def _indent(msg):
<mask>:
            return self.indent_msg(msg)
        return msg
    tb_list = [_indent(tb) for tb in traceback_list]
    return ''.join(tb_list)",msg.strip().startswith('File'),36,self.is_indented(msg),False,6.082317172853824,N/A
"def indent_msg(self, msg, indentation=None):
    """"""Returns `msg` with the indentation specified by `indentation`.

        """"""
<mask>:
        indent = self.TAB * indentation
    else:
        indent = self.TAB * self.indent
    return '{indent}{msg}'.format(indent=indent, msg=msg)",indentation is not None,28,indentation,False,4.9787068367863965,N/A
"def print_traceback(self, err_type, err_obj, err_traceback, file=sys.stdout):
    """"""Prints a color-formatted traceback with appropriate indentation.""""""
<mask>:
        error_msg = err_obj
    elif isinstance(err_obj, bytes):
        error_msg = err_obj.decode('utf8')
    else:
        error_msg = err_obj
    print(self.indent_msg(red(error_msg)), file=file)
    if self.verbosity >= V_NORMAL:
        traceback_msg = traceback.format_exception(err_type, err_obj, err_traceback)
        traceback_msg = self.format_traceback(traceback_msg)
        traceback_msg = '\n{traceback}'.format(traceback=traceback_msg)
        traceback_msg = self.indent_msg(yellow(traceback_msg))
        print(traceback_msg, file=file)","isinstance(err_obj, AssertionError)",49,"isinstance(err_obj, str)",False,70.71067811865478,N/A
"def create_test_case_elements(self, document, parent_node, context):
    topic_node = document.createElement('testcase')
    topic_node.setAttribute('classname', context['name'])
    topic_node.setAttribute('name', 'topic')
    topic_node.setAttribute('time', '0.0')
    stdOutNode = document.createElement('system-out')
    stdOutText = document.createCDATASection(self._safe_cdata(context['stdout']))
    stdOutNode.appendChild(stdOutText)
    stdErrNode = document.createElement('system-err')
    stdErrText = document.createCDATASection(self._safe_cdata(context['stderr']))
    stdErrNode.appendChild(stdErrText)
    topic_node.appendChild(stdOutNode)
    topic_node.appendChild(stdErrNode)
<mask>:
        e = context['error']
        error_msg = 'Error in {0!s}: {1!s}'.format(e.source, e.exc_info[1])
        error_tb = traceback.format_exception(*e.exc_info)
        failure_node = document.createElement('failure')
        failure_node.setAttribute('type', e.exc_info[0].__name__)
        failure_node.setAttribute('message', error_msg)
        failure_text = document.createTextNode(''.join(error_tb))
        failure_node.appendChild(failure_text)
        topic_node.appendChild(failure_node)
    if context.get('skip', None):
        skip_node = document.createElement('skipped')
        skip_node.setAttribute('message', str(context['skip']))
        topic_node.appendChild(skip_node)
    parent_node.appendChild(topic_node)
    for test in context['tests']:
        test_stats = {'context': context['name'], 'name': test['name'], 'taken': 0.0}
        testcase_node = document.createElement('testcase')
        testcase_node.setAttribute('classname', str(test_stats['context']))
        testcase_node.setAttribute('name', str(test_stats['name']))
        testcase_node.setAttribute('time', '{time:.3f}'.format(time=test_stats['taken']))
        stdOutNode = document.createElement('system-out')
        stdOutText = document.createCDATASection(self._safe_cdata(test['stdout']))
        stdOutNode.appendChild(stdOutText)
        stdErrNode = document.createElement('system-err')
        stdErrText = document.createCDATASection(self._safe_cdata(test['stderr']))
        stdErrNode.appendChild(stdErrText)
        testcase_node.appendChild(stdOutNode)
        testcase_node.appendChild(stdErrNode)
        parent_node.appendChild(testcase_node)
        if test.get('error', None):
            error = test['error']
            error_msg = traceback.format_exception(error['type'], error['value'], error['traceback'])
            error_data = {'errtype': error['type'].__name__, 'msg': error['value'], 'tb': ''.join(error_msg)}
            failure_node = document.createElement('failure')
            failure_node.setAttribute('type', str(error_data['errtype']))
            failure_node.setAttribute('message', str(error_data['msg']))
            failure_text = document.createTextNode(str(error_data['tb']))
            failure_node.appendChild(failure_text)
            testcase_node.appendChild(failure_node)
        if test.get('skip', None):
            skip_node = document.createElement('skipped')
            skip_node.setAttribute('message', str(test['skip']))
            testcase_node.appendChild(skip_node)
    for ctx in context['contexts']:
        self.create_test_case_elements(document, parent_node, ctx)","context.get('error', None)",151,"context.get('error', None)",True,100.00000000000004,N/A
"def run(self):
    start_time = time.time()
    result = VowsResult()
<mask>:
        self._capture_streams(True)
    try:
        for suiteName, suitePlan in self.execution_plan.items():
            batches = [batch for batch in self.suites[suiteName] if batch.__name__ in suitePlan['contexts']]
            for batch in batches:
                self.pool.spawn(self.run_context, result.contexts, batch.__name__, batch(None), suitePlan['contexts'][batch.__name__], index=-1, suite=suiteName)
        self.pool.join()
    finally:
        if self.capture_output:
            self._capture_streams(False)
    result.elapsed_time = elapsed(start_time)
    return result",self.capture_output,48,self.capture_output,True,100.00000000000004,N/A
"def _capture_streams(self, capture):
<mask>:
        sys.stdout = AnsiToWin32(_StreamCapture('stdout'), convert=False, strip=True)
        sys.stderr = AnsiToWin32(_StreamCapture('stderr'), convert=False, strip=True)
    else:
        sys.stdout = VowsParallelRunner.orig_stdout
        sys.stderr = VowsParallelRunner.orig_stderr",capture,21,capture,True,100.00000000000004,N/A
"def __init__(self, suites, exclusion_patterns, inclusion_patterns):
    self.suites = suites
<mask>:
        raise Exception('Using both exclusion_patterns and inclusion_patterns is not allowed')
    self.exclusion_patterns = set([re.compile(x) for x in exclusion_patterns])
    self.inclusion_patterns = set([re.compile(x) for x in inclusion_patterns])",exclusion_patterns and inclusion_patterns,32,exclusion_patterns and inclusion_patterns,True,100.00000000000004,N/A
"def plan(self):
    plan = {}
    for suiteName, contextClasses in self.suites.items():
        plan[suiteName] = {'contexts': {}}
        for contextClass in contextClasses:
            contextPlan, isRequired = self.plan_context(contextClass, '')
<mask>:
                plan[suiteName]['contexts'][contextClass.__name__] = contextPlan
    return plan",isRequired and (not self.is_excluded(contextPlan['name'])),29,contextPlan,False,3.0590232050182594e-05,N/A
"def is_excluded(self, name):
    """"""Return whether `name` is in `self.exclusion_patterns`.""""""
    for pattern in self.exclusion_patterns:
<mask>:
            return True
    return False",pattern.search(name),18,pattern.match(name),False,37.99178428257963,N/A
"def is_included(self, name):
    """"""Return whether `name` is in `self.inclusion_patterns`.""""""
<mask>:
        return True
    for pattern in self.inclusion_patterns:
        if pattern.search(name):
            return True
    return False",not self.inclusion_patterns,22,name in self.inclusion_patterns,False,61.47881529512643,N/A
"def plan_context(self, contextClass, idBase):
    context = {'name': contextClass.__name__, 'id': idBase + ('.' if idBase else '') + contextClass.__name__, 'contexts': {}, 'vows': []}
    special_names = set(['setup', 'teardown', 'topic'])
<mask>:
        special_names.update(contextClass.ignored_members)
    contextMembers = [(name, value) for name, value in inspect.getmembers(contextClass) if name not in special_names and (not name.startswith('_'))]
    context['vows'] = [name for name, vow in contextMembers if (inspect.ismethod(vow) or inspect.isfunction(vow)) and self.is_included(context['id'] + '.' + name) and (not self.is_excluded(name))]
    subcontexts = [(name, subcontext) for name, subcontext in contextMembers if inspect.isclass(subcontext) and (not self.is_excluded(name))]
    for name, subcontext in subcontexts:
        subcontextPlan, subcontextContainsIncludedSubcontexts = self.plan_context(subcontext, context['id'])
        if self.is_included(subcontextPlan['id']) or subcontextContainsIncludedSubcontexts:
            context['contexts'][name] = subcontextPlan
    if self.inclusion_patterns:
        contextRequiredBecauseItContainsVowsOrSubcontexts = bool(context['contexts']) or bool(context['vows'])
    else:
        contextRequiredBecauseItContainsVowsOrSubcontexts = True
    return (context, contextRequiredBecauseItContainsVowsOrSubcontexts)","hasattr(contextClass, 'ignored_members')",112,contextClass.ignored_members,False,6.9717291216921975,N/A
"def get_code_for(obj):
    code = None
<mask>:
        code = obj.__code__
    elif hasattr(obj, '__func__'):
        code = obj.__func__.__code__
    return code","hasattr(obj, '__code__')",17,"hasattr(obj, '__code__')",True,100.00000000000004,N/A
"def get_topics_for(topic_function, ctx_obj):
<mask>:
        return []
    if hasattr(topic_function, '_original'):
        _async = getattr(topic_function, '_wrapper_type', None) == 'async_topic'
        topic_function = topic_function._original
    else:
        _async = False
    code = get_code_for(topic_function)
    if not code:
        raise RuntimeError('Function %s does not have a code property' % topic_function)
    expected_args = code.co_argcount - 1
    if _async:
        expected_args -= 1
    topics = []
    child = ctx_obj
    context = ctx_obj.parent
    for i in range(expected_args):
        topic = context.topic_value
        if context.generated_topic:
            topic = topic[child.index]
        topics.append(topic)
        if not context.parent:
            break
        context = context.parent
        child = child.parent
    return topics",not ctx_obj.parent,84,not topic_function,False,11.521590992286539,N/A
"def run_vow(self, tests_collection, topic, ctx_obj, vow, vow_name, enumerated):
    start_time = time.time()
    vow_result = self.get_vow_result(vow, topic, ctx_obj, vow_name, enumerated)
    try:
        result = vow(ctx_obj, topic)
        vow_result['result'] = result
        vow_result['succeeded'] = True
<mask>:
            self.on_vow_success(vow_result)
    except SkipTest as se:
        vow_result['skip'] = se
    except:
        err_type, err_value, err_traceback = sys.exc_info()
        vow_result['error'] = {'type': err_type, 'value': err_value, 'traceback': err_traceback}
        if self.on_vow_error:
            self.on_vow_error(vow_result)
    vow_result['elapsed'] = elapsed(start_time)
    tests_collection.append(vow_result)
    return vow_result",self.on_vow_success,61,self.on_vow_success,True,100.00000000000004,N/A
"def list_subcommands() -> list[str]:
    """"""List all jupyter subcommands

    searches PATH for `jupyter-name`

    Returns a list of jupyter's subcommand names, without the `jupyter-` prefix.
    Nested children (e.g. jupyter-sub-subsub) are not included.
    """"""
    subcommand_tuples = set()
    for d in _path_with_self():
        try:
            names = os.listdir(d)
        except OSError:
            continue
        for name in names:
<mask>:
                if sys.platform.startswith('win'):
                    name = os.path.splitext(name)[0]
                subcommand_tuples.add(tuple(name.split('-')[1:]))
    subcommands = set()
    for sub_tup in subcommand_tuples:
        if not any((sub_tup[:i] in subcommand_tuples for i in range(1, len(sub_tup)))):
            subcommands.add('-'.join(sub_tup))
    return sorted(subcommands)",name.startswith('jupyter-'),76,'-' in name,False,10.122592925934278,N/A
"def _execvp(cmd: str, argv: list[str]) -> None:
    """"""execvp, except on Windows where it uses Popen

    Python provides execvp on Windows, but its behavior is problematic (Python bug#9148).
    """"""
<mask>:
        cmd_path = which(cmd)
        if cmd_path is None:
            msg = f'{cmd!r} not found'
            raise OSError(msg, errno.ENOENT)
        p = Popen([cmd_path] + argv[1:])
        import signal
        signal.signal(signal.SIGINT, signal.SIG_IGN)
        p.wait()
        sys.exit(p.returncode)
    else:
        os.execvp(cmd, argv)",sys.platform.startswith('win'),58,platform.system() == 'Windows',False,13.134549472120788,N/A
"def _jupyter_abspath(subcommand: str) -> str:
    """"""This method get the abspath of a specified jupyter-subcommand with no
    changes on ENV.
    """"""
    search_path = os.pathsep.join(_path_with_self())
    jupyter_subcommand = f'jupyter-{subcommand}'
    abs_path = which(jupyter_subcommand, path=search_path)
<mask>:
        msg = f'\nJupyter command `{jupyter_subcommand}` not found.'
        raise Exception(msg)
    if not os.access(abs_path, os.X_OK):
        msg = f'\nJupyter command `{jupyter_subcommand}` is not executable.'
        raise Exception(msg)
    return abs_path",abs_path is None,56,abs_path is None,True,100.00000000000004,N/A
"def _path_with_self() -> list[str]:
    """"""Put `jupyter`'s dir at the front of PATH

    Ensures that /path/to/jupyter subcommand
    will do /path/to/jupyter-subcommand
    even if /other/jupyter-subcommand is ahead of it on PATH
    """"""
    path_list = (os.environ.get('PATH') or os.defpath).split(os.pathsep)
    try:
        bindir = sysconfig.get_path('scripts')
    except KeyError:
        pass
    else:
        path_list.append(bindir)
    scripts = [sys.argv[0]]
<mask>:
        scripts.append(os.path.realpath(scripts[0]))
    for script in scripts:
        bindir = str(Path(script).parent)
        if Path(bindir).is_dir() and os.access(script, os.X_OK):
            path_list.insert(0, bindir)
    return path_list",Path(scripts[0]).is_symlink(),64,len(scripts) == 1,False,6.168585410281235,N/A
"def _evaluate_argcomplete(parser: JupyterParser) -> list[str]:
    """"""If argcomplete is enabled, trigger autocomplete or return current words

    If the first word looks like a subcommand, return the current command
    that is attempting to be completed so that the subcommand can evaluate it;
    otherwise auto-complete using the main parser.
    """"""
    try:
        from traitlets.config.argcomplete_config import get_argcomplete_cwords, increment_argcomplete_index
        cwords = get_argcomplete_cwords()
<mask>:
            increment_argcomplete_index()
            return cwords
        parser.argcomplete()
    except ImportError:
        parser.argcomplete()
    msg = 'Control flow should not reach end of autocomplete()'
    raise AssertionError(msg)",cwords and len(cwords) > 1 and (not cwords[1].startswith('-')),76,parser.argcomplete_enabled,False,0.4354290028007189,N/A
"def envset(name: str, default: Optional[bool]=False) -> Optional[bool]:
    """"""Return the boolean value of a given environment variable.

    An environment variable is considered set if it is assigned to a value
    other than 'no', 'n', 'false', 'off', '0', or '0.0' (case insensitive)

    If the environment variable is not defined, the default value is returned.
    """"""
<mask>:
        return default
    return os.environ[name].lower() not in ['no', 'n', 'false', 'off', '0', '0.0']",name not in os.environ,66,name not in os.environ,True,100.00000000000004,N/A
"def _do_i_own(path: str) -> bool:
    """"""Return whether the current user owns the given path""""""
    p = Path(path).resolve()
    while not p.exists() and p != p.parent:
        p = p.parent
    try:
        return p.owner() == os.getlogin()
    except Exception:
        pass
<mask>:
        try:
            st = p.stat()
            return st.st_uid == os.geteuid()
        except (NotImplementedError, OSError):
            pass
    return os.access(p, os.W_OK)","hasattr(os, 'geteuid')",51,p.exists(),False,10.400597689005304,N/A
"def prefer_environment_over_user() -> bool:
    """"""Determine if environment-level paths should take precedence over user-level paths.""""""
<mask>:
        return envset('JUPYTER_PREFER_ENV_PATH')
    if sys.prefix != sys.base_prefix and _do_i_own(sys.prefix):
        return True
    if 'CONDA_PREFIX' in os.environ and sys.prefix.startswith(os.environ['CONDA_PREFIX']) and (os.environ.get('CONDA_DEFAULT_ENV', 'base') != 'base') and _do_i_own(sys.prefix):
        return True
    return False",'JUPYTER_PREFER_ENV_PATH' in os.environ,42,envset('JUPYTER_PREFER_ENV_PATH'),False,57.813962871807846,N/A
"def jupyter_config_dir() -> str:
    """"""Get the Jupyter config directory for this platform and user.

    Returns JUPYTER_CONFIG_DIR if defined, otherwise the appropriate
    directory for the platform.
    """"""
    env = os.environ
<mask>:
        return _mkdtemp_once('jupyter-clean-cfg')
    if env.get('JUPYTER_CONFIG_DIR'):
        return env['JUPYTER_CONFIG_DIR']
    if use_platform_dirs():
        return platformdirs.user_config_dir(APPNAME, appauthor=False)
    home_dir = get_home_dir()
    return pjoin(home_dir, '.jupyter')",env.get('JUPYTER_NO_CONFIG'),47,env.get('JUPYTER_TEMP_DIR') and _mkdtemp_once('jupyter-clean-cfg'),False,28.43329181530769,N/A
"def jupyter_data_dir() -> str:
    """"""Get the config directory for Jupyter data files for this platform and user.

    These are non-transient, non-configuration files.

    Returns JUPYTER_DATA_DIR if defined, else a platform-appropriate path.
    """"""
    env = os.environ
<mask>:
        return env['JUPYTER_DATA_DIR']
    if use_platform_dirs():
        return platformdirs.user_data_dir(APPNAME, appauthor=False)
    home = get_home_dir()
    if sys.platform == 'darwin':
        return str(Path(home, 'Library', 'Jupyter'))
    if sys.platform == 'win32':
        appdata = os.environ.get('APPDATA', None)
        if appdata:
            return str(Path(appdata, 'jupyter').resolve())
        return pjoin(jupyter_config_dir(), 'data')
    xdg = env.get('XDG_DATA_HOME', None)
    if not xdg:
        xdg = pjoin(home, '.local', 'share')
    return pjoin(xdg, 'jupyter')",env.get('JUPYTER_DATA_DIR'),84,"env.get('JUPYTER_DATA_DIR', None)",False,72.92571723872932,N/A
"@property
def config_file_paths(self) -> list[str]:
    path = jupyter_config_path()
<mask>:
        path.insert(0, self.config_dir)
    return path",self.config_dir not in path,13,self.config_dir,False,54.88116360940266,N/A
"def _config_file_name_default(self) -> str:
<mask>:
        return ''
    return self.name.replace('-', '_') + '_config'",not self.name,12,not self.name,True,100.00000000000004,N/A
"def write_default_config(self) -> None:
    """"""Write our default config to a .py config file""""""
    config_file: str
<mask>:
        config_file = self.config_file
    else:
        config_file = str(Path(self.config_dir, self.config_file_name + '.py'))
    if Path(config_file).exists() and (not self.answer_yes):
        answer = ''

        def ask() -> str:
            prompt = f'Overwrite {config_file!r} with default config? [y/N]'
            try:
                return input(prompt).lower() or 'n'
            except KeyboardInterrupt:
                print('')
                return 'n'
        answer = ask()
        while not answer.startswith(('y', 'n')):
            print(""Please answer 'yes' or 'no'"")
            answer = ask()
        if answer.startswith('n'):
            return
    config_text = self.generate_config_file()
    print('Writing default config to: {config_file!r}')
    ensure_dir_exists(Path(config_file).parent.resolve(), 448)
    with Path.open(Path(config_file), mode='w', encoding='utf-8') as f:
        f.write(config_text)",self.config_file,91,self.config_file,True,100.00000000000004,N/A
"def migrate_config(self) -> None:
    """"""Migrate config/data from IPython 3""""""
    try:
        f_marker = Path.open(Path(self.config_dir, 'migrated'), 'r+')
    except FileNotFoundError:
        pass
    except OSError:
        return
    else:
        f_marker.close()
        return
    from .migrate import get_ipython_dir, migrate
<mask>:
        return
    migrate()",not Path(get_ipython_dir()).exists(),32,get_ipython_dir(self.config_dir) is None,False,35.00482776497398,N/A
"def load_config_file(self, suppress_errors: bool=True) -> None:
    """"""Load the config file.

        By default, errors in loading config are handled, and a warning
        printed on screen. For testing, the suppress_errors option is set
        to False, so errors will make tests fail.
        """"""
    self.log.debug('Searching %s for config files', self.config_file_paths)
    base_config = 'jupyter_config'
    try:
        super().load_config_file(base_config, path=self.config_file_paths)
    except ConfigFileNotFound:
        self.log.debug('Config file %s not found', base_config)
<mask>:
        path, config_file_name = os.path.split(self.config_file)
    else:
        path = self.config_file_paths
        config_file_name = self.config_file_name
        if not config_file_name or config_file_name == base_config:
            return
    try:
        super().load_config_file(config_file_name, path=path)
    except ConfigFileNotFound:
        self.log.debug('Config file not found, skipping: %s', config_file_name)
    except Exception:
        if not suppress_errors or self.raise_config_file_errors:
            raise
        self.log.warning('Error loading config file: %s', config_file_name, exc_info=True)",self.config_file,107,self.config_file,True,100.00000000000004,N/A
"def get_data() -> dict[str, Any]:
    """"""
    returns a dict of various user environment data
    """"""
    env: dict[str, Any] = {}
    env['path'] = os.environ.get('PATH')
    env['sys_path'] = sys.path
    env['sys_exe'] = sys.executable
    env['sys_version'] = sys.version
    env['platform'] = platform.platform()
<mask>:
        env['where'] = subs(['where', 'jupyter'])
        env['which'] = None
    else:
        env['which'] = subs(['which', '-a', 'jupyter'])
        env['where'] = None
    env['pip'] = subs([sys.executable, '-m', 'pip', 'list'])
    env['conda'] = subs(['conda', 'list'])
    env['conda-env'] = subs(['conda', 'env', 'export'])
    return env",sys.platform == 'win32',69,platform.startswith('win'),False,9.652434877402245,N/A
"def main() -> None:
    """"""
    print out useful info
    """"""
<mask>:
        return
    environment_data = get_data()
    print('$PATH:')
    for directory in environment_data['path'].split(os.pathsep):
        print(f'\t{directory}')
    print('\nsys.path:')
    for directory in environment_data['sys_path']:
        print(f'\t{directory}')
    print('\nsys.executable:')
    print(f""\t{environment_data['sys_exe']}"")
    print('\nsys.version:')
    if '\n' in environment_data['sys_version']:
        for data in environment_data['sys_version'].split('\n'):
            print(f'\t{data}')
    else:
        print(f""\t{environment_data['sys_version']}"")
    print('\nplatform.platform():')
    print(f""\t{environment_data['platform']}"")
    if environment_data['which']:
        print('\nwhich -a jupyter:')
        for line in environment_data['which'].split('\n'):
            print(f'\t{line}')
    if environment_data['where']:
        print('\nwhere jupyter:')
        for line in environment_data['where'].split('\n'):
            print(f'\t{line}')
    if environment_data['pip']:
        print('\npip list:')
        for package in environment_data['pip'].split('\n'):
            print(f'\t{package}')
    if environment_data['conda']:
        print('\nconda list:')
        for package in environment_data['conda'].split('\n'):
            print(f'\t{package}')
    if environment_data['conda-env']:
        print('\nconda env:')
        for package in environment_data['conda-env'].split('\n'):
            print(f'\t{package}')",'_ARGCOMPLETE' in os.environ,89,not os.path.exists(os.pathsep),False,7.495553473355842,N/A
"def migrate_dir(src: str, dst: str) -> bool:
    """"""Migrate a directory from src to dst""""""
    log = get_logger()
<mask>:
        log.debug('No files in %s', src)
        return False
    if Path(dst).exists():
        if os.listdir(dst):
            log.debug('%s already exists', dst)
            return False
        Path(dst).rmdir()
    log.info('Copying %s -> %s', src, dst)
    ensure_dir_exists(Path(dst).parent)
    shutil.copytree(src, dst, symlinks=True)
    return True",not os.listdir(src),48,not Path(src).exists(),False,20.164945583740657,N/A
"def migrate_file(src: str | Path, dst: str | Path, substitutions: Any=None) -> bool:
    """"""Migrate a single file from src to dst

    substitutions is an optional dict of {regex: replacement} for performing replacements on the file.
    """"""
    log = get_logger()
<mask>:
        log.debug('%s already exists', dst)
        return False
    log.info('Copying %s -> %s', src, dst)
    ensure_dir_exists(Path(dst).parent)
    shutil.copy(src, dst)
    if substitutions:
        with Path.open(Path(dst), encoding='utf-8') as f:
            text = f.read()
        for pat, replacement in substitutions.items():
            text = pat.sub(replacement, text)
        with Path.open(Path(dst), 'w', encoding='utf-8') as f:
            f.write(text)
    return True",Path(dst).exists(),83,Path.exists(dst),False,36.40930239806874,N/A
"def migrate_one(src: str, dst: str) -> bool:
    """"""Migrate one item

    dispatches to migrate_dir/_file
    """"""
    log = get_logger()
<mask>:
        return migrate_file(src, dst)
    if Path(src).is_dir():
        return migrate_dir(src, dst)
    log.debug('Nothing to migrate for %s', src)
    return False",Path(src).is_file(),34,Path(src).is_file(),True,100.00000000000004,N/A
"def migrate_static_custom(src: str, dst: str) -> bool:
    """"""Migrate non-empty custom.js,css from src to dst

    src, dst are 'custom' directories containing custom.{js,css}
    """"""
    log = get_logger()
    migrated = False
    custom_js = Path(src, 'custom.js')
    custom_css = Path(src, 'custom.css')
    custom_js_empty = True
<mask>:
        with Path.open(custom_js, encoding='utf-8') as f:
            js = f.read().strip()
            for line in js.splitlines():
                if not (line.isspace() or line.strip().startswith(('/*', '*', '//'))):
                    custom_js_empty = False
                    break
    custom_css_empty = True
    if Path(custom_css).is_file():
        with Path.open(custom_css, encoding='utf-8') as f:
            css = f.read().strip()
            custom_css_empty = css.startswith('/*') and css.endswith('*/')
    if custom_js_empty:
        log.debug('Ignoring empty %s', custom_js)
    if custom_css_empty:
        log.debug('Ignoring empty %s', custom_css)
    if custom_js_empty and custom_css_empty:
        return False
    ensure_dir_exists(dst)
    if not custom_js_empty or not custom_css_empty:
        ensure_dir_exists(dst)
    if not custom_js_empty and migrate_file(custom_js, Path(dst, 'custom.js')):
        migrated = True
    if not custom_css_empty and migrate_file(custom_css, Path(dst, 'custom.css')):
        migrated = True
    return migrated",Path(custom_js).is_file(),129,Path(custom_js).is_file(),True,100.00000000000004,N/A
"def migrate_config(name: str, env: Any) -> list[Any]:
    """"""Migrate a config file.

    Includes substitutions for updated configurable names.
    """"""
    log = get_logger()
    src_base = str(Path(f""{env['profile']}"", f'ipython_{name}_config'))
    dst_base = str(Path(f""{env['jupyter_config']}"", f'jupyter_{name}_config'))
    loaders = {'.py': PyFileConfigLoader, '.json': JSONFileConfigLoader}
    migrated = []
    for ext in ('.py', '.json'):
        src = src_base + ext
        dst = dst_base + ext
<mask>:
            cfg = loaders[ext](src).load_config()
            if cfg:
                if migrate_file(src, dst, substitutions=config_substitutions):
                    migrated.append(src)
            else:
                log.debug('Not migrating empty config file: %s', src)
    return migrated",Path(src).exists(),74,src in loaders,False,5.197112497172873,N/A
"def ensure_dir_exists(path: str | Path, mode: int=511) -> None:
    """"""Ensure that a directory exists

    If it doesn't exist, try to create it, protecting against a race condition
    if another process is doing the same.
    The default permissions are determined by the current umask.
    """"""
    try:
        Path(path).mkdir(parents=True, mode=mode)
    except OSError as e:
<mask>:
            raise
    if not Path(path).is_dir():
        msg = f'{path!r} exists but is not a directory'
        raise OSError(msg)",e.errno != errno.EEXIST,67,e.errno != errno.EEXIST,True,100.00000000000004,N/A
"def _get_frame(level: int) -> FrameType | None:
    """"""Get the frame at the given stack level.""""""
<mask>:
        frame = sys._getframe(level + 1)
    else:
        frame = inspect.stack(context=0)[level + 1].frame
    return frame","hasattr(sys, '_getframe')",29,"isinstance(level + 1, int)",False,7.267884212102741,N/A
"def run(self, coro: Any) -> Any:
    """"""Synchronously run a coroutine on a background thread.""""""
    with self.__lock:
        name = f'{threading.current_thread().name} - runner'
<mask>:
            self.__io_loop = asyncio.new_event_loop()
            self.__runner_thread = threading.Thread(target=self._runner, daemon=True, name=name)
            self.__runner_thread.start()
    fut = asyncio.run_coroutine_threadsafe(coro, self.__io_loop)
    return fut.result(None)",self.__io_loop is None,37,not self.__io_loop,False,74.20884818558928,N/A
"def run_sync(coro: Callable[..., Awaitable[T]]) -> Callable[..., T]:
    """"""Wraps coroutine in a function that blocks until it has executed.

    Parameters
    ----------
    coro : coroutine-function
        The coroutine-function to be executed.

    Returns
    -------
    result :
        Whatever the coroutine-function returns.
    """"""
    assert inspect.iscoroutinefunction(coro)

    def wrapped(*args: Any, **kwargs: Any) -> Any:
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
            asyncio.get_running_loop()
<mask>:
                _runner_map[name] = _TaskRunner()
            return _runner_map[name].run(inner)
        except RuntimeError:
            pass
        loop = ensure_event_loop()
        return loop.run_until_complete(inner)
    wrapped.__doc__ = coro.__doc__
    return wrapped",name not in _runner_map,74,name not in _runner_map,True,100.00000000000004,N/A
"def get_jupyter_output(cmd):
    """"""Get output of a jupyter command""""""
<mask>:
        cmd = [cmd]
    return check_output([sys.executable, '-m', 'jupyter_core', *cmd], stderr=PIPE).decode('utf8').strip()","not isinstance(cmd, list)",18,"not isinstance(cmd, list)",True,100.00000000000004,N/A
"def write_executable(path, source):
<mask>:
        script = path.dirpath() / path.purebasename + '-script.py'
        exe = path.dirpath() / path.purebasename + '.exe'
    else:
        script = path
    script.write(source)
    script.chmod(448)
    if sys.platform == 'win32':
        try:
            import importlib.resources
            if not hasattr(importlib.resources, 'files'):
                raise ImportError
            wp = importlib.resources.files('setuptools').joinpath('cli-32.exe')
            w = wp.read_bytes()
        except (ImportError, FileNotFoundError, SystemError):
            pytest.skip('Need importlib.resources and setuptools to make scripts executable on Windows')
        exe.write(w, 'wb')
        exe.chmod(448)",sys.platform == 'win32',60,path.purebasename,False,10.122592925934278,N/A
"@skip_darwin
def test_not_on_path(tmpdir):
    a = tmpdir.mkdir('a')
    jupyter = a.join('jupyter')
    jupyter.write('from jupyter_core import command; command.main()')
    jupyter.chmod(448)
    witness = a.join('jupyter-witness')
    witness_src = '#!{}\n{}\n'.format(sys.executable, 'print(""WITNESS ME"")')
    write_executable(witness, witness_src)
    env = {'PATH': ''}
<mask>:
        env['SYSTEMROOT'] = os.environ['SYSTEMROOT']
    if sys.platform == 'win32':
        env['PATHEXT'] = '.EXE'
    out = check_output([sys.executable, str(jupyter), 'witness'], env=env)
    assert b'WITNESS' in out",'SYSTEMROOT' in os.environ,50,'SYSTEMROOT' in os.environ,True,100.00000000000004,N/A
"@skip_darwin
def test_path_priority(tmpdir):
    a = tmpdir.mkdir('a')
    jupyter = a.join('jupyter')
    jupyter.write('from jupyter_core import command; command.main()')
    jupyter.chmod(448)
    witness_a = a.join('jupyter-witness')
    witness_a_src = '#!{}\n{}\n'.format(sys.executable, 'print(""WITNESS A"")')
    write_executable(witness_a, witness_a_src)
    b = tmpdir.mkdir('b')
    witness_b = b.join('jupyter-witness')
    witness_b_src = '#!{}\n{}\n'.format(sys.executable, 'print(""WITNESS B"")')
    write_executable(witness_b, witness_b_src)
    env = {'PATH': str(b)}
<mask>:
        env['SYSTEMROOT'] = os.environ['SYSTEMROOT']
    if sys.platform == 'win32':
        env['PATHEXT'] = '.EXE'
    out = check_output([sys.executable, str(jupyter), 'witness'], env=env)
    assert b'WITNESS A' in out",'SYSTEMROOT' in os.environ,64,'SYSTEMROOT' in os.environ,True,100.00000000000004,N/A
"@skip_darwin
def test_argv0(tmpdir):
    a = tmpdir.mkdir('a')
    jupyter = a.join('jupyter')
    jupyter.write('from jupyter_core import command; command.main()')
    jupyter.chmod(448)
    witness_a = a.join('jupyter-witness')
    witness_a_src = f'#!{sys.executable}\nimport sys\nprint(sys.argv[0])\n'
    write_executable(witness_a, witness_a_src)
    env = {}
<mask>:
        env['SYSTEMROOT'] = os.environ['SYSTEMROOT']
    if sys.platform == 'win32':
        env['PATHEXT'] = '.EXE'
    out = check_output([sys.executable, str(jupyter), 'witness'], env=env)
    assert f'{jupyter}-witness'.encode() in out",'SYSTEMROOT' in os.environ,48,'SYSTEMROOT' in os.environ,True,100.00000000000004,N/A
"@macos
@use_platformdirs
def test_runtime_dir_darwin():
    runtime = jupyter_runtime_dir()
<mask>:
        assert runtime == realpath('~/Library/Preferences/Jupyter/runtime')
        return
    assert runtime == realpath('~/Library/Application Support/Jupyter/runtime')",__version_info__[0] < 3,18,platform.system() == 'Darwin',False,0.0,N/A
"def test_is_hidden():
    with tempfile.TemporaryDirectory() as root:
        subdir1 = os.path.join(root, 'subdir')
        os.makedirs(subdir1)
        assert not is_hidden(subdir1, root)
        assert not is_file_hidden(subdir1)
        subdir2 = os.path.join(root, '.subdir2')
        os.makedirs(subdir2)
        assert is_hidden(subdir2, root)
        assert is_file_hidden(subdir2)
        assert not is_hidden(subdir2, subdir2)
        subdir34 = os.path.join(root, 'subdir3', '.subdir4')
        os.makedirs(subdir34)
        assert is_hidden(subdir34, root)
        assert is_hidden(subdir34)
        subdir56 = os.path.join(root, '.subdir5', 'subdir6')
        os.makedirs(subdir56)
        assert is_hidden(subdir56, root)
        assert is_hidden(subdir56)
        assert not is_file_hidden(subdir56)
        assert not is_file_hidden(subdir56, os.stat(subdir56))
        assert not is_file_hidden(os.path.join(root, 'does_not_exist'))
        subdir78 = os.path.join(root, 'subdir7', 'subdir8')
        os.makedirs(subdir78)
        assert not is_hidden(subdir78, root)
<mask>:
            os.chflags(subdir78, UF_HIDDEN)
            assert is_hidden(subdir78, root)","hasattr(os, 'chflags')",81,os.name == 'nt',False,8.116697886877475,N/A
"@pytest.mark.skipif(sys.platform != 'win32', reason='only runs on windows')
def test_secure_write_win32():

    def fetch_win32_permissions(filename):
        """"""Extracts file permissions on windows using icacls""""""
        role_permissions = {}
        proc = os.popen(f'icacls {filename}')
        lines = proc.read().splitlines()
        proc.close()
        for index, line in enumerate(lines):
<mask>:
                line = line.split(filename)[-1].strip().lower()
            match = re.match('\\s*([^:]+):\\(([^\\)]*)\\)', line)
            if match:
                usergroup, permissions = match.groups()
                usergroup = usergroup.lower().split('\\')[-1]
                permissions = {p.lower() for p in permissions.split(',')}
                role_permissions[usergroup] = permissions
            elif not line.strip():
                break
        return role_permissions

    def check_user_only_permissions(fname):
        username = os.environ['USERNAME'].lower()
        permissions = fetch_win32_permissions(fname)
        print(permissions)
        assert username in permissions
        assert permissions[username] == {'r', 'w', 'd'}
        assert 'administrators' in permissions
        assert permissions['administrators'] == {'f'}
        assert 'everyone' not in permissions
        assert len(permissions) == 2
    directory = tempfile.mkdtemp()
    fname = os.path.join(directory, 'check_perms')
    try:
        with secure_write(fname) as f:
            f.write('test 1')
        check_user_only_permissions(fname)
        with open(fname, encoding='utf-8') as f:
            assert f.read() == 'test 1'
    finally:
        shutil.rmtree(directory)",index == 0,130,index == 0,True,100.00000000000004,N/A
"def _win32_longpath(path):
    """"""
    Helper function to add the long path prefix for Windows, so that shutil.copytree
     won't fail while working with paths with 255+ chars.
    """"""
<mask>:
        normalized = os.path.normpath(path)
        if not normalized.startswith('\\\\?\\'):
            is_unc = normalized.startswith('\\\\')
            if is_unc:
                normalized = normalized.replace('\\\\', '\\\\?\\UNC\\')
            else:
                normalized = '\\\\?\\' + normalized
        return normalized
    else:
        return path",sys.platform == 'win32',53,platform.system() == 'Windows',False,13.134549472120788,N/A
"@pytest.fixture
def datadir(original_datadir, tmp_path):
    result = tmp_path / original_datadir.stem
<mask>:
        shutil.copytree(_win32_longpath(str(original_datadir)), _win32_longpath(str(result)))
    else:
        result.mkdir()
    return result",original_datadir.is_dir(),16,os.path.exists(str(result)),False,4.9323515694897075,N/A
"def is_installed(prog):
    """"""Return whether or not a given executable is installed on the machine.""""""
    with open(os.devnull, 'w') as devnull:
        try:
<mask>:
                retcode = subprocess.call(['where', prog], stdout=devnull)
            else:
                retcode = subprocess.call(['which', prog], stdout=devnull)
        except OSError as e:
            if e.errno != errno.ENOENT:
                raise
            retcode = 1
    return retcode == 0",os.name == 'nt',48,prog.startswith('--'),False,8.116697886877475,N/A
"def __init__(self, profiling_data, scale=None):
<mask>:
        self.entries = pstats2entries(pstats.Stats(profiling_data))
    elif isinstance(profiling_data, pstats.Stats):
        self.entries = pstats2entries(profiling_data)
    else:
        self.entries = profiling_data
    self.out_file = None
    self.scale = scale
    if not scale:
        self.scale = Scale('ns')
    self._code_by_position = defaultdict(set)
    self._populate_code_by_position()",is_basestring(profiling_data),34,"isinstance(profiling_data, pstats.PstatsList)",False,27.77619034011791,N/A
"def _populate_code_by_position(self):
    for entry in self.entries:
        self._add_code_by_position(entry.code)
<mask>:
            continue
        for subentry in entry.calls:
            self._add_code_by_position(subentry.code)",not entry.calls,14,entry.calls is None,False,39.76353643835252,N/A
"def munged_function_name(self, code):
    co_filename, co_firstlineno, co_name = cProfile.label(code)
<mask>:
        return co_name
    return '%s:%d' % (co_name, co_firstlineno)","len(self._code_by_position[co_filename, co_name]) == 1",16,co_filename == self.filename,False,4.633429238877301,N/A
"def visualize(self):
    """"""Launch kcachegrind on the converted entries.

        One of the executables listed in KCACHEGRIND_EXECUTABLES
        must be present in the system path.
        """"""
    available_cmd = None
    for cmd in KCACHEGRIND_EXECUTABLES:
<mask>:
            available_cmd = cmd
            break
    if available_cmd is None:
        sys.stderr.write('Could not find kcachegrind. Tried: %s\n' % ', '.join(KCACHEGRIND_EXECUTABLES))
        return
    if self.out_file is None:
        fd, outfile = tempfile.mkstemp('.log', 'pyprof2calltree')
        use_temp_file = True
    else:
        outfile = self.out_file.name
        use_temp_file = False
    try:
        if use_temp_file:
            with io.open(fd, 'w') as f:
                self.output(f)
        subprocess.call([available_cmd, outfile])
    finally:
        if use_temp_file:
            os.remove(outfile)
            self.out_file = None",is_installed(cmd),86,os.path.exists(cmd),False,20.556680845025987,N/A
"def bitcoin_cli_checkoutput(*args):
    """"""
    Run `bitcoin-cli`, fail if OS return code nonzero, return output
    """"""
    cmd_list, retcode, output = run_subprocess('bitcoin-cli', *args)
<mask>:
        raise subprocess.CalledProcessError(retcode, cmd_list, output=output)
    return output",retcode != 0,27,retcode,False,4.9787068367863965,N/A
"def validate_rng_seed(seed, min_length):
    """"""
    Validates random hexadecimal seed
    returns => <boolean>

    seed: <string> hex string to be validated
    min_length: <int> number of characters required.  > 0
    """"""
<mask>:
        print('Error: Computer entropy must be at least {0} characters long'.format(min_length))
        return False
    if len(seed) % 2 != 0:
        print('Error: Computer entropy must contain an even number of characters.')
        return False
    try:
        int(seed, 16)
    except ValueError:
        print('Error: Illegal character. Computer entropy must be composed of hexadecimal characters only (0-9, a-f).')
        return False
    return True",len(seed) < min_length,81,len(seed) < min_length,True,100.00000000000004,N/A
"def validate_dice_seed(dice, min_length):
    """"""
    Validates dice data (i.e. ensures all digits are between 1 and 6).
    returns => <boolean>

    dice: <string> representing list of dice rolls (e.g. ""5261435236..."")
    """"""
<mask>:
        print('Error: You must provide at least {0} dice rolls'.format(min_length))
        return False
    for die in dice:
        try:
            i = int(die)
            if i < 1 or i > 6:
                print('Error: Dice rolls must be between 1 and 6.')
                return False
        except ValueError:
            print('Error: Dice rolls must be numbers between 1 and 6')
            return False
    return True",len(dice) < min_length,84,len(dice) < min_length,True,100.00000000000004,N/A
"def xor_hex_strings(str1, str2):
    """"""
    Return xor of two hex strings.
    An XOR of two pieces of data will be as random as the input with the most randomness.
    We can thus combine two entropy sources in this way as a safeguard against one source being
    compromised in some way.
    For details, see http://crypto.stackexchange.com/a/17660

    returns => <string> in hex format
    """"""
<mask>:
        raise Exception('tried to xor strings of unequal length')
    str1_dec = int(str1, 16)
    str2_dec = int(str2, 16)
    xored = str1_dec ^ str2_dec
    return '{:0{}x}'.format(xored, len(str1))",len(str1) != len(str2),85,len(str1) != len(str2),True,100.00000000000004,N/A
"@pytest.fixture
def setup_teardown():
    data1 = stats.gamma.rvs(2, loc=1.5, scale=2, size=10000)
    data2 = stats.gamma.rvs(1, loc=1.5, scale=3, size=10000)
    with open('test.csv', 'w') as tmp:
        for x, y in zip(data1, data2):
            tmp.write('{},{}\n'.format(x, y))
    yield
    filenames = ['test.csv', 'fitter.log', 'fitter.png']
    for filename in filenames:
        file = Path(filename)
<mask>:
            file.unlink()",file.exists(),43,file.exists(),True,100.00000000000004,N/A
"def __init__(self, data=None, X=None, Y=None, bins=None):
    """""".. rubric:: **Constructor**

        One should provide either the parameter **data** alone, or the X and Y
        parameters, which are the histogram of some data sample.

        :param data: random data
        :param X: evenly spaced X data
        :param Y: probability density of the data
        :param bins: if data is providede, we will compute the probability using
            hist function and bins may be provided.

        """"""
    self.data = data
<mask>:
        Y, X, _ = pylab.hist(self.data, bins=bins, density=True)
        self.N = len(X) - 1
        self.X = [(X[i] + X[i + 1]) / 2 for i in range(self.N)]
        self.Y = Y
        self.A = 1
        self.guess_std = pylab.std(self.data)
        self.guess_mean = pylab.mean(self.data)
        self.guess_amp = 1
    else:
        self.X = X
        self.Y = Y
        self.Y = self.Y / sum(self.Y)
        if len(self.X) == len(self.Y) + 1:
            self.X = [(X[i] + X[i + 1]) / 2 for i in range(len(X) - 1)]
        self.N = len(self.X)
        self.guess_mean = self.X[int(self.N / 2)]
        self.guess_std = sqrt(sum((self.X - mean(self.X)) ** 2) / self.N) / sqrt(2 * 3.14)
        self.guess_amp = 1.0
    self.func = self._func_normal",data,172,X is None or Y is None,False,0.0,N/A
"def fit(self, error_rate=0.05, semilogy=False, Nfit=100, error_kwargs={'lw': 1, 'color': 'black', 'alpha': 0.2}, fit_kwargs={'lw': 2, 'color': 'red'}):
    self.mus = []
    self.sigmas = []
    self.amplitudes = []
    self.fits = []
    pylab.figure(1)
    pylab.clf()
    pylab.bar(self.X, self.Y, width=0.85, ec='k')
    for x in range(Nfit):
        self.E = [scipy.stats.norm.rvs(0, error_rate) for y in self.Y]
        self.result = scipy.optimize.least_squares(self.func, (self.guess_mean, self.guess_std, self.guess_amp))
        mu, sigma, amplitude = self.result['x']
        pylab.plot(self.X, amplitude * scipy.stats.norm.pdf(self.X, mu, sigma), **error_kwargs)
        self.sigmas.append(sigma)
        self.amplitudes.append(amplitude)
        self.mus.append(mu)
        self.fits.append(amplitude * scipy.stats.norm.pdf(self.X, mu, sigma))
    self.sigma = mean(self.sigmas)
    self.amplitude = mean(self.amplitudes)
    self.mu = mean(self.mus)
    pylab.plot(self.X, self.amplitude * scipy.stats.norm.pdf(self.X, self.mu, self.sigma), **fit_kwargs)
<mask>:
        pylab.semilogy()
    pylab.grid()
    pylab.figure(2)
    pylab.clf()
    M = mean(self.fits, axis=0)
    S = pylab.std(self.fits, axis=0)
    pylab.fill_between(self.X, M - 3 * S, M + 3 * S, color='gray', alpha=0.5)
    pylab.fill_between(self.X, M - 2 * S, M + 2 * S, color='gray', alpha=0.5)
    pylab.fill_between(self.X, M - S, M + S, color='gray', alpha=0.5)
    pylab.plot(self.X, self.amplitude * scipy.stats.norm.pdf(self.X, self.mu, self.sigma), **fit_kwargs)
    pylab.grid()
    return (self.mu, self.sigma, self.amplitude)",semilogy,147,semilogy,True,100.00000000000004,N/A
"@main.command()
@click.argument('filename', type=click.STRING)
@click.option('--column-number', type=click.INT, default=1, show_default=True, help='data column to use (first column by default)')
@click.option('--delimiter', type=click.STRING, default=',', show_default=True, help='column delimiter (comma by default)')
@click.option('--distributions', type=click.STRING, default='gamma,beta', show_default=True, help='list of distribution')
@click.option('--tag', type=click.STRING, default='fitter', help='tag to name output files')
@click.option('--progress/--no-progress', default=True, show_default=True)
@click.option('--verbose/--no-verbose', default=True, show_default=True)
@click.option('--output-image', type=click.STRING, default='fitter.png', show_default=True)
def fitdist(**kwargs):
    """"""fit distribution""""""
    from pylab import savefig
    col = kwargs['column_number']
    with open(kwargs['filename'], 'r') as csvfile:
        data = csv.reader(csvfile, delimiter=kwargs['delimiter'])
        data = [float(x[col - 1]) for x in data]
    outfile = kwargs['output_image']
<mask>:
        click.echo('output file must have one of the following extension: png, svg, pdf, jpg', err=True)
        sys.exit(1)
    if kwargs['verbose'] is False:
        kwargs['progress'] = False
    from fitter import Fitter
    distributions = kwargs['distributions'].split(',')
    distributions = [x.strip() for x in distributions]
    fit = Fitter(data, distributions=distributions)
    fit.fit(progress=kwargs['progress'])
    fit.summary()
    if kwargs['verbose']:
        click.echo()
    if kwargs['verbose']:
        click.echo('Saved image in fitter.png; use --output-image to change the name')
    savefig(f'{outfile}', dpi=200)
    best = fit.get_best()
    bestname = list(best.keys())[0]
    values = list(best.values())[0]
    msg = f'Fitter version {version}\nBest fit is {bestname} distribution\nparameters: '
    msg += f'{values}\n The parameters have to be used in that order in scipy'
    if kwargs['verbose']:
        click.echo(msg)
    tag = kwargs['tag']
    with open(f'{tag}.log', 'w') as fout:
        fout.write(msg)","Path(outfile).name.split('.')[-1] not in ['png', 'jpg', 'svg', 'pdf']",186,not outfile,False,0.00026351417291071874,N/A
"def get_distributions():
    distributions = []
    for this in dir(scipy.stats):
<mask>:
            distributions.append(this)
    return distributions",'fit' in eval('dir(scipy.stats.' + this + ')'),13,scipy.stats.has_distribution(this),False,13.575914775035756,N/A
"def __init__(self, data, xmin=None, xmax=None, bins=100, distributions=None, timeout=30, density=True):
    """""".. rubric:: Constructor

        :param list data: a numpy array or a list
        :param float xmin: if None, use the data minimum value, otherwise histogram and
            fits will be cut
        :param float xmax: if None, use the data maximum value, otherwise histogram and
            fits will be cut
        :param int bins: numbers of bins to be used for the cumulative histogram. This has
            an impact on the quality of the fit.
        :param list distributions: give a list of distributions to look at. If none, use
            all scipy distributions that have a fit method. If you want to use
            only one distribution and know its name, you may provide a string (e.g.
            'gamma'). Finally, you may set to 'common' to  include only common
            distributions, which are: cauchy, chi2, expon, exponpow, gamma,
                 lognorm, norm, powerlaw, irayleigh, uniform.
        :param timeout: max time for a given distribution. If timeout is
            reached, the distribution is skipped.

        .. versionchanged:: 1.2.1 remove verbose argument, replacedb by logging module.
        .. versionchanged:: 1.0.8 increase timeout from 10 to 30 seconds.
        """"""
    self.timeout = timeout
    self._data = None
    self._density = True
    self.distributions = distributions
<mask>:
        self._load_all_distributions()
    elif self.distributions == 'common':
        self.distributions = get_common_distributions()
    elif isinstance(distributions, str):
        self.distributions = [distributions]
    self.bins = bins
    self._alldata = np.array(data)
    if xmin == None:
        self._xmin = self._alldata.min()
    else:
        self._xmin = xmin
    if xmax == None:
        self._xmax = self._alldata.max()
    else:
        self._xmax = xmax
    self._trim_data()
    self._update_data_pdf()
    self._init()",self.distributions == None,237,self.distributions == 'all',False,75.98356856515926,N/A
"def _set_xmin(self, value):
<mask>:
        value = self._alldata.min()
    elif value < self._alldata.min():
        value = self._alldata.min()
    self._xmin = value
    self._trim_data()
    self._update_data_pdf()",value == None,19,value > self._alldata.max(),False,4.196114906296549,N/A
"def _set_xmax(self, value):
<mask>:
        value = self._alldata.max()
    elif value > self._alldata.max():
        value = self._alldata.max()
    self._xmax = value
    self._trim_data()
    self._update_data_pdf()",value == None,19,value < 0,False,19.716118825581447,N/A
"def fit(self, progress=False, n_jobs=-1, max_workers=-1, prefer='processes'):
    """"""Loop over distributions and find best parameter to fit the data for each

        When a distribution is fitted onto the data, we populate a set of
        dataframes:

            - :attr:`df_errors`  :sum of the square errors between the data and the fitted
              distribution i.e., :math:`\\sum_i \\left( Y_i - pdf(X_i) \\right)^2`
            - :attr:`fitted_param` : the parameters that best fit the data
            - :attr:`fitted_pdf` : the PDF generated with the parameters that best fit the data

        Indices of the dataframes contains the name of the distribution.

        """"""
    N = len(self.distributions)
    with tqdm_joblib(desc=f'Fitting {N} distributions', total=N, disable=not progress) as progress_bar:
        results = Parallel(n_jobs=max_workers, prefer=prefer)((delayed(Fitter._fit_single_distribution)(dist, self._data, self.x, self.y, self.timeout) for dist in self.distributions))
    for distribution, values in results:
<mask>:
            param, pdf_fitted, sq_error, aic, bic, kullback_leibler, ks_stat, ks_pval = values
            self.fitted_param[distribution] = param
            self.fitted_pdf[distribution] = pdf_fitted
            self._fitted_errors[distribution] = sq_error
            self._aic[distribution] = aic
            self._bic[distribution] = bic
            self._kldiv[distribution] = kullback_leibler
            self._ks_stat[distribution] = ks_stat
            self._ks_pval[distribution] = ks_pval
        else:
            self._fitted_errors[distribution] = np.inf
            self._aic[distribution] = np.inf
            self._bic[distribution] = np.inf
            self._kldiv[distribution] = np.inf
    self.df_errors = pd.DataFrame({'sumsquare_error': self._fitted_errors, 'aic': self._aic, 'bic': self._bic, 'kl_div': self._kldiv, 'ks_statistic': self._ks_stat, 'ks_pvalue': self._ks_pval})
    self.df_errors.sort_index(inplace=True)",values is not None,181,distribution in self.distributions,False,0.0,N/A
"def run(self):
<mask>:
        log.info('running [yarn install]')
        run(['yarn', 'install'], cwd=repo_root)
        log.info('installing webpack')
        run(['npm', 'i', '-D', 'webpack'], cwd=repo_root)
        run(['npm', 'i', 'webpack-cli', 'html-webpack-plugin', 'xterm', 'xterm-addon-attach', 'xterm-addon-search', 'xterm-addon-web-links', 'xterm-addon-fit'], cwd=repo_root)
        log.info('running webpack')
        run(['npm', 'run', 'webpack'], cwd=repo_root)",not osp.isdir(COMPONENTS),32,self.is_installed(),False,8.643019616048525,N/A
"def make_distribution(self):
<mask>:
        print(""\nWARNING: Server components are missing!! We can't proceed further!\n"")
        sys.exit(1)
    return sdist.make_distribution(self)",not osp.isdir(COMPONENTS),15,not self.is_server,False,8.170609724417774,N/A
"def get_version(module='spyder_terminal'):
    """"""Get version.""""""
    with open(os.path.join(HERE, module, '__init__.py'), 'r') as f:
        data = f.read()
    lines = data.split('\n')
    for line in lines:
<mask>:
            version_tuple = ast.literal_eval(line.split('=')[-1].strip())
            version = '.'.join(map(str, version_tuple))
            break
    return version",line.startswith('VERSION_INFO'),32,line.startswith('__version__ = '),False,21.401603033752977,N/A
"def main():
    """"""
    Run pytest tests.
    """"""
    errno = pytest.main(['-x', 'spyder_terminal', '-v', '-rw', '--durations=10', '--cov=spyder_terminal', '--cov-report=term-missing', '--timeout=60'])
<mask>:
        raise SystemExit(errno)",errno != 0,20,errno != 0,True,100.00000000000004,N/A
"def setup_page(self):
    """"""Create configuration page.""""""
    options_layout = QGridLayout()
    shell_group = QGroupBox(_('Terminal shell'))
    shell_layout = QVBoxLayout()
<mask>:
        self.shells = WINDOWS_SHELLS
    else:
        self.shells = UNIX_SHELLS
    valid_shells = []
    for shell in self.shells:
        if find_program(shell) is not None:
            valid_shells.append(shell)
    valid_shells = zip(valid_shells, valid_shells)
    if WINDOWS:
        default_option = 'cmd'
    elif sys.platform.startswith('linux'):
        default_option = 'bash'
    else:
        mac_ver = LooseVersion(platform.mac_ver()[0])
        if mac_ver >= LooseVersion('10.15.0'):
            default_option = 'zsh'
        else:
            default_option = 'bash'
    shell_combo = self.create_combobox(_('Select the shell interpreter:'), valid_shells, 'shell', restart=True, default=default_option)
    shell_combo.combobox.setMinimumContentsLength(15)
    shell_layout.addWidget(shell_combo)
    shell_group.setLayout(shell_layout)
    shell_layout.addStretch(1)
    options_layout.addWidget(shell_group)
    display_group = QGroupBox(_('Terminal display preferences'))
    display_layout = QVBoxLayout()
    self.buffer_sb = self.create_spinbox(_('Buffer limit: '), '', 'buffer_limit', min_=100, default=1000, max_=1000000, step=1)
    display_layout.addWidget(self.buffer_sb)
    display_group.setLayout(display_layout)
    display_layout.addStretch(1)
    options_layout.addWidget(display_group)
    terminal_group = QGroupBox(_('Terminal style preferences'))
    cursor_choices = [(_('Block'), 0), (_('Underline'), 1), (_('Bar'), 2)]
    self.cursor_combo = self.create_combobox(_('Type of cursor:'), cursor_choices, 'cursor_type')
    self.cursor_combo.combobox.setMinimumContentsLength(15)
    options_layout.addWidget(self.cursor_combo)
    self.sound_cb = self.create_checkbox(_('Enable bell sound'), 'sound', tip=_('Enable bell sound on terminal'))
    options_layout.addWidget(self.sound_cb)
    self.cursor_blink_cb = self.create_checkbox(_('Enable cursor blink'), 'cursor_blink', tip=_('_Enable cursor blink on terminal'))
    options_layout.addWidget(self.cursor_blink_cb)
    terminal_group.setLayout(options_layout)
    layout = QVBoxLayout()
    layout.addWidget(shell_group)
    layout.addWidget(display_group)
    layout.addWidget(terminal_group)
    layout.addStretch(1)
    self.setLayout(layout)",WINDOWS,158,sys.platform.startswith('win'),False,0.0,N/A
"def check_compatibility(self):
    """"""Check if current Qt backend version is greater or equal to 5.""""""
    message = ''
    valid = True
<mask>:
        try:
            import winpty
            del winpty
        except:
            message = _(""Unfortunately, the library that <b>spyder-terminal</b> uses to create terminals is failing to work in your system. Therefore, this plugin will be deactivated.<br><br> This usually happens on Windows 7 systems. If that's the case, please consider updating to a newer Windows version."")
            valid = False
    return (valid, message)",WINDOWS,76,sys.platform.startswith('win'),False,0.0,N/A
"def test_terminal_cwd(setup_terminal, qtbot_module):
    """"""Test if the a new terminal supports cwd  with especial characters.""""""
    start_dir = os.getcwd()
    new_dir = osp.join(start_dir, 'this is dir with spaces')
<mask>:
        os.mkdir(new_dir)
    os.chdir(new_dir)
    terminal = setup_terminal
    qtbot_module.waitUntil(lambda: terminal.get_widget().server_is_ready(), timeout=TERM_UP)
    qtbot_module.wait(1000)
    port = terminal.get_widget().port
    status_code = requests.get('http://127.0.0.1:{}'.format(port)).status_code
    assert status_code == 200
    os.chdir(start_dir)",not osp.exists(new_dir),46,not osp.exists(new_dir),True,100.00000000000004,N/A
"def gen_routes(close_future):
    """"""Return a list of HTML redirection routes.""""""
<mask>:
        ws = []
        for route in WS:
            ws.append((route[0], route[1], dict(close_future=close_future)))
        return REST + ws + WEB
    return ROUTES",close_future is not None,28,close_future,False,36.78794411714425,N/A
"@flaky(max_runs=3)
@pytest.mark.timeout(10)
@testing.gen_test
@pytest.mark.skipif(os.name == 'nt', reason=""Doesn't work on Windows"")
def test_terminal_resize(self):
    """"""Test terminal resizing.""""""
    data = {'rows': '25', 'cols': '80'}
    response = (yield self.http_client.fetch(self.get_url('/api/terminals'), method='POST', body=urlencode(data)))
    pid = response.body.decode('utf-8')
    sock = (yield self._mk_connection(pid))
    _ = (yield sock.read_message())
    data = {'rows': '23', 'cols': '73'}
    response = (yield self.http_client.fetch(self.get_url('/api/terminals/{0}/size'.format(pid)), method='POST', body=urlencode(data)))
    sock.write_message('cd {0}{1}'.format(LOCATION_SLASH, LINE_END))
    python_bin = sys.executable or 'python'
    python_exec = python_bin + ' print_size.py' + LINE_END
    sock.write_message(python_exec)
    expected_size = '(73, 23)'
    msg = ''
    fail_retry = 50
    tries = 0
    while expected_size not in msg:
<mask>:
            break
        msg = (yield sock.read_message())
        tries += 1
    self.assertIn(expected_size, msg)
    yield self.close(sock)",tries == fail_retry,98,tries >= fail_retry,False,53.7284965911771,N/A
"def get_terminal_size():
    """"""
    Get width and height of console.

    Works on Linux, OS X, Windows and Cygwin
    Based on:
    http://stackoverflow.com/questions/566746/how-to-get-console-window-width-in-python
    """"""
    current_os = platform.system()
    tuple_xy = None
<mask>:
        tuple_xy = _get_terminal_size_windows()
        if tuple_xy is None:
            tuple_xy = _get_terminal_size_tput()
    if current_os in ['Linux', 'Darwin'] or current_os.startswith('CYGWIN'):
        tuple_xy = _get_terminal_size_linux()
    if tuple_xy is None:
        print('default')
        tuple_xy = (80, 25)
    return tuple_xy",current_os == 'Windows',59,"current_os in ['Windows', 'Windows'] or current_os.startswith('WINDOWS')",False,8.59076483566362,N/A
"def resize_to_smallest(self, rows, cols):
    """"""Set the terminal size to that of the smallest client dimensions.

        A terminal not using the full space available is much nicer than a
        terminal trying to use more than the available space, so we keep it
        sized to the smallest client.
        """"""
    minrows = mincols = 10001
<mask>:
        minrows = rows
    if cols is not None and cols < mincols:
        mincols = cols
    if minrows == 10001 or mincols == 10001:
        return
    rows, cols = self.ptyproc.getwinsize()
    if (rows, cols) != (minrows, mincols):
        self.ptyproc.setwinsize(minrows, mincols)",rows is not None and rows < minrows,89,rows is not None and rows < minrows,True,100.00000000000004,N/A
"@tornado.gen.coroutine
def client_disconnected(self, pid, socket):
    """"""Send terminal SIGHUP when client disconnects.""""""
    self.log.info('Websocket closed, sending SIGHUP to terminal.')
    term = self.consoles[pid]
    term.clients.remove(socket)
    try:
<mask>:
            term.kill()
            self.pty_read(term.ptyproc.fd)
            return
        term.killpg(signal.SIGHUP)
    except Exception:
        pass
    del self.consoles[pid]",WINDOWS,32,term.ptyproc,False,0.0,N/A
"def on_close(self):
    """"""Close console communication.""""""
    LOGGER.info('TTY Off!')
    LOGGER.info('WebSocket closed: {0}'.format(self.pid))
    self.application.term_manager.client_disconnected(self.pid, self)
<mask>:
        self.close_future.set_result('Done!')",self.close_future is not None,14,self.close_future,False,54.88116360940266,N/A
"def __init__(self, name, plugin, parent):
    """"""Widget constructor.""""""
    self.terms = []
    super().__init__(name, plugin, parent)
    self.tab_widget = None
    self.menu_actions = None
    self.server_retries = 0
    self.server_ready = False
    self.font = None
    self.port = select_port(default_port=8071)
    self.stdout_file = None
    self.stderr_file = None
<mask>:
        self.stdout_file = osp.join(os.getcwd(), 'spyder_terminal_out.log')
        self.stderr_file = osp.join(os.getcwd(), 'spyder_terminal_err.log')
    self.project_path = None
    self.current_file_path = None
    self.current_cwd = os.getcwd()
    self.closing = False
    self.main = parent
    self.find_widget = FindTerminal(self)
    self.find_widget.hide()
    layout = QVBoxLayout()
    self.tabwidget = Tabs(self, rename_tabs=True)
    self.tabwidget.currentChanged.connect(self.refresh_plugin)
    self.tabwidget.move_data.connect(self.move_tab)
    self.tabwidget.set_close_function(self.close_term)
    if hasattr(self.tabwidget, 'setDocumentMode') and (not sys.platform == 'darwin'):
        self.tabwidget.setDocumentMode(True)
    layout.addWidget(self.tabwidget)
    layout.addWidget(self.find_widget)
    self.setLayout(layout)
    css = qstylizer.style.StyleSheet()
    css.QTabWidget.pane.setValues(border=0)
    self.setStyleSheet(css.toString())
    self.__wait_server_to_start()",get_debug_level() > 0,93,"osp.isdir(osp.getcwd(), 'spyder_terminal_out.log')",False,5.10809933294318,N/A
"def get_focus_widget(self):
    """"""
        Set focus on current selected terminal.

        Return the widget to give focus to when
        this plugin's dockwidget is raised on top-level.
        """"""
    term = self.tabwidget.currentWidget()
<mask>:
        return term.view",term is not None,31,term is not None,True,100.00000000000004,N/A
"def setup(self):
    """"""Perform the setup of plugin's main menu and signals.""""""
    self.cmd = find_program(self.get_conf('shell'))
    server_args = [sys.executable, '-m', 'spyder_terminal.server', '--port', str(self.port), '--shell', self.cmd]
    self.server = QProcess(self)
    env = self.server.processEnvironment()
    for var in os.environ:
        env.insert(var, os.environ[var])
    self.server.setProcessEnvironment(env)
    self.server.errorOccurred.connect(self.handle_process_errors)
    self.server.setProcessChannelMode(QProcess.SeparateChannels)
<mask>:
        self.server.setStandardOutputFile(self.stdout_file)
        self.server.setStandardErrorFile(self.stderr_file)
    self.server.start(server_args[0], server_args[1:])
    self.color_scheme = self.get_conf('appearance', 'ui_theme')
    self.theme = self.get_conf('appearance', 'selected')
    menu = self.get_options_menu()
    new_terminal_toolbar_action = self.create_toolbutton(TerminalMainWidgetToolbarSections.New, text=_('Open a new terminal'), icon=self.create_icon('expand_selection'), triggered=lambda: self.create_new_term())
    self.add_corner_widget(TerminalMainWidgetCornerToolbar.NewTerm, new_terminal_toolbar_action)
    new_terminal_cwd = self.create_action(TerminalMainWidgetActions.NewTerminalForCWD, text=_('New terminal in current working directory'), tip=_('Sets the pwd at the current working directory'), triggered=lambda: self.create_new_term(), shortcut_context='terminal', register_shortcut=True)
    self.new_terminal_project = self.create_action(TerminalMainWidgetActions.NewTerminalForProject, text=_('New terminal in current project'), tip=_('Sets the pwd at the current project directory'), triggered=lambda: self.create_new_term(path=self.project_path))
    new_terminal_file = self.create_action(TerminalMainWidgetActions.NewTerminalForFile, text=_('New terminal in current Editor file'), tip=_('Sets the pwd at the directory that contains the current opened file'), triggered=lambda: self.create_new_term(path=self.current_file_path))
    rename_tab_action = self.create_action(TerminalMainWidgetActions.RenameTab, text=_('Rename terminal'), triggered=lambda: self.tab_name_editor())
    self.create_action(TerminalMainWidgetActions.Copy, text=_('Copy text'), icon=self.create_icon('editcopy'), shortcut_context='terminal', triggered=lambda: self.copy(), register_shortcut=True)
    self.create_action(TerminalMainWidgetActions.Paste, text=_('Paste text'), icon=self.create_icon('editpaste'), shortcut_context='terminal', triggered=lambda: self.paste(), register_shortcut=True)
    self.create_action(TerminalMainWidgetActions.Clear, text=_('Clear terminal'), shortcut_context='terminal', triggered=lambda: self.clear(), register_shortcut=True)
    self.create_action(TerminalMainWidgetActions.ZoomIn, text=_('Zoom in'), shortcut_context='terminal', triggered=lambda: self.increase_font(), register_shortcut=True)
    self.create_action(TerminalMainWidgetActions.ZoomOut, text=_('Zoom out'), shortcut_context='terminal', triggered=lambda: self.decrease_font(), register_shortcut=True)
    self.create_menu(TermViewMenus.Context)
    for item in [new_terminal_cwd, self.new_terminal_project, new_terminal_file]:
        self.add_item_to_menu(item, menu=menu, section=TerminalMainWidgetMenuSections.New)
    self.add_item_to_menu(rename_tab_action, menu=menu, section=TerminalMainWidgetMenuSections.TabActions)",self.stdout_file and self.stderr_file,185,"self.get_conf('appearance', 'ui_output')",False,8.054496384843702,N/A
"def update_actions(self):
    """"""Setup and update the actions in the options menu.""""""
<mask>:
        self.new_terminal_project.setEnabled(False)",self.project_path is None,13,self.new_terminal_project,False,15.619699684601283,N/A
"def setup_term(self):
    """"""Setup other terminal options after page has loaded.""""""
    print('\x00', end='')
    self.set_font(self.font)
    self.set_dir(self.initial_path)
    self.current_theme = self.set_theme({})
    self.set_scrollbar_style()
    options = self.get_conf_options()
    dict_options = {}
    for option in options:
        dict_options[option] = self.get_conf(option)
    self.apply_settings(dict_options)
    self.apply_zoom()
    shell_name = self.get_conf('shell')
<mask>:
        env_route = self.ENV_ROUTES[shell_name]
        for act_file in env_route:
            if os.path.exists(os.path.expanduser(act_file)):
                self.exec_cmd(f'source {act_file}')
                self.exec_cmd('clear')",os.name != 'nt',49,shell_name,False,10.122592925934278,N/A
"@on_conf_change(section='appearance')
def set_theme(self, _values):
    """"""Set theme for the terminal.""""""
    supported_themes = ANSI_COLORS
    new_theme = {}
    theme = self.get_conf('selected', section='appearance')
    color_scheme = self.get_conf('ui_theme', section='appearance')
<mask>:
        theme = 'spyder' if color_scheme == 'light' else 'spyder/dark'
    new_theme['background'] = self.get_conf('{}/background'.format(theme), section='appearance')
    new_theme['foreground'] = self.get_conf('{}/normal'.format(theme), section='appearance')[0]
    new_theme['cursor'] = self.get_conf('{}/normal'.format(theme), section='appearance')[0]
    new_theme['cursorAccent'] = self.get_conf('{}/ctrlclick'.format(theme), section='appearance')
    new_theme['selection'] = self.get_conf('{}/occurrence'.format(theme), section='appearance')
    theme_colors = ANSI_COLORS[theme]
    for color in theme_colors:
        new_theme[color] = theme_colors[color]
    self.eval_javascript('setOption(""{}"", {})'.format('theme', new_theme))
    self.set_conf('fontFamily', self.get_conf('font/family', section='appearance'))
    return new_theme",theme not in supported_themes,71,theme not in supported_themes,True,100.00000000000004,N/A
"def __alive_loopback(self):
    alive = self.is_alive()
<mask>:
        self.terminal_closed.emit()
    else:
        QTimer.singleShot(250, self.__alive_loopback)",not alive,10,alive,False,36.78794411714425,N/A
"def apply_zoom(self):
    zoom = self.get_conf('zoom')
<mask>:
        for __ in range(0, zoom):
            self.view.increase_font(new_term=True)
    if zoom < 0:
        for __ in range(0, -zoom):
            self.view.decrease_font(new_term=True)",zoom > 0,22,zoom > 0,True,100.00000000000004,N/A
"def find(self, changed=True, forward=True, rehighlight=False, start_highlight_timer=False, multiline_replace_check=False):
    """"""Call the find function""""""
    text = self.search_text.currentText()
<mask>:
        self.search_text.lineEdit().setStyleSheet('')
        return None
    else:
        case = self.case_button.isChecked()
        word = self.words_button.isChecked()
        regexp = self.re_button.isChecked()
        if forward:
            found = self.parent.search_next(text, case=case, regex=regexp, word=word)
        else:
            found = self.parent.search_previous(text, case=case, regex=regexp, word=word)
        found = False if found == -1 else True
        stylesheet = self.STYLE[found]
        tooltip = self.TOOLTIP[found]
        if not found and regexp:
            error_msg = regexp_error_msg(text)
            if error_msg:
                stylesheet = self.STYLE['regexp_error']
                tooltip = self.TOOLTIP['regexp_error'] + ': ' + error_msg
        self.search_text.lineEdit().setStyleSheet(stylesheet)
        self.search_text.setToolTip(tooltip)
        return found",len(text) == 0,83,not changed or not multiline_replace_check,False,0.0,N/A
"def _canvas_grab_run(self):
    config = self._config
    canvas = config.endpoint.login()
    user = canvas.get_current_user()
    self._model.on_update_login_user.emit(str(user))
    courses = list(canvas.get_courses())
    available_courses, not_available = canvas_grab.utils.filter_available_courses(courses)
    filtered_courses = config.course_filter.get_filter().filter_course(available_courses)
    total_course_count = len(courses)
    not_available_count = len(not_available)
    filtered_count = len(available_courses) - len(filtered_courses)
    self._model.on_done_fetching_courses.emit(f'您已经以 {user} 身份登录。共有 {total_course_count} 门课程需同步，其中 {not_available_count} 门无法访问，{filtered_count} 门已被过滤。')
    course_name_parser = canvas_grab.course_parser.CourseParser()
    for idx, course in enumerate(filtered_courses):
        course_name = course.name
        self._model.on_new_course_in_progress.emit(f'({idx + 1}/{len(filtered_courses)}) {course_name} (ID: {course.id})')
        parsed_name = course_name_parser.get_parsed_name(course)
        print(f""  Download to {colored(parsed_name, 'cyan')}"")
        on_disk_path = f'{config.download_folder}/{parsed_name}'
        on_disk_snapshot = canvas_grab.snapshot.OnDiskSnapshot(on_disk_path).take_snapshot()
        mode, canvas_snapshots = config.organize_mode.get_snapshots(course)
        canvas_snapshot = {}
        for canvas_snapshot_obj in canvas_snapshots:
            try:
                for progress_item in canvas_snapshot_obj.yield_take_snapshot():
                    progress, status_text, progress_text = progress_item
                    self._model.on_snapshot_in_progress.emit(progress, status_text, progress_text)
                canvas_snapshot = canvas_snapshot_obj.get_snapshot()
            except ResourceDoesNotExist:
                print(colored(f'{mode} not supported, falling back to alternative mode', 'yellow'))
                continue
            break
        planner = canvas_grab.planner.Planner(config.organize_mode.delete_file)
        plans = planner.plan(canvas_snapshot, on_disk_snapshot, config.file_filter)
        print(colored(f'  Updating {len(plans)} objects '))
        transfer = canvas_grab.transfer.Transfer()
        transfer_task = transfer.yield_transfer(on_disk_path, f'{config.download_folder}/_canvas_grab_archive', plans)
        for progress_item in transfer_task:
            progress, status_text, progress_text = progress_item
            self._model.on_download_in_progress.emit(progress, status_text, progress_text)
        self._model.on_finish_course.emit(f'{course_name} (ID: {course.id})', f'更新了 {len(plans)} 个文件。(远程 {len(canvas_snapshot)} -> 本地 {len(on_disk_snapshot)})')
<mask>:
        canvas_grab.version.check_latest_version()",not self._noupdate,157,mode == 'organize',False,0.0,N/A
"def main(self):
    init()
    try:
        _, self._noupdate, self._config = canvas_grab.get_options.get_options()
    except TypeError:
        return
    app = QGuiApplication(sys.argv)
    app.setQuitOnLastWindowClosed(True)
    app.aboutToQuit.connect(self._exit_handler)
    engine = QQmlApplicationEngine()
    engine.rootContext().setContextProperty('py_sync_model', self._model)
    engine.load(os.path.join(os.path.dirname(__file__), 'ui/main.qml'))
<mask>:
        sys.exit(-1)
    thread = threading.Thread(target=self._canvas_grab_run)
    thread.start()
    sys.exit(app.exec_())",not engine.rootObjects(),31,engine.waitForShutdown(),False,24.736929544091932,N/A
"def data(self, index, role=Qt.DisplayRole):
    row = index.row()
<mask>:
        return self.items[row]['name']
    if role == SyncModel.StatusRole:
        return self.items[row]['status']
    if role == SyncModel.StatusTextRole:
        return self.items[row]['status_text']
    if role == SyncModel.ProgressTextRole:
        return self.items[row]['progress_text']
    if role == SyncModel.ProgressRole:
        return self.items[row]['progress']
    if role == SyncModel.IconNameRole:
        return self.items[row]['icon_name']",role == SyncModel.NameRole,40,role == SyncModel.NameRole,True,100.00000000000004,N/A
"@Slot(float, str, str)
def snapshot_in_progress(self, progress, status_text, progress_text):
    self.beginResetModel()
<mask>:
        self.items[1]['status_text'] = status_text
    if progress_text is not None:
        self.items[1]['progress_text'] = progress_text
    if progress is not None:
        self.items[1]['progress'] = progress
    self.endResetModel()",status_text is not None,30,status_text is not None,True,100.00000000000004,N/A
"@Slot(float, str, str)
def download_in_progress(self, progress, status_text, progress_text):
    self.beginResetModel()
<mask>:
        self.items[1]['status_text'] = status_text
    if progress_text != '':
        self.items[1]['progress_text'] = progress_text
    if progress > 0:
        self.items[1]['progress'] = progress
    self.endResetModel()",status_text != '',28,status_text != '',True,100.00000000000004,N/A
"def plan(self, snapshot_from, snapshot_to, file_filter):
    """"""plan a transfer

        Args:
            snapshot_from (dict): source snapshot
            snapshot_to (dict): target snapshot
            file_filter (canvas_grab.file_filter.FileFilter): file filter

        Returns:
            transfer plan
        """"""
    snapshot_from_filter = file_filter.filter_files(snapshot_from)
    plans = []
    for key, from_item in snapshot_from.items():
<mask>:
            plans.append(('ignore', key, from_item))
        elif key not in snapshot_to:
            plans.append(('add', key, from_item))
        else:
            to_item = snapshot_to[key]
            if isinstance(from_item, SnapshotFile):
                if to_item.size != from_item.size or to_item.modified_at != from_item.modified_at:
                    plans.append(('update', key, from_item))
            if isinstance(from_item, SnapshotLink):
                content_length = len(from_item.content().encode('utf-8'))
                if to_item.size != content_length:
                    plans.append(('update', key, from_item))
    for key, to_item in snapshot_to.items():
        if key not in snapshot_from_filter:
            if self.remove_local_file:
                plans.append(('delete', key, to_item))
            else:
                plans.append(('try-remove', key, to_item))
    return plans",key not in snapshot_from_filter,100,key in snapshot_from_filter,False,72.89545183625967,N/A
"def get_parsed_name(self, course):
    r = re.search('\\((?P<semester_id>[0-9\\-]+)\\)-(?P<sjtu_id>[A-Za-z0-9]+)-(?P<classroom_id>.+)-(?P<name>.+)\\Z', course.course_code)
<mask>:
        r = r.groupdict()
    else:
        return normalize_path(course.name)
    if hasattr(course, 'original_name'):
        course_name = course.original_name
        course_nickname = course.name
    else:
        course_name = course.name
        course_nickname = course.name
    template_map = {'{CANVAS_ID}': str(course.id), '{SJTU_ID}': r.get('sjtu_id', ''), '{SEMESTER_ID}': r.get('semester_id', ''), '{CLASSROOM_ID}': r.get('classroom_id', ''), '{NAME}': normalize_path(course_name.replace('（', '(').replace('）', ')'), file_regex), '{NICKNAME}': normalize_path(course_nickname.replace('（', '(').replace('）', ')'), file_regex), '{COURSE_CODE}': course.course_code}
    folder_name = '{SJTU_ID}-{NAME}'
    for old, new in template_map.items():
        folder_name = folder_name.replace(old, new)
    folder_name = normalize_path(folder_name)
    return folder_name",r is not None,72,r,False,4.9787068367863965,N/A
"def check_latest_version():
    version_obj = {}
    print()
    try:
        version_obj = requests.get(GITHUB_RELEASE_URL, timeout=3).json()
    except Exception as e:
        print(f""{colored('Failed to check update.', 'red')} It's normal if you don't have a stable network connection."")
        print(f'You may report the following message to developer: {e}')
        return
    version = version_obj.get('tag_name', 'unknown')
<mask>:
        print('Failed to check update: unknown remote version')
    elif ver_parser.parse(version) > ver_parser.parse(VERSION):
        print(f""You're using version {colored(VERSION, 'green')}, but the latest release is {colored(version, 'green')}."")
        print(f""Please visit {colored('https://github.com/skyzh/canvas_grab/releases', 'blue')} to download the latest version."")
        print()
        print(version_obj.get('body', ''))
        print()
    elif ver_parser.parse(version) < ver_parser.parse(VERSION):
        print(""Just checked update. You're using development version of canvas_grab. :)"")
    else:
        print(""Just checked update. You're using latest version of canvas_grab. :)"")",version == 'unknown',106,ver_parser.parse(version) is None,False,4.196114906296549,N/A
"def get_tabs(self):
<mask>:
        self.cache['tabs'] = [tab.id for tab in self.course.get_tabs()]
    return self.cache['tabs']",'tabs' not in self.cache,12,'tabs' not in self.cache,True,100.00000000000004,N/A
"def get_files(self):
<mask>:
        return None
    if 'files' not in self.cache:
        self.cache['files'] = {file.id: file for file in self.course.get_files()}
    return self.cache['files']",'files' not in self.get_tabs(),20,not self.course,False,7.888842466409752,N/A
"def get_folders(self):
<mask>:
        return None
    if 'folders' not in self.cache:
        self.cache['folders'] = {folder.id: folder for folder in self.course.get_folders()}
    return self.cache['folders']",'files' not in self.get_tabs(),20,not self.course,False,7.888842466409752,N/A
"def get_file(self, file_id):
    files = self.get_files()
<mask>:
        return self.course.get_file(file_id)
    else:
        return files.get(file_id, self.course.get_file(file_id))",files is None,13,not files,False,30.326532985631665,N/A
"def get_modules(self):
<mask>:
        return None
    if 'modules' not in self.cache:
        self.cache['modules'] = {module.id: module for module in self.course.get_modules()}
    return self.cache['modules']",'modules' not in self.get_tabs(),20,not self.course,False,7.888842466409752,N/A
"def filter_files(self, snapshot):
<mask>:
        return snapshot
    allowed = self.allowed_extensions()
    return {k: v for k, v in snapshot.items() if any(map(lambda ext: k.endswith(ext), allowed)) or isinstance(v, SnapshotLink)}",'All' in self.allowed_group,25,not self.allowed_extensions(),False,34.57207846419409,N/A
"def interact(self):
    choices = []
    for key, group in FILE_GROUP.items():
        choices.append(questionary.Choice(f""{key} ({', '.join(group)})"", key, checked=key in self.allowed_group))
    choices.append(questionary.Choice(f'Allow all', 'All', checked='All' in self.allowed_group))
    choices.append(questionary.Choice(f'Custom', 'custom', disabled='Please set extra allowed extensions in `allowed_extra` config'))
    while True:
        self.allowed_group = questionary.checkbox('Select allowed extensions', choices).unsafe_ask()
<mask>:
            print('At least one extension group must be selected.')
        elif 'All' in self.allowed_group and len(self.allowed_group) != 1:
            print('Invalid choice.')
        else:
            break",len(self.allowed_group) == 0,62,'Custom' in self.allowed_group and len(self.allowed_group) == 0,False,54.017258985951415,N/A
"def yield_transfer(self, base_path, archive_base_path, plans):
    yield (None, '传输文件中...', None)
    for idx, (op, key, plan) in enumerate(plans):
        path = f'{base_path}/{key}'
        archive_path = f'{archive_base_path}/{path}'
<mask>:
            self.create_parent_folder(path)
            file_obj = Path(path)
            if file_obj.exists():
                self.create_parent_folder(archive_path)
                file_obj.replace(archive_path)
            if plan.url == '':
                print(f""  {colored('? (not available)', 'yellow')} {key}"")
                continue
            if isinstance(plan, SnapshotFile):
                download_file_task = download_file(plan.url, f'({idx + 1}/{len(plans)}) ' + truncate_name(plan.name), path, plan.size)
                for progress in download_file_task:
                    yield (self.sub_transfer_progress(idx, len(plans), progress), None, None)
                apply_datetime_attr(path, plan.created_at, plan.modified_at)
            elif isinstance(plan, SnapshotLink):
                Path(path).write_text(plan.content(), encoding='utf-8')
            else:
                print(colored('Unsupported snapshot type', 'red'))
        if op == 'delete':
            file_obj = Path(path)
            if file_obj.exists():
                self.create_parent_folder(archive_path)
                file_obj.rename(archive_path)
        if op == 'add':
            print(f""  {colored('+', 'green')} {key}"")
            yield (None, None, f'下载 {key}')
        if op == 'update':
            print(f""  {colored('=', 'green')} {key}"")
            yield (None, None, f'更新 {key}')
        if op == 'delete':
            print(f""  {colored('-', 'yellow')} {key}"")
            yield (None, None, f'删除 {key}')
        if op == 'ignore':
            print(f""  {colored('? (ignored)', 'yellow')} {key}"")
            yield (None, None, f'忽略 {key}')
        if op == 'try-remove':
            print(f""  {colored('? (not on remote)', 'yellow')} {key}"")
            yield (None, None, f'忽略 {key}')
    self.clean_tree(base_path)",op == 'add' or op == 'update',160,op == 'create',False,17.035677145427364,N/A
"def clean_tree(self, path) -> bool:
    """"""Remove empty folder recursively.
        Returns True if folder is deleted.
        """"""
    path = Path(path)
<mask>:
        return True
    children = cleaned_children = list(path.glob('*'))
    for child_idx in reversed(range(len(children))):
        if children[child_idx].is_dir() and self.clean_tree(children[child_idx]):
            del cleaned_children[child_idx]
    if not cleaned_children:
        path.rmdir()
        return True
    return False",not path.is_dir(),45,not path.is_dir(),True,100.00000000000004,N/A
"def summarize_courses(courses, number=5):
    joins = []
    for idx, course in enumerate(courses):
<mask>:
            break
        joins.append(course.name)
    if len(courses) > number:
        joins.append('...')
    return ', '.join(joins) + f'  ({len(courses)} courses)'",idx >= number,26,idx > number,False,45.13864405503391,N/A
"def filter_available_courses(courses):
    available_courses = []
    not_available_courses = []
    for course in courses:
<mask>:
            available_courses.append(course)
        else:
            not_available_courses.append(course)
    return (available_courses, not_available_courses)","hasattr(course, 'name')",19,course.is_available(),False,8.643019616048525,N/A
"def apply_datetime_attr(path, c_time: int, m_time: int):
    a_time = time()
<mask>:
        setctime(path, c_time)
    os.utime(path, (a_time, m_time))",is_windows(),15,c_time != 0,False,8.116697886877475,N/A
"def truncate_name(name, length=40):
<mask>:
        return name[:length - 3] + '...'
    else:
        return name",len(name) > length,13,len(name) > length,True,100.00000000000004,N/A
"def find_choice(choices, value):
    for choice in choices:
<mask>:
            return choice
    return None",choice.value == value,12,choice == value,False,38.75385825373298,N/A
"def get_options():
    parser = argparse.ArgumentParser(description='Grab all files on Canvas LMS to local directory.', epilog='Configuration file variables specified with program arguments will override the original settings at runtime, but will not be written to the original configuration file. If you specify a configuration file with the --config-file argument when you configure it, it will be overwritten with the new content.')
    interactive_group = parser.add_mutually_exclusive_group()
    interactive_group.add_argument('-i', '--interactive', dest='interactive', action='store_true', default=True, help='Set the program to run in interactive mode (default action)')
    interactive_group.add_argument('-I', '--non-interactive', '--no-input', dest='interactive', action='store_false', default=True, help='Set the program to run in non-interactive mode. This can be used to exit immediately in case of profile corruption without getting stuck with the input.')
    parser.add_argument('-r', '--reconfigure', '--configure', dest='reconfigure', help='Reconfigure the tool.', action='store_true')
    parser.add_argument('-o', '--download-folder', '--output', dest='download', help='Specify alternative download folder.')
    parser.add_argument('-c', '--config-file', dest='config_file', default='config.toml', help='Specify alternative configuration file.')
    parser.add_argument('--version', action='version', version=VERSION)
    parser.add_argument('-k', '--keep-version', '--no-update', dest='noupdate', action='store_true', default=False, help='Skip update checking. This will be helpful without a stable network connection and prevent reconfiguration.')
    args = parser.parse_args()
    greeting()
    print(f'Using config {args.config_file}')
    config_file = Path(args.config_file)
    config = Config()
    config_fail = False
<mask>:
        try:
            config.from_config(toml.loads(config_file.read_text(encoding='utf8')))
        except KeyError as e:
            print(f""It seems that you have upgraded canvas_grab. Please reconfigure. ({colored(e, 'red')} not found)"")
            config_fail = True
    if config_fail or args.reconfigure or (not config_file.exists()):
        if not args.interactive:
            print('configuration file corrupted or not exist, and non interactive flag is set. Quit immediately.')
            exit(-1)
        try:
            config.interact()
        except KeyboardInterrupt:
            print('User canceled the configuration process')
            return
        config_file.write_text(toml.dumps(config.to_config()), encoding='utf8')
    if args.download:
        config.download_folder = args.download
    return (args.interactive, args.noupdate, config)",config_file.exists(),242,config_file.exists(),True,100.00000000000004,N/A
"def download_file(url, desc, filename, file_size, verbose=False, req_timeout=(5, None)):
    with requests.get(url, stream=True, timeout=req_timeout) as r:
        r.raise_for_status()
        chunk_size = 1024
<mask>:
            print('size = %d, url = %s' % (file_size, url))
        download_size = 0
        with open(filename + '.canvas_tmp', 'wb') as fp:
            with tqdm(total=file_size, unit='B', unit_scale=True, unit_divisor=1024, desc=desc, bar_format='{l_bar}{bar}{r_bar}', ascii=is_windows(), leave=False) as pbar:
                lst_update = current_milli_time()
                for chunk in r.iter_content(chunk_size=chunk_size):
                    fp.write(chunk)
                    download_size += len(chunk)
                    current_time = current_milli_time()
                    if current_time - lst_update > 100:
                        yield (float(download_size) / file_size)
                        lst_update = current_time
                    pbar.update(len(chunk))
        if download_size != file_size:
            raise Exception(f'Incomplete file: expected {file_size}, downloaded {download_size}')
        os.replace(filename + '.canvas_tmp', filename)
    return",verbose,93,verbose,True,100.00000000000004,N/A
"def main():
    init()
    try:
        interactive, noupdate, config = canvas_grab.get_options.get_options()
    except TypeError:
        return
    canvas = config.endpoint.login()
    print(f""You are logged in as {colored(canvas.get_current_user(), 'cyan')}"")
    courses = list(canvas.get_courses())
    available_courses, not_available = canvas_grab.utils.filter_available_courses(courses)
    filtered_courses = config.course_filter.get_filter().filter_course(available_courses)
    total_course_count = len(courses)
    not_available_count = len(not_available)
    filtered_count = len(available_courses) - len(filtered_courses)
    print(colored(f'{total_course_count} courses in total, {not_available_count} not available, {filtered_count} filtered', 'cyan'))
    course_name_parser = canvas_grab.course_parser.CourseParser()
    for idx, course in enumerate(filtered_courses):
        course_name = course.name
        print(f""({idx + 1}/{len(filtered_courses)}) Course {colored(course_name, 'cyan')} (ID: {course.id})"")
        parsed_name = course_name_parser.get_parsed_name(course)
        print(f""  Download to {colored(parsed_name, 'cyan')}"")
        on_disk_path = f'{config.download_folder}/{parsed_name}'
        on_disk_snapshot = canvas_grab.snapshot.OnDiskSnapshot(on_disk_path).take_snapshot()
        mode, canvas_snapshots = config.organize_mode.get_snapshots(course)
        canvas_snapshot = {}
        for canvas_snapshot_obj in canvas_snapshots:
            try:
                canvas_snapshot = canvas_snapshot_obj.take_snapshot()
            except ResourceDoesNotExist:
                print(colored(f'{mode} not supported, falling back to alternative mode', 'yellow'))
                continue
            break
        planner = canvas_grab.planner.Planner(config.organize_mode.delete_file)
        plans = planner.plan(canvas_snapshot, on_disk_snapshot, config.file_filter)
        print(colored(f'  Updating {len(plans)} objects ({len(canvas_snapshot)} remote objects -> {len(on_disk_snapshot)} local objects)'))
        transfer = canvas_grab.transfer.Transfer()
        transfer.transfer(on_disk_path, f'{config.download_folder}/_canvas_grab_archive', plans)
<mask>:
        canvas_grab.version.check_latest_version()",not noupdate,141,not available_courses,False,15.97357760615681,N/A
"def add_to_snapshot(self, key, value):
    """"""Add a key-value pair into snapshot. If duplicated, this function will report error and ignore the pair.

        Args:
            key (str): key or path of the object
            value (any): content of the object
        """"""
<mask>:
        print(colored(f'  Duplicated file found: {key}, please download it using web browser.', 'yellow'))
        return
    self.snapshot[key] = value",key in self.snapshot,54,key in self.snapshot,True,100.00000000000004,N/A
"def yield_take_snapshot(self):
    course = self.course
    request_batcher = RequestBatcher(course)
    yield (0, '请稍候', '正在获取文件列表')
    files = request_batcher.get_files()
<mask>:
        raise ResourceDoesNotExist('File tab is not supported.')
    folders = request_batcher.get_folders()
    for _, file in files.items():
        folder = normalize_path(folders[file.folder_id].full_name) + '/'
        if folder.startswith('course files/'):
            folder = folder[len('course files/'):]
        snapshot_file = from_canvas_file(file)
        filename = f'{folder}{normalize_path(snapshot_file.name, file_regex)}'
        self.add_to_snapshot(filename, snapshot_file)
    print(f'  {len(files)} files in total')
    yield (0.1, None, f'共 {len(files)} 个文件')
    if self.with_link:
        yield (None, '正在解析链接', None)
        pages = request_batcher.get_pages() or []
        for page in pages:
            key = f'pages/{normalize_path(page.title, file_regex)}.html'
            value = SnapshotLink(page.title, page.html_url, 'Page')
            self.add_to_snapshot(key, value)
        print(f'  {len(pages)} pages in total')
        yield (0.2, '请稍候', f'共 {len(pages)} 个链接')",files is None,99,not files,False,30.326532985631665,N/A
"def yield_take_snapshot(self):
    course = self.course
    request_batcher = RequestBatcher(course)
    accessed_files = []
    yield (0, '请稍候', '正在获取模块列表')
    modules = (request_batcher.get_modules() or {}).items()
    download_idx = 0
    for _, module in modules:
        name = re.sub(file_regex, '_', module.name)
        name = ' '.join(name.split())
        idx = str(module.position)
        module_name = f'{idx} {name}'
        module_item_count = module.items_count
        print(f""  Module {colored(module_name, 'cyan')} ({module_item_count} items)"")
        yield (download_idx / len(modules) * 0.2, '正在获取模块列表', f'{module_name} (包含 {module_item_count} 个对象)')
        download_idx += 1
        for item in module.get_module_items():
<mask>:
                file_id = item.content_id
                snapshot_file = from_canvas_file(request_batcher.get_file(file_id))
                accessed_files.append(file_id)
                filename = f'{module_name}/{normalize_path(snapshot_file.name, file_regex)}'
                self.add_to_snapshot(filename, snapshot_file)
            if self.with_link:
                if item.type == 'ExternalUrl' or item.type == 'Page':
                    key = f'{module_name}/{normalize_path(item.title, file_regex)}.html'
                    value = SnapshotLink(item.title, item.html_url, module_name)
                    self.add_to_snapshot(key, value)
    files = request_batcher.get_files()
    if files:
        unmoduled_files = 0
        for file_id, file in files.items():
            if file_id not in accessed_files:
                snapshot_file = from_canvas_file(file)
                filename = f'unmoduled/{normalize_path(snapshot_file.name, file_regex)}'
                self.add_to_snapshot(filename, snapshot_file)
                unmoduled_files += 1
        print(f""  {colored('Unmoduled files', 'cyan')} ({unmoduled_files} items)"")
        yield (0.2, '正在获取模块列表', f'还有 {unmoduled_files} 个不在模块中的文件')",item.type == 'File',148,item.content_id,False,17.491650626361256,N/A
"def take_snapshot(self):
    """"""Take an on-disk snapshot

        Returns:
            dict: snapshot on disk. All objects are of type `SnapshotFile`.
        """"""
    base = Path(self.base_path)
    for item in base.rglob('*'):
<mask>:
            stat = item.stat()
            self.snapshot[item.relative_to(base).as_posix()] = SnapshotFile(item.name, stat.st_size, int(stat.st_mtime))
    return self.snapshot",item.is_file() and (not item.name.startswith('.')) and (not item.name.endswith('.canvas_tmp')),36,item.is_file(),False,1.3763786733050407,N/A
"def from_config(self, config):
    final_err = None
    self.download_folder, err = self.try_from_config(lambda: config.get('download_folder', self.download_folder))
    final_err = final_err or err
    _, err = self.try_from_config(lambda: self.endpoint.from_config(config['endpoint']))
    final_err = final_err or err
    _, err = self.try_from_config(lambda: self.organize_mode.from_config(config['organize_mode']))
    final_err = final_err or err
    _, err = self.try_from_config(lambda: self.course_filter.from_config(config['course_filter']))
    final_err = final_err or err
    _, err = self.try_from_config(lambda: self.file_filter.from_config(config['file_filter']))
    final_err = final_err or err
<mask>:
        raise final_err",final_err,60,final_err,True,100.00000000000004,N/A
"def get_snapshots(self, course):
<mask>:
        canvas_snapshot_module = CanvasModuleSnapshot(course, True)
    else:
        canvas_snapshot_module = CanvasModuleSnapshot(course)
    if self.mode == 'file_link':
        canvas_snapshot_file = CanvasFileSnapshot(course, True)
    else:
        canvas_snapshot_file = CanvasFileSnapshot(course)
    if self.mode == 'module' or self.mode == 'module_link':
        canvas_snapshots = [canvas_snapshot_module, canvas_snapshot_file]
    elif self.mode == 'file' or self.mode == 'file_link':
        canvas_snapshots = [canvas_snapshot_file, canvas_snapshot_module]
    else:
        raise CanvasGrabCliError(f'Unsupported organize mode {self.mode}')
    return (self.mode, canvas_snapshots)",self.mode == 'module_link',57,self.mode == 'module',False,54.44460596606694,N/A
"def get_name(course_filter):
<mask>:
        return 'term'
    if isinstance(course_filter, AllFilter):
        return 'all'
    if isinstance(course_filter, PerFilter):
        return 'per'
    return ''","isinstance(course_filter, TermFilter)",17,"isinstance(course_filter, TermFilter)",True,100.00000000000004,N/A
"def get_filter(self):
<mask>:
        return self.term_filter
    if self.filter_name == 'all':
        return self.all_filter
    if self.filter_name == 'per':
        return self.per_filter
    return None",self.filter_name == 'term',19,self.filter_name == 'term',True,100.00000000000004,N/A
"def filter_course(self, courses):
    terms = self.terms.copy()
<mask>:
        terms = [max(map(lambda course: course.enrollment_term_id, courses))]
        print(f'TermFilter: Select latest term {terms[0]}')
    return list(filter(lambda course: course.enrollment_term_id in terms, courses))",-1 in terms,25,self.verbose,False,0.0,N/A
"def interact(self, courses):
    groups = group_by(courses, lambda course: course.enrollment_term_id)
    choices = []
    for term, courses in groups.items():
        choices.append(questionary.Choice(f'Term {term}: {summarize_courses(courses)}', term, checked=term in self.terms))
    choices = sorted(choices, key=lambda choice: choice.value)
    choices.append(questionary.Choice('Latest term only', -1, checked=-1 in self.terms))
    choices.reverse()
    while True:
        self.terms = questionary.checkbox('Select terms to download', choices).unsafe_ask()
<mask>:
            print('At least one term must be selected.')
        elif -1 in self.terms and len(self.terms) != 1:
            print('Invalid choice')
        else:
            break",len(self.terms) == 0,67,self.terms is None,False,17.86690863748233,N/A
"def interact(self, courses):
    choices = []
    sorted_courses = sorted(courses, key=lambda course: course.enrollment_term_id)
    sorted_courses.reverse()
    for course in sorted_courses:
        choices.append(questionary.Choice(f'{course.name} (Term {course.enrollment_term_id})', course.id, checked=course.id in self.course_id))
    while True:
        self.course_id = questionary.checkbox('Select courses to download', choices).unsafe_ask()
<mask>:
            print('At least one course must be selected.')
        else:
            break",len(self.course_id) == 0,43,not self.course_id,False,33.02232277439296,N/A
"def maybe_update_state(ctx: click.Context) -> None:
    path_or_module = ctx.params.get('path_or_module')
<mask>:
        file_path = Path(path_or_module)
        if file_path.exists() and file_path.is_file():
            state.file = file_path
        else:
            if not re.fullmatch('[a-zA-Z_]\\w*(\\.[a-zA-Z_]\\w*)*', path_or_module):
                typer.echo(f'Not a valid file or Python module: {path_or_module}', err=True)
                sys.exit(1)
            state.module = path_or_module
    app_name = ctx.params.get('app')
    if app_name:
        state.app = app_name
    func_name = ctx.params.get('func')
    if func_name:
        state.func = func_name",path_or_module,53,path_or_module,True,100.00000000000004,N/A
"def get_typer_from_module(module: Any) -> Optional[typer.Typer]:
<mask>:
        obj = getattr(module, state.app, None)
        if not isinstance(obj, typer.Typer):
            typer.echo(f'Not a Typer object: --app {state.app}', err=True)
            sys.exit(1)
        return obj
    if state.func:
        func_obj = getattr(module, state.func, None)
        if not callable(func_obj):
            typer.echo(f'Not a function: --func {state.func}', err=True)
            sys.exit(1)
        sub_app = typer.Typer()
        sub_app.command()(func_obj)
        return sub_app
    local_names = dir(module)
    local_names_set = set(local_names)
    for name in default_app_names:
        if name in local_names_set:
            obj = getattr(module, name, None)
            if isinstance(obj, typer.Typer):
                return obj
    for name in local_names_set - set(default_app_names):
        obj = getattr(module, name)
        if isinstance(obj, typer.Typer):
            return obj
    for func_name in default_func_names:
        func_obj = getattr(module, func_name, None)
        if callable(func_obj):
            sub_app = typer.Typer()
            sub_app.command()(func_obj)
            return sub_app
    for func_name in local_names_set - set(default_func_names):
        func_obj = getattr(module, func_name)
        if callable(func_obj):
            sub_app = typer.Typer()
            sub_app.command()(func_obj)
            return sub_app
    return None",state.app,124,state.app,True,100.00000000000004,N/A
"def get_typer_from_state() -> Optional[typer.Typer]:
    spec = None
<mask>:
        module_name = state.file.name
        spec = importlib.util.spec_from_file_location(module_name, str(state.file))
    elif state.module:
        spec = importlib.util.find_spec(state.module)
    if spec is None:
        if state.file:
            typer.echo(f'Could not import as Python file: {state.file}', err=True)
        else:
            typer.echo(f'Could not import as Python module: {state.module}', err=True)
        sys.exit(1)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    obj = get_typer_from_module(module)
    return obj",state.file,53,state.file,True,100.00000000000004,N/A
"def maybe_add_run_to_cli(cli: click.Group) -> None:
<mask>:
        if state.file or state.module:
            obj = get_typer_from_state()
            if obj:
                obj._add_completion = False
                click_obj = typer.main.get_command(obj)
                click_obj.name = 'run'
                if not click_obj.help:
                    click_obj.help = 'Run the provided Typer app.'
                cli.add_command(click_obj)",'run' not in cli.commands,35,not cli.has_command('run'),False,11.339582221952005,N/A
"def print_version(ctx: click.Context, param: Option, value: bool) -> None:
<mask>:
        return
    typer.echo(f'Typer CLI version: {__version__}')
    raise typer.Exit()",not value or ctx.resilient_parsing,17,value,False,0.09118819655545167,N/A
"@app.command()
def hello(name: str='World', formal: bool=False):
    """"""
    Say hi
    """"""
<mask>:
        typer.echo(f'Good morning Ms. {name}')
    else:
        typer.echo(f'Hello {name}!')",formal,18,formal,True,100.00000000000004,N/A
"@app.command()
def bye(friend: bool=False):
    """"""
    Say bye
    """"""
<mask>:
        typer.echo('Goodbye my friend')
    else:
        typer.echo('Goodbye')",friend,14,friend,True,100.00000000000004,N/A
"def ui_width():
    """"""Return the Width of the UI Panel.

    Args:
        None.

    Returns:
        UI Width.
    """"""
    area = bpy.context.area
    resolution = bpy.context.preferences.system.ui_scale
    for reg in area.regions:
<mask>:
            region_width = reg.width
    return region_width",reg.type == 'UI',31,reg.resolution > resolution,False,17.491650626361256,N/A
"def draw(self, context):
    ui_cutoff = bpy.context.preferences.addons[__package__].preferences.pdt_ui_width
    layout = self.layout
    pdt_pg = context.scene.pdt_pg
    row = layout.row()
    row.label(text=f'Working {PDT_LAB_PLANE}:')
    row.prop(pdt_pg, 'plane', text='')
    row = layout.row()
    row.label(text=f'Move {PDT_LAB_MODE}:')
    row.prop(pdt_pg, 'select', text='')
    row = layout.row()
    row.prop(pdt_pg, 'extend', text='All Selected Entities (Off: Active Only)')
    row = layout.row()
    box_1 = row.box()
    row = box_1.row()
    row.label(text=f'(1) Select {PDT_LAB_OPERATION}:')
    row.prop(pdt_pg, 'operation', text='')
    row = box_1.row()
    box_1a = row.box()
    box_1a.label(text=f'(a) Either Set Coordinates + [Place »]')
    row = box_1a.row()
    box = row.box()
    row = box.row()
    split = row.split(factor=0.35, align=True)
    split.label(text=PDT_LAB_CVALUE)
    split.prop(pdt_pg, 'cartesian_coords', text='')
    row = box.row()
    row.operator('pdt.absolute', icon='EMPTY_AXIS', text=f'{PDT_LAB_ABS} »')
    row.operator('pdt.delta', icon='EMPTY_AXIS', text=f'{PDT_LAB_DEL} »')
    row = box_1a.row()
    box = row.box()
    row = box.row()
    row.prop(pdt_pg, 'distance', text=PDT_LAB_DISVALUE)
    row.prop(pdt_pg, 'angle', text=PDT_LAB_ANGLEVALUE)
    row = box.row()
    row.operator('pdt.distance', icon='EMPTY_AXIS', text=f'{PDT_LAB_DIR} »')
    row.operator('pdt.view_axis', icon='EMPTY_AXIS', text=f'{PDT_LAB_VIEW} »')
    row = box.row()
    row.prop(pdt_pg, 'flip_angle', text=PDT_LAB_FLIPANGLE)
    row = box_1.row()
    box_1b = row.box()
    box_1b.label(text='(b) Or Select |n| Entities + [Place »]')
    row = box_1b.row()
    row.operator('pdt.normal', text=f'|3| {PDT_LAB_NOR} »')
    row.operator('pdt.centre', text=f'|3| {PDT_LAB_ARCCENTRE} »')
    box = box_1b.box()
    row = box.row()
    row.operator('pdt.intersect', text=f'|4| {PDT_LAB_INTERSECT} »')
<mask>:
        row = box.row()
    row.prop(pdt_pg, 'object_order', text=PDT_LAB_ORDER)
    row = box_1b.row()
    box = row.box()
    box.label(text=f'Do (1) at % between selected points')
    row = box.row()
    row.operator('pdt.percent', text=f'|2| % »')
    row.prop(pdt_pg, 'percent', text=PDT_LAB_PERCENTS)
    if ui_width() < ui_cutoff:
        row = box.row()
    row.prop(pdt_pg, 'flip_percent', text=PDT_LAB_FLIPPERCENT)",ui_width() < ui_cutoff,202,pdt_pg.plane == 'active',False,4.8734989388136185,N/A
"def draw(self, context):
    ui_cutoff = bpy.context.preferences.addons[__package__].preferences.pdt_ui_width
    pdt_pg = context.scene.pdt_pg
    layout = self.layout
    row = layout.row()
    col = row.column()
<mask>:
        icon = 'PLAY'
        txt = 'Show'
    else:
        icon = 'PAUSE'
        txt = 'Hide'
    col.operator('pdt.modaldraw', icon=icon, text=txt)
    col = row.column()
    col.prop(pdt_pg, 'pivot_size', text=PDT_LAB_PIVOTSIZE)
    if ui_width() < ui_cutoff:
        row = layout.row()
    col = row.column()
    col.prop(pdt_pg, 'pivot_width', text=PDT_LAB_PIVOTWIDTH)
    col = row.column()
    col.prop(pdt_pg, 'pivot_alpha', text=PDT_LAB_PIVOTALPHA)
    row = layout.row()
    split = row.split(factor=0.35, align=True)
    split.label(text=PDT_LAB_PIVOTLOCH)
    split.prop(pdt_pg, 'pivot_loc', text=PDT_LAB_PIVOTLOC)
    row = layout.row()
    col = row.column()
    col.operator('pdt.pivotselected', icon='EMPTY_AXIS', text='Selection')
    col = row.column()
    col.operator('pdt.pivotcursor', icon='EMPTY_AXIS', text='Cursor')
    col = row.column()
    col.operator('pdt.pivotorigin', icon='EMPTY_AXIS', text='Origin')
    row = layout.row()
    col = row.column()
    col.operator('pdt.viewplanerot', icon='EMPTY_AXIS', text='Rotate')
    col = row.column()
    col.prop(pdt_pg, 'pivot_ang', text='Angle')
    row = layout.row()
    col = row.column()
    col.operator('pdt.viewscale', icon='EMPTY_AXIS', text='Scale')
    col = row.column()
    col.operator('pdt.cursorpivot', icon='EMPTY_AXIS', text='Cursor To Pivot')
    row = layout.row()
    col = row.column()
    col.prop(pdt_pg, 'pivot_dis', text='Scale Distance')
    col = row.column()
    col.prop(pdt_pg, 'distance', text='System Distance')
    row = layout.row()
    split = row.split(factor=0.35, align=True)
    split.label(text='Scale')
    split.prop(pdt_pg, 'pivot_scale', text='')
    row = layout.row()
    col = row.column()
    col.operator('pdt.pivotwrite', icon='FILE_TICK', text='PP Write')
    col = row.column()
    col.operator('pdt.pivotread', icon='FILE', text='PP Read')",context.window_manager.pdt_run_opengl is False,169,bpy.context.preferences.pdt_ui_layout,False,15.307938151800226,N/A
"def draw(self, context):
    ui_cutoff = context.preferences.addons[__package__].preferences.pdt_ui_width
    layout = self.layout
    pdt_pg = context.scene.pdt_pg
    row = layout.row()
    row.prop(pdt_pg, 'pdt_library_path')
    row = layout.row()
    col = row.column()
    col.operator('pdt.append', text='Append')
    col = row.column()
    col.operator('pdt.link', text='Link')
<mask>:
        row = layout.row()
    col = row.column()
    col.prop(pdt_pg, 'lib_mode', text='')
    box = layout.box()
    row = box.row()
    col = row.column()
    col.label(text='Objects')
    col = row.column()
    col.prop(pdt_pg, 'object_search_string')
    row = box.row()
    row.prop(pdt_pg, 'lib_objects', text='')
    box = layout.box()
    row = box.row()
    col = row.column()
    col.label(text='Collections')
    col = row.column()
    col.prop(pdt_pg, 'collection_search_string')
    row = box.row()
    row.prop(pdt_pg, 'lib_collections', text='')
    box = layout.box()
    row = box.row()
    col = row.column()
    col.label(text='Materials')
    col = row.column()
    col.prop(pdt_pg, 'material_search_string')
    row = box.row()
    row.prop(pdt_pg, 'lib_materials', text='')
    row = box.row()",ui_width() < ui_cutoff,106,ui_cutoff > 0,False,17.86690863748233,N/A
"def draw(self, context):
    ui_cutoff = context.preferences.addons[__package__].preferences.pdt_ui_width
    layout = self.layout
    ui_groups = self._ui_groups
    pdt_pg = context.scene.pdt_pg
    box = layout.box()
    row = box.row()
    col = row.column()
    col.label(text='View Rotation')
    col = row.column()
    col.operator('pdt.viewrot', text='Rotate Abs')
    row = box.row()
    split = row.split(factor=0.35, align=True)
    split.label(text='Rotation')
    split.prop(pdt_pg, 'rotation_coords', text='')
    row = box.row()
    col = row.column()
    col.prop(pdt_pg, 'vrotangle', text='Angle')
<mask>:
        row = box.row()
    col = row.column()
    col.operator('pdt.viewleft', text='', icon='TRIA_LEFT')
    col = row.column()
    col.operator('pdt.viewright', text='', icon='TRIA_RIGHT')
    col = row.column()
    col.operator('pdt.viewup', text='', icon='TRIA_UP')
    col = row.column()
    col.operator('pdt.viewdown', text='', icon='TRIA_DOWN')
    col = row.column()
    col.operator('pdt.viewroll', text='', icon='RECOVER_LAST')
    row = box.row()
    col = row.column()
    col.operator('pdt.viewiso', text='Isometric')
    col = row.column()
    col.operator('pdt.reset_3d_view', text='Reset View')",ui_width() < ui_cutoff,100,pdt_pg.vrotangle == 'Abs',False,4.8734989388136185,N/A
"def get_intersection(edge1, edge2):
    """"""Get Intersections of 2 Edges.

    Args:
        edge1, edge2: tuples containing 2 vectors.

    Returns:
        The point halfway on line. See intersect_line_line.
    """"""
    line = line_from_edge_intersect(edge1, edge2)
<mask>:
        return (line[0] + line[1]) / 2
    return None",line,37,line,True,100.00000000000004,N/A
"def test_coplanar(edge1, edge2):
    """"""Test 2 Edges are Co-planar.

    Note:
        The line that describes the shortest line between the two edges would be short if the
        lines intersect mathematically. If this line is longer than 1.0e-5 then they are either
        coplanar or parallel

    Args:
        edge1, edge2: tuples containing 2 vectors.

    Returns:
        True if edge1 and edge2 or coplanar, False otherwise.
    """"""
    line = line_from_edge_intersect(edge1, edge2)
<mask>:
        return (line[0] - line[1]).length < 1e-05
    return None",line,73,line,True,100.00000000000004,N/A
"def closest_idx(intersect_point, edge):
    """"""Get Closest Vertex to input point.

    Note:
        If both points in edge are equally far from intersect_point, then v1 is returned.

    Args:
        intersect_point:       vector
        edge:        bmesh edge

    Returns:
        Index of vertex closest to intersect_point.
    """"""
<mask>:
        edge_verts = edge.verts
        vector_a = edge_verts[0].co
        vector_b = edge_verts[1].co
        distance_test = (vector_a - intersect_point).length <= (vector_b - intersect_point).length
        return edge_verts[0].index if distance_test else edge_verts[1].index
    debug(f'Received {edge}, check expected input in docstring ')
    return None","isinstance(edge, bmesh.types.BMEdge)",73,"isinstance(edge, bmesh.BMEvent)",False,55.0695314903184,N/A
"def closest_vector(intersect_point, edge):
    """"""Return Closest Vector to input Point.

    Note:
        If both points in e are equally far from intersect_point, then v1 is returned.

    Args:
        intersect_point:       vector
        edge:        tuple containing 2 vectors

    Returns:
        Vector closest to intersect_point.
    """"""
<mask>:
        vector_a, vector_b = edge
        distance_test = (vector_a - intersect_point).length <= (vector_b - intersect_point).length
        return vector_a if distance_test else vector_b
    debug(f'Received {edge}, check expected input in docstring ')
    return None","isinstance(edge, tuple) and all([isinstance(co, Vector) for co in edge])",68,"isinstance(edge, tuple)",False,6.948345122280157,N/A
"def find_intersecting_edges(bm, intersect_point, idx1, idx2):
    """"""Find Intercecting Edges.

    Args:
        intersect_point: Vector describing 3D coordinates of intersection point
        idx1, idx2:    edge indices

    Returns:
        The list of edge indices where intersect_point is on those edges.
    """"""
<mask>:
        return []
    idxs = [idx1, idx2]
    edges = [coords_tuple_from_edge_idx(bm, idx) for idx in idxs]
    return [idx for edge, idx in zip(edges, idxs) if point_on_edge(intersect_point, edge)]",not intersect_point,60,not intersect_point,True,100.00000000000004,N/A
"def enumlist_objects(self, context):
    """"""Populate Objects List from Parts Library.

    Creates list of objects that optionally have search string contained in them
    to populate variable pdt_lib_objects enumerator.

    Args:
        context: Blender bpy.context instance.

    Returns:
        list of Object Names.
    """"""
    scene = context.scene
    pg = scene.pdt_pg
    file_path = pg.pdt_library_path
    path = Path(bpy.path.abspath(file_path))
    _pdt_obj_items.clear()
<mask>:
        with bpy.data.libraries.load(str(path)) as (data_from, _):
            if len(pg.object_search_string) == 0:
                object_names = [obj for obj in data_from.objects]
            else:
                object_names = [obj for obj in data_from.objects if pg.object_search_string in obj]
        for object_name in object_names:
            _pdt_obj_items.append((object_name, object_name, ''))
    else:
        _pdt_obj_items.append(('MISSING', 'Library Not Set', ''))
    return _pdt_obj_items",path.is_file() and '.blend' in str(path),94,path.exists(),False,3.3477793662538136,N/A
"def enumlist_collections(self, context):
    """"""Populate Collections List from Parts Library.

    Creates list of collections that optionally have search string contained in them
    to populate variable pg.lib_collections enumerator

    Args:
        context: Blender bpy.context instance.

    Returns:
        list of Collections Names.
    """"""
    scene = context.scene
    pg = scene.pdt_pg
    file_path = pg.pdt_library_path
    path = Path(bpy.path.abspath(file_path))
    _pdt_col_items.clear()
<mask>:
        with bpy.data.libraries.load(str(path)) as (data_from, _):
            if len(pg.collection_search_string) == 0:
                object_names = [obj for obj in data_from.collections]
            else:
                object_names = [obj for obj in data_from.collections if pg.collection_search_string in obj]
        for object_name in object_names:
            _pdt_col_items.append((object_name, object_name, ''))
    else:
        _pdt_col_items.append(('MISSING', 'Library Not Set', ''))
    return _pdt_col_items",path.is_file() and '.blend' in str(path),94,path.exists(),False,3.3477793662538136,N/A
"def enumlist_materials(self, context):
    """"""Populate Materials List from Parts Library.

    Creates list of materials that optionally have search string contained in them
    to populate variable pg.lib_materials enumerator.

    Args:
        context: Blender bpy.context instance.

    Returns:
        list of Object Names.
    """"""
    scene = context.scene
    pg = scene.pdt_pg
    file_path = pg.pdt_library_path
    path = Path(bpy.path.abspath(file_path))
    _pdt_mat_items.clear()
<mask>:
        with bpy.data.libraries.load(str(path)) as (data_from, _):
            if len(pg.material_search_string) == 0:
                object_names = [obj for obj in data_from.materials]
            else:
                object_names = [obj for obj in data_from.materials if pg.material_search_string in obj]
        for object_name in object_names:
            _pdt_mat_items.append((object_name, object_name, ''))
    else:
        _pdt_mat_items.append(('MISSING', 'Library Not Set', ''))
    return _pdt_mat_items",path.is_file() and '.blend' in str(path),94,path.exists(),False,3.3477793662538136,N/A
"def unregister():
    """"""Unregister Classes and Delete Scene Variables.

    Operates on the classes list defined above.
    """"""
    from bpy.utils import unregister_class
    pdt_pivot_point.PDT_OT_ModalDrawOperator.handle_remove(pdt_pivot_point.PDT_OT_ModalDrawOperator, bpy.context)
    window_manager = bpy.context.window_manager
    pdt_wm = 'pdt_run_opengl'
<mask>:
        del window_manager[pdt_wm]
    for cls in reversed(classes):
        unregister_class(cls)
    del Scene.pdt_pg",pdt_wm in window_manager,38,window_manager.has_key(pdt_wm),False,21.200626759025184,N/A
"def execute(self, context):
    """"""Reset 3D View to Blender Defaults.

        Args:
            context: Blender bpy.context instance.

        Returns:
            Status Set.
        """"""
    default_view_distance = 17.986562728881836
    default_view_distance = bpy.data.screens['Layout'].areas[-1].spaces[0].region_3d.view_distance
    default_view_matrix = ((0.41, -0.4017, 0.8188, 0.0), (0.912, 0.1936, -0.3617, 0.0), (-0.0133, 0.895, 0.4458, 0.0), (0.0, 0.0, -17.9866, 1.0))
    view = context.region_data
    debug(f'is_orthographic_side_view: {view.is_orthographic_side_view}')
<mask>:
        debug(f'view_distance before reset: {view.view_distance}')
        debug(f'view_location before reset: {view.view_location}')
        view.view_distance = default_view_distance
        view.view_location = (-0.0, -0.0, -0.0)
        view.update()
        debug(f'view_distance AFTER reset: {view.view_distance}')
        debug(f'view_location AFTER reset: {view.view_location}')
    else:
        debug(f'view_matrix before reset:\n{view.view_matrix}')
        view.view_matrix = default_view_matrix
        view.view_distance = default_view_distance
        view.update()
        debug(f'view_matrix AFTER reset:\n{view.view_matrix}')
    return {'FINISHED'}",view.is_orthographic_side_view,89,view.is_orthographic_side_view,True,100.00000000000004,N/A
"def execute(self, context):
    """"""Manipulates Geometry, or Objects by Absolute (World) Coordinates.

        Note:
            - Reads pg.operate from Operation Mode Selector as 'operation'
            - Reads pg.cartesian_coords scene variables to:
            -- set position of Cursor      (CU)
            -- set position of Pivot Point (PP)
            -- MoVe geometry/objects       (MV)
            -- Extrude Vertices            (EV)
            -- Split Edges                 (SE)
            -- add a New Vertex            (NV)

            Invalid Options result in self.report Error.

        Args:
            context: Blender bpy.context instance.

        Returns:
            Status Set.
        """"""
    pg = context.scene.pdt_pg
    operation = pg.operation
    decimal_places = context.preferences.addons[__package__].preferences.pdt_input_round
<mask>:
        pg.command = f'ca{str(round(pg.cartesian_coords.x, decimal_places))},{str(round(pg.cartesian_coords.y, decimal_places))},{str(round(pg.cartesian_coords.z, decimal_places))}'
    elif operation == 'PP':
        pg.command = f'pa{str(round(pg.cartesian_coords.x, decimal_places))},{str(round(pg.cartesian_coords.y, decimal_places))},{str(round(pg.cartesian_coords.z, decimal_places))}'
    elif operation == 'MV':
        pg.command = f'ga{str(round(pg.cartesian_coords.x, decimal_places))},{str(round(pg.cartesian_coords.y, decimal_places))},{str(round(pg.cartesian_coords.z, decimal_places))}'
    elif operation == 'SE':
        pg.command = f'sa{str(round(pg.cartesian_coords.x, decimal_places))},{str(round(pg.cartesian_coords.y, decimal_places))},{str(round(pg.cartesian_coords.z, decimal_places))}'
    elif operation == 'NV':
        pg.command = f'na{str(round(pg.cartesian_coords.x, decimal_places))},{str(round(pg.cartesian_coords.y, decimal_places))},{str(round(pg.cartesian_coords.z, decimal_places))}'
    elif operation == 'EV':
        pg.command = f'va{str(round(pg.cartesian_coords.x, decimal_places))},{str(round(pg.cartesian_coords.y, decimal_places))},{str(round(pg.cartesian_coords.z, decimal_places))}'
    else:
        error_message = f'{operation} {PDT_ERR_NON_VALID} {PDT_LAB_ABS}'
        self.report({'ERROR'}, error_message)
    return {'FINISHED'}",operation == 'CU',149,operation == 'CU',True,100.00000000000004,N/A
"def execute(self, context):
    """"""Manipulates Geometry, or Objects by Delta Offset (Increment).

        Note:
            - Reads pg.operation from Operation Mode Selector as 'operation'
            - Reads pg.select, pg.plane, pg.cartesian_coords scene variables to:
            -- set position of CUrsor       (CU)
            -- set position of Pivot Point  (PP)
            -- MoVe geometry/objects        (MV)
            -- Extrude Vertices             (EV)
            -- Split Edges                  (SE)
            -- add a New Vertex             (NV)
            -- Duplicate Geometry           (DG)
            -- Extrude Geometry             (EG)

            Invalid Options result in self.report Error.

        Args:
            context: Blender bpy.context instance.

        Returns:
            Status Set.
        """"""
    pg = context.scene.pdt_pg
    operation = pg.operation
    decimal_places = context.preferences.addons[__package__].preferences.pdt_input_round
<mask>:
        pg.command = f'cd{str(round(pg.cartesian_coords.x, decimal_places))},{str(round(pg.cartesian_coords.y, decimal_places))},{str(round(pg.cartesian_coords.z, decimal_places))}'
    elif operation == 'PP':
        pg.command = f'pd{str(round(pg.cartesian_coords.x, decimal_places))},{str(round(pg.cartesian_coords.y, decimal_places))},{str(round(pg.cartesian_coords.z, decimal_places))}'
    elif operation == 'MV':
        pg.command = f'gd{str(round(pg.cartesian_coords.x, decimal_places))},{str(round(pg.cartesian_coords.y, decimal_places))},{str(round(pg.cartesian_coords.z, decimal_places))}'
    elif operation == 'SE':
        pg.command = f'sd{str(round(pg.cartesian_coords.x, decimal_places))},{str(round(pg.cartesian_coords.y, decimal_places))},{str(round(pg.cartesian_coords.z, decimal_places))}'
    elif operation == 'NV':
        pg.command = f'nd{str(round(pg.cartesian_coords.x, decimal_places))},{str(round(pg.cartesian_coords.y, decimal_places))},{str(round(pg.cartesian_coords.z, decimal_places))}'
    elif operation == 'EV':
        pg.command = f'vd{str(round(pg.cartesian_coords.x, decimal_places))},{str(round(pg.cartesian_coords.y, decimal_places))},{str(round(pg.cartesian_coords.z, decimal_places))}'
    elif operation == 'DG':
        pg.command = f'dd{str(round(pg.cartesian_coords.x, decimal_places))},{str(round(pg.cartesian_coords.y, decimal_places))},{str(round(pg.cartesian_coords.z, decimal_places))}'
    elif operation == 'EG':
        pg.command = f'ed{str(round(pg.cartesian_coords.x, decimal_places))},{str(round(pg.cartesian_coords.y, decimal_places))},{str(round(pg.cartesian_coords.z, decimal_places))}'
    else:
        error_message = f'{operation} {PDT_ERR_NON_VALID} {PDT_LAB_DEL}'
        self.report({'ERROR'}, error_message)
    return {'FINISHED'}",operation == 'CU',179,operation == 'CU',True,100.00000000000004,N/A
"def execute(self, context):
    """"""Manipulates Geometry, or Objects by Distance at Angle (Direction).

        Note:
            - Reads pg.operation from Operation Mode Selector as 'operation'
            - Reads pg.select, pg.distance, pg.angle, pg.plane & pg.flip_angle scene variables to:
            -- set position of CUrsor       (CU)
            -- set position of Pivot Point  (PP)
            -- MoVe geometry/objects        (MV)
            -- Extrude Vertices             (EV)
            -- Split Edges                  (SE)
            -- add a New Vertex             (NV)
            -- Duplicate Geometry           (DG)
            -- Extrude Geometry             (EG)

            Invalid Options result in self.report Error.

        Args:
            context: Blender bpy.context instance.

        Returns:
            Status Set.
        """"""
    pg = context.scene.pdt_pg
    operation = pg.operation
    decimal_places = context.preferences.addons[__package__].preferences.pdt_input_round
<mask>:
        pg.command = f'ci{str(round(pg.distance, decimal_places))},{str(round(pg.angle, decimal_places))}'
    elif operation == 'PP':
        pg.command = f'pi{str(round(pg.distance, decimal_places))},{str(round(pg.angle, decimal_places))}'
    elif operation == 'MV':
        pg.command = f'gi{str(round(pg.distance, decimal_places))},{str(round(pg.angle, decimal_places))}'
    elif operation == 'SE':
        pg.command = f'si{str(round(pg.distance, decimal_places))},{str(round(pg.angle, decimal_places))}'
    elif operation == 'NV':
        pg.command = f'ni{str(round(pg.distance, decimal_places))},{str(round(pg.angle, decimal_places))}'
    elif operation == 'EV':
        pg.command = f'vi{str(round(pg.distance, decimal_places))},{str(round(pg.angle, decimal_places))}'
    elif operation == 'DG':
        pg.command = f'di{str(round(pg.distance, decimal_places))},{str(round(pg.angle, decimal_places))}'
    elif operation == 'EG':
        pg.command = f'ei{str(round(pg.distance, decimal_places))},{str(round(pg.angle, decimal_places))}'
    else:
        error_message = f'{operation} {PDT_ERR_NON_VALID} {PDT_LAB_DIR}'
        self.report({'ERROR'}, error_message)
    return {'FINISHED'}",operation == 'CU',175,operation == 'CU',True,100.00000000000004,N/A
"def execute(self, context):
    """"""Manipulates Geometry, or Objects by View Normal Axis Offset (Increment).

        Note:
            - Reads pg.operation from Operation Mode Selector as 'operation'
            - Reads pg.select, pg.plane, pg.cartesian_coords scene variables to:
            -- set position of CUrsor       (CU)
            -- set position of Pivot Point  (PP)
            -- MoVe geometry/objects        (MV)
            -- Extrude Vertices             (EV)
            -- Split Edges                  (SE)
            -- add a New Vertex             (NV)
            -- Duplicate Geometry           (DG)
            -- Extrude Geometry             (EG)

            Invalid Options result in self.report Error.

        Args:
            context: Blender bpy.context instance.

        Returns:
            Status Set.
        """"""
    pg = context.scene.pdt_pg
    operation = pg.operation
    decimal_places = context.preferences.addons[__package__].preferences.pdt_input_round
<mask>:
        pg.command = f'cn{str(round(pg.distance, decimal_places))}'
    elif operation == 'PP':
        pg.command = f'pn{str(round(pg.distance, decimal_places))}'
    elif operation == 'MV':
        pg.command = f'gn{str(round(pg.distance, decimal_places))}'
    elif operation == 'NV':
        pg.command = f'nn{str(round(pg.distance, decimal_places))}'
    elif operation == 'EV':
        pg.command = f'vn{str(round(pg.distance, decimal_places))}'
    elif operation == 'DG':
        pg.command = f'dn{str(round(pg.distance, decimal_places))}'
    elif operation == 'EG':
        pg.command = f'en{str(round(pg.distance, decimal_places))}'
    else:
        error_message = f'{operation} {PDT_ERR_NON_VALID} {PDT_LAB_DEL}'
        self.report({'ERROR'}, error_message)
    return {'FINISHED'}",operation == 'CU',157,operation == 'CU',True,100.00000000000004,N/A
"def execute(self, context):
    """"""Manipulates Geometry, or Objects by Percentage between 2 points.

        Note:
            - Reads pg.operation from Operation Mode Selector as 'operation'
            - Reads pg.percent, pg.extend & pg.flip_percent scene variables to:
            -- set position of CUrsor       (CU)
            -- set position of Pivot Point  (PP)
            -- MoVe geometry/objects        (MV)
            -- Extrude Vertices             (EV)
            -- Split Edges                  (SE)
            -- add a New Vertex             (NV)

            Invalid Options result in self.report Error.

        Args:
            context: Blender bpy.context instance.

        Returns:
            Status Set.
        """"""
    pg = context.scene.pdt_pg
    operation = pg.operation
    decimal_places = context.preferences.addons[__package__].preferences.pdt_input_round
<mask>:
        pg.command = f'cp{str(round(pg.percent, decimal_places))}'
    elif operation == 'PP':
        pg.command = f'pp{str(round(pg.percent, decimal_places))}'
    elif operation == 'MV':
        pg.command = f'gp{str(round(pg.percent, decimal_places))}'
    elif operation == 'SE':
        pg.command = f'sp{str(round(pg.percent, decimal_places))}'
    elif operation == 'NV':
        pg.command = f'np{str(round(pg.percent, decimal_places))}'
    elif operation == 'EV':
        pg.command = f'vp{str(round(pg.percent, decimal_places))}'
    else:
        error_message = f'{operation} {PDT_ERR_NON_VALID} {PDT_LAB_PERCENT}'
        self.report({'ERROR'}, error_message)
    return {'FINISHED'}",operation == 'CU',141,operation == 'CU',True,100.00000000000004,N/A
"def execute(self, context):
    """"""Generate Trig Waves in Active Object.

        Note:
            Uses all the PDT trig_* variables.

            This function will draw a trigonometrical wave based upon cycle length
            One cycle is assumed to be 180 degrees, so half a revolution of an imaginary
            rotating object. If a full cycle from 0 to 360 degrees is required, the cycles
            number should be set to 2.

        Args:
            context: Blender bpy.context instance.

        Returns:
            Nothing.
        """"""
    pg = context.scene.pdt_pg
    plane = pg.plane
    a1, a2, a3 = set_mode(plane)
    for obj in bpy.data.objects:
        obj.select_set(state=False)
    context.view_layer.objects.active = pg.trig_obj
    x_inc = pg.trig_len / pg.trig_res
<mask>:
        bpy.ops.object.mode_set(mode='EDIT')
        for v in pg.trig_obj.data.vertices:
            v.select = True
        bpy.ops.mesh.delete(type='VERT')
        bpy.ops.object.mode_set(mode='OBJECT')
    if pg.trig_obj.mode != 'EDIT':
        bpy.ops.object.mode_set(mode='EDIT')
    bm = bmesh.from_edit_mesh(pg.trig_obj.data)
    for i in range(pg.trig_res * pg.trig_cycles + 1):
        if pg.trig_type == 'sin':
            if pg.trig_abs:
                z_val = abs(sin(i / pg.trig_res * pi) * pg.trig_amp)
            else:
                z_val = sin(i / pg.trig_res * pi) * pg.trig_amp
        elif pg.trig_type == 'cos':
            if pg.trig_abs:
                z_val = abs(cos(i / pg.trig_res * pi) * pg.trig_amp)
            else:
                z_val = cos(i / pg.trig_res * pi) * pg.trig_amp
        else:
            if pg.trig_abs:
                z_val = abs(tan(i / pg.trig_res * pi) * pg.trig_amp)
            else:
                z_val = tan(i / pg.trig_res * pi) * pg.trig_amp
            if abs(z_val) > pg.trig_tanmax:
                if z_val >= 0:
                    z_val = pg.trig_tanmax
                elif pg.trig_abs:
                    z_val = pg.trig_tanmax
                else:
                    z_val = -pg.trig_tanmax
        vert_loc = Vector(pg.trig_off)
        vert_loc[a1] = vert_loc[a1] + i * x_inc
        vert_loc[a2] = vert_loc[a2] + z_val
        if plane == 'LO':
            vert_loc = view_coords(vert_loc[a1], vert_loc[a2], vert_loc[a3])
        vertex_new = bm.verts.new(vert_loc)
        bm.verts.ensure_lookup_table()
        if i > 0:
            bm.edges.new([bm.verts[-2], vertex_new])
    bmesh.update_edit_mesh(pg.trig_obj.data)
    bpy.ops.object.mode_set(mode='OBJECT')
    return {'FINISHED'}",pg.trig_del,252,pg.trig_obj.mode != 'EDIT',False,26.269098944241588,N/A
"@staticmethod
def handle_add(self, context):
    """"""Draw Pivot Point Graphic if not displayed.

        Note:
            Draws 7 element Pivot Point Graphic

        Args:
            context: Blender bpy.context instance.

        Returns:
            Nothing.
        """"""
<mask>:
        PDT_OT_ModalDrawOperator._handle = SpaceView3D.draw_handler_add(draw_callback_3d, (self, context), 'WINDOW', 'POST_VIEW')
        context.window_manager.pdt_run_opengl = True",PDT_OT_ModalDrawOperator._handle is None,37,not context.window_manager.pdt_run_opengl,False,4.789232204309912,N/A
"@staticmethod
def handle_remove(self, context):
    """"""Remove Pivot Point Graphic if displayed.

        Note:
            Removes 7 element Pivot Point Graphic

        Args:
            context: Blender bpy.context instance.

        Returns:
            Nothing.
        """"""
<mask>:
        SpaceView3D.draw_handler_remove(PDT_OT_ModalDrawOperator._handle, 'WINDOW')
    PDT_OT_ModalDrawOperator._handle = None
    context.window_manager.pdt_run_opengl = False",PDT_OT_ModalDrawOperator._handle is not None,34,PDT_OT_ModalDrawOperator._handle,False,68.72892787909726,N/A
"def execute(self, context):
    """"""Pivot Point Show/Hide Button Function.

        Note:
            Operational execute function for Show/Hide Pivot Point function

        Args:
            context: Blender bpy.context instance.

        Returns:
            Status Set.
        """"""
<mask>:
        if context.window_manager.pdt_run_opengl is False:
            self.handle_add(self, context)
            context.area.tag_redraw()
        else:
            self.handle_remove(self, context)
            context.area.tag_redraw()
        return {'FINISHED'}
    self.report({'ERROR'}, PDT_ERR_NO3DVIEW)
    return {'CANCELLED'}",context.area.type == 'VIEW_3D',44,not context.area.is_active(),False,27.77619034011791,N/A
"@classmethod
def poll(cls, context):
    """"""Check Object Status.

        Args:
            context: Blender bpy.context instance.

        Returns:
            Nothing.
        """"""
    obj = context.object
<mask>:
        return False
    return all([bool(obj), obj.type == 'MESH', obj.mode == 'EDIT'])",obj is None,29,obj is None,True,100.00000000000004,N/A
"def execute(self, context):
    """"""Rotate Selected Vertices about Pivot Point.

        Note:
            Rotates any selected vertices about the Pivot Point
            in View Oriented coordinates, works in any view orientation.

        Args:
            context: Blender bpy.context instance.

        Note:
            Uses pg.pivot_loc, pg.pivot_ang scene variables

        Returns:
            Status Set.
        """"""
    scene = context.scene
    pg = scene.pdt_pg
    obj = bpy.context.view_layer.objects.active
<mask>:
        self.report({'ERROR'}, PDT_ERR_NO_ACT_OBJ)
        return {'FINISHED'}
    if obj.mode != 'EDIT':
        error_message = f'{PDT_ERR_EDIT_MODE} {obj.mode})'
        self.report({'ERROR'}, error_message)
        return {'FINISHED'}
    bm = bmesh.from_edit_mesh(obj.data)
    v1 = Vector((0, 0, 0))
    v2 = view_coords(0, 0, 1)
    axis = (v2 - v1).normalized()
    rot = Matrix.Rotation(pg.pivot_ang * pi / 180, 4, axis)
    verts = verts = [v for v in bm.verts if v.select]
    bmesh.ops.rotate(bm, cent=pg.pivot_loc - obj.matrix_world.decompose()[0], matrix=rot, verts=verts)
    bmesh.update_edit_mesh(obj.data)
    return {'FINISHED'}",obj is None,115,obj is None,True,100.00000000000004,N/A
"def vector_build(context, pg, obj, operation, values, num_values):
    """"""Build Movement Vector from Input Fields.

    Args:
        context: Blender bpy.context instance.
        pg: PDT Parameters Group - our variables
        obj: The Active Object
        operation: The Operation e.g. Create New Vertex
        values: The parameters passed e.g. 1,4,3 for Cartesian Coordinates
        num_values: The number of values passed - determines the function

    Returns:
        Vector to position, or offset, items.
    """"""
    scene = context.scene
    plane = pg.plane
    flip_angle = pg.flip_angle
    flip_percent = pg.flip_percent
<mask>:
        output_vector = Vector((float(values[0]), float(values[1]), float(values[2])))
    elif num_values == 2 and len(values) == 2:
        output_vector = dis_ang(values, flip_angle, plane, scene)
    elif num_values == 1 and len(values) == 1:
        output_vector = get_percent(obj, flip_percent, float(values[0]), operation, scene)
    else:
        if num_values == 3:
            pg.error = PDT_ERR_BAD3VALS
        elif num_values == 2:
            pg.error = PDT_ERR_BAD2VALS
        else:
            pg.error = PDT_ERR_BAD1VALS
        context.window_manager.popup_menu(oops, title='Error', icon='ERROR')
        raise PDT_InvalidVector
    return output_vector",num_values == 3 and len(values) == 3,136,num_values == 4 and len(values) == 4,False,69.97522298221911,N/A
"def join_two_vertices(context):
    """"""Joins 2 Free Vertices that do not form part of a Face.

    Note:
        Joins two vertices that do not form part of a single face
        It is designed to close open Edge Loops, where a face is not required
        or to join two disconnected Edges.

    Args:
        context: Blender bpy.context instance.

    Returns:
        Status Set.
    """"""
    scene = context.scene
    pg = scene.pdt_pg
    obj = context.view_layer.objects.active
<mask>:
        bm = bmesh.from_edit_mesh(obj.data)
        verts = [v for v in bm.verts if v.select]
        if len(verts) == 2:
            try:
                bm.edges.new([verts[-1], verts[-2]])
                bmesh.update_edit_mesh(obj.data)
                bm.select_history.clear()
                return
            except ValueError:
                pg.error = PDT_ERR_CONNECTED
                context.window_manager.popup_menu(oops, title='Error', icon='ERROR')
                raise PDT_VerticesConnected
        else:
            pg.error = f'{PDT_ERR_SEL_2_VERTS} {len(verts)})'
            context.window_manager.popup_menu(oops, title='Error', icon='ERROR')
            raise PDT_SelectionError
    else:
        pg.error = f'{PDT_ERR_EDOB_MODE},{obj.mode})'
        context.window_manager.popup_menu(oops, title='Error', icon='ERROR')
        raise PDT_ObjectModeError","all([bool(obj), obj.type == 'MESH', obj.mode == 'EDIT'])",117,obj.mode == PDT_OBJECT_MODE_EDGE,False,12.676166246311745,N/A
"def extend_vertex(context):
    """"""Computes Edge Extension to Face.

    Args:
        context: Blender bpy.context instance.

    Returns:
        Nothing.
    """"""
    obj = bpy.context.edit_object
    pg = context.scene.pdt_pg
<mask>:
        object_data = obj.data
        bm = bmesh.from_edit_mesh(object_data)
        verts = bm.verts
        faces = bm.faces
        planes = [f for f in faces if f.select]
        if not len(planes) == 1:
            failure_message(context)
            return
        plane = planes[0]
        plane_vert_indices = plane.verts[:]
        all_selected_vert_indices = [v for v in verts if v.select]
        plane_verts = set(plane_vert_indices)
        all_verts = set(all_selected_vert_indices)
        diff_verts = all_verts.difference(plane_verts)
        diff_verts = list(diff_verts)
        if not len(diff_verts) == 2:
            failure_message(context)
            return
        (v1_ref, v1), (v2_ref, v2) = [(i, i.co) for i in diff_verts]
        plane_co = plane.calc_center_median()
        plane_no = plane.normal
        new_co = intersect_line_plane(v1, v2, plane_co, plane_no, False)
        if new_co:
            new_vertex = verts.new(new_co)
            a_len = (v1 - new_co).length
            b_len = (v2 - new_co).length
            vertex_reference = v1_ref if a_len < b_len else v2_ref
            bm.edges.new([vertex_reference, new_vertex])
            bmesh.update_edit_mesh(object_data, loop_triangles=True)
        else:
            failure_message_on_plane(context)
    else:
        pg.error = f'{PDT_ERR_EDOB_MODE},{obj.mode})'
        context.window_manager.popup_menu(oops, title='Error', icon='ERROR')
        return","all([bool(obj), obj.type == 'MESH', obj.mode == 'EDIT'])",146,obj.mode == PDT_EDOB_MODE_EDGE,False,12.676166246311745,N/A
"@classmethod
def poll(cls, context):
    """"""Only allow this to work if a mesh is selected in EDIT mode.

        Args:
            context: Blender bpy.context instance.

        Returns:
            Boolean.
        """"""
    obj = context.object
<mask>:
        return False
    return all([bool(obj), obj.type == 'MESH', obj.mode == 'EDIT'])",obj is None,39,"obj.type not in ['MESH', 'EDIT']",False,4.196114906296549,N/A
"def debug(msg, prefix=''):
    """"""Print a debug message to the console if PDT's or Blender's debug flags are set.

    Note:
        The printed message will be of the form:

        {prefix}{caller file name:line number}| {msg}

    Args:
        msg: Incoming message to display
        prefix: Always Blank

    Returns:
        Nothing.
    """"""
    pdt_debug = bpy.context.preferences.addons[__package__].preferences.debug
<mask>:
        import traceback

        def extract_filename(fullpath):
            """"""Return only the filename part of fullpath (excluding its path).

            Args:
                fullpath: Filename's full path

            Returns:
                filename.
            """"""
            filename = fullpath.split('/')[-1]
            if len(filename) < 1:
                return fullpath
            return filename
        laststack = traceback.extract_stack()[-2]
        print(f'{prefix}{extract_filename(laststack[0])}:{laststack[1]}| {msg}')",bpy.app.debug or bpy.app.debug_python or pdt_debug,86,pdt_debug and pdt_debug.get('blender'),False,9.88274679095246,N/A
"def check_selection(num, bm, obj):
    """"""Check that the Object's select_history has sufficient entries.

    Note:
        If selection history is not Verts, clears selection and history.

    Args:
        num: The number of entries required for each operation
        bm: The Bmesh from the Object
        obj: The Object

    Returns:
        list of 3D points as Vectors.
    """"""
<mask>:
        return None
    active_vertex = bm.select_history[-1]
    if isinstance(active_vertex, bmesh.types.BMVert):
        vector_a = active_vertex.co
        if num == 1:
            return vector_a
        if num == 2:
            vector_b = bm.select_history[-2].co
            return (vector_a, vector_b)
        if num == 3:
            vector_b = bm.select_history[-2].co
            vector_c = bm.select_history[-3].co
            return (vector_a, vector_b, vector_c)
        if num == 4:
            vector_b = bm.select_history[-2].co
            vector_c = bm.select_history[-3].co
            vector_d = bm.select_history[-4].co
            return (vector_a, vector_b, vector_c, vector_d)
    else:
        for f in bm.faces:
            f.select_set(False)
        for e in bm.edges:
            e.select_set(False)
        for v in bm.verts:
            v.select_set(False)
        bmesh.update_edit_mesh(obj.data)
        bm.select_history.clear()
    return None",len(bm.select_history) < num,130,len(bm.select_history) == 0,False,67.86502681586727,N/A
"def view_coords(x_loc, y_loc, z_loc):
    """"""Converts input Vector values to new Screen Oriented Vector.

    Args:
        x_loc: X coordinate from vector
        y_loc: Y coordinate from vector
        z_loc: Z coordinate from vector

    Returns:
        Vector adjusted to View's Inverted Transformation Matrix.
    """"""
    areas = [a for a in bpy.context.screen.areas if a.type == 'VIEW_3D']
<mask>:
        view_matrix = areas[0].spaces.active.region_3d.view_matrix
        view_matrix = view_matrix.to_3x3().normalized().inverted()
        view_location = Vector((x_loc, y_loc, z_loc))
        new_view_location = view_matrix @ view_location
        return new_view_location
    return Vector((0, 0, 0))",len(areas) > 0,72,len(areas) > 0,True,100.00000000000004,N/A
"def view_coords_i(x_loc, y_loc, z_loc):
    """"""Converts Screen Oriented input Vector values to new World Vector.

    Note:
        Converts View transformation Matrix to Rotational Matrix

    Args:
        x_loc: X coordinate from vector
        y_loc: Y coordinate from vector
        z_loc: Z coordinate from vector

    Returns:
        Vector adjusted to View's Transformation Matrix.
    """"""
    areas = [a for a in bpy.context.screen.areas if a.type == 'VIEW_3D']
<mask>:
        view_matrix = areas[0].spaces.active.region_3d.view_matrix
        view_matrix = view_matrix.to_3x3().normalized()
        view_location = Vector((x_loc, y_loc, z_loc))
        new_view_location = view_matrix @ view_location
        return new_view_location
    return Vector((0, 0, 0))",len(areas) > 0,80,len(areas) > 0,True,100.00000000000004,N/A
"def view_dir(dis_v, ang_v):
    """"""Converts Distance and Angle to View Oriented Vector.

    Note:
        Converts View Transformation Matrix to Rotational Matrix (3x3)
        Angles are Converts to Radians from degrees.

    Args:
        dis_v: Scene PDT distance
        ang_v: Scene PDT angle

    Returns:
        World Vector.
    """"""
    areas = [a for a in bpy.context.screen.areas if a.type == 'VIEW_3D']
<mask>:
        view_matrix = areas[0].spaces.active.region_3d.view_matrix
        view_matrix = view_matrix.to_3x3().normalized().inverted()
        view_location = Vector((0, 0, 0))
        view_location.x = dis_v * cos(ang_v * pi / 180)
        view_location.y = dis_v * sin(ang_v * pi / 180)
        new_view_location = view_matrix @ view_location
        return new_view_location
    return Vector((0, 0, 0))",len(areas) > 0,92,areas,False,0.673794699908547,N/A
"def command_maths(context, mode, pg, expression, output_target):
    """"""Evaluates Maths Input.

    Args:
        context: Blender bpy.context instance.
        mode: The Operation Mode, e.g. a for Absolute
        pg: PDT Parameters Group - our variables
        expression: The Maths component of the command input e.g. sqrt(56)
        output_target: The output variable box on the UI

    Returns:
        Nothing.
    """"""
    namespace = {}
    namespace.update(vars(math))
    try:
        maths_result = eval(expression, namespace, namespace)
    except:
        pg.error = PDT_ERR_BADMATHS
        context.window_manager.popup_menu(oops, title='Error', icon='ERROR')
        raise PDT_MathsError
    decimal_places = context.preferences.addons[__package__].preferences.pdt_input_round
<mask>:
        pg.cartesian_coords.x = round(maths_result, decimal_places)
    elif output_target == 'y':
        pg.cartesian_coords.y = round(maths_result, decimal_places)
    elif output_target == 'z':
        pg.cartesian_coords.z = round(maths_result, decimal_places)
    elif output_target == 'd':
        pg.distance = round(maths_result, decimal_places)
    elif output_target == 'a':
        pg.angle = round(maths_result, decimal_places)
    elif output_target == 'p':
        pg.percent = round(maths_result, decimal_places)
    else:
        pg.maths_output = round(maths_result, decimal_places)",output_target == 'x',122,output_target == 'x',True,100.00000000000004,N/A
"def move_cursor_pivot(context, pg, operation, mode, obj, verts, values):
    """"""Moves Cursor & Pivot Point.

    Args:
        context: Blender bpy.context instance.
        pg: PDT Parameters Group - our variables
        operation: The Operation e.g. Create New Vertex
        mode: The Operation Mode, e.g. a for Absolute
        obj: The Active Object
        verts: The object's selected vertices, or selected history vertices
        values: The parameters passed e.g. 1,4,3 for Cartesian Coordinates

    Returns:
        Nothing.
    """"""
<mask>:
        try:
            vector_delta = vector_build(context, pg, obj, operation, values, 3)
        except:
            raise PDT_InvalidVector
    elif mode == 'i':
        try:
            vector_delta = vector_build(context, pg, obj, operation, values, 2)
        except:
            raise PDT_InvalidVector
    else:
        try:
            vector_delta = vector_build(context, pg, obj, operation, values, 1)
        except:
            raise PDT_InvalidVector
    scene = context.scene
    mode_sel = pg.select
    obj_loc = Vector((0, 0, 0))
    if obj is not None:
        obj_loc = obj.matrix_world.decompose()[0]
    if mode == 'a':
        if operation == 'C':
            scene.cursor.location = vector_delta
        elif operation == 'P':
            pg.pivot_loc = vector_delta
    elif mode in {'d', 'i', 'n'}:
        if pg.plane == 'LO' and mode in {'d', 'n'}:
            vector_delta = view_coords(vector_delta.x, vector_delta.y, vector_delta.z)
        elif pg.plane == 'LO' and mode == 'i':
            vector_delta = view_dir(pg.distance, pg.angle)
        if mode_sel == 'REL':
            if operation == 'C':
                scene.cursor.location = scene.cursor.location + vector_delta
            else:
                pg.pivot_loc = pg.pivot_loc + vector_delta
        elif mode_sel == 'SEL':
            if obj.mode == 'EDIT':
                if operation == 'C':
                    scene.cursor.location = verts[-1].co + obj_loc + vector_delta
                else:
                    pg.pivot_loc = verts[-1].co + obj_loc + vector_delta
            if obj.mode == 'OBJECT':
                if operation == 'C':
                    scene.cursor.location = obj_loc + vector_delta
                else:
                    pg.pivot_loc = obj_loc + vector_delta
    else:
        if obj.mode == 'EDIT':
            if operation == 'C':
                scene.cursor.location = obj_loc + vector_delta
            else:
                pg.pivot_loc = obj_loc + vector_delta
        if obj.mode == 'OBJECT':
            if operation == 'C':
                scene.cursor.location = vector_delta
            else:
                pg.pivot_loc = vector_delta","mode in {'a', 'd', 'n'}",276,mode == 'a',False,5.4424142191183185,N/A
"def move_entities(context, pg, operation, mode, obj, bm, verts, values):
    """"""Moves Entities.

    Args:
        context: Blender bpy.context instance.
        pg: PDT Parameters Group - our variables
        operation: The Operation e.g. Create New Vertex
        mode: The Operation Mode, e.g. a for Absolute
        obj: The Active Object
        bm: The object's Bmesh
        verts: The object's selected vertices, or selected history vertices
        values: The parameters passed e.g. 1,4,3 for Cartesian Coordinates

    Returns:
        Nothing.
    """"""
    obj_loc = obj.matrix_world.decompose()[0]
<mask>:
        try:
            vector_delta = vector_build(context, pg, obj, operation, values, 3)
        except:
            raise PDT_InvalidVector
        if obj.mode == 'EDIT':
            for v in [v for v in bm.verts if v.select]:
                v.co = vector_delta - obj_loc
            bmesh.ops.remove_doubles(bm, verts=[v for v in bm.verts if v.select], dist=0.0001)
        if obj.mode == 'OBJECT':
            for ob in context.view_layer.objects.selected:
                ob.location = vector_delta
    elif mode in {'d', 'i', 'n'}:
        if mode in {'d', 'n'}:
            try:
                vector_delta = vector_build(context, pg, obj, operation, values, 3)
            except:
                raise PDT_InvalidVector
        else:
            try:
                vector_delta = vector_build(context, pg, obj, operation, values, 2)
            except:
                raise PDT_InvalidVector
        if pg.plane == 'LO' and mode in {'d', 'n'}:
            vector_delta = view_coords(vector_delta.x, vector_delta.y, vector_delta.z)
        elif pg.plane == 'LO' and mode == 'i':
            vector_delta = view_dir(pg.distance, pg.angle)
        if obj.mode == 'EDIT':
            bmesh.ops.translate(bm, verts=[v for v in bm.verts if v.select], vec=vector_delta)
        if obj.mode == 'OBJECT':
            for ob in context.view_layer.objects.selected:
                ob.location = obj_loc + vector_delta
    else:
        try:
            vector_delta = vector_build(context, pg, obj, operation, values, 1)
        except:
            raise PDT_InvalidVector
        if obj.mode == 'EDIT':
            verts[-1].co = vector_delta
        if obj.mode == 'OBJECT':
            obj.location = vector_delta
    if obj.mode == 'EDIT':
        bmesh.update_edit_mesh(obj.data)
        bm.select_history.clear()",mode == 'a',243,"mode in {'n', 'i', 'n'}",False,4.767707020457095,N/A
"def execute(self, context):
    """"""Appends Objects from PDT Library file.

        Note:
            Appended Objects are placed at Cursor Location.
            Uses pg.lib_objects, pg.lib_collections & pg.lib_materials

        Args:
            context: Blender bpy.context instance.

        Returns:
            Status Set.
        """"""
    scene = context.scene
    pg = scene.pdt_pg
    obj = context.view_layer.objects.active
<mask>:
        if obj.mode != 'OBJECT':
            error_message = PDT_ERR_OBJECTMODE
            self.report({'ERROR'}, error_message)
            return {'FINISHED'}
    obj_names = [o.name for o in context.view_layer.objects].copy()
    file_path = pg.pdt_library_path
    path = Path(file_path)
    if path.is_file() and str(path).endswith('.blend'):
        if pg.lib_mode == 'OBJECTS':
            bpy.ops.wm.append(filepath=str(path), directory=str(path) + '/Object', filename=pg.lib_objects)
            for obj in context.view_layer.objects:
                if obj.name not in obj_names:
                    obj.select_set(False)
                    obj.location = Vector((scene.cursor.location.x, scene.cursor.location.y, scene.cursor.location.z))
            return {'FINISHED'}
        if pg.lib_mode == 'COLLECTIONS':
            bpy.ops.wm.append(filepath=str(path), directory=str(path) + '/Collection', filename=pg.lib_collections)
            for obj in context.view_layer.objects:
                if obj.name not in obj_names:
                    obj.select_set(False)
                    obj.location = Vector((scene.cursor.location.x, scene.cursor.location.y, scene.cursor.location.z))
            return {'FINISHED'}
        if pg.lib_mode == 'MATERIALS':
            bpy.ops.wm.append(filepath=str(path), directory=str(path) + '/Material', filename=pg.lib_materials)
            return {'FINISHED'}
    error_message = PDT_ERR_NO_LIBRARY
    self.report({'ERROR'}, error_message)
    return {'FINISHED'}",obj is not None,139,obj,False,4.9787068367863965,N/A
"def execute(self, context):
    """"""Links Objects from PDT Library file.

        Note:
            Linked Objects are placed at Cursor Location
            Uses pg.lib_objects, pg.lib_collections & pg.lib_materials

        Args:
            context: Blender bpy.context instance.

        Returns:
            Status Set.
        """"""
    scene = context.scene
    pg = scene.pdt_pg
    obj = context.view_layer.objects.active
<mask>:
        if obj.mode != 'OBJECT':
            error_message = PDT_ERR_OBJECTMODE
            self.report({'ERROR'}, error_message)
            return {'FINISHED'}
    file_path = pg.pdt_library_path
    path = Path(file_path)
    if path.is_file() and str(path).endswith('.blend'):
        if pg.lib_mode == 'OBJECTS':
            bpy.ops.wm.link(filepath=str(path), directory=str(path) + '/Object', filename=pg.lib_objects)
            for obj in context.view_layer.objects:
                obj.select_set(False)
            return {'FINISHED'}
        if pg.lib_mode == 'COLLECTIONS':
            bpy.ops.wm.link(filepath=str(path), directory=str(path) + '/Collection', filename=pg.lib_collections)
            for obj in context.view_layer.objects:
                obj.select_set(False)
            return {'FINISHED'}
        if pg.lib_mode == 'MATERIALS':
            bpy.ops.wm.link(filepath=str(path), directory=str(path) + '/Material', filename=pg.lib_materials)
            return {'FINISHED'}
    error_message = PDT_ERR_NO_LIBRARY
    self.report({'ERROR'}, error_message)
    return {'FINISHED'}",obj is not None,112,obj,False,4.9787068367863965,N/A
"def add_line_to_bisection(context):
    """"""Computes Bisector of 2 Co-Planar Edges.

    Args:
        context: Blender bpy.context instance

    Returns:
        Nothing.
    """"""
    obj = context.object
<mask>:
        pg = context.scene.pdt_pg
        obj_data = obj.data
        bm = bmesh.from_edit_mesh(obj_data)
        bm.verts.ensure_lookup_table()
        bm.edges.ensure_lookup_table()
        edges = [e for e in bm.edges if e.select and (not e.hide)]
        if not len(edges) == 2:
            pg.error = f'{PDT_ERR_2CPNPE}'
            context.window_manager.popup_menu(oops, title='Error', icon='ERROR')
            return
        [[vector_a, vector_b], [vector_c, vector_d]] = [[v.co for v in e.verts] for e in edges]
        debug(f'vectors found:\n {vector_a}\n {vector_b}\n {vector_c}\n {vector_d}')
        dist1 = (vector_a - vector_b).length
        dist2 = (vector_c - vector_d).length
        bdist = min([dist1, dist2])
        edge1 = (vector_a, vector_b)
        edge2 = (vector_c, vector_d)
        if not cm.test_coplanar(edge1, edge2):
            pg.error = PDT_ERR_NCEDGES
            context.window_manager.popup_menu(oops, title='Error', icon='ERROR')
            return
        intersect_point = cm.get_intersection(edge1, edge2)
        far1 = vector_b if (vector_a - intersect_point).length < (vector_b - intersect_point).length else vector_a
        far2 = vector_d if (vector_c - intersect_point).length < (vector_d - intersect_point).length else vector_c
        dex1 = far1 - intersect_point
        dex2 = far2 - intersect_point
        dex1 = dex1 * (bdist / dex1.length)
        dex2 = dex2 * (bdist / dex2.length)
        intersect_point2 = intersect_point + dex1.lerp(dex2, 0.5)
        intersect_point3 = intersect_point2.lerp(intersect_point, 2.0)
        vec1 = bm.verts.new(intersect_point2)
        vec2 = bm.verts.new(intersect_point)
        vec3 = bm.verts.new(intersect_point3)
        bm.edges.new((vec1, vec2))
        bm.edges.new((vec2, vec3))
        bmesh.ops.remove_doubles(bm, verts=bm.verts, dist=0.0001)
        bmesh.update_edit_mesh(obj_data)
    else:
        pg.error = f'{PDT_ERR_EDOB_MODE},{obj.mode})'
        context.window_manager.popup_menu(oops, title='Error', icon='ERROR')
        return","all([bool(obj), obj.type == 'MESH', obj.mode == 'EDIT'])",197,obj.type == 'Co-Planar',False,4.469085405803316,N/A
"@classmethod
def poll(cls, context):
    """"""Only allow operation on a mesh object in EDIT mode.

        Args:
            context: Blender bpy.context instance.

        Returns:
            Boolean.
        """"""
    obj = context.active_object
<mask>:
        return False
    return all([obj is not None, obj.type == 'MESH', obj.mode == 'EDIT'])",obj is None,39,obj is None,True,100.00000000000004,N/A
"def make_vectors(coords, a1, a2, a3, pg):
    """"""Return Vectors of the Tangent Points.

    Args:
        coords: A List of Coordinates in 2D space of the tangent points
                & a third dimension for the vectors
        a1: Index of horizontal axis
        a2: Index of vertical axis
        a3: Index of depth axis
        pg: PDT Parameters Group - our variables

    Returns:
        tangent_vector_o1: Location of First Tangent Point
        tangent_vector_o2: Location of Second Tangent Point
        tangent_vector_o3: Location of First Tangent Point
        tangent_vector_o4: Location of Second Tangent Point
    """"""
    tangent_vector_o1 = Vector((0, 0, 0))
    tangent_vector_o1[a1] = coords[0]
    tangent_vector_o1[a2] = coords[1]
    tangent_vector_o1[a3] = coords[8]
    tangent_vector_o2 = Vector((0, 0, 0))
    tangent_vector_o2[a1] = coords[2]
    tangent_vector_o2[a2] = coords[3]
    tangent_vector_o2[a3] = coords[8]
    tangent_vector_o3 = Vector((0, 0, 0))
    tangent_vector_o3[a1] = coords[4]
    tangent_vector_o3[a2] = coords[5]
    tangent_vector_o3[a3] = coords[8]
    tangent_vector_o4 = Vector((0, 0, 0))
    tangent_vector_o4[a1] = coords[6]
    tangent_vector_o4[a2] = coords[7]
    tangent_vector_o4[a3] = coords[8]
<mask>:
        tangent_vector_o1 = view_coords(tangent_vector_o1[a1], tangent_vector_o1[a2], tangent_vector_o1[a3])
        tangent_vector_o2 = view_coords(tangent_vector_o2[a1], tangent_vector_o2[a2], tangent_vector_o2[a3])
        tangent_vector_o3 = view_coords(tangent_vector_o3[a1], tangent_vector_o3[a2], tangent_vector_o3[a3])
        tangent_vector_o4 = view_coords(tangent_vector_o4[a1], tangent_vector_o4[a2], tangent_vector_o4[a3])
    return (tangent_vector_o1, tangent_vector_o2, tangent_vector_o3, tangent_vector_o4)",pg.plane == 'LO',162,pg == 0,False,21.444097124017667,N/A
"def draw_tangents(tangent_vectors, obj_data):
    """"""Add Edges Representing the Tangents.

    Note:
        The length of the tanget_vectors determines which tangents will be
        drawn, 3 gives Point Tangents, 4 gives Inner/Outer tangents

    Args:
        tangent_vectors: A list of vectors representing the tangents
        obj_data: A list giving Object, Object Location and Object Bmesh

    Returns:
        Nothing.
    """"""
    obj = obj_data[0]
    obj_loc = obj_data[1]
    bm = obj_data[2]
<mask>:
        point_vertex_outer = bm.verts.new(tangent_vectors[0] - obj_loc)
        tangent_vertex_o1 = bm.verts.new(tangent_vectors[1] - obj_loc)
        tangent_vertex_o2 = bm.verts.new(tangent_vectors[2] - obj_loc)
        bm.edges.new([tangent_vertex_o1, point_vertex_outer])
        bm.edges.new([tangent_vertex_o2, point_vertex_outer])
    else:
        tangent_vertex_o1 = bm.verts.new(tangent_vectors[0] - obj_loc)
        tangent_vertex_o2 = bm.verts.new(tangent_vectors[2] - obj_loc)
        tangent_vertex_o3 = bm.verts.new(tangent_vectors[1] - obj_loc)
        tangent_vertex_o4 = bm.verts.new(tangent_vectors[3] - obj_loc)
        bm.edges.new([tangent_vertex_o1, tangent_vertex_o2])
        bm.edges.new([tangent_vertex_o3, tangent_vertex_o4])
    bmesh.update_edit_mesh(obj.data)",len(tangent_vectors) == 3,105,obj.type == 'Point',False,9.846052248031867,N/A
"def analyse_arc(context, pg):
    """"""Analyses an Arc inferred from Selected Vertices.

    Note:
        Will work if more than 3 vertices are selected, taking the
        first, the nearest to the middle and the last.

    Args:
        context: Blender bpy.context instance
        pg: PDT Parameters Group - our variables

    Returns:
        vector_delta: Location of Arc Centre
        radius: Radius of Arc.
    """"""
    obj = context.view_layer.objects.active
<mask>:
        pg.error = PDT_ERR_NO_ACT_OBJ
        context.window_manager.popup_menu(oops, title='Error', icon='ERROR')
        raise PDT_ObjectModeError
    if obj.mode == 'EDIT':
        obj_loc = obj.matrix_world.decompose()[0]
        bm = bmesh.from_edit_mesh(obj.data)
        verts = [v for v in bm.verts if v.select]
        if len(verts) < 3:
            pg.error = f'{PDT_ERR_SEL_3_VERTS} {len(verts)})'
            context.window_manager.popup_menu(oops, title='Error', icon='ERROR')
            raise PDT_SelectionError
        vector_a = verts[0].co
        vector_b = verts[int(floor(len(verts) / 2))].co
        vector_c = verts[-1].co
        vector_delta, radius = arc_centre(vector_a, vector_b, vector_c)
        return (vector_delta, radius)",obj is None,118,obj is None,True,100.00000000000004,N/A
"@classmethod
def poll(cls, context):
    ob = context.object
<mask>:
        return False
    return all([bool(ob), ob.type == 'MESH', ob.mode == 'EDIT'])",ob is None,18,ob is None,True,100.00000000000004,N/A
"def remove_permutations_that_share_a_vertex(bm, permutations):
    """"""Get useful Permutations.

    Args:
        bm: Object's Bmesh
        permutations: Possible Intersection Edges as a list

    Returns:
        List of Edges.
    """"""
    final_permutations = []
    for edges in permutations:
        raw_vert_indices = cm.vertex_indices_from_edges_tuple(bm, edges)
<mask>:
            continue
        final_permutations.append(edges)
    return final_permutations",len(set(raw_vert_indices)) < 4,38,cm.is_edge_share(raw_vert_indices),False,43.24227075463214,N/A
"def can_skip(closest_points, vert_vectors):
    """"""Check if the intersection lies on both edges and return True
    when criteria are not met, and thus this point can be skipped.

    Args:
        closest_points: List of Coordinates of points to consider
        vert_vectors: List of Coordinates of vertices to consider

    Returns:
        Boolean.
    """"""
<mask>:
        return True
    if not isinstance(closest_points[0].x, float):
        return True
    if cm.num_edges_point_lies_on(closest_points[0], vert_vectors) < 2:
        return True
    cpa, cpb = closest_points
    return (cpa - cpb).length > 1e-05",not closest_points,72,not closest_points,True,100.00000000000004,N/A
"def get_intersection_dictionary(bm, edge_indices):
    """"""Return a dictionary of edge indices and points found on those edges.

    Args:
        bm, Object's Bmesh
        edge_indices: List of Edge Indices

    Returns:
        Dictionary of Vectors.
    """"""
    bm.verts.ensure_lookup_table()
    bm.edges.ensure_lookup_table()
    permutations = get_valid_permutations(bm, edge_indices)
    list_k = defaultdict(list)
    list_d = defaultdict(list)
    for edges in permutations:
        raw_vert_indices = cm.vertex_indices_from_edges_tuple(bm, edges)
        vert_vectors = cm.vectors_from_indices(bm, raw_vert_indices)
        points = LineIntersect(*vert_vectors)
<mask>:
            continue
        [list_k[edge].append(points[0]) for edge in edges]
    for edge_idx, unordered_points in list_k.items():
        tv1, tv2 = bm.edges[edge_idx].verts
        v1 = bm.verts[tv1.index].co
        v2 = bm.verts[tv2.index].co
        ordered_points = order_points((v1, v2), unordered_points)
        list_d[edge_idx].extend(ordered_points)
    return list_d","can_skip(points, vert_vectors)",86,len(points) == 0,False,9.469167282754096,N/A
"def unselect_nonintersecting(bm, d_edges, edge_indices):
    """"""Deselects Non-Intersection Edges.

    Args:
        bm, Object's Bmesh
        d_edges: List of Intersecting Edges
        edge_indices: List of Edge Indices to consider

    Returns:
        Nothing.
    """"""
<mask>:
        reserved_edges = set(edge_indices) - set(d_edges)
        for edge in reserved_edges:
            bm.edges[edge].select = False",len(edge_indices) > len(d_edges),39,edge_indices,False,3.567399334725242,N/A
"def intersect_all(context):
    """"""Computes All intersections with Crossing Geometry.

    Note:
        Deletes original edges and replaces with new intersected edges

    Args:
        context: Blender bpy.context instance.

    Returns:
        Status Set.
    """"""
    pg = context.scene.pdt_pg
    obj = context.active_object
<mask>:
        bpy.context.tool_settings.mesh_select_mode = (False, True, False)
        if obj.mode == 'EDIT':
            bm = bmesh.from_edit_mesh(obj.data)
            selected_edges = [edge for edge in bm.edges if edge.select]
            edge_indices = [i.index for i in selected_edges]
            int_dict = get_intersection_dictionary(bm, edge_indices)
            unselect_nonintersecting(bm, int_dict.keys(), edge_indices)
            update_mesh(bm, int_dict)
            bmesh.update_edit_mesh(obj.data)
        else:
            pg.error = f'{PDT_ERR_EDOB_MODE},{obj.mode})'
            context.window_manager.popup_menu(oops, title='Error', icon='ERROR')
            return
        return
    else:
        pg.error = f'{PDT_ERR_EDOB_MODE},{obj.mode})'
        context.window_manager.popup_menu(oops, title='Error', icon='ERROR')
        return","all([bool(obj), obj.type == 'MESH', obj.mode == 'EDIT'])",89,"obj.mode in ('BMESH', 'EDIT', 'EDIT')",False,5.988463445657318,N/A
"def if_exists_load_env(name: str) -> None:
    current_frame = inspect.currentframe()
<mask>:
        return
    inspect_file = inspect.getfile(current_frame)
    env_path = os.path.dirname(os.path.abspath(inspect_file))
    env_file = '{env_path}/{name}'.format(env_path=env_path, name=name)
    if os.path.exists(env_file):
        dotenv.load_dotenv(env_file, override=True)",not current_frame,24,current_frame is None,False,39.76353643835252,N/A
"def test_google_maps_field_contains_constuct_regular(self):
    widget = GoogleMapsField()
    html = widget.render('field', '', {'id': 'X'})
<mask>:
        self.assertIn('<input type=""hidden"" name=""field"" id=""X"" data-controller=""google-maps-field""', html)
    else:
        self.assertIn('new GoogleMapsField', html)","WAGTAIL_VERSION >= (6, 0)",22,widget.is_google_maps_field_constuct,False,3.7477767366779213,N/A
"def test_google_maps_field_js_init_contains_construct(self):
<mask>:
        pytest.skip('Test only runs on Wagtail versions lower than 6.0')
    widget = GoogleMapsField()
    html = widget.render_js_init('id', 'field', '')
    self.assertIn('new GoogleMapsField', html)","WAGTAIL_VERSION >= (6, 0)",23,sys.version_info[0] < 6,False,5.522397783539471,N/A
"def test_leaflet_field_contains_constuct_regular(self):
    widget = LeafletField()
    html = widget.render('field', '', {'id': 'X'})
<mask>:
        self.assertIn('<input type=""hidden"" name=""field"" id=""X"" data-controller=""leaflet-field""', html)
    else:
        self.assertIn('new LeafletField', html)","WAGTAIL_VERSION >= (6, 0)",22,widget.is_leaflet,False,3.929752628321626,N/A
"def test_leaflet_field_js_init_contains_construct(self):
    widget = LeafletField()
<mask>:
        html = widget.render('field', '', {'id': 'X'})
        self.assertIn('<input type=""hidden"" name=""field"" id=""X"" data-controller=""leaflet-field""', html)
    else:
        html = widget.render_js_init('id', 'field', '')
        self.assertIn('new LeafletField', html)","WAGTAIL_VERSION >= (6, 0)",27,sys.version_info[0] == 2,False,4.9323515694897075,N/A
"def test_value_are_parsed_properly(self):
    widget = LeafletField()
<mask>:
        from html import escape
        html = widget.render('field', 'SRID=5432;POINT(12.0 13.0)', {'id': 'X'})
        self.assertIn(escape('""lat"": ""13.0""'), html)
        self.assertIn(escape('""lng"": ""12.0""'), html)
    else:
        html = widget.render_js_init('id', 'field', 'SRID=5432;POINT(12.0 13.0)')
        self.assertIn('""lat"": ""13.0""', html)
        self.assertIn('""lng"": ""12.0""', html)","WAGTAIL_VERSION >= (6, 0)",36,sys.version_info[0] == 2,False,4.9323515694897075,N/A
"def get_env(name, default=None):
    """"""Get the environment variable or return exception""""""
<mask>:
        return os.environ[name]
    if default is not None:
        return default
    error_msg = 'Set the {} env variable'.format(name)
    raise ImproperlyConfigured(error_msg)",name in os.environ,29,name in os.environ,True,100.00000000000004,N/A
"def handle(self, *args, **options):
    User = get_user_model()
<mask>:
        return
    username = options['user']
    password = options['password']
    email = options['email']
    User.objects.create_superuser(username=username, password=password, email=email)
    self.stdout.write('Local user ""{}"" was created'.format(username))",User.objects.exists(),26,User.objects.filter(user=self.user).exists(),False,29.615165360116254,N/A
"def search(request):
    search_query = request.GET.get('query', None)
    page = request.GET.get('page', 1)
<mask>:
        search_results = Page.objects.live().search(search_query)
        query = Query.get(search_query)
        query.add_hit()
    else:
        search_results = Page.objects.none()
    paginator = Paginator(search_results, 10)
    try:
        search_results = paginator.page(page)
    except PageNotAnInteger:
        search_results = paginator.page(1)
    except EmptyPage:
        search_results = paginator.page(paginator.num_pages)
    return render(request, 'search/search.html', {'search_query': search_query, 'search_results': search_results})",search_query,47,search_query,True,100.00000000000004,N/A
"def build_attrs(self, *args, **kwargs):
    data = {'defaultLocation': GEO_WIDGET_DEFAULT_LOCATION, 'addressField': self.address_field, 'zoomField': self.zoom_field, 'zoom': self.zoom, 'srid': self.srid, 'showEmptyLocation': GEO_WIDGET_EMPTY_LOCATION, 'translations': translations}
<mask>:
        result = geosgeometry_str_to_struct(self.value_data)
        if result:
            data['defaultLocation'] = {'lat': result['y'], 'lng': result['x']}
    if self.value_data and Point and isinstance(self.value_data, Point):
        data['defaultLocation'] = {'lat': self.value_data.y, 'lng': self.value_data.x}
    attrs = super().build_attrs(*args, **kwargs)
    attrs['data-controller'] = 'google-maps-field'
    attrs['data-google-maps-field-options-value'] = json.dumps(data)
    return attrs","self.value_data and isinstance(self.value_data, str)",57,"self.value_data and (not Point) and isinstance(self.value_data, Point)",False,60.54783271684541,N/A
"@cached_property
def media(self):
    from django.utils.module_loading import import_string
    from wagtailgeowidget.app_settings import GOOGLE_MAPS_V3_APIKEY, GOOGLE_MAPS_V3_APIKEY_CALLBACK, GOOGLE_MAPS_V3_LANGUAGE
    google_maps_apikey = GOOGLE_MAPS_V3_APIKEY
<mask>:
        if isinstance(GOOGLE_MAPS_V3_APIKEY_CALLBACK, str):
            callback = import_string(GOOGLE_MAPS_V3_APIKEY_CALLBACK)
        else:
            callback = GOOGLE_MAPS_V3_APIKEY_CALLBACK
        google_maps_apikey = callback()
    return forms.Media(css={'all': ('wagtailgeowidget/css/google-maps-field.css',)}, js=('wagtailgeowidget/js/google-maps-field.js', 'wagtailgeowidget/js/google-maps-field-controller.js', 'https://maps.google.com/maps/api/js?key={}&libraries=places&language={}'.format(google_maps_apikey, GOOGLE_MAPS_V3_LANGUAGE)))",GOOGLE_MAPS_V3_APIKEY_CALLBACK,37,GOOGLE_MAPS_V3_APIKEY_CALLBACK,True,100.00000000000004,N/A
"def render(self, name, value, attrs=None, renderer=None):
    try:
        id_ = attrs['id']
    except (KeyError, TypeError):
        raise TypeError(""GoogleMapsField cannot be rendered without an 'id' attribute"")
    self.value_data = value
    widget_html = super().render(name, self.value_data, attrs)
    input_classes = 'google-maps-location'
<mask>:
        input_classes = '{} {}'.format(input_classes, 'google-maps-field-location--hide')
    location = format_html('<div class=""input""><input id=""{0}_latlng"" class=""{1}"" maxlength=""250"" type=""text""></div>', id_, input_classes)
    return mark_safe(widget_html + location + '<div id=""{0}_map"" class=""google-maps-field""></div>'.format(id_))",self.hide_latlng,57,self.value_data.location,False,14.535768424205482,N/A
"def build_attrs(self, *args, **kwargs):
    options = {'translations': translations}
    params = {}
<mask>:
        from wagtailgeowidget.app_settings import MAPBOX_ACCESS_TOKEN, MAPBOX_LANGUAGE
        params['accessToken'] = MAPBOX_ACCESS_TOKEN
        params['language'] = MAPBOX_LANGUAGE
    options['params'] = params
    attrs = super().build_attrs(*args, **kwargs)
    attrs['data-controller'] = 'geocoder-field'
    attrs['data-geocoder-field-geocoder-value'] = self.geocoder
    attrs['data-geocoder-field-options-value'] = json.dumps(options)
    return attrs",self.geocoder == geocoders.MAPBOX,41,self.geocoder,False,18.887560283756194,N/A
"@property
def media(self):
    js = ['wagtailgeowidget/js/geocoder-field.js', 'wagtailgeowidget/js/geocoder-field-controller.js']
    from wagtailgeowidget.app_settings import GOOGLE_MAPS_V3_APIKEY, GOOGLE_MAPS_V3_LANGUAGE
<mask>:
        js = [*js, 'https://maps.google.com/maps/api/js?key={}&libraries=places&language={}'.format(GOOGLE_MAPS_V3_APIKEY, GOOGLE_MAPS_V3_LANGUAGE)]
    return forms.Media(js=js)",self.geocoder == geocoders.GOOGLE_MAPS,20,GOOGLE_MAPS_V3_APIKEY,False,15.925177647011354,N/A
"def render_form(self, value, prefix='', errors=None):
<mask>:
        value = 'SRID={};POINT({} {})'.format(value['srid'], value['lng'], value['lat'])
    return super().render_form(value, prefix, errors)","value and isinstance(value, dict)",16,value,False,0.09118819655545167,N/A
"def value_for_form(self, value):
<mask>:
        return None
    if value and isinstance(value, str):
        return value
    val = 'SRID={};POINT({} {})'.format(4326, value['lng'], value['lat'])
    return val",not value,21,value is None,False,27.516060407455225,N/A
"def to_python(self, value):
<mask>:
        return value
    value = geosgeometry_str_to_struct(value)
    if not value:
        raise Exception(""Error: Cannot parse '{}' into struct"".format(value))
    value = {'lat': value['y'], 'lng': value['x'], 'srid': value['srid']}
    return super().to_python(value)","isinstance(value, dict)",29,"isinstance(value, dict)",True,100.00000000000004,N/A
"def geosgeometry_str_to_struct(value: str) -> Optional[Dict]:
    """"""
    Parses a geosgeometry string into struct.

    Example:
        SRID=5432;POINT(12.0 13.0)
    Returns:
        >> [5432, 12.0, 13.0]
    """"""
    result = geos_ptrn.match(value)
<mask>:
        return None
    return {'srid': result.group(1), 'x': result.group(2), 'y': result.group(3)}",not result,34,not result,True,100.00000000000004,N/A
"def get_test_dir():
    """"""Return a module specific tmpdir.""""""
    frame = inspect.currentframe()
<mask>:
        caller = frame.f_back
        module_name = caller.f_globals['__name__']
    else:
        module_name = 'unknown'
    return TMP_ROOT / Path('picopt-' + module_name)",frame and frame.f_back,27,frame,False,0.24787521766663595,N/A
"def test_containers_noop(self) -> None:
    """"""Test containers noop.""""""
    args = (PROGRAM_NAME, '-r', str(TMP_ROOT))
    cli.main(args)
    for name, sizes in FNS.items():
<mask>:
            path = TMP_ROOT / name
            with ZipFile(path, 'r') as zf:
                namelist = zf.namelist()
            assert BMP_FN in namelist
        path = TMP_ROOT / name
        assert path.stat().st_size == sizes[0]",name == EPUB_FN,45,BMP_FN in name,False,19.3576934939088,N/A
"def test_containers_no_convert(self) -> None:
    """"""Test containers no convert.""""""
    args = (PROGRAM_NAME, '-rx', 'GIF,CBZ,ZIP,EPUB', '-c WEBP', str(TMP_ROOT))
    cli.main(args)
    for name, sizes in FNS.items():
        path = TMP_ROOT / name
<mask>:
            with ZipFile(path, 'r') as zf:
                namelist = zf.namelist()
            assert BMP_FN in namelist
        assert path.stat().st_size == sizes[1]",name == EPUB_FN,44,path.exists(),False,0.0,N/A
"@staticmethod
def _write_timestamp(path, ts=None, config=None):
    """"""Write timestamp.""""""
<mask>:
        ts = datetime.now(tz=timezone.utc).timestamp()
    if config is None:
        config = DEFAULT_CONFIG
    ts_config = {**TREESTAMPS_CONFIG}
    for key in TREESTAMPS_CONFIG:
        ts_config[key] = config[key]
    yaml = {'config': config, 'treestamps_config': ts_config, str(path): ts}
    YAML().dump(yaml, TIMESTAMPS_PATH)
    assert TIMESTAMPS_PATH.exists()
    assert not WAL_PATH.exists()
    print(yaml)",ts is None,44,ts is None,True,100.00000000000004,N/A
"@staticmethod
def _write_wal(path, ts=None, config=None):
    """"""Write wal.""""""
<mask>:
        ts = datetime.now(tz=timezone.utc).timestamp()
    if config is None:
        config = DEFAULT_CONFIG
    ts_config = {**TREESTAMPS_CONFIG}
    for key in TREESTAMPS_CONFIG:
        ts_config[key] = config[key]
    yaml = {'config': config, 'treestamps_config': ts_config, 'wal': [{str(path): ts}]}
    YAML().dump(yaml, WAL_PATH)",ts is None,39,ts is None,True,100.00000000000004,N/A
"def _print_formats_config(verbose: int, handled_format_strs: set[str], convert_format_strs: dict[str, set[str]], is_modern_cwebp: bool, cwebp_version: str) -> None:
    """"""Print verbose init messages.""""""
    handled_format_list = ', '.join(sorted(handled_format_strs))
    cprint(f'Optimizing formats: {handled_format_list}')
    for convert_to_format_str, format_strs in convert_format_strs.items():
        convert_from_list = sorted(format_strs)
<mask>:
            return
        convert_from = ', '.join(convert_from_list)
        cprint(f'Converting {convert_from} to {convert_to_format_str}', 'cyan')
    if verbose > 1 and (not is_modern_cwebp) and (WebPAnimatedLossless.OUTPUT_FORMAT_STR in convert_format_strs):
        to_webp_strs = MODERN_CWEBP_FORMAT_STRS & handled_format_strs
        if to_webp_strs:
            to_web_str = ' & '.join(sorted(to_webp_strs))
            cprint(f'Converting {to_web_str} with an extra step for older cwebp {cwebp_version}', 'cyan')",not convert_from_list,78,not convert_from_list,True,100.00000000000004,N/A
"def _get_handler_stages(handler_class: type[Handler], disabled_programs: frozenset) -> dict[str, tuple[str, ...]]:
    """"""Get the program stages for each handler.""""""
    stages = {}
    for program_priority_list in handler_class.PROGRAMS:
        for program in program_priority_list:
<mask>:
                continue
            if program.startswith(('pil2', Handler.INTERNAL)):
                exec_args = None
            elif program.startswith('npx_'):
                bin_path = shutil.which('npx')
                if not bin_path:
                    continue
                exec_args = (bin_path, '--no', *program.split('_')[1:])
                try:
                    subprocess.run(exec_args, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                except (subprocess.CalledProcessError, FileNotFoundError, OSError):
                    continue
            else:
                bin_path = shutil.which(program)
                if not bin_path:
                    continue
                exec_args = (bin_path,)
            stages[program] = exec_args
            break
    return stages",program in disabled_programs,76,program in disabled_programs,True,100.00000000000004,N/A
"def _set_format_handler_map_entry(handler_type: str, handler_class: type[Handler], convert_to: frozenset[str], handler_stages: dict[type[Handler], dict[str, tuple[str, ...]]], convert_format_strs: dict, file_format: FileFormat, convert_handlers: dict[FileFormat, type[Handler]], native_handlers: dict[FileFormat, type[Handler]], handled_format_strs: set[str], disabled_programs: frozenset) -> bool:
    """"""Create an entry for the format handler maps.""""""
<mask>:
        return False
    if handler_class not in handler_stages:
        handler_stages[handler_class] = _get_handler_stages(handler_class, disabled_programs)
    stages = handler_stages.get(handler_class)
    if not stages:
        return False
    if handler_type == 'convert':
        if handler_class.OUTPUT_FORMAT_STR not in convert_format_strs:
            convert_format_strs[handler_class.OUTPUT_FORMAT_STR] = set()
        convert_format_strs[handler_class.OUTPUT_FORMAT_STR].add(file_format.format_str)
        convert_handlers[file_format] = handler_class
    else:
        native_handlers[file_format] = handler_class
    handled_format_strs.add(file_format.format_str)
    return True",handler_type == 'convert' and handler_class.OUTPUT_FILE_FORMAT.format_str not in convert_to,79,not handler_type,False,0.33528656955623864,N/A
"def _get_cwebp_version(handler_stages: dict):
    """"""Get the version of cwebp.""""""
    cwebp_version = ''
    bin_path = handler_stages.get(WebPLossless, {}).get('cwebp')
<mask>:
        return cwebp_version
    args = (*bin_path, '-version')
    result = subprocess.run(args, capture_output=True, text=True, check=True)
    if result.returncode == 0:
        cwebp_version = result.stdout.splitlines()[0].strip()
    return cwebp_version",not bin_path,37,bin_path is None,False,39.76353643835252,N/A
"def _is_cwebp_modern(handler_stages: dict) -> tuple[bool, str]:
    cwebp_version = 'Unknown'
    try:
        cwebp_version = _get_cwebp_version(handler_stages)
<mask>:
            return (False, cwebp_version)
        parts = cwebp_version.split('.')
        for index in range(len(MIN_CWEBP_VERSION)):
            test_part = int(parts[index])
            ref_part = MIN_CWEBP_VERSION[index]
            if test_part > ref_part:
                return (True, cwebp_version)
            if test_part < ref_part:
                return (False, cwebp_version)
    except Exception:
        return (False, cwebp_version)
    return (True, cwebp_version)",not cwebp_version,52,not cwebp_version.startswith('.cwebp'),False,23.462350320528007,N/A
"def _init_run(self):
    """"""Init Run.""""""
<mask>:
        msg = 'No paths to optimize.'
        raise PicoptError(msg)
    for path in self._top_paths:
        if not path.exists():
            msg = f'Path does not exist: {path}'
            raise PicoptError(msg)
    if self._config.timestamps:
        self._init_run_timestamps()",not self._top_paths,32,len(self._top_paths) == 0,False,41.72261448611505,N/A
"def _is_skippable(self, path_info: PathInfo) -> bool:
    """"""Handle things that are not optimizable files.""""""
    reason = None
    color = 'white'
    attrs: list = ['dark']
<mask>:
        reason = f'Skip archive directory {path_info.full_name()}'
    elif not self._config.symlinks and path_info.path and path_info.path.is_symlink():
        reason = f'Skip symlink {path_info.full_name()}'
    elif path_info.name() in self.TIMESTAMPS_FILENAMES:
        legacy = 'legacy ' if path_info.name() == OLD_TIMESTAMPS_NAME else ''
        reason = f'Skip {legacy}timestamp {path_info.full_name()}'
    elif not path_info.zipinfo and path_info.path and (not path_info.path.exists()):
        reason = f'WARNING: {path_info.full_name()} not found.'
        color = 'yellow'
        attrs = []
    elif is_path_ignored(self._config, Path(path_info.name())):
        reason = f'Skip ignored {path_info.full_name()}'
    if reason and self._config.verbose > 1:
        cprint(reason, color, attrs=attrs)
    return bool(reason)",path_info.zipinfo and path_info.is_dir(),100,not path_info.zipinfo and path_info.path and (not path_info.path.is_dir()),False,49.45703137422567,N/A
"def _is_older_than_timestamp(self, path_info: PathInfo) -> bool:
    """"""Is the file older than the timestamp.""""""
<mask>:
        walk_after = self._config.after
    elif path_info.path and self._config.timestamps:
        timestamps = self._timestamps.get(path_info.top_path, {})
        walk_after = timestamps.get(path_info.path)
    else:
        walk_after = None
    if walk_after is None:
        return False
    mtime = path_info.mtime()
    return bool(mtime <= walk_after)",self._config.after is not None,45,path_info.after and self._config.after,False,41.72261448611505,N/A
"def _clean_up_working_files(self, path: Path) -> None:
    """"""Auto-clean old working temp files if encountered.""""""
    try:
<mask>:
            shutil.rmtree(path, ignore_errors=True)
        else:
            path.unlink(missing_ok=True)
        if self._config.verbose > 1:
            cprint(f'Deleted {path}', 'yellow')
    except Exception as exc:
        cprint(str(exc), 'red')",path.is_dir(),32,path.exists(),False,20.252884954471366,N/A
"def _finish_results(self, results: list[ApplyResult], container_mtime: float | None, top_path: Path) -> None:
    """"""Get the async results and total them.""""""
    for result in results:
        final_result = result.get()
<mask>:
            final_result.report()
            self._totals.errors.append(final_result)
        else:
            self._totals.bytes_in += final_result.bytes_in
            if final_result.saved > 0 and (not self._config.bigger):
                self._totals.bytes_out += final_result.bytes_out
            else:
                self._totals.bytes_out += final_result.bytes_in
        if self._config.timestamps and (not container_mtime):
            timestamps = self._timestamps[top_path]
            timestamps.set(final_result.path)",final_result.exc,56,final_result.errors,False,66.87403049764218,N/A
"def __call__(self, _parser, namespace, values, _option_string=None):
    """"""Split values string into list.""""""
<mask>:
        values = tuple(sorted(values.strip().split(_LIST_DELIMETER)))
    setattr(namespace, self.dest, values)","isinstance(values, str)",18,values.strip(),False,11.51015341649912,N/A
"def _report_saved(self) -> str:
    """"""Return the percent saved.""""""
    report = f'{self._get_full_path()}: '
    report += self._new_percent_saved()
<mask>:
        report += ' would be'
    if self.saved > 0:
        report += ' saved'
    elif self.saved < 0:
        report += ' lost'
    if self.saved <= 0 and (not self.bigger):
        report += ', kept original'
    return report",self.test,51,self.saved > 0,False,21.3643503198117,N/A
"def _report_error(self) -> str:
    """"""Return the error report string.""""""
    report = f'ERROR: {self._get_full_path()}'
<mask>:
        report += f'\n{self._TAB}retcode: {self.exc.returncode}'
        if self.exc.cmd:
            cmd = ' '.join(self.exc.cmd)
            report += f'\n{self._TAB}command: {cmd}'
        if self.exc.stdout:
            report += f'\n{self._TAB}stdout: {self.exc.stdout}'
        if self.exc.stderr:
            report += f'\n{self._TAB}stderr: {self.exc.stderr}'
    else:
        report += f'\n{self._TAB}{self.exc!s}'
    return report","isinstance(self.exc, CalledProcessError)",46,self.exc.returncode,False,21.82269148961668,N/A
"def report(self) -> None:
    """"""Record the percent saved & print it.""""""
    attrs = []
<mask>:
        report = self._report_error()
        color: Color = 'red'
    else:
        report = self._report_saved()
        color: Color = 'cyan' if self.convert else 'white'
        if self.saved <= 0:
            color: Color = 'blue'
            attrs: list[Attribute] = ['bold']
    cprint(report, color, attrs=attrs)",self.exc,49,self.convert,False,55.03212081491043,N/A
"def _report_bytes_in(self) -> None:
    """"""Report Totals if there were bytes in.""""""
<mask>:
        return
    bytes_saved = self.bytes_in - self.bytes_out
    percent_bytes_saved = bytes_saved / self.bytes_in * 100
    msg = ''
    if self._config.test:
        if percent_bytes_saved > 0:
            msg += 'Could save'
        elif percent_bytes_saved == 0:
            msg += 'Could even out for'
        else:
            msg += 'Could lose'
    elif percent_bytes_saved > 0:
        msg += 'Saved'
    elif percent_bytes_saved == 0:
        msg += 'Evened out'
    else:
        msg = 'Lost'
    natural_saved = naturalsize(bytes_saved)
    msg += f' a total of {natural_saved} or {percent_bytes_saved:.2f}%'
    cprint(msg)
    if self._config.test:
        cprint('Test run did not change any files.')",not self._config.verbose and (not self._config.test),94,self.bytes_out is None,False,3.4835119683384828,N/A
"def report(self) -> None:
    """"""Report the total number and percent of bytes saved.""""""
<mask>:
        cprint('')
    if self.bytes_in:
        self._report_bytes_in()
    elif self._config.verbose:
        cprint(""Didn't optimize any files."")
    if self.errors:
        cprint('Errors with the following files:', 'red')
        for rs in self.errors:
            rs.report()",self._config.verbose == 1,37,self._config.verbose,False,60.653065971263366,N/A
"def _add_old_timestamp(self, old_timestamp_path: Path) -> None:
    """"""Get the timestamp from a old style timestamp file.""""""
<mask>:
        return
    path = old_timestamp_path.parent
    mtime = old_timestamp_path.stat().st_mtime
    self._timestamps.set(path, mtime)",not old_timestamp_path.exists(),25,not old_timestamp_path.exists(),True,100.00000000000004,N/A
"def _import_old_parent_timestamps(self, path: Path) -> None:
    """"""Walk up to the root eating old style timestamps.""""""
<mask>:
        return
    old_timestamp_path = path / OLD_TIMESTAMPS_NAME
    self._add_old_timestamp(old_timestamp_path)
    if path.parent != path:
        self._import_old_parent_timestamps(path.parent)","is_path_ignored(self._config, path) or (not self._config.symlinks and path.is_symlink())",28,not path.is_dir(),False,2.410588336100134,N/A
"def _import_old_child_timestamps(self, path: Path) -> None:
<mask>:
        return
    for root, dirnames, filenames in os.walk(path):
        root_path = Path(root)
        if OLD_TIMESTAMPS_NAME in filenames:
            old_timestamp_path = root_path / OLD_TIMESTAMPS_NAME
            self._add_old_timestamp(old_timestamp_path)
            self._timestamps._consumed_paths.add(old_timestamp_path)
        for dirname in dirnames:
            self._import_old_child_timestamps(root_path / dirname)","is_path_ignored(self._config, path) or (not self._config.symlinks and path.is_symlink())",35,not path,False,3.5662765227733845e-05,N/A
"def is_dir(self) -> bool:
    """"""Is the file a directory.""""""
<mask>:
        if self.zipinfo:
            self._is_dir = self.zipinfo.is_dir()
        elif self.path:
            self._is_dir = self.path.is_dir()
        else:
            self._is_dir = False
    return self._is_dir",self._is_dir is None,26,not self._is_dir,False,70.1396726799769,N/A
"def is_container_child(self) -> bool:
    """"""Is this path inside a container.""""""
<mask>:
        self._is_container_child = self.frame is not None or bool(self.container_mtime)
    return self._is_container_child",self._is_container_child is None,21,self._is_container_child is None,True,100.00000000000004,N/A
"def stat(self) -> stat_result | bool:
    """"""Return fs_stat if possible.""""""
<mask>:
        self._stat = self.path.stat() if self.path else False
    return self._stat",self._stat is None,20,self._stat is None,True,100.00000000000004,N/A
"def data(self) -> bytes:
    """"""Get the data from the file.""""""
<mask>:
        if not self.path or self.path.is_dir():
            self._data = b''
        else:
            with self.path.open('rb') as fp:
                self._data = fp.read()
    return self._data",self._data is None,29,not self._data,False,54.75182535069452,N/A
"def fp_or_buffer(self) -> BufferedReader | BytesIO:
    """"""Return an file pointer for chunking or buffer.""""""
<mask>:
        return self.path.open('rb')
    return self._buffer()",self.path,19,self.path is not None,False,30.213753973567677,N/A
"def _get_xmp(info, image, path_name):
    """"""Get xmp data if not automatically included in info.""""""
    try:
<mask>:
            with suppress(AttributeError):
                if (xmp := image.getxmp()):
                    info['xmp'] = xmp
    except Exception as exc:
        cprint(f'WARNING: Failed to extract xmp data for {path_name} {exc}', 'yellow')",'xmp' not in info,38,'xmp' not in info,True,100.00000000000004,N/A
"def _get_exif_bytes(info, image, path_name):
    """"""Get exif in bytes format for possible webp in the future.""""""
    try:
        with suppress(AttributeError):
<mask>:
                info['exif_bytes'] = exif.tobytes()
    except Exception as exc:
        cprint(f'WARNING: Failed to extract exif bytes data for {path_name} {exc}', 'yellow')",(exif := image.getexif()),37,exif is not None,False,3.564186929405141,N/A
"def extract_info_for_webp(keep_metadata, info, image, path_info):
    """"""Extract info manually for webp later in handler.""""""
<mask>:
        return
    full_name = path_info.full_name()
    _get_xmp(info, image, full_name)
    _get_exif_bytes(info, image, full_name)",not keep_metadata,24,not keep_metadata,True,100.00000000000004,N/A
"def webp_convert_info_metadata(config, info):
    """"""Convert info values into webp bytes.""""""
<mask>:
        return
    if (xmp := info.get('xmp')) and isinstance(xmp, Mapping):
        info['xmp'] = _xmp_to_bytes(xmp)
    if (exif := info.get('exif')) and (not isinstance(exif, bytes)):
        info['exif'] = info.pop('exif_bytes', b'')
    if (icc_profile := info.get('icc_profile')) and (not isinstance(icc_profile, bytes)):
        info['icc_profile'] = icc_profile.encode()",not config.keep_metadata,44,config.get('webp_info') is None,False,9.287528999566801,N/A
"def get_jpeg_xmp(image: JpegImageFile) -> str | None:
    """"""Get raw jpeg xml from Pillow.""""""
    xmp = None
    for segment, content in image.applist:
<mask>:
            sections = content.split(APP1_SECTION_DELIMETER)
            marker, xmp_tags = sections[:2]
            if marker == XAP_MARKER:
                xmp = xmp_tags.decode()
                break
    return xmp",segment == 'APP1',39,segment == APP1_SECTION_DELIMETER,False,20.556680845025987,N/A
"def png_bit_depth(buffer: BinaryIO) -> int | None:
    """"""If a file is a png, get the bit depth from the standard position.""""""
<mask>:
        buffer.seek(BIT_DEPTH_OFFSET)
        depth = buffer.read(1)
        result = int.from_bytes(depth, byteorder='little')
    else:
        cprint('WARNING: cannot find bit depts of non png.', 'yellow')
        result = None
    return result",PNG_HEADER.compare(buffer),45,buffer.is_png(),False,8.513058489093439,N/A
"def is_lossless(input_buffer: BytesIO | BufferedReader) -> bool:
    """"""Compare header types against lossless types.""""""
    result = True
    buffer: BytesIO | mmap = mmap(input_buffer.fileno(), 0, access=ACCESS_READ) if isinstance(input_buffer, BufferedReader) else input_buffer
<mask>:
        result = False
    else:
        x = buffer.read(1)
        if x == b'L':
            result = True
        elif x == b'X':
            finder = buffer if isinstance(buffer, mmap) else bytearray(buffer.read(SEARCH_LEN))
            result = finder.find(VP8L_HEADER) != -1
        else:
            result = False
    input_buffer.close()
    buffer.close()
    return result",not VP8_HEADER.compare(buffer),69,buffer.fileno() == 0,False,6.892168295481103,N/A
"def set_jpeg_xmp(jpeg_data: bytes, xmp: str) -> bytes:
    """"""Insert xmp data into jpeg.""""""
    jpeg_buffer = bytearray(jpeg_data)
    soi_index = jpeg_buffer.find(SOI_MARKER)
<mask>:
        reason = 'SOI marker not found in JPEG buffer.'
        raise ValueError(reason)
    xmp_bytes = XAP_MARKER + APP1_SECTION_DELIMETER + xmp.encode('utf-8') + APP1_SECTION_DELIMETER
    return bytes(jpeg_buffer[:soi_index + len(SOI_MARKER)]) + EOI_MARKER + struct.pack('<H', len(xmp_bytes) + len(EOI_MARKER)) + xmp_bytes + bytes(jpeg_buffer[soi_index + len(SOI_MARKER):])",soi_index == -1,56,soi_index < 0,False,32.555630133216134,N/A
"def cwebp(self, exec_args: tuple[str, ...], input_buffer: BinaryIO, opts: tuple[str, ...] | None=None) -> BinaryIO:
    """"""Optimize using cwebp.""""""
    args = [*exec_args]
<mask>:
        args += [*opts]
    args += [*self.CWEBP_ARGS_PREFIX]
    args += ['-metadata']
    if self.config.keep_metadata:
        args += ['all']
    else:
        args += ['none']
    input_path_tmp = isinstance(input_buffer, BytesIO)
    input_path: Path | None = self.get_working_path('cwebp-input') if input_path_tmp else self.path_info.path
    if not input_path:
        reason = 'No input path for cwebp'
        raise ValueError(reason)
    output_path = self.get_working_path('cwebp-output')
    output_path_tmp = bool(self.path_info.path)
    args += [str(input_path), '-o', str(output_path)]
    return self.run_ext_fs(tuple(args), input_buffer, input_path, output_path, input_path_tmp, output_path_tmp)",opts,83,opts,True,100.00000000000004,N/A
"def __init__(self, config: AttrDict, path_info: PathInfo, input_file_format: FileFormat, info: Mapping[str, Any]):
    """"""Initialize extra input formats.""""""
    super().__init__(config, path_info, input_file_format, info)
<mask>:
        self._input_file_formats |= MODERN_CWEBP_FORMATS",config.computed.is_modern_cwebp,23,input_file_format.get_mode() == MODERN_CWEBP_FORMATS,False,2.8265205879007453,N/A
"@classmethod
def run_ext(cls, args: tuple[str, ...], input_buffer: BinaryIO) -> BytesIO:
    """"""Run EXTERNAL program.""""""
    for arg in args:
<mask>:
            reason = f'{args}'
            raise ValueError(reason)
    input_buffer.seek(0)
    result = subprocess.run(args, check=True, input=input_buffer.read(), stdout=subprocess.PIPE)
    return BytesIO(result.stdout)","arg in (None, '')",32,arg != '__main__',False,4.196114906296549,N/A
"def get_working_path(self, identifier: str) -> Path:
    """"""Return a working path with a custom suffix.""""""
<mask>:
        path_tail = '__'.join((*cps[1:], str(self.original_path)))
        path_tail = path_tail.translate(WORKING_PATH_TRANS_TABLE)
        path = Path(cps[0] + '__' + path_tail)
    else:
        path = self.original_path
    suffixes = [self.original_path.suffix, self.WORKING_SUFFIX]
    if identifier:
        suffixes += [identifier]
    suffix = '.'.join(suffixes)
    suffix += self.output_suffix
    return path.with_suffix(suffix)",(cps := self.path_info.container_paths),50,self.is_working,False,3.9082509128279828,N/A
"def __init__(self, config: AttrDict, path_info: PathInfo, input_file_format: FileFormat, info: Mapping[str, Any]):
    """"""Initialize handler.""""""
    self.config: AttrDict = config
    self.path_info: PathInfo = path_info
    self.original_path: Path = path_info.path if path_info.path else Path(path_info.name())
    self.working_path = self.original_path
    default_suffix = self.get_default_suffix()
    self._suffixes = self.get_suffixes(default_suffix)
    suffix = path_info.suffix()
    self.output_suffix: str = suffix if suffix.lower() in self._suffixes else default_suffix
    self.final_path: Path = self.original_path.with_suffix(self.output_suffix)
    self.input_file_format: FileFormat = input_file_format
    self.info: dict[str, Any] = dict(info)
<mask>:
        self.path_info.stat()
    self._input_file_formats = self.INPUT_FILE_FORMATS",self.config.preserve,69,self.path_info.stat,False,14.535768424205482,N/A
"def _prepare_info_webp(self):
    """"""Transform info for webp.""""""
    background = self.info.pop('background', None)
<mask>:
        rgb = _gif_palette_index_to_rgb(background)
        self.info['background'] = (*rgb, 0)","isinstance(background, int)",18,background is not None,False,9.688464563433238,N/A
"def _prepare_info_png(self):
    """"""Transform info for png.""""""
    transparency = self.info.get('transparency')
<mask>:
        self.info.pop('transparency', None)
    if (xmp := self.info.get('xmp', None)):
        pnginfo = self.info.get('pnginfo', PngInfo())
        pnginfo.add_text(PNGINFO_XMP_KEY, xmp, zip=True)
        self.info['pnginfo'] = pnginfo","isinstance(transparency, int)",27,transparency is not None,False,9.688464563433238,N/A
"@classmethod
def identify_format(cls, path_info: PathInfo) -> FileFormat | None:
    """"""Return the format if this handler can handle this path.""""""
    suffix = path_info.suffix().lower()
<mask>:
        return cls.OUTPUT_FILE_FORMAT
    return None",suffix == cls.get_default_suffix(),27,suffix.endswith('.py'),False,4.736913377107212,N/A
"def _mpo2jpeg_get_frame(self, input_buffer: BinaryIO) -> bytes:
    """"""Get Primary JPEG Offsets.""""""
    mpo_info = self.info.pop('mpinfo', {})
    mpo_metadata_list = mpo_info.get(MPO_METADATA, ())
    for mpo_metadata in mpo_metadata_list:
        attr = mpo_metadata.get('Attribute', {})
        mp_type = attr.get('MPType')
<mask>:
            offset = mpo_metadata.get('DataOffset', -1)
            size = mpo_metadata.get('Size', -1)
            break
    else:
        offset = size = -1
    if -1 in (offset, size):
        reason = f'{self.original_path} could not find {MPO_TYPE_PRIMARY} in MPO'
        raise ValueError(reason)
    input_buffer.seek(offset)
    return input_buffer.read(size)",mp_type == MPO_TYPE_PRIMARY,64,mp_type == MPO_TYPE_PRIMARY,True,100.00000000000004,N/A
"def _mpo2jpeg_copy_exif(self, jpeg_data: bytes) -> bytes:
    """"""Copy MPO EXIF into JPEG.""""""
    output_buffer = BytesIO()
<mask>:
        piexif.insert(exif, jpeg_data, output_buffer)
        return output_buffer.read()
    return jpeg_data",(exif := self.info.get('exif')),22,self.exif_exists(exif),False,8.840282257465129,N/A
"def _mpo2jpeg_copy_xmp(self, jpeg_data: bytes) -> bytes:
    """"""Copy MPO XMP into JPEG manually.""""""
<mask>:
        jpeg_data = set_jpeg_xmp(jpeg_data, xmp)
    return jpeg_data",(xmp := self.info.get('xmp')),19,self.mp_type == 'xmp',False,7.030417713400723,N/A
"def pil2jpeg(self, exec_args: tuple[str, ...], input_buffer: BinaryIO) -> BinaryIO:
    """"""Convert MPOs with primary images to jpeg.""""""
<mask>:
        return input_buffer
    jpeg_data = self._mpo2jpeg_get_frame(input_buffer)
    try:
        jpeg_data = self._mpo2jpeg_copy_exif(jpeg_data)
    except Exception as exc:
        cprint(f'WARNING: could not copy EXIF data for {self.path_info.full_name()}: {exc}')
    try:
        jpeg_data = self._mpo2jpeg_copy_xmp(jpeg_data)
    except Exception as exc:
        cprint(f'WARNING: could not copy XMP data for {self.path_info.full_name()}: {exc}')
    self.input_file_format = self.OUTPUT_FILE_FORMAT
    return BytesIO(jpeg_data)",self.input_file_format != MPO_FILE_FORMAT,61,self.input_file_format == self.OUTPUT_FILE_FORMAT,False,54.237828377183035,N/A
"def unpack_into(self) -> Generator[PathInfo, None, None]:
    """"""Unpack webp into temp dir.""""""
    frame_index = 1
    frame_info = {}
    with Image.open(self.original_path) as image:
        for frame in ImageSequence.Iterator(image):
            with BytesIO() as frame_buffer:
                frame.save(frame_buffer, **self.PIL2_FRAME_KWARGS)
                for key in ANIMATED_INFO_KEYS:
                    value = frame.info.get(key)
<mask>:
                        if key not in frame_info:
                            frame_info[key] = []
                        frame_info[key].append(value)
                frame_buffer.seek(0)
                frame_path_info = PathInfo(self.path_info.top_path, self.path_info.mtime(), self.path_info.convert, self.path_info.is_case_sensitive, frame=frame_index, data=frame_buffer.read(), container_paths=self.get_container_paths())
            if self.config.verbose:
                cprint('.', end='')
            yield frame_path_info
            frame_index += 1
    image.close()
    for key in tuple(frame_info):
        value = frame_info[key]
        if value is not None:
            frame_info[key] = tuple(value)
    self.frame_info = frame_info",value is not None,86,value is not None,True,100.00000000000004,N/A
"def pack_into(self) -> BytesIO:
    """"""Remux the optimized frames into an animated webp.""""""
    sorted_pairs = sorted(self._optimized_contents.items(), key=lambda pair: 0 if pair[0].frame is None else pair[0].frame)
    head_image_data = sorted_pairs.pop()[1]
    total_len = 0
    append_images = []
    while sorted_pairs:
        _, frame_data = sorted_pairs.pop()
        total_len += len(frame_data)
        frame = Image.open(BytesIO(frame_data))
        append_images.append(frame)
<mask>:
            cprint('.', end='')
    info = dict(self.prepare_info(self.OUTPUT_FORMAT_STR))
    info.update(self.frame_info)
    output_buffer = BytesIO()
    with Image.open(BytesIO(head_image_data)) as image:
        image.save(output_buffer, self.OUTPUT_FORMAT_STR, save_all=True, append_images=append_images, **self.PIL2_KWARGS, **info)
    return output_buffer",self.config.verbose,68,self.verbose,False,40.74994374254097,N/A
"def _extract_image_info(path_info: PathInfo, keep_metadata: bool) -> tuple[str | None, dict[str, Any]]:
    """"""Get image format and info from a file.""""""
    image_format_str = None
    info = {}
    n_frames = 1
    animated = False
    try:
        fp = path_info.path_or_buffer()
        with Image.open(fp) as image:
            image.verify()
        image.close()
        with suppress(AttributeError):
            fp.close()
        fp = path_info.path_or_buffer()
        with Image.open(fp) as image:
            image_format_str = image.format
<mask>:
                info: dict[str, Any] = image.info if keep_metadata else {}
                animated = getattr(image, 'is_animated', False)
                info['animated'] = animated
                if animated and (n_frames := getattr(image, 'n_frames', None)):
                    info['n_frames'] = n_frames
                with suppress(AttributeError):
                    info['mpinfo'] = image.mpinfo
        image.close()
        with suppress(AttributeError):
            fp.close()
    except UnidentifiedImageError:
        pass
    return (image_format_str, info)",image_format_str,98,image_format_str is None,False,61.47881529512643,N/A
"def _is_lossless(image_format_str: str, path_info: PathInfo, info: Mapping[str, Any]) -> bool:
    """"""Determine if image format is lossless.""""""
<mask>:
        lossless = is_lossless(path_info.fp_or_buffer())
    elif image_format_str == TiffImageFile.format:
        lossless = info.get('compression') in TIFF_LOSSLESS_COMPRESSION
    else:
        lossless = image_format_str in LOSSLESS_FORMAT_STRS
    return lossless",image_format_str == WebPLossless.OUTPUT_FORMAT_STR,37,image_format_str == PathInfo.format,False,44.28354515428592,N/A
"def _get_image_format(path_info: PathInfo, keep_metadata: bool) -> tuple[FileFormat | None, Mapping[str, Any]]:
    """"""Construct the image format with PIL.""""""
    image_format_str, info = _extract_image_info(path_info, keep_metadata)
    file_format = None
<mask>:
        lossless = _is_lossless(image_format_str, path_info, info)
        file_format = FileFormat(image_format_str, lossless, info.get('animated', False))
    return (file_format, info)",image_format_str,40,image_format_str,True,100.00000000000004,N/A
"def _get_non_pil_format(path_info: PathInfo) -> FileFormat | None:
    """"""Get the container format by querying each handler.""""""
    file_format = None
    for handler in _NON_PIL_HANDLERS:
        file_format = handler.identify_format(path_info)
<mask>:
            break
    else:
        file_format = None
    return file_format",file_format is not None,33,file_format,False,36.78794411714425,N/A
"def _create_handler_get_format(config: AttrDict, path_info: PathInfo) -> tuple[FileFormat | None, Mapping[str, Any]]:
    file_format, info = _get_image_format(path_info, config.keep_metadata)
<mask>:
        file_format = _get_non_pil_format(path_info)
    return (file_format, info)",not file_format,23,file_format is None,False,39.76353643835252,N/A
"def unpack(self) -> Generator[PathInfo, None, None]:
    """"""Create directory and unpack container.""""""
<mask>:
        cprint(f'Unpacking {self.original_path}...', end='')
    yield from self.unpack_into()
    if self.config.verbose:
        cprint('done')",self.config.verbose,21,self.config.verbose,True,100.00000000000004,N/A
"def set_task(self, path_info: PathInfo, mp_result: ApplyResult | None) -> None:
    """"""Store the mutiprocessing task.""""""
<mask>:
        self._optimized_contents[path_info] = path_info.data()
    else:
        self._tasks[path_info] = mp_result",mp_result is None,22,mp_result is None,True,100.00000000000004,N/A
"def repack(self) -> ReportStats:
    """"""Create a new container and clean up the tmp dir.""""""
<mask>:
        cprint(f'Repacking {self.final_path}...', end='')
    report_stats = self.optimize_wrapper()
    if self.config.verbose:
        cprint('done')
    return report_stats",self.config.verbose,26,self.config.verbose,True,100.00000000000004,N/A
"@classmethod
def identify_format(cls, path_info: PathInfo) -> FileFormat | None:
    """"""Return the format if this handler can handle this path.""""""
<mask>:
        return super().identify_format(path_info)
    return None",is_zipfile(path_info.path_or_buffer()),24,path_info.path_type == PathInfo.FileType.FILE,False,30.329987166688387,N/A
"def _get_archive(self) -> ZipFile:
    """"""Use the zipfile builtin for this archive.""""""
<mask>:
        archive = ZipFile(self.original_path, 'r')
    else:
        msg = f'Unknown archive type: {self.original_path}'
        raise ValueError(msg)
    return archive",is_zipfile(self.original_path),27,self.archive_type == 'zip',False,9.51934081834847,N/A
"def _set_comment(self, comment: bytes | None) -> None:
    """"""Set the comment from the archive.""""""
<mask>:
        self.comment = comment",comment,18,comment is not None,False,15.97357760615681,N/A
"def unpack_into(self) -> Generator[PathInfo, None, None]:
    """"""Uncompress archive.""""""
    with self._get_archive() as archive:
        self._set_comment(archive.comment)
        for archive_info in archive.infolist():
            zipinfo = self.to_zipinfo(archive_info)
<mask>:
                continue
            path_info = PathInfo(self.path_info.top_path, self.path_info.mtime(), self.path_info.convert, self.path_info.is_case_sensitive, zipinfo=zipinfo, data=archive.read(zipinfo.filename), container_paths=self.get_container_paths())
            yield path_info",zipinfo.is_dir(),33,not zipinfo,False,4.104249931194939,N/A
"def pack_into(self) -> BytesIO:
    """"""Zip up the files in the tempdir into the new filename.""""""
    output_buffer = BytesIO()
    with ZipFile(output_buffer, 'w', compression=ZIP_DEFLATED, compresslevel=9) as new_zf:
        for path_info in tuple(self._optimized_contents):
            data = self._optimized_contents.pop(path_info)
            zipinfo = path_info.zipinfo
<mask>:
                continue
            if path_info.container_filename and path_info.container_filename != zipinfo.filename:
                zipinfo.filename = path_info.container_filename
            if not self.config.keep_metadata and zipinfo and (zipinfo.compress_type == ZIP_STORED):
                zipinfo.compress_type = ZIP_DEFLATED
            new_zf.writestr(zipinfo, data)
            if self.config.verbose:
                cprint('.', end='')
        if self.comment:
            new_zf.comment = self.comment
            if self.config.verbose:
                cprint('.', end='')
    return output_buffer",not zipinfo,75,not zipinfo,True,100.00000000000004,N/A
"def internal_oxipng(self, _exec_args: tuple[str, ...], input_buffer: BufferedReader | BytesIO) -> BytesIO:
    """"""Run internal oxipng on the file.""""""
    oxipng_kwargs = self._OXIPNG_KWARGS
<mask>:
        oxipng_kwargs = dict(oxipng_kwargs)
        oxipng_kwargs.update(self._OXIPNG_MAX_KWARGS)
    opts = {**self._OXIPNG_KWARGS}
    if not self.config.keep_metadata:
        opts['strip'] = oxipng.StripChunks.safe()
    input_buffer.seek(0)
    with input_buffer:
        result = oxipng.optimize_from_memory(input_buffer.read(), **opts)
    return BytesIO(result)",self.config.png_max,43,self.config.max_size > 0,False,33.03164318013809,N/A
"def pngout(self, exec_args: tuple[str, ...], input_buffer: BufferedReader | BytesIO) -> BytesIO | BufferedReader:
    """"""Run the external program pngout on the file.""""""
    depth = png_bit_depth(input_buffer)
<mask>:
        cprint(f'Skipped pngout for {depth} bit PNG: {self.original_path}', 'white', attrs=['dark'])
        return input_buffer
    opts = ('-k1',) if self.config.keep_metadata else ('-k0',)
    args = (*exec_args, *self._PNGOUT_ARGS, *opts)
    return self.run_ext(args, input_buffer)",not depth or depth > self._PNGOUT_DEPTH_MAX or depth < 1,51,depth != 0,False,0.6193628179172647,N/A
"def optimize(self) -> BinaryIO:
    """"""Use the correct optimizing functions in sequence.

        And report back statistics.
        """"""
    stages = self.config.computed.handler_stages.get(self.__class__, {})
<mask>:
        cprint(f'Tried to execute handler {self.__class__.__name__} with no available stages.', 'yellow')
        raise ValueError
    image_buffer: BinaryIO = self.path_info.fp_or_buffer()
    for func, exec_args in stages.items():
        new_image_buffer: BinaryIO = getattr(self, func)(exec_args, image_buffer)
        if image_buffer != new_image_buffer:
            image_buffer.close()
        image_buffer = new_image_buffer
    return image_buffer",not stages,58,stages is None,False,27.516060407455225,N/A
"def pil2native(self, _exec_args: tuple[str, tuple[str, ...]], input_buffer: BytesIO | BufferedReader, format_str: None | str=None, opts: None | Mapping[str, Any]=None) -> BytesIO | BufferedReader:
    """"""Use PIL to save the image.""""""
<mask>:
        return input_buffer
    if format_str is None:
        format_str = self.OUTPUT_FORMAT_STR
    if opts is None:
        opts = self.PIL2_KWARGS
    info = self.prepare_info(format_str)
    output_buffer = BytesIO()
    with Image.open(input_buffer) as image:
        image.save(output_buffer, format_str, save_all=True, **opts, **info)
    image.close()
    self.input_file_format = self.OUTPUT_FILE_FORMAT
    return output_buffer",self.input_file_format in self._input_file_formats,67,self.is_interactive,False,2.619778931805682,N/A
"def fixedAdsSearchQuery(*args, **kwargs):
    q = ads.SearchQuery(*args, **kwargs)
    q.session.headers.pop('Content-Type', None)
<mask>:
        q.session.verify = False
    return q",_DISABLE_SSL,15,not q.session.verify,False,0.0,N/A
"def fixedAdsExportQuery(*args, **kwargs):
    q = ads.ExportQuery(*args, **kwargs)
<mask>:
        q.session.verify = False
    return q",_DISABLE_SSL,13,q.session,False,0.0,N/A
"def _match_name_prefix(name):
    for prefix in _name_prefix:
        p = prefix.replace(' ', '')
<mask>:
            return ' '.join((prefix, name[len(p):]))",name.lower().startswith(p),16,name.startswith(p),False,43.17306149243962,N/A
"def _split_authors(fa):
    fa = fa.strip(':').split(':')
<mask>:
        return (fa[0], fa[1:])
    return (fa[0], None)",_USE_COAUTHORS and len(fa) > 1,12,len(fa) > 1,False,43.45982085070784,N/A
"def search_keys(files, find_bib=False):
<mask>:
        files = [files]
    bib = None
    keys = set()
    for f in files:
        with open(f) as fp:
            text = fp.read()
        text = _re_comment.sub('', text)
        if find_bib and (not bib):
            m = _re_bib.search(text)
            if m:
                dirpath = os.path.dirname(f)
                bib = []
                for b in m.groups()[0].split(','):
                    b = b.strip()
                    if not b.lower().endswith('.bib'):
                        b += '.bib'
                    bib.append(os.path.join(dirpath, b))
        for m in _re_cite.finditer(text):
            for k in m.groups()[0].split(','):
                keys.add(k.strip())
    return (keys, bib)",_is_like_string(files),71,"isinstance(files, str)",False,10.89644800332157,N/A
"def detect(frame: numpy.ndarray) -> bool:
    """"""Detects if a given frame is showing Nook Shopping catalog.""""""
    side_color = frame[150:160, -20:].mean(axis=(0, 1))
<mask>:
        raise AssertionError('Wardell catalog is not supported.')
    if numpy.linalg.norm(side_color - NOOK_MILES_COLOR) < 10:
        raise AssertionError('Nook Miles catalog is not supported.')
    return numpy.linalg.norm(side_color - SIDE_COLOR) < 10",numpy.linalg.norm(side_color - WARDELL_COLOR) < 10,46,numpy.linalg.norm(side_color - Wardell_COLOR) < 10,False,81.53551038173119,N/A
"def parse_video(filename: str, for_sale: bool=False) -> List[numpy.ndarray]:
    """"""Parses a whole video and returns an image containing all the items found.""""""
    unfinished_page = False
    item_scroll_count = 0
    all_rows: List[numpy.ndarray] = []
    for i, frame in enumerate(_read_frames(filename)):
<mask>:
            continue
        new_rows = list(_parse_frame(frame, for_sale))
        if _is_duplicate_rows(all_rows, new_rows):
            continue
        unfinished_page = any((r.min() > 150 for r in new_rows))
        item_scroll_count += _is_item_scroll(all_rows, new_rows)
        assert item_scroll_count < 20, 'Video is scrolling too slowly.'
        all_rows.extend(new_rows)
    assert all_rows, 'No items found, invalid video?'
    return _dedupe_rows(all_rows)",not unfinished_page and i % 3 != 0,77,frame.ndim != 2,False,7.0550047212602784,N/A
"def run_ocr(item_rows: List[numpy.ndarray], lang: str='eng') -> Set[str]:
    """"""Runs tesseract OCR on an image of item names and returns all items found.""""""
<mask>:
        return set()
    item_rows, remaining_rows = (item_rows[:900], item_rows[900:])
    logging.debug('Running Tesseract on %s rows', len(item_rows))
    parsed_text = pytesseract.image_to_string(Image.fromarray(cv2.vconcat(item_rows)), lang=lang, config=_get_tesseract_config(lang))
    assert isinstance(parsed_text, str), 'Tesseract returned bytes'
    clean_names = {_cleanup_name(item, lang) for item in parsed_text.split('\n')}
    remaining_names = run_ocr(remaining_rows, lang)
    return (clean_names | remaining_names) - {''}",not item_rows,64,len(item_rows) == 0,False,17.747405280050266,N/A
"def match_items(item_names: Set[str], locale: str='en-us') -> Tuple[List[str], List[str]]:
    """"""Matches a list of names against a database of items, finding best matches.""""""
    no_match_items = []
    matched_items = set()
    item_db = _get_item_db(locale)
    for item in sorted(item_names):
<mask>:
            matched_items.add(item)
            continue
        matches = difflib.get_close_matches(item, item_db, n=1, cutoff=0.5)
        if not matches:
            no_match_items.append(item)
            assert len(no_match_items) <= 0.3 * len(item_names), 'Failed to match multiple items, wrong language?'
            continue
        ratio = difflib.SequenceMatcher(None, item, matches[0]).ratio()
        logging.debug('Matched %r to %r (%.2f)', item, matches[0], ratio)
        matched_items.add(matches[0])
    if no_match_items:
        logging.warning('Failed to match %d items: %s', len(no_match_items), no_match_items)
    return (sorted(matched_items), no_match_items)",item in item_db,88,item in matched_items,False,23.643540225079384,N/A
"def _read_frames(filename: str) -> Iterator[numpy.ndarray]:
    """"""Parses frames of the given video and returns the relevant region in grayscale.""""""
    scroll_positions = []
    cap = cv2.VideoCapture(filename)
    while True:
        ret, frame = cap.read()
<mask>:
            break
        assert frame.shape[:2] == (720, 1280), 'Invalid resolution: {1}x{0}'.format(*frame.shape)
        if not detect(frame):
            scroll_positions = []
            continue
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        scrollbar = gray[160:570, 1235:1245].mean(axis=1)
        scroll_positions.append(numpy.argmax(scrollbar < 150))
        if _is_inconsistent_scroll(scroll_positions):
            raise AssertionError('Video is scrolling inconsistently.')
        yield gray[150:630, 635:1220]
    cap.release()",not ret,69,ret is None,False,27.516060407455225,N/A
"def scan_media(filename: str, mode: str='auto', locale: str='auto', for_sale: bool=False) -> ScanResult:
<mask>:
        mode = _detect_media_type(filename)
        logging.info('Detected scan mode: %s', mode)
    if mode not in SCANNERS:
        raise RuntimeError('Invalid mode: %r' % mode)
    assert mode != 'storage', 'Storage scanning is not supported.'
    kwargs = {}
    if mode == 'catalog':
        kwargs['for_sale'] = for_sale
    return SCANNERS[mode].scan(filename, locale=locale, **kwargs)",mode == 'auto',54,mode == 'auto',True,100.00000000000004,N/A
"def _detect_media_type(filename: str) -> str:
    video_capture = cv2.VideoCapture(filename)
    for _ in range(100):
        success, frame = video_capture.read()
<mask>:
            break
        if filename.endswith('.jpg') and frame.shape[:2] == (1080, 1920):
            frame = cv2.resize(frame, (1280, 720))
        assert frame.shape[:2] == (720, 1280), 'Invalid resolution: {1}x{0}'.format(*frame.shape)
        for mode, scanner in SCANNERS.items():
            if scanner.detect(frame):
                return mode
    raise AssertionError('Media is not showing a known scan type.')",not success or frame is None,56,not success,False,13.533528323661276,N/A
"def main(argv):
<mask>:
        media_file = argv[1]
    elif FLAGS.mode == 'recipes':
        media_file = 'examples/recipes.mp4'
    elif FLAGS.mode == 'critters':
        media_file = 'examples/critters.mp4'
    elif FLAGS.mode == 'reactions':
        media_file = 'examples/reactions.jpg'
    elif FLAGS.mode == 'music':
        media_file = 'examples/music.mp4'
    else:
        media_file = 'examples/catalog.mp4'
    result = scan_media(media_file, mode=FLAGS.mode, locale=FLAGS.locale, for_sale=FLAGS.for_sale)
    result_count, result_mode = (len(result.items), result.mode.name.lower())
    print(f'Found {result_count} items in {result_mode} [{result.locale}]')
    print('\n'.join(result.items))",len(argv) > 1,56,len(argv) > 1,True,100.00000000000004,N/A
"def upload_to_datastore(result, discord_user_id=None) -> datastore.Entity:
    datastore_client = datastore.Client.from_service_account_json('catalog-scanner.json')
    temp_key = datastore_client.key('Catalog')
    key = datastore_client.allocate_ids(temp_key, 1)[0]
    catalog = datastore.Entity(key, exclude_from_indexes=['data', 'unmatched'])
    key_hash = hashid_client.encode(key.id)
    catalog_data = '\n'.join(result.items).encode('utf-8')
    catalog.update({'hash': key_hash, 'data': catalog_data, 'locale': result.locale, 'type': result.mode.name.lower(), 'created': datetime.datetime.utcnow(), 'discord_user': discord_user_id})
<mask>:
        unmatched_data = '\n'.join(result.unmatched).encode('utf-8')
        catalog['unmatched'] = unmatched_data
    datastore_client.put(catalog)
    return catalog",result.unmatched,48,result.unmatched,True,100.00000000000004,N/A
"def parse_image(filename: str) -> List[numpy.ndarray]:
    """"""Parses a screenshot and returns icons for all reactions found.""""""
    icon_pages: Dict[int, List[numpy.ndarray]] = {}
    assertion_error: Optional[AssertionError] = None
    cap = cv2.VideoCapture(filename)
    while True:
        ret, frame = cap.read()
<mask>:
            break
        if frame.shape[:2] == (1080, 1920):
            frame = cv2.resize(frame, (1280, 720))
        if not detect(frame):
            continue
        try:
            new_icons = list(_parse_frame(frame))
            icon_pages[len(new_icons)] = new_icons
        except AssertionError as e:
            assertion_error = e
    if assertion_error and (filename.endswith('.jpg') or not icon_pages):
        raise assertion_error
    return [icon for page in icon_pages.values() for icon in page]",not ret,82,ret is None,False,27.516060407455225,N/A
"def translate_names(reaction_names: List[str], locale: str) -> List[str]:
    """"""Translates a list of reaction names to the given locale.""""""
<mask>:
        return reaction_names
    translation_path = os.path.join('reactions', 'translations.json')
    with open(translation_path, encoding='utf-8') as fp:
        translations = json.load(fp)
    return [translations[name][locale] for name in reaction_names]","locale in ['auto', 'en-us']",38,not os.path.exists('reactions'),False,0.0,N/A
"def _parse_frame(frame: numpy.ndarray) -> Iterator[numpy.ndarray]:
    """"""Extracts the individual reaction icons from the frame.""""""
    for x, y in REACTION_POSITIONS:
        center_color = frame[y - 6:y + 6, x - 6:x + 6].mean(axis=(0, 1))
<mask>:
            break
        if numpy.linalg.norm(center_color - SELECT_COLOR) < 20:
            break
        icon = frame[y - 32:y + 32, x - 32:x + 32]
        assert icon[34:42, 10:18].mean() < 250, 'Cursor is blocking a reaction.'
        assert icon[-5:, :, 2].mean() > 200, 'Tooltip is blocking a reaction.'
        if icon[-3, -5, 1] > 227:
            icon = cv2.copyMakeBorder(icon, top=8, bottom=8, left=8, right=8, borderType=cv2.BORDER_CONSTANT, value=BG_COLOR)
            icon = cv2.resize(icon, (64, 64))
        yield icon",numpy.linalg.norm(center_color - EMPTY_COLOR) < 10,95,center_color < SELECT_COLOR,False,8.894308445355128,N/A
"def _find_best_match(icon: numpy.ndarray, reactions: List[ReactionImage]) -> ReactionImage:
    """"""Finds the closest matching reaction for the given icon.""""""
    fast_similarity_metric = lambda r: cv2.absdiff(icon, r.img).mean()
    similarities = list(map(fast_similarity_metric, reactions))
    sim1, sim2 = numpy.partition(similarities, kth=2)[:2]
<mask>:
        return reactions[numpy.argmin(similarities)]

    def slow_similarity_metric(reaction):
        diffs = []
        for x, y in itertools.product([-1, 0, 1], repeat=2):
            shifted = numpy.roll(numpy.roll(icon, x, axis=1), y, axis=0)
            diffs.append(cv2.absdiff(shifted, reaction.img).sum())
        return min(diffs)
    similarities = list(map(slow_similarity_metric, reactions))
    return reactions[numpy.argmin(similarities)]",abs(sim1 - sim2) > 3,64,sim1[0] < sim2[0],False,5.669791110976001,N/A
"def detect(frame: numpy.ndarray) -> bool:
    """"""Detects if a given frame is showing the music list.""""""
    color = frame[:20, 1220:1250].mean(axis=(0, 1))
<mask>:
        return True
    return False",_is_background(color),25,color.any() == 0,False,7.267884212102741,N/A
"def parse_video(filename: str) -> List[numpy.ndarray]:
    """"""Parses a whole video and returns images for all song covers found.""""""
    all_covers: List[numpy.ndarray] = []
    for frame in _read_frames(filename):
        for new_covers in _parse_frame(frame):
<mask>:
                continue
            all_covers.extend(new_covers)
    return _remove_blanks(all_covers)","_is_duplicate_cards(all_covers, new_covers)",34,new_covers is None,False,5.78270080339587,N/A
"def translate_names(song_names: List[str], locale: str) -> List[str]:
    """"""Translates a list of song names to the given locale.""""""
<mask>:
        return song_names
    translation_path = os.path.join('music', 'translations.json')
    with open(translation_path, encoding='utf-8') as fp:
        translations = json.load(fp)
    return [translations[name][locale] for name in song_names]","locale in ['auto', 'en-us']",38,not locale,False,4.104249931194939,N/A
"def _read_frames(filename: str) -> Iterator[numpy.ndarray]:
    """"""Parses frames of the given video and returns the relevant region.""""""
    cap = cv2.VideoCapture(filename)
    while True:
        ret, frame = cap.read()
<mask>:
            break
        assert frame.shape[:2] == (720, 1280), 'Invalid resolution: {1}x{0}'.format(*frame.shape)
        if not detect(frame):
            continue
        yield frame[95:670, 40:1240]
    cap.release()",not ret,43,ret is None,False,27.516060407455225,N/A
"def _parse_frame(frame: numpy.ndarray) -> Iterator[List[numpy.ndarray]]:
    """"""Parses an individual frame and extracts song covers from the music list.""""""
    x_positions = [40, 327, 614, 900]
    bg_color = frame[:20, :20].mean(axis=(0, 1))
    end_row_color = frame[100:200, 1000:1100].mean(axis=(0, 1))
<mask>:
        yield [frame[15:275, x:x + 260] for x in x_positions]
        return
    thresh = cv2.inRange(frame[:410], bg_color - 30, bg_color + 30)
    separators = numpy.nonzero(numpy.diff(thresh.mean(axis=1) > 100))[0]
    if len(separators) < 2:
        return
    y_centers = []
    for y1, y2 in zip(separators, separators[1:]):
        if 259 < y2 - y1 < 266:
            y_centers.extend([y1 % 287, (y2 + 25) % 287])
        if 20 < y2 - y1 < 27:
            y_centers.extend([y2 % 287, (y1 + 25) % 287])
    if not y_centers:
        return
    y_centroid = numpy.mean(y_centers) + 1
    y_positions = numpy.arange(y_centroid, 575, 287).astype(int)
    for y in y_positions:
        if y + 260 > frame.shape[0]:
            continue
        yield [frame[y:y + 260, x:x + 260] for x in x_positions]",numpy.linalg.norm(end_row_color - bg_color) < 5,140,bg_color < 15 or end_row_color < 255,False,27.114729525976227,N/A
"def translate_names(critter_names: List[str], locale: str) -> List[str]:
    """"""Translates a list of critter names to the given locale.""""""
<mask>:
        return critter_names
    translation_path = os.path.join('critters', 'translations.json')
    with open(translation_path, encoding='utf-8') as fp:
        translations = json.load(fp)
    return [translations[name][locale] for name in critter_names]","locale in ['auto', 'en-us']",38,not os.path.exists('critters'),False,0.0,N/A
"def _read_frames(filename: str) -> Iterator[Tuple[CritterType, numpy.ndarray]]:
    """"""Parses frames of the given video and returns the relevant region.""""""
    frame_skip = 0
    last_section = None
    last_frame = None
    good_frames: Dict[Tuple[CritterType, int], numpy.ndarray] = {}
    cap = cv2.VideoCapture(filename)
    while True:
        ret, frame = cap.read()
<mask>:
            break
        if frame_skip > 0:
            frame_skip -= 1
            continue
        if frame.shape[:2] == (1080, 1920):
            frame = cv2.resize(frame, (1280, 720))
        assert frame.shape[:2] == (720, 1280), 'Invalid resolution: {1}x{0}'.format(*frame.shape)
        if not detect(frame):
            continue
        mode_detector = frame[20:24, 600:800].mean(axis=(0, 1))
        if numpy.linalg.norm(mode_detector - (199, 234, 237)) > 50:
            raise AssertionError('Critterpedia is in Pictures Mode.')
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        if filename.endswith('.jpg'):
            yield (_detect_critter_section(gray), frame[149:623, :])
            continue
        if last_frame is None:
            last_frame = frame
            continue
        critter_section = _detect_critter_section(gray)
        if critter_section != last_section:
            if last_section is not None:
                frame_skip = 15
            last_section = critter_section
            continue
        if last_frame[570:600, :70, 2].min() > 230:
            good_frames[critter_section, 0] = last_frame
        elif last_frame[570:600, -70:, 2].min() > 230:
            good_frames[critter_section, 1] = last_frame
        last_frame = frame
    cap.release()
    for (critter_type, _), frame in good_frames.items():
        yield (critter_type, frame[149:623, :])",not ret,164,ret is None,False,27.516060407455225,N/A
"def _detect_critter_section(gray_frame: numpy.ndarray) -> CritterType:
    for i, critter_type in enumerate(CritterType):
        start_x, end_x = (65 + i * 65, 65 + (i + 1) * 65)
        section_icon = gray_frame[70:75, start_x:end_x]
<mask>:
            return critter_type
    raise AssertionError('Invalid Critterpedia page')",section_icon.min() > 150,36,section_icon.startswith('Critterpedia'),False,33.764591090632756,N/A
"def _parse_frame(frame: numpy.ndarray) -> Iterator[numpy.ndarray]:
    """"""Parses an individual frame and extracts icons from the Critterpedia page.""""""
    y_positions = [0, 95, 190, 285, 379]
    y_offsets = [5, 89]
    rows = []
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    for y_pos, offset in itertools.product(y_positions, y_offsets):
        line = gray[y_pos + offset - 3:y_pos + offset + 3, :]
<mask>:
            continue
        rows.append(line)
    if not rows:
        return
    thresh = cv2.threshold(cv2.vconcat(rows), 210, 255, 0)[1]
    separators = thresh.mean(axis=0) < 240
    x_lines = list(separators.nonzero()[0])
    centers = [numpy.fmod(x, 112.7) for x in x_lines]
    centroid = round(numpy.median(centers))
    x_positions = numpy.arange(centroid, 1280, 112.7).astype(int)
    for x, y in itertools.product(x_positions, y_positions):
        if x + 96 > frame.shape[1]:
            continue
        yield frame[y + 8:y + 88, x + 16:x + 96]",line.min() < 170 or line.max() > 240,113,line.startswith('#'),False,5.4752948205155585,N/A
"def _remove_blanks(all_icons: List[CritterIcon]) -> List[CritterIcon]:
    """"""Remove all icons that show empty critter boxes.""""""
    filtered_icons = []
    for icon in all_icons:
<mask>:
            continue
        filtered_icons.append(icon)
    return filtered_icons","icon[20:60, 20:60, 2].min() > 100",25,icon.is_blank(),False,3.2449642932033895,N/A
"def parse_video(filename: str) -> List[numpy.ndarray]:
    """"""Parses a whole video and returns images for all storage items found.""""""
    all_rows: List[numpy.ndarray] = []
    for i, frame in enumerate(_read_frames(filename)):
<mask>:
            continue
        for new_row in _parse_frame(frame):
            if _is_duplicate_row(all_rows, new_row):
                continue
            all_rows.extend(new_row)
    return _remove_blanks(all_rows)",i % 4 != 0,39,i != len(frames) - 1,False,10.552670315936318,N/A
"def _read_frames(filename: str) -> Iterator[numpy.ndarray]:
    """"""Parses frames of the given video and returns the relevant region.""""""
    cap = cv2.VideoCapture(filename)
    while True:
        ret, frame = cap.read()
<mask>:
            break
        assert frame.shape[:2] == (720, 1280), 'Invalid resolution: {1}x{0}'.format(*frame.shape)
        if not detect(frame):
            continue
        yield frame[150:675, 112:1168]
    cap.release()",not ret,43,ret is None,False,27.516060407455225,N/A
"def _parse_frame(frame: numpy.ndarray) -> Iterator[List[numpy.ndarray]]:
    """"""Parses an individual frame and extracts cards from the storage.""""""
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    x_positions = list(range(0, 1056, 132))
    cols = []
    for x in x_positions[1:]:
        empty_col = gray[:, x - 10:x + 10]
<mask>:
            continue
        cols.append(empty_col)
    thresh = cv2.threshold(cv2.hconcat(cols), 236, 255, 0)[1]
    separators = list(numpy.nonzero(thresh.mean(axis=1) < 240)[0])
    centroid = int(numpy.median([s % 127 for s in separators]))
    y_positions = list(range(centroid, 525, 127))
    for y in y_positions:
        if y + 127 > frame.shape[0]:
            continue
        tooltip = cv2.inRange(frame[y + 122:y + 127, :], (160, 195, 80), (180, 205, 100))
        if tooltip.mean() > 10:
            continue
        yield [frame[y + 13:y + 113, x + 16:x + 116] for x in x_positions]",empty_col.min() < 200,112,empty_col.sum() < 255,False,43.167001068522545,N/A
"def _is_duplicate_row(all_rows: List[numpy.ndarray], new_row: List[numpy.ndarray]) -> bool:
    """"""Checks if the new row is the same as the previous seen rows.""""""
<mask>:
        return False
    new_concat = cv2.hconcat(new_row)
    for ind in [slice(-8, None), slice(-16, -8), slice(-24, -16), slice(-32, -24)]:
        old_concat = cv2.hconcat(all_rows[ind])
        if old_concat is None:
            continue
        if cv2.absdiff(new_concat, old_concat).mean() < 12:
            if old_concat[-5:].min() < 50:
                all_rows[ind] = new_row
            return True
    return False",not new_row or len(all_rows) < len(new_row),61,new_row.shape[0] != all_rows.shape[1],False,13.445273575332964,N/A
"def _remove_blanks(all_icons: List[numpy.ndarray]) -> List[numpy.ndarray]:
    """"""Remove all icons that show empty critter boxes.""""""
    filtered_icons = []
    for icon in all_icons:
        center_color = icon[40:60, 40:60].mean(axis=(0, 1))
<mask>:
            continue
        filtered_icons.append(icon)
    return filtered_icons",numpy.linalg.norm(center_color - BLANK_COLOR) < 5,30,center_color.any() < 0.5,False,11.152372863964953,N/A
"def detect(frame: numpy.ndarray) -> bool:
    """"""Detects if a given frame is showing DIY recipes.""""""
    color = frame[:20, 1200:1240].mean(axis=(0, 1))
<mask>:
        raise AssertionError('Workbench scanning is not supported.')
    if numpy.linalg.norm(color - KITCHEN_COLOR) < 10:
        raise AssertionError('Kitchen scanning is not supported.')
    return numpy.linalg.norm(color - BG_COLOR) < 10",numpy.linalg.norm(color - WOOD_COLOR) < 10,44,numpy.linalg.norm(color - WORKBROWSER_COLOR) < 10,False,78.25422900366432,N/A
"def parse_video(filename: str) -> List[numpy.ndarray]:
    """"""Parses a whole video and returns images for all recipe cards found.""""""
    all_cards: List[numpy.ndarray] = []
    for i, frame in enumerate(_read_frames(filename)):
<mask>:
            continue
        for new_cards in _parse_frame(frame):
            if _is_duplicate_cards(all_cards, new_cards):
                continue
            all_cards.extend(new_cards)
    return all_cards",i % 4 != 0,39,i != len(filename) - 1,False,10.552670315936318,N/A
"def match_recipes(recipe_cards: List[numpy.ndarray]) -> List[str]:
    """"""Matches icons against database of recipe images, finding best matches.""""""
    matched_recipes = set()
    for card in recipe_cards:
        card_center_color = card[28:84, 28:84].mean(axis=(0, 1))
<mask>:
            continue
        possible_recipes = list(_get_candidate_recipes(card))
        best_match = _find_best_match(card, possible_recipes)
        item_name = best_match.name
        if item_name in matched_recipes and item_name in CONFUSED_ITEMS:
            item_name = CONFUSED_ITEMS[item_name]
        matched_recipes.add(item_name)
    return sorted(matched_recipes)",numpy.linalg.norm(card_center_color - BG_COLOR) < 5,53,card_center_color.sum() < 1e-06,False,22.739998404778618,N/A
"def translate_names(recipe_names: List[str], locale: str) -> List[str]:
    """"""Translates a list of recipe names to the given locale.""""""
<mask>:
        return recipe_names
    translation_path = os.path.join('recipes', 'translations.json')
    with open(translation_path, encoding='utf-8') as fp:
        translations = json.load(fp)
    return [translations[name][locale] for name in recipe_names]","locale in ['auto', 'en-us']",38,"locale not in ['en', 'en']",False,13.888095170058955,N/A
"def _read_frames(filename: str) -> Iterable[numpy.ndarray]:
    """"""Parses frames of the given video and returns the relevant region.""""""
    cap = cv2.VideoCapture(filename)
    while True:
        ret, frame = cap.read()
<mask>:
            break
        assert frame.shape[:2] == (720, 1280), 'Invalid resolution: {1}x{0}'.format(*frame.shape)
        if not detect(frame):
            continue
        yield frame[110:720, 45:730]
    cap.release()",not ret,43,ret is None,False,27.516060407455225,N/A
"def annotate_frame(self, frame: numpy.ndarray) -> numpy.ndarray:
    """"""Parses various parts of a frame for catalog items and annotates it.""""""
<mask>:
        text = 'Navigate to Nook catalog to start!'
        opts = {'org': (200, 70), 'fontFace': cv2.FONT_HERSHEY_PLAIN, 'lineType': cv2.LINE_AA, 'fontScale': 3}
        frame = cv2.putText(frame, text, color=(0, 0, 0), thickness=7, **opts)
        return cv2.putText(frame, text, color=(100, 100, 255), thickness=3, **opts)
    cv2.rectangle(frame, (0, 0), (300, 130), (106, 226, 240), -1)
    for i, line in enumerate(TUTORIAL_LINES):
        if line.startswith('F'):
            line += ' (%s)' % ('ON' if self.for_sale else 'OFF')
        frame = cv2.putText(frame, line, (30, 25 + i * 30), 0, 0.8, 0, 2)
    count_text = 'Item count: %d' % len(self.items)
    if not self.section_name:
        count_text = 'Items saved to disk'
    frame = cv2.putText(frame, count_text, (500, 700), 0, 1, 0)
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    section = numpy.where((154 < gray[20, 250:]) & (gray[20, 250:] < 162))[0]
    if section.any() and abs(self.active_section - section[0]) > 10:
        x1, *_, x2 = 250 + section
        section_region = 255 - gray[8:32, x1 + 5:x2 - 5]
        self.section_name = self.image_to_text(section_region)
        self.active_section = section[0]
        self.items = set()
    elif not self.active_section:
        return frame
    item_name = None
    variation_name = None
    selected = self.get_selected_item(frame)
    if not selected:
        return frame
    price_region = gray[selected.y1:selected.y2, 1070:1220]
    if self.for_sale and price_region.min() > 100:
        p1, p2 = ((selected.x1, selected.y1 + 20), (selected.x2, selected.y1 + 20))
        return cv2.line(frame, p1, p2, color=(0, 0, 255), thickness=2)
    item_name = self.image_to_text(gray[selected.slice])
    frame = cv2.putText(frame, item_name, selected.p1, 0, 1, 0)
    variation = self.get_variation(gray)
    if variation:
        frame = cv2.rectangle(frame, variation.p1, variation.p2, 0)
        variation_name = self.image_to_text(gray[variation.slice])
        frame = cv2.putText(frame, variation_name, variation.p1, 1, 2, 0)
    full_name = self.resolve_name(item_name, variation_name)
    if full_name:
        self.items.add(full_name)
    return frame","numpy.linalg.norm(frame[500, 20] - (182, 249, 255)) > 5",259,self.for_sale,False,0.2918767892026305,N/A
"def get_selected_item(self, frame: numpy.ndarray) -> Optional[Rectangle]:
    """"""Returns the rectangle around the selected item name if there is one.""""""
    select_region = numpy.where(frame[140:640, 1052, 0] < 100)[0]
<mask>:
        return None
    rect = Rectangle(x1=635, x2=1050)
    rect.y1 = 140 + select_region[0] + 8
    rect.y2 = 140 + select_region[-1] - 4
    if rect.y2 - rect.y1 < 35:
        return None
    item_region = frame[rect.y1:rect.y2, rect.x1:rect.x2, 1]
    detected_text = numpy.where(item_region.min(axis=0) < 55)[0]
    if not detected_text.any():
        return None
    rect.x2 = 635 + detected_text[-1] + 10
    return rect",not select_region.any(),78,not select_region.any(),True,100.00000000000004,N/A
"def get_variation(self, gray: numpy.ndarray) -> Optional[Rectangle]:
    """"""Returns the rectangle around the variation text if there is one.""""""
<mask>:
        return None
    variation = Rectangle(x1=30, y1=628, y2=665)
    variation.x2 = numpy.argmax(gray[variation.y1, :] < 250) - 15
    return variation","gray[650, 25] < 250",35,gray.shape[0] == 0,False,6.27465531099474,N/A
"def resolve_name(self, item: Optional[str], variation: Optional[str]) -> Optional[str]:
    """"""Resolves an item and optional variation name against the item database.""""""
    key = (item, variation)
<mask>:
        item = best_match(item, self.item_db)
        variation = best_match(variation, self.item_db.get(item))
        if variation:
            self._item_cache[key] = f'{item} [{variation}]'
        elif item and (not self.item_db[item]):
            self._item_cache[key] = item
        else:
            self._item_cache[key] = None
    return self._item_cache[key]",key not in self._item_cache,52,not key in self._item_cache,False,77.30551756939455,N/A
"def image_to_text(self, text_area: numpy.ndarray) -> str:
    """"""Runs OCR over a given image and returns the parsed text.""""""
    img_hash = str(cv2.img_hash.averageHash(text_area)[0])
<mask>:
        image = Image.fromarray(text_area)
        self.tesseract.SetImage(image)
        text = self.tesseract.GetUTF8Text().strip()
        self._tesseract_cache[img_hash] = text
    return self._tesseract_cache[img_hash]",img_hash not in self._tesseract_cache,33,img_hash not in self._tesseract_cache,True,100.00000000000004,N/A
"@property
def portal(self) -> Xdp.Portal:
<mask>:
        from .app import CoBangApplication
    app = self.get_application()
    assert app is not None
    return cast('CoBangApplication', app).portal",TYPE_CHECKING,21,self.is_py3,False,10.682175159905848,N/A
"@property
def zbar_scanner(self) -> zbar.ImageScanner:
<mask>:
        from .app import CoBangApplication
    app = self.get_application()
    assert app is not None
    return cast('CoBangApplication', app).zbar_scanner",TYPE_CHECKING,21,self.is_py3,False,10.682175159905848,N/A
"@property
def nm_client(self) -> NM.Client | None:
<mask>:
        from .app import CoBangApplication
    app = self.get_application()
    assert app is not None
    return cast('CoBangApplication', app).nm_client",TYPE_CHECKING,23,self.is_py3,False,10.682175159905848,N/A
"@Gtk.Template.Callback()
def on_job_viewstack_visible_child_changed(self, viewstack: Adw.ViewStack, *args):
    visible_child_name = viewstack.get_visible_child_name()
<mask>:
        self.stop_webcam()
        return
    if not self.gst_pipeline:
        self.request_camera_access()
        return
    if not self.btn_pause.get_active() and self.scan_source_viewstack.get_visible_child_name() == ScanSourceName.WEBCAM:
        self.play_webcam()",visible_child_name != JobName.SCANNER,25,visible_child_name == ScanSourceName.WEBCAM,False,42.7287006396234,N/A
"@Gtk.Template.Callback()
def scanning_result_title(self, wd: Self, value: int) -> str:
<mask>:
        return _('Found a wifi configuration')
    if value == ScannerState.URL_FOUND:
        return _('Found a URL. Click to open:')
    if value == ScannerState.TEXT_FOUND:
        return _('Found unrecognized text.')
    return _('Unknown')",value == ScannerState.WIFI_FOUND,36,value == ScannerState.WIFI_CONFIG_FOUND,False,68.65890479690394,N/A
"def build_wifi_info_display(wifi: WifiInfoMessage, nm_client: NM.Client | None) -> Gtk.Box | None:
    builder = Gtk.Builder.new_from_resource('/vn/hoabinh/quan/CoBang/gtk/wifi-display.ui')
    box = cast(Gtk.Box | None, builder.get_object('wifi_form'))
<mask>:
        return None
    if (label_ssid_value := cast(Gtk.Label | None, builder.get_object('ssid_value'))):
        label_ssid_value.set_text(wifi.ssid)
    if (label_password_value := cast(Gtk.Label | None, builder.get_object('password_value'))):
        label_password_value.set_text(wifi.password or '')
    btn = cast(Gtk.Button | None, builder.get_object('btn_connect'))
    if nm_client and is_connected_same_wifi(wifi, nm_client) and btn:
        log.debug('Set sensitive for {}', btn)
        btn.set_sensitive(False)
        btn.set_label(_('Connected'))
    log.debug('Connect handlers for Wifi UI')
    if (password_entry := builder.get_object('password_value')):
        password_entry.connect('icon-press', on_secondary_icon_pressed)
    if nm_client and btn:
        btn.connect_after('clicked', on_btn_connect_clicked, wifi, nm_client)
    return box",not box,82,not wifi,False,49.99999999999999,N/A
"def build_url_display(url: SplitResult) -> Gtk.Box | None:
    builder = Gtk.Builder.new_from_resource('/vn/hoabinh/quan/CoBang/gtk/url-display.ui')
    btn = cast(Gtk.LinkButton | None, builder.get_object('btn_link'))
    box = cast(Gtk.Box | None, builder.get_object('box_url'))
<mask>:
        btn.set_uri(url.geturl())
        btn.set_label(url.netloc)
    return box",btn,27,url.geturl(),False,0.0,N/A
"def wifi_connect_done(client: NM.Client, res: Gio.AsyncResult, button: Gtk.Button):
    created = client.add_connection_finish(res)
    log.debug('NetworkManager created connection: {}', created)
<mask>:
        button.set_label(_('Saved'))
        button.set_sensitive(False)",created,18,created,True,100.00000000000004,N/A
"def add_wifi_connection(info: WifiInfoMessage, callback: Callable, btn: Any, nm_client: NM.Client):
    conn = NM.RemoteConnection()
    base = NM.SettingConnection.new()
    connection_name = f'{info.ssid} ({BRAND_NAME})'
    base.set_property(NM.SETTING_CONNECTION_ID, connection_name)
    conn.add_setting(base)
    ssid = GLib.Bytes.new(info.ssid.encode())
    wireless = NM.SettingWireless.new()
    wireless.set_property(NM.SETTING_WIRELESS_SSID, ssid)
    wireless.set_property(NM.SETTING_WIRELESS_HIDDEN, info.hidden)
    secure = NM.SettingWirelessSecurity.new()
    try:
        key_mn = NMWifiKeyMn[info.auth_type.name] if info.auth_type else None
    except KeyError:
        key_mn = None
<mask>:
        secure.set_property(NM.SETTING_WIRELESS_SECURITY_KEY_MGMT, key_mn)
    if info.password:
        if key_mn == NMWifiKeyMn.WPA:
            secure.set_property(NM.SETTING_WIRELESS_SECURITY_PSK, info.password)
        elif key_mn == NMWifiKeyMn.WEP:
            secure.set_property(NM.SETTING_WIRELESS_SECURITY_WEP_KEY0, info.password)
    conn.add_setting(wireless)
    conn.add_setting(secure)
    nm_client.add_connection_async(conn, True, None, callback, btn)",key_mn,72,key_mn,True,100.00000000000004,N/A
"def parse_wifi_message(string: str) -> WifiInfoMessage | None:
    string = string.strip()
<mask>:
        return None
    parts = string[5:].split(';')
    winfo = WifiInfoMessage()
    for p in parts:
        if p.startswith('S:'):
            winfo.ssid = mecard_unescape(p[2:])
        elif p.startswith('P:'):
            winfo.password = mecard_unescape(p[2:])
        elif p.startswith('T:'):
            auth_type = p[2:]
            winfo.auth_type = WifiAuthType(auth_type) if auth_type != 'nopass' else None
        elif p.startswith('H:'):
            winfo.hidden = parse_to_boolean(p[2:])
    if not winfo.auth_type:
        winfo.password = None
    return winfo",not string.startswith('WIFI:'),60,len(string) < 5,False,6.4790667469036025,N/A
"def do_activate(self):
    """"""Called when the application is activated.

        We raise the application's main window, creating it if
        necessary.
        """"""
    win = self.props.active_window
<mask>:
        win = CoBangWindow(application=self)
    win.present()",not win,27,win is None,False,27.516060407455225,N/A
"def do_open(self, files: list[Gio.File], n_file: int, hint: str):
    """"""Called when the application is opened with files.""""""
<mask>:
        return
    file = files[0]
    log.debug('Opening file: {}', file)
    mime_type = guess_mimetype(file)
    log.info('MIME type: {}', mime_type)
    if not mime_type or not mime_type.startswith('image/'):
        log.info('Not an image. Ignore.')
        return
    win = self.props.active_window
    if not win:
        win = CoBangWindow(application=self)
    win.process_file_from_commandline(file, mime_type)
    win.present()",not n_file,55,len(files) == 0,False,0.0,N/A
"def on_about_action(self, *args):
    """"""Callback for the app.about action.""""""
<mask>:
        win.btn_pause.set_active(True)
    version = self.get_version() or '0.0'
    year = datetime.now().year
    about = Adw.AboutDialog(application_name=BRAND_NAME, application_icon=APP_ID, developer_name='Nguyễn Hồng Quân', version=version, developers=DEVELOPPERS, artists=ARTISTS, license_type=Gtk.License.GPL_3_0, copyright=f'© 2020 - {year} Nguyễn Hồng Quân', website='https://github.com/hongquan/CoBang', issue_url='https://github.com/hongquan/CoBang/issues', comments=COMMENTS)
    about.present(self.props.active_window)",(win := self.props.active_window),40,self.props.active_window,False,48.95416595569534,N/A
"def create_action(self, name, callback, shortcuts=None):
    """"""Add an application action.

        Args:
            name: the name of the action
            callback: the function to be called when the action is
              activated
            shortcuts: an optional list of accelerators
        """"""
    action = Gio.SimpleAction.new(name, None)
    action.connect('activate', callback)
    self.add_action(action)
<mask>:
        self.set_accels_for_action(f'app.{name}', shortcuts)",shortcuts,44,shortcuts,True,100.00000000000004,N/A
"def cb_networkmanager_client_init_done(self, client: NM.Client, res: Gio.AsyncResult):
<mask>:
        log.error('Failed to initialize NetworkManager client')
        return
    client.new_finish(res)
    self.nm_client = client
    log.debug('NM client: {}', client)",not client,21,client.error,False,27.516060407455225,N/A
"def __init__(self, hass, config):
    """"""Initialize the sensor.""""""
    self.config = config
    self._name = config.get(CONF_NAME)
    self._id_prefix = config.get(CONF_ID_PREFIX)
<mask>:
        self._id_prefix = 'anniversary_'
    self.entity_id = generate_entity_id(ENTITY_ID_FORMAT, self._id_prefix + self._name, [])
    self._unknown_year = False
    self._date = ''
    self._show_half_anniversary = config.get(CONF_HALF_ANNIVERSARY)
    self._half_days_remaining = 0
    self._half_date = ''
    self._template_sensor = False
    self._date_template = config.get(CONF_DATE_TEMPLATE)
    if self._date_template is not None:
        self._template_sensor = True
    else:
        self._date, self._unknown_year = validate_date(config.get(CONF_DATE))
        self._date = self._date.replace(tzinfo=dt_util.DEFAULT_TIME_ZONE)
        if self._show_half_anniversary:
            self._half_date = self._date + relativedelta(months=+6)
    self._icon_normal = config.get(CONF_ICON_NORMAL)
    self._icon_today = config.get(CONF_ICON_TODAY)
    self._icon_soon = config.get(CONF_ICON_SOON)
    self._soon = config.get(CONF_SOON)
    self._icon = self._icon_normal
    self._years_next = 0
    self._years_current = 0
    self._state = 0
    self._weeks_remaining = 0
    self._unit_of_measurement = config.get(CONF_UNIT_OF_MEASUREMENT)
    if self._unit_of_measurement is None:
        self._unit_of_measurement = DEFAULT_UNIT_OF_MEASUREMENT
    self._one_time = config.get(CONF_ONE_TIME)
    self._count_up = config.get(CONF_COUNT_UP)",self._id_prefix is None,114,self._id_prefix is None,True,100.00000000000004,N/A
"@property
def extra_state_attributes(self):
    """"""Return the state attributes.""""""
    res = {}
    res[ATTR_ATTRIBUTION] = ATTRIBUTION
<mask>:
        return res
    if not self._unknown_year:
        res[ATTR_YEARS_NEXT] = self._years_next
        res[ATTR_YEARS_CURRENT] = self._years_current
    res[ATTR_DATE] = self._date
    res[ATTR_NEXT_DATE] = self._next_date
    res[ATTR_WEEKS] = self._weeks_remaining
    if self._show_half_anniversary:
        res[ATTR_HALF_DATE] = self._half_date
        res[ATTR_HALF_DAYS] = self._half_days_remaining
    return res","self._state in ['Invalid Date', 'Invalid Template']",44,self._unknown_month and self._unknown_month,False,11.498759556447217,N/A
"@property
def unit_of_measurement(self):
    """"""Return the unit this state is expressed in.""""""
<mask>:
        return
    return self._unit_of_measurement","self._state in ['Invalid Date', 'Invalid Template']",15,self._unit_of_measurement is None,False,12.788328485625525,N/A
"@property
def extra_state_attributes(self) -> dict | None:
    """"""Return the device state attributes.""""""
<mask>:
        return None
    return {}",self.hass.data[DOMAIN][CALENDAR_PLATFORM].event is None,17,self.device_state_attributes is None,False,5.861955497706755,N/A
"def add_entity(self, entity_id: str) -> None:
    """"""Append entity ID to the calendar.""""""
<mask>:
        self.entities.append(entity_id)",entity_id not in self.entities,14,entity_id not in self.entities,True,100.00000000000004,N/A
"def remove_entity(self, entity_id: str) -> None:
    """"""Remove entity ID from the calendar.""""""
<mask>:
        self.entities.remove(entity_id)",entity_id in self.entities,14,entity_id in self.entities,True,100.00000000000004,N/A
"@staticmethod
@callback
def async_get_options_flow(config_entry):
<mask>:
        return OptionsFlowHandler(config_entry)
    else:
        return EmptyOptions(config_entry)","config_entry.options.get('unique_id', None) is not None",10,config_entry.options_flow,False,15.42055884967951,N/A
"def is_not_date(date, one_time):
    try:
        datetime.strptime(date, '%Y-%m-%d')
        return False
    except ValueError:
<mask>:
            pass
        else:
            return True
    try:
        datetime.strptime(date, '%m-%d')
        return False
    except ValueError:
        return True",not one_time,24,one_time,False,71.65313105737896,N/A
"def filterJurnals(papers, csv_path):
    result = []
    df = pd.read_csv(csv_path, sep=';')
    journal_list = list(df['journal_list'])
    include_list = list(df['include_list'])
    for p in papers:
        good = not (p.jurnal is not None and len(p.jurnal) > 0)
<mask>:
            for jurnal, include in zip(journal_list, include_list):
                if include == 1 and similarStrings(p.jurnal, jurnal) >= 0.8:
                    good = True
        if good:
            result.append(p)
    return result",p.jurnal is not None,55,good,False,0.0,N/A
"def filter_min_date(list_papers, min_year):
    new_list = []
    for paper in list_papers:
<mask>:
            new_list.append(paper)
    return new_list",paper.year is not None and int(paper.year) >= min_year,14,paper['start_year'] < min_year,False,7.541743856652242,N/A
"def setSciHubUrl():
    print('Searching for a sci-hub mirror')
    r = requests.get(NetInfo.SciHub_URLs_repo, headers=NetInfo.HEADERS)
    links = SciHubUrls(r.text)
    for l in links:
        try:
            print('Trying with {}...'.format(l))
            r = requests.get(l, headers=NetInfo.HEADERS)
<mask>:
                NetInfo.SciHub_URL = l
                break
        except:
            pass
    else:
        print('\nNo working Sci-Hub instance found!\nIf in your country Sci-Hub is not available consider using a VPN or a proxy\nYou can use a specific mirror mirror with the --scihub-mirror argument')
        NetInfo.SciHub_URL = 'https://sci-hub.st'",r.status_code == 200,66,r.status_code == 200,True,100.00000000000004,N/A
"def downloadPapers(papers, dwnl_dir, num_limit, SciHub_URL=None, SciDB_URL=None):
    NetInfo.SciHub_URL = SciHub_URL
<mask>:
        setSciHubUrl()
    if SciDB_URL is not None:
        NetInfo.SciDB_URL = SciDB_URL
    print('\nUsing Sci-Hub mirror {}'.format(NetInfo.SciHub_URL))
    print('Using Sci-DB mirror {}'.format(NetInfo.SciDB_URL))
    print(""You can use --scidb-mirror and --scidb-mirror to specify your're desired mirror URL\n"")
    num_downloaded = 0
    paper_number = 1
    paper_files = []
    for p in papers:
        if p.canBeDownloaded() and (num_limit is None or num_downloaded < num_limit):
            print('Download {} of {} -> {}'.format(paper_number, len(papers), p.title))
            paper_number += 1
            pdf_dir = getSaveDir(dwnl_dir, p.getFileName())
            failed = 0
            url = ''
            while not p.downloaded and failed != 5:
                try:
                    dwn_source = 1
                    if failed == 0 and p.DOI is not None:
                        url = URLjoin(NetInfo.SciDB_URL, p.DOI)
                    if failed == 1 and p.DOI is not None:
                        url = URLjoin(NetInfo.SciHub_URL, p.DOI)
                        dwn_source = 2
                    if failed == 2 and p.scholar_link is not None:
                        url = URLjoin(NetInfo.SciHub_URL, p.scholar_link)
                    if failed == 3 and p.scholar_link is not None and (p.scholar_link[-3:] == 'pdf'):
                        url = p.scholar_link
                        dwn_source = 3
                    if failed == 4 and p.pdf_link is not None:
                        url = p.pdf_link
                        dwn_source = 3
                    if url != '':
                        r = requests.get(url, headers=NetInfo.HEADERS)
                        content_type = r.headers.get('content-type')
                        if (dwn_source == 1 or dwn_source == 2) and 'application/pdf' not in content_type and ('application/octet-stream' not in content_type):
                            time.sleep(random.randint(1, 4))
                            pdf_link = getSchiHubPDF(r.text)
                            if pdf_link is not None:
                                r = requests.get(pdf_link, headers=NetInfo.HEADERS)
                                content_type = r.headers.get('content-type')
                        if 'application/pdf' in content_type or 'application/octet-stream' in content_type:
                            paper_files.append(saveFile(pdf_dir, r.content, p, dwn_source))
                except Exception:
                    pass
                failed += 1",NetInfo.SciHub_URL is None,234,SciHub_URL is None,False,67.03200460356396,N/A
"def schoolarParser(html):
    result = []
    soup = BeautifulSoup(html, 'html.parser')
    for element in soup.findAll('div', class_='gs_r gs_or gs_scl'):
<mask>:
            title = None
            link = None
            link_pdf = None
            cites = None
            year = None
            authors = None
            for h3 in element.findAll('h3', class_='gs_rt'):
                found = False
                for a in h3.findAll('a'):
                    if not found:
                        title = a.text
                        link = a.get('href')
                        found = True
            for a in element.findAll('a'):
                if 'Cited by' in a.text:
                    cites = int(a.text[8:])
                if '[PDF]' in a.text:
                    link_pdf = a.get('href')
            for div in element.findAll('div', class_='gs_a'):
                try:
                    authors, source_and_year, source = div.text.replace('\xa0', ' ').split(' - ')
                except ValueError:
                    continue
                if not authors.strip().endswith('…'):
                    authors = authors.replace(', ', ';')
                else:
                    authors = None
                try:
                    year = int(source_and_year[-4:])
                except ValueError:
                    continue
                if not 1000 <= year <= 3000:
                    year = None
                else:
                    year = str(year)
            if title is not None:
                result.append({'title': title, 'link': link, 'cites': cites, 'link_pdf': link_pdf, 'year': year, 'authors': authors})
    return result",not isBook(element),148,element.get('id') == 'schoolar',False,6.27465531099474,N/A
"def isBook(tag):
    result = False
    for span in tag.findAll('span', class_='gs_ct2'):
<mask>:
            result = True
    return result",span.text == '[B]',16,span.get('class') == 'gs_ct2',False,10.600313379512592,N/A
"def getSchiHubPDF(html):
    result = None
    soup = BeautifulSoup(html, 'html.parser')
    iframe = soup.find(id='pdf')
    plugin = soup.find(id='plugin')
    download_scidb = soup.find('a', text=lambda text: text and 'Download' in text, href=re.compile('\\.pdf$'))
    embed_scihub = soup.find('embed')
<mask>:
        result = iframe.get('src')
    if plugin is not None and result is None:
        result = plugin.get('src')
    if result is not None and result[0] != 'h':
        result = 'https:' + result
    if download_scidb is not None and result is None:
        result = download_scidb.get('href')
    if embed_scihub is not None and result is None:
        result = embed_scihub.get('original-url')
    return result",iframe is not None,85,iframe is not None and result is None,False,34.57207846419409,N/A
"def SciHubUrls(html):
    result = []
    soup = BeautifulSoup(html, 'html.parser')
    for ul in soup.findAll('ul'):
        for a in ul.findAll('a'):
            link = a.get('href')
<mask>:
                result.append(link)
    return result",link.startswith('https://sci-hub.') or link.startswith('http://sci-hub.'),24,link,False,3.7751345442790995e-09,N/A
"def waithIPchange():
    while True:
        inp = input('You have been blocked, try changing your IP or using a VPN. Press Enter to continue downloading, or type ""exit"" to stop and exit....')
<mask>:
            return False
        elif not inp.strip():
            print('Wait 30 seconds...')
            time.sleep(30)
            return True",inp.strip().lower() == 'exit',42,inp.startswith('#'),False,7.966506956353643,N/A
"def scholar_requests(scholar_pages, url, restrict, chrome_version, scholar_results=10):
    javascript_error = ""Sorry, we can't verify that you're not a robot when JavaScript is turned off""
    to_download = []
    driver = None
    for i in scholar_pages:
        while True:
            res_url = url % (scholar_results * (i - 1))
<mask>:
                if driver is None:
                    print('Using Selenium driver')
                    options = Options()
                    options.add_argument('--headless')
                    driver = uc.Chrome(headless=True, use_subprocess=False, version_main=chrome_version)
                driver.get(res_url)
                html = driver.page_source
            else:
                html = requests.get(res_url, headers=NetInfo.HEADERS)
                html = html.text
            if javascript_error in html:
                is_continue = waithIPchange()
                if not is_continue:
                    return to_download
            else:
                break
        papers = schoolarParser(html)
        if len(papers) > scholar_results:
            papers = papers[0:scholar_results]
        print('\nGoogle Scholar page {} : {} papers found'.format(i, scholar_results))
        if len(papers) > 0:
            papersInfo = getPapersInfo(papers, url, restrict, scholar_results)
            info_valids = functools.reduce(lambda a, b: a + 1 if b.DOI is not None else a, papersInfo, 0)
            print('Papers found on Crossref: {}/{}\n'.format(info_valids, len(papers)))
            to_download.append(papersInfo)
        else:
            print('Paper not found...')
    return to_download",chrome_version is not None,145,res_url.startswith('/selenium/'),False,3.3864985683445354,N/A
"def parseSkipList(skip_words):
    skip_list = skip_words.split(',')
    print('Skipping results containing {}'.format(skip_list))
    output_param = ''
    for skip_word in skip_list:
        skip_word = skip_word.strip()
<mask>:
            output_param += '+-""' + skip_word + '""'
        else:
            output_param += '+-' + skip_word
    return output_param",' ' in skip_word,35,"skip_word.startswith('""')",False,17.747405280050266,N/A
"def ScholarPapersInfo(query, scholar_pages, restrict, min_date=None, scholar_results=10, chrome_version=None, cites=None, skip_words=None):
    url = 'https://scholar.google.com/scholar?hl=en&as_vis=1&as_sdt=1,5&start=%d'
<mask>:
        if len(query) > 7 and (query.startswith('http://') or query.startswith('https://')):
            url = query
        else:
            url += f'&q={query}'
        if skip_words:
            url += parseSkipList(skip_words)
            print(url)
    if cites:
        url += f'&cites={cites}'
    if min_date:
        url += f'&as_ylo={min_date}'
    to_download = scholar_requests(scholar_pages, url, restrict, chrome_version, scholar_results)
    return [item for sublist in to_download for item in sublist]",query,61,query,True,100.00000000000004,N/A
"def getFileName(self):
    try:
<mask>:
            return urllib.parse.quote(self.DOI, safe='') + '.pdf'
        else:
            return re.sub('[^\\w\\-_. ]', '_', self.title) + '.pdf'
    except:
        return 'none.pdf'",self.use_doi_as_filename,20,self.DOI,False,7.4477876575973925,N/A
"def setBibtex(self, bibtex):
    x = bibtexparser.loads(bibtex, parser=None)
    x = x.entries
    self.bibtex = bibtex
    try:
<mask>:
            self.year = x[0]['year']
        if 'author' in x[0]:
            self.authors = x[0]['author']
        self.jurnal = x[0]['journal'].replace('\\', '') if 'journal' in x[0] else None
        if self.jurnal is None:
            self.jurnal = x[0]['publisher'].replace('\\', '') if 'publisher' in x[0] else None
    except:
        pass",'year' in x[0],51,'year' in x[0],True,100.00000000000004,N/A
"def generateReport(papers, path):
    columns = ['Name', 'Scholar Link', 'DOI', 'Bibtex', 'PDF Name', 'Year', 'Scholar page', 'Journal', 'Downloaded', 'Downloaded from', 'Authors']
    data = []
    for p in papers:
        pdf_name = p.getFileName() if p.downloaded else ''
        bibtex_found = p.bibtex is not None
        dwn_from = ''
<mask>:
            dwn_from = 'SciDB'
        elif p.downloadedFrom == 2:
            dwn_from = 'SciHub'
        elif p.downloadedFrom == 3:
            dwn_from = 'Scholar'
        data.append({'Name': p.title, 'Scholar Link': p.scholar_link, 'DOI': p.DOI, 'Bibtex': bibtex_found, 'PDF Name': pdf_name, 'Year': p.year, 'Scholar page': p.scholar_page, 'Journal': p.jurnal, 'Downloaded': p.downloaded, 'Downloaded from': dwn_from, 'Authors': p.authors})
    df = pd.DataFrame(data, columns=columns)
    df.to_csv(path, index=False, encoding='utf-8')",p.downloadedFrom == 1,94,p.downloadedFrom == 1,True,100.00000000000004,N/A
"def generateBibtex(papers, path):
    content = ''
    for p in papers:
<mask>:
            content += p.bibtex + '\n'
    relace_list = ['\x07st', '*', '#']
    for c in relace_list:
        content = content.replace(c, '')
    f = open(path, 'w', encoding='latin-1', errors='ignore')
    f.write(str(content))
    f.close()",p.bibtex is not None,37,"hasattr(p, 'bibtex')",False,8.116697886877475,N/A
"def getBibtex(DOI):
    try:
        url_bibtex = 'http://api.crossref.org/works/' + DOI + '/transform/application/x-bibtex'
        x = requests.get(url_bibtex)
<mask>:
            return ''
        return str(x.text)
    except Exception as e:
        print(e)
        return ''",x.status_code == 404,25,x.status_code != 200,False,54.10822690539397,N/A
"def getPapersInfoFromDOIs(DOI, restrict):
    paper_found = Paper()
    paper_found.DOI = DOI
    try:
        paper = get_entity(DOI, EntityType.PUBLICATION, OutputType.JSON)
<mask>:
            if 'title' in paper:
                paper_found.title = paper['title'][0]
            if 'short-container-title' in paper and len(paper['short-container-title']) > 0:
                paper_found.jurnal = paper['short-container-title'][0]
            if restrict is None or restrict != 1:
                paper_found.setBibtex(getBibtex(paper_found.DOI))
    except:
        print('Paper not found ' + DOI)
    return paper_found",paper is not None and len(paper) > 0,52,paper,False,0.004539992976248487,N/A
"def getPapersInfo(papers, scholar_search_link, restrict, scholar_results):
    papers_return = []
    num = 1
    for paper in papers:
        title = paper['title']
        queries = {'query.bibliographic': title.lower(), 'sort': 'relevance', 'select': 'DOI,title,deposited,author,short-container-title'}
        print('Searching paper {} of {} on Crossref...'.format(num, len(papers)))
        num += 1
        found_timestamp = 0
        paper_found = Paper(title, paper['link'], scholar_search_link, paper['cites'], paper['link_pdf'], paper['year'], paper['authors'])
        while True:
            try:
                for el in iterate_publications_as_json(max_results=30, queries=queries):
                    el_date = 0
<mask>:
                        el_date = int(el['deposited']['timestamp'])
                    if (paper_found.DOI is None or el_date > found_timestamp) and 'title' in el and (similarStrings(title.lower(), el['title'][0].lower()) > 0.75):
                        found_timestamp = el_date
                        if 'DOI' in el:
                            paper_found.DOI = el['DOI'].strip().lower()
                        if 'short-container-title' in el and len(el['short-container-title']) > 0:
                            paper_found.jurnal = el['short-container-title'][0]
                        if restrict is None or restrict != 1:
                            paper_found.setBibtex(getBibtex(paper_found.DOI))
                break
            except ConnectionError as e:
                print('Wait 10 seconds and try again...')
                time.sleep(10)
        papers_return.append(paper_found)
        time.sleep(random.randint(1, 10))
    return papers_return",'deposited' in el and 'timestamp' in el['deposited'],128,'deposited' in el and 'timestamp' in el,False,65.14390575310559,N/A
"def checkVersion():
    try:
        print('PyPaperBot v' + __version__)
        response = requests.get('https://pypi.org/pypi/pypaperbot/json')
        latest_version = response.json()['info']['version']
<mask>:
            print(""NEW VERSION AVAILABLE!\nUpdate with 'pip install PyPaperBot —upgrade' to get the latest features!\n"")
    except:
        pass",latest_version != __version__,29,latest_version != __version__,True,100.00000000000004,N/A
"def start(query, scholar_results, scholar_pages, dwn_dir, proxy, min_date=None, num_limit=None, num_limit_type=None, filter_jurnal_file=None, restrict=None, DOIs=None, SciHub_URL=None, chrome_version=None, cites=None, use_doi_as_filename=False, SciDB_URL=None, skip_words=None):
<mask>:
        SciDB_URL = urljoin(SciDB_URL, '/scidb/')
    to_download = []
    if DOIs is None:
        print('Query: {}'.format(query))
        print('Cites: {}'.format(cites))
        to_download = ScholarPapersInfo(query, scholar_pages, restrict, min_date, scholar_results, chrome_version, cites, skip_words)
    else:
        print('Downloading papers from DOIs\n')
        num = 1
        i = 0
        while i < len(DOIs):
            DOI = DOIs[i]
            print('Searching paper {} of {} with DOI {}'.format(num, len(DOIs), DOI))
            papersInfo = getPapersInfoFromDOIs(DOI, restrict)
            papersInfo.use_doi_as_filename = use_doi_as_filename
            to_download.append(papersInfo)
            num += 1
            i += 1
    if restrict != 0 and to_download:
        if filter_jurnal_file is not None:
            to_download = filterJurnals(to_download, filter_jurnal_file)
        if min_date is not None:
            to_download = filter_min_date(to_download, min_date)
        if num_limit_type is not None and num_limit_type == 0:
            to_download.sort(key=lambda x: int(x.year) if x.year is not None else 0, reverse=True)
        if num_limit_type is not None and num_limit_type == 1:
            to_download.sort(key=lambda x: int(x.cites_num) if x.cites_num is not None else 0, reverse=True)
        downloadPapers(to_download, dwn_dir, num_limit, SciHub_URL, SciDB_URL)
    Paper.generateReport(to_download, dwn_dir + 'result.csv')
    Paper.generateBibtex(to_download, dwn_dir + 'bibtex.bib')",SciDB_URL is not None and '/scidb' not in SciDB_URL,163,SciDB_URL is not None,False,22.31301601484299,N/A
"def __init__(self):
    Gtk.Dialog.__init__(self, title='Emote Guide', window_position=Gtk.WindowPosition.CENTER, resizable=False)
    header = Gtk.HeaderBar(title='Guide', show_close_button=True)
    self.set_titlebar(header)
    box = self.get_content_area()
    hbox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL)
    vbox = Gtk.Box(orientation=Gtk.Orientation.VERTICAL)
    launching = Gtk.Label()
    launching.set_markup('<span size=""large"" font_weight=""bold"" underline=""single"">Launching</span>')
    launching.set_alignment(0, 0.5)
    vbox.pack_start(launching, True, True, GRID_SIZE)
    background = Gtk.Label()
    background.set_markup('Emote runs in the background and automatically starts when you log in.')
    background.set_line_wrap(True)
    background.set_alignment(0, 0.5)
    vbox.pack_start(background, True, True, GRID_SIZE)
<mask>:
        opening = Gtk.Label()
        opening.set_markup('The emoji picker can be opened by clicking the app icon again, or by\nsetting a custom app shortcut. See <a href=""https://github.com/tom-james-watson/Emote/wiki/Hotkey-In-Wayland"" title=""See Wayland shortcut instructions"">the wiki</a> for details.')
        opening.set_line_wrap(True)
        opening.set_alignment(0, 0.5)
        vbox.pack_start(opening, True, True, GRID_SIZE)
    else:
        opening = Gtk.Label()
        opening.set_markup('The emoji picker can be opened with either the keyboard shortcut or by\nclicking the app icon again.')
        opening.set_line_wrap(True)
        opening.set_alignment(0, 0.5)
        vbox.pack_start(opening, True, True, GRID_SIZE)
    usage = Gtk.Label()
    usage.set_markup('<span size=""large"" font_weight=""bold"" underline=""single"">Usage</span>')
    usage.set_alignment(0, 0.5)
    vbox.pack_start(usage, True, True, GRID_SIZE)
    if config.is_wayland:
        copying = Gtk.Label()
        copying.set_markup('Select an emoji to have it copied to your clipboard. You can then paste the\nemoji wherever you need.')
        copying.set_line_wrap(True)
        copying.set_alignment(0, 0.5)
        vbox.pack_start(copying, True, True, GRID_SIZE)
    else:
        copying = Gtk.Label()
        copying.set_markup('Select an emoji to have it pasted to your currently focused app. The\nemoji is also copied to the clipboard so you can then paste the emoji\nwherever you need.')
        copying.set_line_wrap(True)
        copying.set_alignment(0, 0.5)
        vbox.pack_start(copying, True, True, GRID_SIZE)
    multiple = Gtk.Label()
    multiple.set_markup('You can select multiple emojis by selecting them with shift left click\nor with right click.')
    multiple.set_line_wrap(True)
    multiple.set_alignment(0, 0.5)
    vbox.pack_start(multiple, True, True, GRID_SIZE)
    hbox.pack_start(vbox, True, True, GRID_SIZE)
    box.pack_start(hbox, True, True, GRID_SIZE)
    self.show_all()
    self.present()",config.is_wayland,240,platform.system() == 'Windows',False,5.522397783539471,N/A
"def init_menu_button(self):
    self.menu_popover = Gtk.Popover()
    vbox = Gtk.Box(orientation=Gtk.Orientation.VERTICAL)
    hbox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL)
    items_box = Gtk.Box(orientation=Gtk.Orientation.VERTICAL)
<mask>:
        prefs_btn = Gtk.ModelButton('Preferences')
        prefs_btn.set_alignment(0, 0.5)
        prefs_btn.connect('clicked', lambda prefs_btn: self.open_preferences())
        items_box.pack_start(prefs_btn, False, True, 0)
    keyboard_shortcuts_btn = Gtk.ModelButton('Keyboard Shortcuts')
    keyboard_shortcuts_btn.set_alignment(0, 0.5)
    keyboard_shortcuts_btn.connect('clicked', lambda keyboard_shortcuts_btn: self.open_keyboard_shortcuts())
    items_box.pack_start(keyboard_shortcuts_btn, False, True, 0)
    guide_btn = Gtk.ModelButton('Guide')
    guide_btn.set_alignment(0, 0.5)
    guide_btn.connect('clicked', lambda guide_btn: self.open_guide())
    items_box.pack_start(guide_btn, False, True, 0)
    about_btn = Gtk.ModelButton('About')
    about_btn.set_alignment(0, 0.5)
    about_btn.connect('clicked', lambda about_btn: self.open_about())
    items_box.pack_start(about_btn, False, True, 0)
    vbox.pack_start(items_box, False, False, GRID_SIZE)
    hbox.pack_start(vbox, False, False, GRID_SIZE)
    hbox.show_all()
    self.menu_popover.add(hbox)
    self.menu_popover.set_position(Gtk.PositionType.BOTTOM)
    menu_button = Gtk.MenuButton(name='menu_button')
    menu_button.set_popover(self.menu_popover)
    menu_button.show()
    menu_button.add(Gtk.Image.new_from_gicon(Gio.ThemedIcon(name='open-menu-symbolic'), Gtk.IconSize.BUTTON))
    return menu_button",config.is_snap or config.is_dev,88,self.settings.get('prefs'),False,4.513617516969122,N/A
"def on_skintone_combo_changed(self, combo):
    char = combo.get_active_text()
    skintone_index = None
<mask>:
        skintone_index = 0
    elif char == '✋🏻':
        skintone_index = 1
    elif char == '✋🏼':
        skintone_index = 2
    elif char == '✋🏽':
        skintone_index = 3
    elif char == '✋🏾':
        skintone_index = 4
    elif char == '✋🏿':
        skintone_index = 5
    if skintone_index is not None:
        user_data.update_skintone_index(skintone_index)
        query = self.search_entry.props.text
        if query == '':
            if hasattr(self, 'selected_emoji_category'):
                self.render_selected_emoji_category()
        else:
            self.render_emoji_search_results(query)",char == '✋',67,char == '✋🏏�',False,59.460355750136046,N/A
"def init_category_selectors(self):
    self.categories_box = Gtk.Box(margin_bottom=GRID_SIZE, margin_top=GRID_SIZE)
    self.category_selectors = []
    self.selected_emoji_category = 'recent'
    for category, _, category_image in emojis.get_category_order():
        category_selector = Gtk.ToggleButton(label=category_image, name='category_selector_button')
        category_selector.set_tooltip_text(self.get_category_display_name(category))
        category_selector.category = category
<mask>:
            category_selector.set_active(True)
        self.category_selectors.append(category_selector)
        category_selector.connect('toggled', self.on_category_selector_toggled)
        self.categories_box.pack_start(category_selector, True, False, GRID_SIZE)
    self.app_container.add(self.categories_box)",category == self.selected_emoji_category,36,self.selected_emoji_category == category,False,78.56293018010261,N/A
"def get_skintone_char(self, emoji):
    char = emoji['char']
<mask>:
        return char
    skintone = user_data.load_skintone_index()
    if skintone == 0:
        return char
    try:
        return emoji['skintone'][str(skintone)]['char']
    except Exception:
        return char",emoji['skintone'] is None,25,not emoji['skintone'],False,54.75182535069452,N/A
"def reset_emoji_preview(self):
<mask>:
        self.show_emoji_preview(self.target_emoji)
    else:
        self.previewed_emoji_label.set_text(' ')
        self.previewed_emoji_name_label.set_text(' ')
        self.previewed_emoji_shortcode_label.set_text(' ')",len(self.current_emojis) > 0,11,self.target_emoji,False,8.697972365316721,N/A
"def search(self, query: str):
<mask>:
        self.timer.cancel()
    self.timer = Timer(DEBOUNCE_INTERVAL, self.callback, args=(query,))
    self.timer.start()",self.timer,12,self.timer,True,100.00000000000004,N/A
"def __init__(self, update_accelerator):
    Gtk.Dialog.__init__(self, title='Emote Keyboard Shortcuts', window_position=Gtk.WindowPosition.CENTER, resizable=False)
    self.update_accelerator = update_accelerator
    header = Gtk.HeaderBar(title='Keyboard Shortcuts', show_close_button=True)
    self.set_titlebar(header)
    box = self.get_content_area()
    shortcuts_grid = Gtk.Grid(orientation=Gtk.Orientation.VERTICAL, margin=GRID_SIZE, row_spacing=GRID_SIZE)
    shortcuts_grid.set_row_homogeneous(False)
    shortcuts_grid.set_column_homogeneous(True)
    row = 1
<mask>:
        open_label = Gtk.Label('Open Emoji Picker')
        open_label.set_alignment(0, 0.5)
        shortcuts_grid.attach(open_label, 1, row, 1, 1)
        open_keybinding = ButtonKeybinding()
        open_keybinding.set_size_request(150, -1)
        open_keybinding.connect('accel-edited', self.on_kb_changed)
        open_keybinding.connect('accel-cleared', self.on_kb_changed)
        accel_string, _ = user_data.load_accelerator()
        open_keybinding.set_accel_string(accel_string)
        shortcuts_grid.attach(open_keybinding, 2, row, 1, 1)
        row += 1
    select_label = Gtk.Label('Select Emoji')
    select_label.set_alignment(0, 0.5)
    shortcuts_grid.attach(select_label, 1, row, 1, 1)
    select_shortcut = Gtk.ShortcutsShortcut(accelerator='Return')
    shortcuts_grid.attach(select_shortcut, 2, row, 1, 1)
    row += 1
    select_multi_label = Gtk.Label('Add Emoji to Selection')
    select_multi_label.set_alignment(0, 0.5)
    shortcuts_grid.attach(select_multi_label, 1, row, 1, 1)
    select_multi_shortcut = Gtk.ShortcutsShortcut(accelerator='<Shift>+Return')
    shortcuts_grid.attach(select_multi_shortcut, 2, row, 1, 1)
    row += 1
    search_label = Gtk.Label('Focus Search')
    search_label.set_alignment(0, 0.5)
    shortcuts_grid.attach(search_label, 1, row, 1, 1)
    search_shortcut = Gtk.ShortcutsShortcut(accelerator='<Ctrl>+F')
    shortcuts_grid.attach(search_shortcut, 2, row, 1, 1)
    row += 1
    next_cat_label = Gtk.Label('Next Emoji Category')
    next_cat_label.set_alignment(0, 0.5)
    shortcuts_grid.attach(next_cat_label, 1, row, 1, 1)
    next_cat_shortcut = Gtk.ShortcutsShortcut(accelerator='<Ctrl>+Tab')
    shortcuts_grid.attach(next_cat_shortcut, 2, row, 1, 1)
    row += 1
    prev_cat_label = Gtk.Label('Previous Emoji Category')
    prev_cat_label.set_alignment(0, 0.5)
    shortcuts_grid.attach(prev_cat_label, 1, row, 1, 1)
    prev_cat_label = Gtk.ShortcutsShortcut(accelerator='<Ctrl>+<Shift>+Tab')
    shortcuts_grid.attach(prev_cat_label, 2, row, 1, 1)
    row += 1
    box.pack_start(shortcuts_grid, True, True, GRID_SIZE)
    self.show_all()
    self.present()",not config.is_wayland,186,user_data.has_accelerator(),False,5.669791110976001,N/A
"def start_daemon(self):
    setproctitle('emote')
<mask>:
        Keybinder.init()
        self.set_accelerator()
    css.load_css()
    emojis.init()
    self.activated = True
    if not user_data.load_shown_welcome():
        self.create_picker_window(True)
        user_data.update_shown_welcome()
    if config.is_flatpak:
        self.flatpak_autostart()
    self.set_theme()
    Gtk.main()",not config.is_wayland,21,config.is_keybinder,False,54.75182535069452,N/A
"def set_accelerator(self):
    """"""Register global shortcut for invoking the emoji picker""""""
    accel_string, _ = user_data.load_accelerator()
<mask>:
        Keybinder.bind(accel_string, self.handle_accelerator)",accel_string,17,accel_string,True,100.00000000000004,N/A
"def set_theme(self):
    """"""Set the GTK theme to be used for the app windows""""""
    theme = user_data.load_theme()
    print(f""Setting theme New=[{theme}] Current=[{settings.get_property('gtk-theme-name')}]"")
<mask>:
        print(f'Setting theme to {theme}')
        settings.set_property('gtk-theme-name', theme)
    else:
        settings.reset_property('gtk-theme-name')",theme != user_data.DEFAULT_THEME,29,theme,False,0.012340980408667962,N/A
"def process_emoji_row(row):
    global all_emojis
    global emojis_by_category
    category = row['group']
<mask>:
        return
    if category in ['smileys-emotion', 'people-body']:
        category = 'smileys-people'
    if row['skintone'] != '':
        for emoji in all_emojis:
            if row['skintone_base_emoji'] == emoji['char'] and emoji['skintone'] is not None:
                emoji['skintone'][row['skintone']] = make_emoji_data(row)
                return
    emoji = make_emoji_data(row)
    emojis_by_category[category].append(emoji)
    all_emojis.append(emoji)",category in EMOJI_CATEGORY_BLOCKLIST,45,category not in emojis_by_category,False,7.809849842300637,N/A
"def get_emoji_by_char(char):
    char = strip_qualified_variant(strip_char_skintone(char))
    for emoji in all_emojis:
<mask>:
            return emoji
    raise Exception(f""Couldn't find emoji by char {char}"")",strip_qualified_variant(emoji['char']) == char,19,emoji.char == char,False,8.558153335723478,N/A
"def load_css():
    """"""
    Load associated CSS for the window.
    """"""
    css_provider = Gtk.CssProvider()
<mask>:
        css_provider.load_from_path(f'{config.snap_root}/static/style.css')
    elif config.is_flatpak:
        css_provider.load_from_path(f'{config.flatpak_root}/static/style.css')
    else:
        css_provider.load_from_path('static/style.css')
    screen = Gdk.Screen.get_default()
    styleContext = Gtk.StyleContext()
    styleContext.add_provider_for_screen(screen, css_provider, Gtk.STYLE_PROVIDER_PRIORITY_USER)",config.is_snap,29,config.is_snap,True,100.00000000000004,N/A
"def update_recent_emojis(char):
    char = emojis.strip_char_skintone(char)
    recent_emojis = load_recent_emojis()
<mask>:
        recent_emojis.remove(char)
        new_recent_emojis = [char] + recent_emojis[:MAX_RECENT_EMOJIS - 2]
    else:
        new_recent_emojis = [char] + recent_emojis[:MAX_RECENT_EMOJIS - 1]
    with shelve.open(SHELVE_PATH) as db:
        db[RECENT_EMOJIS] = new_recent_emojis",char in recent_emojis,32,char in recent_emojis,True,100.00000000000004,N/A
"def do_get_property(self, prop):
<mask>:
        return self.accel_string
    else:
        raise AttributeError('unknown property %s' % prop.name)",prop.name == 'accel-string',13,prop.name == 'accel_string',False,51.697315395717055,N/A
"def do_set_property(self, prop, value):
<mask>:
        if value != self.accel_string:
            self.accel_string = value
            self.keybinding_cell.set_value(value)
    else:
        raise AttributeError('unknown property %s' % prop.name)",prop.name == 'accel-string',20,prop.name == 'accel',False,75.98356856515926,N/A
"def do_set_property(self, prop, value):
<mask>:
        if value != self.accel_string:
            self.accel_string = value
            self.update_label()
    else:
        raise AttributeError('unknown property %s' % prop.name)",prop.name == 'accel-string',20,prop.name == 'accel',False,75.98356856515926,N/A
"def create_collection(self, collection_name, parent_collection_id=None, parent_collection_name=None, return_results=False):
    """"""
    Create an empty collection, in the given location, utilizing the endpoint 'POST /api/collection/'. 

    Parameters
    ----------
    collection_name : the name used for the created collection.
    parent_collection_id : id of the collection where the created collection resides in.
    parent_collection_name : name of the collection where the created collection resides in (use 'Root' for the root collection).
    return_results : whether to return the info of the created collection.
    """"""
<mask>:
        if not parent_collection_name:
            print('Either the name of id of the parent collection must be provided.')
        if parent_collection_name == 'Root':
            parent_collection_id = None
        else:
            parent_collection_id = self.get_item_id('collection', parent_collection_name)
    res = self.post('/api/collection', json={'name': collection_name, 'parent_id': parent_collection_id, 'color': '#509EE3'})
    if return_results:
        return res",not parent_collection_id,114,not parent_collection_id,True,100.00000000000004,N/A
"def create_segment(self, segment_name, column_name, column_values, segment_description='', db_name=None, db_id=None, table_name=None, table_id=None, return_segment=False):
    """"""
    Create a segment using the given arguments utilizing the endpoint 'POST /api/segment/'. 

    Parameters
    ----------
    segment_name : the name used for the created segment.
    column_name : name of the column used for filtering.
    column_values : list of values for filtering in the given column.
    segment_description : description of the segment (default '') 
    db_name : name of the db that is used as the source of data (default None)    
    db_id : id of the db used as the source of data (default None) 
    table_name : name of the table used for creating the segmnet on it (default None)    
    table_id : id of the table used for creating the segmnet on it (default None)    
    return_segment :    whather to return the created segment info (default False)
    """"""
<mask>:
        raise ValueError('Either the name or id of the table must be provided.')
    if not table_id:
        table_id = self.get_item_id('table', table_name, db_id=db_id, db_name=db_name)
    if not table_name:
        table_name = self.get_item_name(item_type='table', item_id=table_id)
    db_id = self.get_db_id_from_table_id(table_id)
    colmuns_name_id_mapping = self.get_columns_name_id(table_name=table_name, db_id=db_id)
    column_id = colmuns_name_id_mapping[column_name]
    segment_blueprint = {'name': segment_name, 'description': segment_description, 'table_id': table_id, 'definition': {'source-table': table_id, 'filter': ['=', ['field-id', column_id]]}}
    segment_blueprint['definition']['filter'].extend(column_values)
    res = self.post('/api/segment/', json=segment_blueprint)
    if return_segment:
        return res",not table_name and (not table_id),198,not table_name and (not db_name),False,60.767958081376904,N/A
"def get(self, endpoint, *args, **kwargs):
    self.validate_session()
    res = requests.get(self.domain + endpoint, headers=self.header, **kwargs, auth=self.auth)
<mask>:
        return res
    else:
        return res.json() if res.ok else False",'raw' in args,24,res.status_code == 200,False,0.0,N/A
"def post(self, endpoint, *args, **kwargs):
    self.validate_session()
    res = requests.post(self.domain + endpoint, headers=self.header, **kwargs, auth=self.auth)
<mask>:
        return res
    else:
        return res.json() if res.ok else False",'raw' in args,24,res.status_code == 200,False,0.0,N/A
"def put(self, endpoint, *args, **kwargs):
    """"""Used for updating objects (cards, dashboards, ...)""""""
    self.validate_session()
    res = requests.put(self.domain + endpoint, headers=self.header, **kwargs, auth=self.auth)
<mask>:
        return res
    else:
        return res.status_code",'raw' in args,27,res.status_code == 200,False,0.0,N/A
"def delete(self, endpoint, *args, **kwargs):
    self.validate_session()
    res = requests.delete(self.domain + endpoint, headers=self.header, **kwargs, auth=self.auth)
<mask>:
        return res
    else:
        return res.status_code",'raw' in args,20,res.status_code == 404,False,0.0,N/A
"def __init__(self, domain, email=None, password=None, api_key=None, basic_auth=False, is_admin=True):
    assert email is not None or api_key is not None
    self.domain = domain.rstrip('/')
    self.email = email
    self.auth = None
<mask>:
        self.password = getpass.getpass(prompt='Please enter your password: ') if password is None else password
        self.session_id = None
        self.header = None
        if basic_auth:
            self.auth = (self.email, self.password)
        self.authenticate()
    else:
        self.header = {'X-API-KEY': api_key}
        res = requests.get(self.domain + '/api/database/1', headers=self.header)
        if res.status_code == 401:
            raise ValueError('The provided API key is not correct.')
    self.is_admin = is_admin
    if not self.is_admin:
        print('\n                Ask your Metabase admin to disable ""Friendly Table and Field Names"" (in Admin Panel > Settings > General).\n                Without this some of the functions of the current package may not work as expected.\n            ')",email,118,password is not None,False,0.0,N/A
"def authenticate(self):
    """"""Get a Session ID""""""
    conn_header = {'username': self.email, 'password': self.password}
    res = requests.post(self.domain + '/api/session', json=conn_header, auth=self.auth)
<mask>:
        raise Exception(res)
    self.session_id = res.json()['id']
    self.header = {'X-Metabase-Session': self.session_id}",not res.ok,29,res.status_code != 200,False,11.044795567078939,N/A
"def validate_session(self):
    """"""Get a new session ID if the previous one has expired""""""
<mask>:
        return
    res = requests.get(self.domain + '/api/user/current', headers=self.header, auth=self.auth)
    if res.ok:
        return True
    elif res.status_code == 401:
        return self.authenticate()
    else:
        raise Exception(res)",not self.email,35,self.session_id is None,False,13.134549472120788,N/A
"def search(self, q, item_type=None):
    """"""
        Search for Metabase objects and return their basic info. 
        We can limit the search to a certain item type by providing a value for item_type keyword. 

        Parameters
        ----------
        q : search input
        item_type : to limit the search to certain item types (default:None, means no limit)
        """"""
    assert item_type in [None, 'card', 'dashboard', 'collection', 'table', 'pulse', 'segment', 'metric']
    res = self.get(endpoint='/api/search/', params={'q': q})
<mask>:
        res = res['data']
    if item_type is not None:
        res = [item for item in res if item['model'] == item_type]
    return res",type(res) == dict,90,'data' in res,False,7.253154775624655,N/A
"def get_card_data(self, card_name=None, card_id=None, collection_name=None, collection_id=None, data_format='json', parameters=None, format_rows=False):
    """"""
        Run the query associated with a card and get the results.

        Parameters
        ----------
        data_format : specifies the format of the returned data:
            - 'json': every row is a dictionary of <column-header, cell> key-value pairs    
            - 'csv': the entire result is returned as a string, where rows are separated by newlines and cells with commas.
        parameters : can be used to pass filter values:
            The format is like [{""type"":""category"",""value"":[""val1"",""val2""],""target"":[""dimension"",[""template-tag"",""filter_variable_name""]]}]
            See the network tab when exporting the results using the web interface to get the proper format pattern.
        format_rows : whether the returned results should be formatted or not
        """"""
    assert data_format in ['json', 'csv']
<mask>:
        assert type(parameters) == list
    if card_id is None:
        if card_name is None:
            raise ValueError('Either card_id or card_name must be provided.')
        card_id = self.get_item_id(item_name=card_name, collection_name=collection_name, collection_id=collection_id, item_type='card')
    import json
    params_json = {'parameters': json.dumps(parameters), 'format_rows': 'true' if format_rows else 'false'}
    res = self.post('/api/card/{}/query/{}'.format(card_id, data_format), 'raw', data=params_json)
    if data_format == 'json':
        return json.loads(res.text)
    if data_format == 'csv':
        return res.text.replace('null', '')",parameters,171,parameters is not None,False,15.97357760615681,N/A
"def get_item_info(self, item_type, item_id=None, item_name=None, collection_id=None, collection_name=None, params=None):
    """"""
    Return the info for the given item.
    Use 'params' for providing arguments. E.g. to include tables in the result for databases, use: params={'include':'tables'}
    """"""
    assert item_type in ['database', 'table', 'card', 'collection', 'dashboard', 'pulse', 'segment']
<mask>:
        assert type(params) == dict
    if not item_id:
        if not item_name:
            raise ValueError('Either the name or id of the {} must be provided.'.format(item_type))
        item_id = self.get_item_id(item_type, item_name, collection_id=collection_id, collection_name=collection_name)
    res = self.get('/api/{}/{}'.format(item_type, item_id), params=params)
    if res:
        return res
    else:
        raise ValueError('There is no {} with the id ""{}""'.format(item_type, item_id))",params,92,params,True,100.00000000000004,N/A
"def get_item_name(self, item_type, item_id):
    assert item_type in ['database', 'table', 'card', 'collection', 'dashboard', 'pulse', 'segment']
    res = self.get('/api/{}/{}'.format(item_type, item_id))
<mask>:
        return res['name']
    else:
        raise ValueError('There is no {} with the id ""{}""'.format(item_type, item_id))",res,32,res,True,100.00000000000004,N/A
"def get_collection_id(self, collection_name):
    import warnings
    warnings.warn('The function get_collection_id will be removed in the next version. Use get_item_id function instead.', DeprecationWarning)
    collection_IDs = [i['id'] for i in self.get('/api/collection/') if i['name'] == collection_name]
<mask>:
        raise ValueError('There is more than one collection with the name ""{}""'.format(collection_name))
    if len(collection_IDs) == 0:
        raise ValueError('There is no collection with the name ""{}""'.format(collection_name))
    return collection_IDs[0]",len(collection_IDs) > 1,58,len(collection_IDs) > 1,True,100.00000000000004,N/A
"def get_db_id(self, db_name):
    import warnings
    warnings.warn('The function get_db_id will be removed in the next version. Use get_item_id function instead.', DeprecationWarning)
    res = self.get('/api/database/')
<mask>:
        res = res['data']
    db_IDs = [i['id'] for i in res if i['name'] == db_name]
    if len(db_IDs) > 1:
        raise ValueError('There is more than one DB with the name ""{}""'.format(db_name))
    if len(db_IDs) == 0:
        raise ValueError('There is no DB with the name ""{}""'.format(db_name))
    return db_IDs[0]",type(res) == dict,68,'data' in res,False,7.253154775624655,N/A
"def copy_card(self, source_card_name=None, source_card_id=None, source_collection_name=None, source_collection_id=None, destination_card_name=None, destination_collection_name=None, destination_collection_id=None, postfix='', verbose=False, return_card=False):
    """"""
    Copy the card with the given name/id to the given destination collection. 

    Parameters
    ----------
    source_card_name : name of the card to copy (default None) 
    source_card_id : id of the card to copy (default None) 
    source_collection_name : name of the collection the source card is located in (default None) 
    source_collection_id : id of the collection the source card is located in (default None) 
    destination_card_name : name used for the card in destination (default None).
                                                        If None, it will use the name of the source card + postfix.
    destination_collection_name : name of the collection to copy the card to (default None) 
    destination_collection_id : id of the collection to copy the card to (default None) 
    postfix : if destination_card_name is None, adds this string to the end of source_card_name 
                            to make destination_card_name
    """"""
<mask>:
        if not source_card_name:
            raise ValueError('Either the name or id of the source card must be provided.')
        else:
            source_card_id = self.get_item_id(item_type='card', item_name=source_card_name, collection_id=source_collection_id, collection_name=source_collection_name)
    if not destination_collection_id:
        if not destination_collection_name:
            raise ValueError('Either the name or id of the destination collection must be provided.')
        else:
            destination_collection_id = self.get_item_id('collection', destination_collection_name)
    if not destination_card_name:
        if not source_card_name:
            source_card_name = self.get_item_name(item_type='card', item_id=source_card_id)
        destination_card_name = source_card_name + postfix
    source_card = self.get('/api/card/{}'.format(source_card_id))
    card_json = source_card
    card_json['collection_id'] = destination_collection_id
    card_json['name'] = destination_card_name
    if card_json.get('description') == '':
        card_json['description'] = None
    res = self.create_card(custom_json=card_json, verbose=verbose, return_card=True)
    return res if return_card else res['id']",not source_card_id,236,not source_card_id,True,100.00000000000004,N/A
"def copy_pulse(self, source_pulse_name=None, source_pulse_id=None, source_collection_name=None, source_collection_id=None, destination_pulse_name=None, destination_collection_id=None, destination_collection_name=None, postfix=''):
    """"""
    Copy the pulse with the given name/id to the given destination collection. 

    Parameters
    ----------
    source_pulse_name : name of the pulse to copy (default None) 
    source_pulse_id : id of the pulse to copy (default None) 
    source_collection_name : name of the collection the source card is located in (default None) 
    source_collection_id : id of the collection the source card is located in (default None) 
    destination_pulse_name : name used for the pulse in destination (default None).
                                                        If None, it will use the name of the source pulse + postfix.
    destination_collection_name : name of the collection to copy the pulse to (default None) 
    destination_collection_id : id of the collection to copy the pulse to (default None) 
    postfix : if destination_pulse_name is None, adds this string to the end of source_pulse_name 
                            to make destination_pulse_name
    """"""
<mask>:
        if not source_pulse_name:
            raise ValueError('Either the name or id of the source pulse must be provided.')
        else:
            source_pulse_id = self.get_item_id(item_type='pulse', item_name=source_pulse_name, collection_id=source_collection_id, collection_name=source_collection_name)
    if not destination_collection_id:
        if not destination_collection_name:
            raise ValueError('Either the name or id of the destination collection must be provided.')
        else:
            destination_collection_id = self.get_item_id('collection', destination_collection_name)
    if not destination_pulse_name:
        if not source_pulse_name:
            source_pulse_name = self.get_item_name(item_type='pulse', item_id=source_pulse_id)
        destination_pulse_name = source_pulse_name + postfix
    source_pulse = self.get('/api/pulse/{}'.format(source_pulse_id))
    pulse_json = source_pulse
    pulse_json['collection_id'] = destination_collection_id
    pulse_json['name'] = destination_pulse_name
    self.post('/api/pulse', json=pulse_json)",not source_pulse_id,218,not source_pulse_id,True,100.00000000000004,N/A
"def copy_dashboard(self, source_dashboard_name=None, source_dashboard_id=None, source_collection_name=None, source_collection_id=None, destination_dashboard_name=None, destination_collection_name=None, destination_collection_id=None, deepcopy=False, postfix='', collection_position=1, description=''):
    """"""
    Copy the dashboard with the given name/id to the given destination collection. 

    Parameters
    ----------
    source_dashboard_name : name of the dashboard to copy (default None) 
    source_dashboard_id : id of the dashboard to copy (default None) 
    source_collection_name : name of the collection the source dashboard is located in (default None) 
    source_collection_id : id of the collection the source dashboard is located in (default None) 
    destination_dashboard_name : name used for the dashboard in destination (default None). 
        If None, it will use the name of the source dashboard + postfix. 
    destination_collection_name : name of the collection to copy the dashboard to (default None) 
    destination_collection_id : id of the collection to copy the dashboard to (default None) 
    deepcopy : whether to duplicate the cards inside the dashboard (default False). 
    postfix : if destination_dashboard_name is None, adds this string to the end of source_dashboard_name 
        to make destination_dashboard_name 
    """"""
<mask>:
        if not source_dashboard_name:
            raise ValueError('Either the name or id of the source dashboard must be provided.')
        else:
            source_dashboard_id = self.get_item_id(item_type='dashboard', item_name=source_dashboard_name, collection_id=source_collection_id, collection_name=source_collection_name)
    if not destination_collection_id:
        if not destination_collection_name:
            raise ValueError('Either the name or id of the destination collection must be provided.')
        else:
            destination_collection_id = self.get_item_id('collection', destination_collection_name)
    if not destination_dashboard_name:
        if not source_dashboard_name:
            source_dashboard_name = self.get_item_name(item_type='dashboard', item_id=source_dashboard_id)
        destination_dashboard_name = source_dashboard_name + postfix
    parameters = {'collection_id': destination_collection_id, 'name': destination_dashboard_name, 'is_deep_copy': deepcopy, 'collection_position': collection_position, 'description': description}
    res = self.post('/api/dashboard/{}/copy'.format(source_dashboard_id), 'raw', json=parameters)
    if res.status_code != 200:
        raise ValueError('Error copying the dashboard: {}'.format(res.text))
    dup_dashboard_id = res.json()['id']
    return dup_dashboard_id",not source_dashboard_id,251,not source_dashboard_name,False,75.98356856515926,N/A
"def tearDown(self):
    for item_type, item_id_list in Metabase_API_Test.cleanup_objects.items():
<mask>:
            for item_id in item_id_list:
                mb.delete_item(item_type=item_type, item_id=item_id)
        if item_type in ['collection', 'segment']:
            for item_id in item_id_list:
                mb.move_to_archive(item_type=item_type, item_id=item_id)","item_type in ['card', 'dashboard', 'pulse']",25,"item_type in ['collection', 'segment']",False,38.875142041440206,N/A
"def __getattr__(self, item):
    """"""
        Finds functions in window.WAPI

        :param item: Function name
        :return: Callable function object
        :rtype: JsFunction
        """"""
    wapi_functions = dir(self)
<mask>:
        raise AttributeError(""Function {0} doesn't exist"".format(item))
    return JsFunction(item, self.driver, self)",item not in wapi_functions,32,item not in wapi_functions,True,100.00000000000004,N/A
"def __dir__(self):
    """"""
        Load wapi.js and returns its functions

        :return: List of functions in window.WAPI
        """"""
<mask>:
        return self.available_functions
    'Sleep wait until WhatsApp loads and creates webpack objects'
    time.sleep(5)
    try:
        script_path = os.path.dirname(os.path.abspath(__file__))
    except NameError:
        script_path = os.getcwd()
    result = self.driver.execute_script(""if (document.querySelector('*[data-icon=chat]') !== null) { return true } else { return false }"")
    if result:
        with open(os.path.join(script_path, 'js', 'wapi.js'), 'r') as script:
            self.driver.execute_script(script.read())
        result = self.driver.execute_script('return Object.keys(window.WAPI)')
        if result:
            self.available_functions = result
            return self.available_functions
        else:
            return []",self.available_functions is not None,77,self.available_functions,False,54.88116360940266,N/A
"def __str__(self):
    """"""
        Casts self.obj from python type to valid JS literal

        :return: JS literal represented in a string
        """"""
<mask>:
        return repr(str(self.obj))
    if isinstance(self.obj, bool):
        return str(self.obj).lower()
    return str(self.obj)","isinstance(self.obj, string_types)",30,"isinstance(self.obj, str)",False,55.0695314903184,N/A
"def __call__(self, *args, **kwargs):
<mask>:
        command = 'return WAPI.{0}({1}, arguments[0])'.format(self.function_name, ','.join([str(JsArg(arg)) for arg in args]))
    else:
        command = 'return WAPI.{0}(arguments[0])'.format(self.function_name)
    try:
        return self.driver.execute_async_script(command)
    except JavascriptException as e:
        if 'WAPI is not defined' in e.msg and self.is_a_retry is not True:
            self.wapi_wrapper.available_functions = None
            retry_command = getattr(self.wapi_wrapper, self.function_name)
            retry_command.is_a_retry = True
            retry_command(*args, **kwargs)
        else:
            raise JsException('Error in function {0} ({1}). Command: {2}'.format(self.function_name, e.msg, command))
    except WebDriverException as e:
        if e.msg == 'Timed out':
            raise WapiPhoneNotConnectedException('Phone not connected to Internet')
        raise JsException('Error in function {0} ({1}). Command: {2}'.format(self.function_name, e.msg, command))",len(args),87,"isinstance(args[0], list)",False,10.552670315936318,N/A
"def run(self):
    self.running = True
    while self.running:
        try:
            new_js_messages = self.wapi_js_wrapper.getBufferedNewMessages()
<mask>:
                new_messages = []
                for js_message in new_js_messages:
                    new_messages.append(factory_message(js_message, self.wapi_driver))
                self._inform_all(new_messages)
        except Exception as e:
            pass
        time.sleep(2)","isinstance(new_js_messages, (collections.Sequence, np.ndarray)) and len(new_js_messages) > 0",28,new_js_messages,False,0.8229747049020034,N/A
"def save_firefox_profile(self, remove_old=False):
    """"""Function to save the firefox profile to the permanant one""""""
    self.logger.info('Saving profile from %s to %s' % (self._profile.path, self._profile_path))
<mask>:
        if os.path.exists(self._profile_path):
            try:
                shutil.rmtree(self._profile_path)
            except OSError:
                pass
        shutil.copytree(os.path.join(self._profile.path), self._profile_path, ignore=shutil.ignore_patterns('parent.lock', 'lock', '.parentlock'))
    else:
        for item in os.listdir(self._profile.path):
            if item in ['parent.lock', 'lock', '.parentlock']:
                continue
            s = os.path.join(self._profile.path, item)
            d = os.path.join(self._profile_path, item)
            if os.path.isdir(s):
                shutil.copytree(s, d, ignore=shutil.ignore_patterns('parent.lock', 'lock', '.parentlock'))
            else:
                shutil.copy2(s, d)
    with open(os.path.join(self._profile_path, self._LOCAL_STORAGE_FILE), 'w') as f:
        f.write(dumps(self.get_local_storage()))",remove_old,72,remove_old,True,100.00000000000004,N/A
"def __init__(self, client='firefox', username='API', proxy=None, command_executor=None, loadstyles=False, profile=None, headless=False, autoconnect=True, logger=None, extra_params=None, chrome_options=None, executable_path=None):
    """"""Initialises the webdriver""""""
    self.logger = logger or self.logger
    extra_params = extra_params or {}
<mask>:
        self._profile_path = profile
        self.logger.info('Checking for profile at %s' % self._profile_path)
        if not os.path.exists(self._profile_path):
            self.logger.critical('Could not find profile at %s' % profile)
            raise WhatsAPIException('Could not find profile at %s' % profile)
    else:
        self._profile_path = None
    self.client = client.lower()
    if self.client == 'firefox':
        if self._profile_path is not None:
            self._profile = webdriver.FirefoxProfile(self._profile_path)
        else:
            self._profile = webdriver.FirefoxProfile()
        if not loadstyles:
            self._profile.set_preference('permissions.default.stylesheet', 2)
            self._profile.set_preference('permissions.default.image', 2)
            self._profile.set_preference('dom.ipc.plugins.enabled.libflashplayer.so', 'false')
        if proxy is not None:
            self.set_proxy(proxy)
        options = Options()
        if headless:
            options.set_headless()
        options.profile = self._profile
        capabilities = DesiredCapabilities.FIREFOX.copy()
        capabilities['webStorageEnabled'] = True
        self.logger.info('Starting webdriver')
        if executable_path is not None:
            executable_path = os.path.abspath(executable_path)
            self.logger.info('Starting webdriver')
            self.driver = webdriver.Firefox(capabilities=capabilities, options=options, executable_path=executable_path, **extra_params)
        else:
            self.logger.info('Starting webdriver')
            self.driver = webdriver.Firefox(capabilities=capabilities, options=options, **extra_params)
    elif self.client == 'chrome':
        self._profile = webdriver.ChromeOptions()
        if self._profile_path is not None:
            self._profile.add_argument('user-data-dir=%s' % self._profile_path)
        if proxy is not None:
            self._profile.add_argument('--proxy-server=%s' % proxy)
        if headless:
            self._profile.add_argument('headless')
        if chrome_options is not None:
            for option in chrome_options:
                self._profile.add_argument(option)
        self.logger.info('Starting webdriver')
        if executable_path is not None:
            executable_path = os.path.abspath(executable_path)
            self.driver = webdriver.Chrome(chrome_options=self._profile, executable_path=executable_path, **extra_params)
        else:
            self.driver = webdriver.Chrome(chrome_options=self._profile, **extra_params)
    elif client == 'remote':
        if self._profile_path is not None:
            self._profile = webdriver.FirefoxProfile(self._profile_path)
        else:
            self._profile = webdriver.FirefoxProfile()
        capabilities = DesiredCapabilities.FIREFOX.copy()
        self.driver = webdriver.Remote(command_executor=command_executor, desired_capabilities=capabilities, **extra_params)
    else:
        self.logger.error('Invalid client: %s' % client)
    self.username = username
    self.wapi_functions = WapiJsWrapper(self.driver, self)
    self.driver.set_script_timeout(500)
    self.driver.implicitly_wait(10)
    if autoconnect:
        self.connect()",profile is not None,235,profile is not None,True,100.00000000000004,N/A
"def connect(self):
    self.driver.get(self._URL)
    profilePath = ''
<mask>:
        profilePath = ''
    else:
        profilePath = self._profile.path
    local_storage_file = os.path.join(profilePath, self._LOCAL_STORAGE_FILE)
    if os.path.exists(local_storage_file):
        with open(local_storage_file) as f:
            self.set_local_storage(loads(f.read()))
        self.driver.refresh()",self.client == 'chrome',26,self._profile is None,False,16.233395773754953,N/A
"def get_qr(self, filename=None):
    """"""Get pairing QR code from client""""""
<mask>:
        self.reload_qr()
    qr = self.driver.find_element_by_css_selector(self._SELECTORS['qrCode'])
    if filename is None:
        fd, fn_png = tempfile.mkstemp(prefix=self.username, suffix='.png')
    else:
        fd = os.open(filename, os.O_RDWR | os.O_CREAT)
        fn_png = os.path.abspath(filename)
    self.logger.debug('QRcode image saved at %s' % fn_png)
    qr.screenshot(fn_png)
    os.close(fd)
    return fn_png",'Click to reload QR code' in self.driver.page_source,44,self.driver is None,False,8.028119550056974,N/A
"def safe_str(text):
<mask>:
        return '(empty)'
    assert isinstance(text, six.string_types), 'obj is not a string: %r' % text
    return str(text.encode('utf-8').decode('ascii', 'ignore')) if text else '(empty)'",not text,23,text is None,False,27.516060407455225,N/A
"def driver_needed(func):
    """"""
    Decorator for WhatsappObjectWithId methods that need to communicate with the browser

    It ensures that the object receives a driver instance at construction

    :param func: WhatsappObjectWithId method
    :return: Wrapped method
    """"""

    def wrapped(self, *args):
<mask>:
            raise AttributeError('No driver passed to object')
        return func(self, *args)
    return wrapped",not self.driver,48,not self.driver,True,100.00000000000004,N/A
"def __init__(self, js_obj, driver=None):
    """"""
        Constructor

        :param js_obj: Whatsapp JS object to wrap
        :type js_obj: dict
        :param driver: Optional driver instance
        :type driver: WhatsAPIDriver
        """"""
    super(WhatsappObjectWithId, self).__init__(js_obj, driver)
<mask>:
        try:
            self.id = js_obj['id']['_serialized']
        except Exception:
            self.id = js_obj['id']
    if 'name' in js_obj:
        self.name = js_obj['name']",'id' in js_obj,45,'id' in js_obj,True,100.00000000000004,N/A
"def __init__(self, js_obj, driver=None):
    super(NumberStatus, self).__init__(js_obj, driver)
<mask>:
        self.status = js_obj['status']
    if 'isBusiness' in js_obj:
        self.is_business = js_obj['isBusiness']
    if 'canReceiveMessage' in js_obj:
        self.can_receive_message = js_obj['canReceiveMessage']",'status' in js_obj,25,'status' in js_obj,True,100.00000000000004,N/A
"def factory_message(js_obj, driver):
    """"""Factory function for creating appropriate object given selenium JS object""""""
<mask>:
        return
    if 'lat' in js_obj and 'lng' in js_obj and js_obj['lat'] and js_obj['lng']:
        return GeoMessage(js_obj, driver)
    if js_obj['isMedia']:
        return MediaMessage(js_obj, driver)
    if js_obj['isNotification']:
        return NotificationMessage(js_obj, driver)
    if 'isMMS' in js_obj and js_obj['isMMS']:
        return MMSMessage(js_obj, driver)
    if js_obj['type'] in ['vcard', 'multi_vcard']:
        return VCardMessage(js_obj, driver)
    return Message(js_obj, driver)",js_obj is None,60,"js_obj['type'] in ['geo', 'geo_geo']",False,10.571070857151538,N/A
"def __init__(self, js_obj, driver=None):
    """"""
        Constructor

        :param js_obj: Raw JS message obj
        :type js_obj: dict
        """"""
    super(Message, self).__init__(js_obj, driver)
    self.id = js_obj['id']
    self.type = js_obj['type']
    self.sender = Contact(js_obj['sender'], driver) if js_obj['sender'] else False
    self.timestamp = datetime.fromtimestamp(js_obj['timestamp'])
    self.chat_id = js_obj['chatId']
<mask>:
        self.content = js_obj['content']
        self.safe_content = safe_str(self.content[0:25]) + '...'
    elif self.type == 'revoked':
        self.content = ''
        self.safe_content = '...'",js_obj['content'],58,self.type == 'sent',False,0.0,N/A
"def __init__(self, js_obj, driver=None):
    super(MediaMessage, self).__init__(js_obj, driver)
    self.size = self._js_obj['size']
    self.mime = self._js_obj['mimetype']
<mask>:
        self.caption = self._js_obj['caption'] or ''
    self.media_key = self._js_obj.get('mediaKey')
    self.client_url = self._js_obj.get('clientUrl')
    extension = mimetypes.guess_extension(self.mime)
    self.filename = ''.join([str(id(self)), extension or ''])",'caption' in self._js_obj,34,self.mime == 'caption',False,12.872632311973014,N/A
"def __init__(self, js_obj, driver=None):
    super(VCardMessage, self).__init__(js_obj, driver)
    self.type = js_obj['type']
    self.contacts = list()
<mask>:
        self.contacts.append(js_obj['content'].encode('ascii', 'ignore'))
    else:
        for card in js_obj['vcardList']:
            self.contacts.append(card['vcard'].encode('ascii', 'ignore'))",js_obj['content'],23,'content' in js_obj,False,34.98330125272253,N/A
"def __init__(self, js_obj, driver=None):
    super(NotificationMessage, self).__init__(js_obj, driver)
    self.type = js_obj['type']
    self.subtype = js_obj['subtype']
<mask>:
        self.recipients = [getContacts(x, driver) for x in js_obj['recipients']]",js_obj['recipients'],22,'recipients' in js_obj,False,34.98330125272253,N/A
"def factory_chat(js_obj, driver=None):
    """"""Factory function for creating appropriate object given selenium JS object""""""
<mask>:
        raise AssertionError('Expected chat, group or broadcast object, got {0}'.format(js_obj['kind']))
    if js_obj['isGroup']:
        return GroupChat(js_obj, driver)
    if js_obj['kind'] == 'broadcast':
        return BroadcastChat(js_obj, driver)
    return UserChat(js_obj, driver)","js_obj['kind'] not in ['chat', 'group', 'broadcast']",38,"js_obj['kind'] not in ('chat', 'group', 'broadcast')",False,72.41577342575832,N/A
"def __init__(self, js_obj, driver=None):
    """"""

        :param js_obj:
        :param driver:
        :type driver: WhatsAPIDriver
        """"""
    self.short_name = None
    self.push_name = None
    self.formatted_name = None
    self.profile_pic = None
    self.verified_name = None
    self.is_business = False
    super(Contact, self).__init__(js_obj, driver)
<mask>:
        self.short_name = js_obj['shortName']
    if 'pushname' in js_obj:
        self.push_name = js_obj['pushname']
    if 'formattedName' in js_obj:
        self.formatted_name = js_obj['formattedName']
    if 'profilePicThumbObj' in js_obj:
        self.profile_pic = js_obj['profilePicThumbObj'].get('eurl', None)
    if 'verifiedName' in js_obj:
        self.verified_name = js_obj['verifiedName']
        self.is_business = js_obj['isBusiness']",'shortName' in js_obj,70,'shortName' in js_obj,True,100.00000000000004,N/A
"def get_safe_name(self):
    """"""

        :return: String used for representation of the Contact

        :rtype: String

        """"""
    name = self.short_name or self.push_name or self.formatted_name
<mask>:
        if self.is_business:
            safe_name = self.verified_name
        else:
            safe_name = safe_str(name)
    else:
        safe_name = 'Unknown'
    return safe_name","isinstance(name, string_types)",37,name,False,0.09118819655545167,N/A
"def process_command(command):
    print_and_log('Processing command: {cmd}'.format(cmd=command))
<mask>:
        send_message_to_master('I am still alive')
    elif command.lower() == 'quit':
        quit()
    elif command.lower() == 'ping':
        send_message_to_master('The counter is now {ping}'.format(ping=pinger))
    elif command.lower() == 'uptime':
        uptime = datetime.datetime.now() - start_time
        send_message_to_master('Up since {start}, hence for a total time of {upt} by now'.format(start=start_time, upt=uptime))
    else:
        send_message_to_master(""I am sorry but I can't understand '{cmd}'"".format(cmd=command))",command.lower() == 'status',55,command.lower() == 'still alive',False,72.59795291154772,N/A
"def on_message_received(self, new_messages):
    for message in new_messages:
<mask>:
            print(""New message '{}' received from number {}"".format(message.content, message.sender.id))
        else:
            print(""New message of type '{}' received from number {}"".format(message.type, message.sender.id))",message.type == 'chat',27,message.type == 'message',False,75.98356856515926,N/A
"def start(self):
    """"""Creates a timer and start it""""""
<mask>:
        self._timer = threading.Timer(self.interval, self._run)
        self._timer.start()
        self.is_running = True",not self.is_running,17,not self.is_running,True,100.00000000000004,N/A
"def default(self, obj):
<mask>:
        return obj.get_js_obj()
    if isinstance(obj, MessageGroup):
        return obj.chat
    return super(WhatsAPIJSONEncoder, self).default(obj)","isinstance(obj, WhatsappObject)",14,"isinstance(obj, JSObject)",False,53.7284965911771,N/A
"def login_required(f):

    @wraps(f)
    def decorated_function(*args, **kwargs):
<mask>:
            return jsonify({'error': 'client is not logged in'})
        return f(*args, **kwargs)
    return decorated_function",g.driver_status != WhatsAPIDriverStatus.LoggedIn,19,not client.logged_in,False,4.955725306405571,N/A
"def init_driver(client_id):
    """"""Initialises a new driver via webwhatsapi module
    
    @param client_id: ID of user client
    @return webwhatsapi object
    """"""
    profile_path = CHROME_CACHE_PATH + str(client_id)
<mask>:
        os.makedirs(profile_path)
    chrome_options = ['window-size=' + CHROME_WINDOW_SIZE, '--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/60.0.3112.78 Chrome/60.0.3112.78 Safari/537.36']
    if CHROME_IS_HEADLESS:
        chrome_options.append('--headless')
    if CHROME_DISABLE_GPU:
        chrome_options.append('--disable-gpu')
    d = WhatsAPIDriver(username=client_id, profile=profile_path, client='chrome', chrome_options=chrome_options)
    return d",not os.path.exists(profile_path),57,not os.path.exists(profile_path),True,100.00000000000004,N/A
"def init_client(client_id):
    """"""Initialse a driver for client and store for future reference
    
    @param client_id: ID of client user
    @return whebwhatsapi object
    """"""
<mask>:
        drivers[client_id] = init_driver(client_id)
    return drivers[client_id]",client_id not in drivers,28,client_id not in drivers,True,100.00000000000004,N/A
"def __init__(self, wkhtmltopdf='', meta_tag_prefix='pdfkit-', environ=''):
    self.meta_tag_prefix = meta_tag_prefix
    self.wkhtmltopdf = wkhtmltopdf
    try:
<mask>:
            if sys.platform == 'win32':
                startupinfo = subprocess.STARTUPINFO()
                startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
                startupinfo.wShowWindow = subprocess.SW_HIDE
                self.wkhtmltopdf = subprocess.Popen(['where.exe', 'wkhtmltopdf'], stdout=subprocess.PIPE, startupinfo=startupinfo).communicate()[0]
            else:
                self.wkhtmltopdf = subprocess.Popen(['which', 'wkhtmltopdf'], stdout=subprocess.PIPE).communicate()[0]
        lines = self.wkhtmltopdf.splitlines()
        if len(lines) > 0:
            self.wkhtmltopdf = lines[0].strip()
        with open(self.wkhtmltopdf) as f:
            pass
    except (IOError, FileNotFoundError) as e:
        raise IOError('No wkhtmltopdf executable found: ""%s""\nIf this file exists please check that this process can read it or you can pass path to it manually in method call, check README. Otherwise please install wkhtmltopdf - https://github.com/JazzCore/python-pdfkit/wiki/Installing-wkhtmltopdf' % self.wkhtmltopdf)
    self.environ = environ
    if not self.environ:
        self.environ = os.environ
    for key in self.environ.keys():
        if not isinstance(self.environ[key], str):
            self.environ[key] = str(self.environ[key])",not self.wkhtmltopdf,116,not self.wkhtmltopdf,True,100.00000000000004,N/A
"def __init__(self, url_or_file, type_, options=None, toc=None, cover=None, css=None, configuration=None, cover_first=False, verbose=False):
    self.source = Source(url_or_file, type_)
    self.configuration = Configuration() if configuration is None else configuration
    try:
        self.wkhtmltopdf = self.configuration.wkhtmltopdf.decode('utf-8')
    except AttributeError:
        self.wkhtmltopdf = self.configuration.wkhtmltopdf
    self.options = OrderedDict()
<mask>:
        self.options.update(self._find_options_in_meta(url_or_file))
    self.environ = self.configuration.environ
    if options is not None:
        self.options.update(options)
    self.toc = {} if toc is None else toc
    self.cover = cover
    self.cover_first = cover_first
    self.verbose = verbose
    self.css = css
    self.stylesheets = []",self.source.isString(),71,type_ == 'meta',False,0.0,N/A
"def _genargs(self, opts):
    """"""
        Generator of args parts based on options specification.

        Note: Empty parts will be filtered out at _command generator
        """"""
    for optkey, optval in self._normalize_options(opts):
        yield optkey
<mask>:
            assert len(optval) == 2 and optval[0] and optval[1], 'Option value can only be either a string or a (tuple, list) of 2 items'
            yield optval[0]
            yield optval[1]
        else:
            yield optval","isinstance(optval, (list, tuple))",61,"isinstance(optval, (tuple, list))",False,49.39382737115372,N/A
"def _command(self, path=None):
    """"""
        Generator of all command parts
        """"""
<mask>:
        self._prepend_css(self.css)
    yield self.wkhtmltopdf
    if not self.verbose:
        self.options.update({'--quiet': ''})
    for argpart in self._genargs(self.options):
        if argpart:
            yield argpart
    if self.cover and self.cover_first:
        yield 'cover'
        yield self.cover
    if self.toc:
        yield 'toc'
        for argpart in self._genargs(self.toc):
            if argpart:
                yield argpart
    if self.cover and (not self.cover_first):
        yield 'cover'
        yield self.cover
    if self.source.isString() or self.source.isFileObj():
        yield '-'
    elif isinstance(self.source.source, str):
        yield self.source.to_s()
    else:
        for s in self.source.source:
            yield s
    if path:
        yield path
    else:
        yield '-'",self.css,81,self.css,True,100.00000000000004,N/A
"@staticmethod
def handle_error(exit_code, stderr):
<mask>:
        return
    stderr_lines = stderr.splitlines()
    if len(stderr_lines) > 1 and stderr.splitlines()[-2].strip() == 'Done':
        return
    if 'cannot connect to X server' in stderr:
        raise IOError('%s\nYou will need to run wkhtmltopdf within a ""virtual"" X server.\nGo to the link below for more information\nhttps://github.com/JazzCore/python-pdfkit/wiki/Using-wkhtmltopdf-without-X-server' % stderr)
    if 'Error' in stderr:
        raise IOError('wkhtmltopdf reported an error:\n' + stderr)
    error_msg = stderr or 'Unknown Error'
    raise IOError('wkhtmltopdf exited with non-zero code {0}. error:\n{1}'.format(exit_code, error_msg))",exit_code == 0,73,exit_code == 0,True,100.00000000000004,N/A
"def to_pdf(self, path=None):
    args = self.command(path)
<mask>:
        startupinfo = subprocess.STARTUPINFO()
        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
        startupinfo.wShowWindow = subprocess.SW_HIDE
        result = subprocess.Popen(args, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=self.environ, startupinfo=startupinfo)
    else:
        result = subprocess.Popen(args, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=self.environ)
    if self.source.isString() or (self.source.isFile() and self.css):
        input = self.source.to_s().encode('utf-8')
    elif self.source.isFileObj():
        input = self.source.source.read().encode('utf-8')
    else:
        input = None
    stdout, stderr = result.communicate(input=input)
    stderr = stderr or stdout or b''
    stderr = stderr.decode('utf-8', errors='replace')
    exit_code = result.returncode
    self.handle_error(exit_code, stderr)
    if '--quiet' not in args:
        sys.stdout.write(stderr)
    if not path:
        return stdout
    try:
        with codecs.open(path, encoding='utf-8') as f:
            text = f.read(4)
            if text == '':
                raise IOError(""Command failed: %s\nCheck whhtmltopdf output without 'quiet' option"" % ' '.join(args))
            return True
    except (IOError, OSError) as e:
        raise IOError(""Command failed: %s\nCheck whhtmltopdf output without 'quiet' option\n%s "" % (' '.join(args), e))",sys.platform == 'win32',127,self.isString() and self.css,False,4.767707020457095,N/A
"def isFile(self, path=None):
<mask>:
        return isinstance(path, io.IOBase) or path.__class__.__name__ == 'StreamReaderWriter'
    else:
        return 'file' in self.type",path,16,path,True,100.00000000000004,N/A
"def checkFiles(self):
<mask>:
        for path in self.source:
            if not os.path.exists(path):
                raise IOError('No such file: %s' % path)
    elif not hasattr(self.source, 'read') and (not os.path.exists(self.source)):
        raise IOError('No such file: %s' % self.source)","isinstance(self.source, list)",31,"isinstance(self.source, list)",True,100.00000000000004,N/A
"def _find_vc2015():
    import _winreg
    try:
        key = _winreg.OpenKeyEx(_winreg.HKEY_LOCAL_MACHINE, 'Software\\Microsoft\\VisualStudio\\SxS\\VC7', 0, _winreg.KEY_READ | _winreg.KEY_WOW64_32KEY)
    except OSError:
        log.debug('Visual C++ is not registered')
        return (None, None)
    best_version = 0
    best_dir = None
    with key:
        for i in count():
            try:
                v, vc_dir, vt = _winreg.EnumValue(key, i)
            except OSError:
                break
<mask>:
                try:
                    version = int(float(v))
                except (ValueError, TypeError):
                    continue
                if version >= 14 and version > best_version:
                    best_version, best_dir = (version, vc_dir)
    return (best_version, best_dir)",v and vt == _winreg.REG_SZ and os.path.isdir(vc_dir),70,vt == _winreg.KEY_READ,False,14.399672813486239,N/A
"def _find_vc2017():
    import threading
    best_version = None
    best_dir = None
    vcvarsall_path = ['C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\BuildTools', 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community']
    for path in vcvarsall_path:
        vc_dir = os.path.join(path, 'VC', 'Auxiliary', 'Build')
<mask>:
            version = 0
            best_version, best_dir = (version, vc_dir)
            break
    return (best_version, best_dir)",os.path.isdir(vc_dir),45,not os.path.exists(vc_dir),False,58.77283725105324,N/A
"def _find_vcvarsall(plat_spec):
    best_version, best_dir = _find_vc2017()
    vcruntime = None
    vcruntime_plat = 'x64' if 'amd64' in plat_spec else 'x86'
<mask>:
        vcredist = os.path.join(best_dir, '..', '..', 'Redist', 'MSVC', '**', 'Microsoft.VC141.CRT', 'vcruntime140.dll')
        try:
            import glob
            vcruntime = glob.glob(vcredist, recursive=True)[-1]
        except (ImportError, OSError, LookupError):
            vcruntime = None
    if not best_version:
        best_version, best_dir = _find_vc2015()
        if best_version:
            vcruntime = os.path.join(best_dir, 'redist', vcruntime_plat, 'Microsoft.VC140.CRT', 'vcruntime140.dll')
    if not best_version:
        log.debug('No suitable Visual C++ version found')
        return (None, None)
    vcvarsall = os.path.join(best_dir, 'vcvarsall.bat')
    if not os.path.isfile(vcvarsall):
        log.debug('%s cannot be found', vcvarsall)
        return (None, None)
    if not vcruntime or not os.path.isfile(vcruntime):
        log.debug('%s cannot be found', vcruntime)
        vcruntime = None
    return (vcvarsall, vcruntime)",best_version,103,not best_version or not vcruntime_plat,False,17.747405280050266,N/A
"def _get_vc_env(plat_spec):
<mask>:
        return {key.lower(): value for key, value in os.environ.items()}
    vcvarsall, vcruntime = _find_vcvarsall(plat_spec)
    if not vcvarsall:
        raise DistutilsPlatformError('Unable to find vcvarsall.bat')
    try:
        out = subprocess.check_output('cmd /u /c ""{}"" {} && set'.format(vcvarsall, plat_spec), stderr=subprocess.STDOUT).decode('utf-16le', errors='replace')
    except subprocess.CalledProcessError as exc:
        log.error(exc.output)
        raise DistutilsPlatformError('Error executing {}'.format(exc.cmd))
    env = {key.lower(): value for key, _, value in (line.partition('=') for line in out.splitlines()) if key and value}
    if vcruntime:
        env['py_vcruntime_redist'] = vcruntime
    return env",os.getenv('DISTUTILS_USE_SDK'),70,plat_spec == 'env',False,4.167251645138561,N/A
"def _find_exe(exe, paths=None):
    """"""Return path to an MSVC executable program.

    Tries to find the program in several places: first, one of the
    MSVC program search paths from the registry; next, the directories
    in the PATH environment variable.  If any of those work, return an
    absolute path that is known to exist.  If none of them work, just
    return the original program name, 'exe'.
    """"""
<mask>:
        paths = os.getenv('path').split(os.pathsep)
    for p in paths:
        fn = os.path.join(os.path.abspath(p), exe)
        if os.path.isfile(fn):
            return fn
    return exe",not paths,82,paths is None,False,27.516060407455225,N/A
"def test_HarrisKeyPoint3D(self):
    base_point_count = 397
    self.assertEqual(self.p.size, base_point_count)
    self.kp.set_NonMaxSupression(True)
    self.kp.set_Radius(0.01)
    keypoints = self.kp.compute()
    self.assertNotEqual(keypoints.size, 0)
    count = 0
    minIts = 999.0
    maxIts = -999.0
    points = np.zeros((keypoints.size, 3), dtype=np.float32)
    for i in range(0, keypoints.size):
        points[i][0] = keypoints[i][0]
        points[i][1] = keypoints[i][1]
        points[i][2] = keypoints[i][2]
        intensity = keypoints[i][3]
<mask>:
            print('coords: ' + str(keypoints[i][0]) + ';' + str(keypoints[i][1]) + ';' + str(keypoints[i][2]))
            maxIts = intensity
        if intensity < minIts:
            minIts = intensity
        count = count + 1
    self.assertGreaterEqual(maxIts, 0.0)
    self.assertEqual(minIts, 0.0)",intensity > maxIts,77,intensity > maxIts,True,100.00000000000004,N/A
"def test_ModelPlane(self):
    model_p = pcl.SampleConsensusModelPlane(self.p)
    ransac = pcl.RandomSampleConsensus(model_p)
    ransac.set_DistanceThreshold(0.01)
    ransac.computeModel()
    inliers = ransac.get_Inliers()
    self.assertNotEqual(len(inliers), 0)
    final = pcl.PointCloud()
<mask>:
        finalpoints = np.zeros((len(inliers), 3), dtype=np.float32)
        for i in range(0, len(inliers)):
            finalpoints[i][0] = self.p[inliers[i]][0]
            finalpoints[i][1] = self.p[inliers[i]][1]
            finalpoints[i][2] = self.p[inliers[i]][2]
        final.from_array(finalpoints)
    self.assertNotEqual(final.size, 0)
    pass",len(inliers) != 0,42,len(inliers) > 0,False,45.48019047027906,N/A
"def test_ModelSphere(self):
    model_s = pcl.SampleConsensusModelSphere(self.p)
    ransac = pcl.RandomSampleConsensus(model_s)
    ransac.set_DistanceThreshold(0.01)
    ransac.computeModel()
    inliers = ransac.get_Inliers()
    self.assertNotEqual(len(inliers), 0)
    final = pcl.PointCloud()
<mask>:
        finalpoints = np.zeros((len(inliers), 3), dtype=np.float32)
        for i in range(0, len(inliers)):
            finalpoints[i][0] = self.p[inliers[i]][0]
            finalpoints[i][1] = self.p[inliers[i]][1]
            finalpoints[i][2] = self.p[inliers[i]][2]
        final.from_array(finalpoints)
    self.assertNotEqual(final.size, 0)
    pass",len(inliers) != 0,42,len(inliers) > 0,False,45.48019047027906,N/A
"def _import_object_from_name(module_name, fullname):
    obj = sys.modules.get(module_name)
<mask>:
        return None
    for comp in fullname.split('.'):
        obj = getattr(obj, comp)
    return obj",obj is None,19,obj is None,True,100.00000000000004,N/A
"def linkcode_resolve(domain, info):
<mask>:
        return None
    rtd_version = os.environ.get('READTHEDOCS_VERSION')
    if rtd_version == 'latest':
        tag = 'master'
    else:
        tag = 'v{}'.format(__version__)
    repo_root_dir = os.path.realpath('..')
    obj = _import_object_from_name(info['module'], info['fullname'])
    try:
        filename = inspect.getsourcefile(obj)
    except TypeError:
        return None
    if filename is None:
        return None
    _, linenum = inspect.getsourcelines(obj)
    assert isinstance(linenum, six.integer_types)
    filename = os.path.realpath(filename)
    if not filename.startswith(repo_root_dir):
        return None
    relpath = os.path.relpath(filename, repo_root_dir)
    return 'https://github.com/Sirokujira/python-pcl/blob/{}/{}#L{}'.format(tag, relpath, linenum)",domain != 'py' or not info['module'],64,domain != 'Sirokujira',False,13.267398701010466,N/A
"def main():
    cloud = pcl.load('./tests/tutorials/bunny.pcd')
    print('cloud points : ' + str(cloud.size))
    detector = cloud.make_HarrisKeypoint3D()
    detector.set_NonMaxSupression(True)
    detector.set_Radius(0.01)
    keypoints = detector.compute()
    print('keypoints detected: ' + str(keypoints.size))
    keypoints3D = pcl.PointCloud()
    max = -999
    min = 999
    count = 0
    points = np.zeros((keypoints.size, 3), dtype=np.float32)
    for i in range(0, keypoints.size):
        points[i][0] = keypoints[i][0]
        points[i][1] = keypoints[i][1]
        points[i][2] = keypoints[i][2]
        intensity = keypoints[i][3]
<mask>:
            print('coords: ' + str(keypoints[i][0]) + ';' + str(keypoints[i][1]) + ';' + str(keypoints[i][2]))
            max = intensity
        if intensity < min:
            min = intensity
        count = count + 1
    points.resize(count, 3)
    print(points)
    keypoints3D.from_array(points)
    print('maximal responce: ' + str(max) + ' min responce:  ' + str(min))
    viewer = pcl.pcl_visualization.PCLVisualizering('3D Viewer')
    pccolor = pcl.pcl_visualization.PointCloudColorHandleringCustom(cloud, 255, 255, 255)
    kpcolor = pcl.pcl_visualization.PointCloudColorHandleringCustom(keypoints3D, 255, 0, 0)
    viewer.AddPointCloud_ColorHandler(cloud, pccolor)
    viewer.AddPointCloud_ColorHandler(keypoints3D, kpcolor, b'keypoints')
    v = True
    while v:
        v = not viewer.WasStopped()
        viewer.SpinOnce()",intensity > max,133,intensity > max,True,100.00000000000004,N/A
"def main():
<mask>:
        print('Syntax is: "" + """" +"" object.pcd scene.pcd\n')
        return 1
    print('Loading point clouds...\n')
    object = pcl.load('')
    scene = pcl.load('')
    print('Downsampling...\n')
    grid_obj = object.make_voxel_grid_filter()
    leaf = 0.005
    grid_obj.set_leaf_size(leaf, leaf, leaf)
    object = grid_obj.filter()
    scene_obj = scene.make_voxel_grid_filter()
    grid_sce = scene_obj.filter()
    print('Estimating scene normals...\n')
    nest = scene.make_NormalEstimationOMP()
    nest.set_RadiusSearch(0.01)
    scene = nest.compute()
    print('Estimating features...\n')
    fest_obj = object.make_FeatureEstimation()
    fest_obj.setRadiusSearch(0.025)
    object_features = fest_obj.compute()
    fest_sce = scene.make_FeatureEstimation()
    fest_sce.setRadiusSearch(0.025)
    scene_features = fest_sce.compute()
    print('Starting alignment...\n')
    align = object.make_SampleConsensusPrerejective()
    align.setSourceFeatures(object_features)
    align.setTargetFeatures(scene_features)
    align.set_MaximumIterations(50000)
    align.set_NumberOfSamples(3)
    align.set_CorrespondenceRandomness(5)
    align.set_SimilarityThreshold(0.9)
    align.set_MaxCorrespondenceDistance(2.5 * leaf)
    if align.hasConverged() == True:
        print('\n')
        transformation = align.getFinalTransformation()
        visu = pcl.PCLVisualization('Alignment')
        visu.add_PointCloud(scene, ColorHandlerT(scene, 0.0, 255.0, 0.0), 'scene')
        visu.add_PointCloud(object_aligned, ColorHandlerT(object_aligned, 0.0, 0.0, 255.0), 'object_aligned')
        visu.spin()
    else:
        print('Alignment failed!\n')
        return 1",args.n != 3,110,pcl.check_syntax('pcd') == True,False,4.456882760699063,N/A
"def main():
    cloud = pcl.PointCloud()
    cloud_filtered = pcl.PointCloud()
    points = np.zeros((5, 3), dtype=np.float32)
    RAND_MAX = 1024.0
    for i in range(0, 5):
        points[i][0] = 1024 * random.random() / RAND_MAX
        points[i][1] = 1024 * random.random() / RAND_MAX
        points[i][2] = 1024 * random.random() / RAND_MAX
    cloud.from_array(points)
<mask>:
        outrem = cloud.make_RadiusOutlierRemoval()
        outrem.set_radius_search(0.8)
        outrem.set_MinNeighborsInRadius(2)
        cloud_filtered = outrem.filter()
    elif args.Removal == 'Condition':
        range_cond = cloud.make_ConditionAnd()
        range_cond.add_Comparison2('z', pcl.CythonCompareOp_Type.GT, 0.0)
        range_cond.add_Comparison2('z', pcl.CythonCompareOp_Type.LT, 0.8)
        condrem = cloud.make_ConditionalRemoval(range_cond)
        condrem.set_KeepOrganized(True)
        cloud_filtered = condrem.filter()
    else:
        print(""please specify command line arg paramter 'Radius' or 'Condition'"")
    print('Cloud before filtering: ')
    for i in range(0, cloud.size):
        print('x: ' + str(cloud[i][0]) + ', y : ' + str(cloud[i][1]) + ', z : ' + str(cloud[i][2]))
    print('Cloud after filtering: ')
    for i in range(0, cloud_filtered.size):
        print('x: ' + str(cloud_filtered[i][0]) + ', y : ' + str(cloud_filtered[i][1]) + ', z : ' + str(cloud_filtered[i][2]))",args.Removal == 'Radius',136,args.Removal == 'Radius',True,100.00000000000004,N/A
"def main():
    cloud = pcl.load('./examples/pcldata/tutorials/table_scene_mug_stereo_textured.pcd')
    print('PointCloud has: ' + str(cloud.size) + ' data points.')
    passthrough = cloud.make_passthrough_filter()
    passthrough.set_filter_field_name('z')
    passthrough.set_filter_limits(0, 1.5)
    cloud_filtered = passthrough.filter()
    print('PointCloud has: ' + str(cloud_filtered.size) + ' data points.')
    ne = cloud_filtered.make_NormalEstimation()
    tree = cloud_filtered.make_kdtree()
    ne.set_SearchMethod(tree)
    ne.set_KSearch(50)
    seg = cloud_filtered.make_segmenter_normals(ksearch=50)
    seg.set_optimize_coefficients(True)
    seg.set_model_type(pcl.SACMODEL_NORMAL_PLANE)
    seg.set_normal_distance_weight(0.1)
    seg.set_method_type(pcl.SAC_RANSAC)
    seg.set_max_iterations(100)
    seg.set_distance_threshold(0.03)
    [inliers_plane, coefficients_plane] = seg.segment()
    cloud_plane = cloud_filtered.extract(inliers_plane, False)
    print('PointCloud representing the planar component: ' + str(cloud_plane.size) + ' data points.\n')
    pcl.save(cloud_plane, 'table_scene_mug_stereo_textured_plane.pcd')
    cloud_filtered2 = cloud_filtered.extract(inliers_plane, True)
    seg = cloud_filtered2.make_segmenter_normals(ksearch=50)
    seg.set_optimize_coefficients(True)
    seg.set_model_type(pcl.SACMODEL_CYLINDER)
    seg.set_normal_distance_weight(0.1)
    seg.set_method_type(pcl.SAC_RANSAC)
    seg.set_max_iterations(10000)
    seg.set_distance_threshold(0.05)
    seg.set_radius_limits(0, 0.1)
    [inliers_cylinder, coefficients_cylinder] = seg.segment()
    cloud_cylinder = cloud_filtered2.extract(inliers_cylinder, False)
<mask>:
        print(""Can't find the cylindrical component."")
    else:
        print('PointCloud representing the cylindrical component: ' + str(cloud_cylinder.size) + ' data points.')
        pcl.save(cloud_cylinder, 'table_scene_mug_stereo_textured_cylinder.pcd')",cloud_cylinder.size == 0,115,len(coefficients_plane) > 0,False,6.567274736060395,N/A
"def main():
    cloud = pcl.PointCloud()
    points = np.zeros((15, 3), dtype=np.float32)
    RAND_MAX = 1024.0
    for i in range(0, 15):
        points[i][0] = 1024 * random.random() / (RAND_MAX + 1.0)
        points[i][1] = 1024 * random.random() / (RAND_MAX + 1.0)
        points[i][2] = 1.0
    points[0][2] = 2.0
    points[3][2] = -2.0
    points[6][2] = 4.0
    cloud.from_array(points)
    print('Point cloud data: ' + str(cloud.size) + ' points')
    for i in range(0, cloud.size):
        print('x: ' + str(cloud[i][0]) + ', y : ' + str(cloud[i][1]) + ', z : ' + str(cloud[i][2]))
    seg = cloud.make_segmenter_normals(ksearch=50)
    seg.set_optimize_coefficients(True)
    seg.set_model_type(pcl.SACMODEL_NORMAL_PLANE)
    seg.set_method_type(pcl.SAC_RANSAC)
    seg.set_distance_threshold(0.01)
    seg.set_normal_distance_weight(0.01)
    seg.set_max_iterations(100)
    indices, coefficients = seg.segment()
<mask>:
        print('Could not estimate a planar model for the given dataset.')
        exit(0)
    print('Model coefficients: ' + str(coefficients[0]) + ' ' + str(coefficients[1]) + ' ' + str(coefficients[2]) + ' ' + str(coefficients[3]))
    print('Model inliers: ' + str(len(indices)))
    for i in range(0, len(indices)):
        print(str(indices[i]) + ', x: ' + str(cloud[indices[i]][0]) + ', y : ' + str(cloud[indices[i]][1]) + ', z : ' + str(cloud[indices[i]][2]))",len(indices) == 0,157,not indices,False,4.104249931194939,N/A
"def main():
    argvs = sys.argv
    argc = len(argvs)
<mask>:
        exit(-1)
    cloud = pcl.load('')
    support_radius = 0.0285
    number_of_partition_bins = 5
    number_of_rotations = 3
    search_method = cloud.make_kdtree()
    feature_estimator = cloud.make_ROPSEstimation()
    feature_estimator.setSearchMethod(search_method)
    feature_estimator.setSearchSurface(cloud)
    feature_estimator.setInputCloud(cloud)
    feature_estimator.setIndices(indices)
    feature_estimator.setTriangles(triangles)
    feature_estimator.setRadiusSearch(support_radius)
    feature_estimator.setNumberOfPartitionBins(number_of_partition_bins)
    feature_estimator.setNumberOfRotations(number_of_rotations)
    feature_estimator.setSupportRadius(support_radius)
    histograms = feature_estimator.compute()",argc != 4,40,len(argvs) != argc,False,14.535768424205482,N/A
"def main():
    cloud = pcl.load('./examples/pcldata/tutorials/table_scene_mug_stereo_textured.pcd')
    passthrough = cloud.make_passthrough_filter()
    passthrough.set_filter_field_name('z')
    passthrough.set_filter_limits(0.0, 1.1)
    cloud_filtered = passthrough.filter()
    print('PointCloud after filtering has: ' + str(cloud_filtered.size) + ' data points.')
    seg = cloud_filtered.make_segmenter_normals(ksearch=50)
    seg.set_optimize_coefficients(True)
    seg.set_model_type(pcl.SACMODEL_NORMAL_PLANE)
    seg.set_method_type(pcl.SAC_RANSAC)
    seg.set_distance_threshold(0.01)
    indices, model = seg.segment()
    print('PointCloud after segmentation has: ' + str(indices.count) + ' inliers.')
    proj = cloud_filtered.make_ProjectInliers()
    proj.set_model_type(pcl.SACMODEL_PLANE)
    cloud_projected = proj.filter()
    print('PointCloud after projection has: ' + str(cloud_projected.size) + ' data points.')
    chull = cloud_projected.make_ConcaveHull()
    chull.set_Alpha(0.1)
    cloud_hull = chull.reconstruct()
    print('Concave hull has: ' + str(cloud_hull.size) + ' data points.')
<mask>:
        pcl.save(cloud_hull, 'table_scene_mug_stereo_textured_hull.pcd')",cloud_hull.size != 0,84,cloud_hull.size > 0,False,55.780028607687655,N/A
"def main():
    parser = argparse.ArgumentParser(description='StrawPCL example: range image border extraction')
    parser.add_argument('--UnseenToMaxRange', '-m', default=True, type=bool, help='Setting unseen values in range image to maximum range readings')
    parser.add_argument('--CoordinateFrame', '-c', default=-1, type=int, help='Using coordinate frame = ')
    parser.add_argument('--AngularResolution', '-r', default=0, type=int, help='Setting angular resolution to = ')
    parser.add_argument('--Help', help='Usage: narf_keypoint_extraction.py [options] <scene.pcd>\n\nOptions:\n-------------------------------------------\n-r <float>   angular resolution in degrees (default = angular_resolution)\n-c <int>     coordinate frame (default = coordinate_frame)\n-m           Treat all unseen points as max range\n-h           this help\n\n\n;')
    args = parser.parse_args()
    setUnseenToMaxRange = args.UnseenToMaxRange
    point_cloud = pcl.PointCloud()
    pcd_filename_indices = ''
<mask>:
        point_cloud = pcl.load('test_pcd.pcd')
        far_ranges_filename = 'test_pcd.pcd'
        far_ranges = pcl.load_PointWithViewpoint(far_ranges_filename)
    else:
        setUnseenToMaxRange = True
        print('No *.pcd file given = Genarating example point cloud.\n')
        count = 0
        points = np.zeros((100 * 100, 3), dtype=np.float32)
        for x in range(-50, 50, 1):
            for y in range(-50, 50, 1):
                points[count][0] = x * 0.01
                points[count][1] = y * 0.01
                points[count][2] = 2.0 - y * 0.01
                count = count + 1
        point_cloud = pcl.PointCloud()
        point_cloud.from_array(points)
        far_ranges = pcl.PointCloud_PointWithViewpoint()
    noise_level = 0.0
    min_range = 0.0
    border_size = 1
    range_image = point_cloud.make_RangeImage()
    range_image.CreateFromPointCloud(point_cloud, angular_resolution, pcl.deg2rad(360.0), pcl.deg2rad(180.0), coordinate_frame, noise_level, min_range, border_size)
    print('range_image::integrateFarRanges.\n')
    if setUnseenToMaxRange == True:
        range_image.SetUnseenToMaxRange()
    viewer = pcl.pcl_visualization.PCLVisualizering()
    viewer.SetBackgroundColor(1.0, 1.0, 1.0)
    range_image_color_handler = pcl.pcl_visualization.PointCloudColorHandleringCustom(point_cloud, 0, 0, 0)
    viewer.AddPointCloud(point_cloud, 'range image')
    v = True
    while v:
        v = not viewer.WasStopped()
        viewer.SpinOnce()",len(pcd_filename_indices) != 0,208,os.path.exists('test_pcd.pcd'),False,4.789232204309912,N/A
"def main():
    parser = argparse.ArgumentParser(description='StrawPCL example: How to visualize a range image')
    parser.add_argument('--UnseenToMaxRange', '-m', default=True, type=bool, help='Setting unseen values in range image to maximum range readings')
    parser.add_argument('--CoordinateFrame', '-c', default=-1, type=int, help='Using coordinate frame = ')
    parser.add_argument('--AngularResolution', '-r', default=0, type=int, help='Setting angular resolution to = ')
    parser.add_argument('--Help', help='Usage: narf_keypoint_extraction.py [options] <scene.pcd>\n\nOptions:\n-------------------------------------------\n-r <float>   angular resolution in degrees (default = angular_resolution)\n-c <int>     coordinate frame (default = coordinate_frame)\n-m           Treat all unseen points as max range\n-h           this help\n\n\n;')
    args = parser.parse_args()
    pcd_filename_indices = ''
<mask>:
        point_cloud = pcl.load('./examples/official/IO/test_pcd.pcd')
        far_ranges_filename = 'test_pcd.pcd'
        far_ranges = pcl.load_PointWithViewpoint(far_ranges_filename)
    else:
        setUnseenToMaxRange = True
        print('No *.pcd file given = Genarating example point cloud.\n')
        count = 0
        points = np.zeros((100 * 100, 3), dtype=np.float32)
        for x in range(-50, 50, 1):
            for y in range(-50, 50, 1):
                points[count][0] = x * 0.01
                points[count][1] = y * 0.01
                points[count][2] = 2.0 - y * 0.01
                count = count + 1
        point_cloud = pcl.PointCloud()
        point_cloud.from_array(points)
        far_ranges = pcl.PointCloud_PointWithViewpoint()
    noise_level = 0.0
    min_range = 0.0
    border_size = 1
    range_image = point_cloud.make_RangeImage()
    range_image.CreateFromPointCloud(point_cloud, angular_resolution_x, pcl.deg2rad(360.0), pcl.deg2rad(180.0), coordinate_frame, noise_level, min_range, border_size)
    print('range_image::integrateFarRanges.\n')
    viewer = pcl.pcl_visualization.PCLVisualizering()
    viewer.SetBackgroundColor(1.0, 1.0, 1.0)
    range_image_color_handler = pcl.pcl_visualization.PointCloudColorHandleringCustom(point_cloud, 0, 0, 0)
    cloudname = str('cloud')
    viewer.AddPointCloud(range_image, range_image_color_handler, cloudname)
    viewer.SetPointCloudRenderingProperties(pcl.pcl_visualization.PCLVISUALIZER_POINT_SIZE, 1, cloudname)
    range_image_widget = pcl.pcl_visualization.RangeImageVisualization()
    range_image_widget.ShowRangeImage(range_image)",len(pcd_filename_indices) != 0,199,os.path.exists('./examples/official/IO/test_pcd.pcd'),False,2.568331954752977,N/A
"def main():
    f = file.File('28W0608011101-1.las', mode='r')
<mask>:
        red = f.red
        green = f.green
        blue = f.blue
        red = np.right_shift(red, 8).astype(np.uint8)
        green = np.right_shift(green, 8).astype(np.uint8)
        blue = np.right_shift(blue, 8).astype(np.uint8)
        red = red.astype(np.uint32)
        green = green.astype(np.uint32)
        blue = blue.astype(np.uint32)
        rgb = np.left_shift(red, 16) + np.left_shift(green, 8) + np.left_shift(blue, 0)
        ptcloud = np.vstack((f.x, f.y, f.z, rgb)).transpose()
        cloud = pcl.PointCloud_PointXYZRGBA()
        mean_param = np.concatenate([np.mean(ptcloud, 0)[0:3], np.zeros(1)])
        ptcloud_centred = ptcloud - mean_param
        cloud.from_array(np.array(ptcloud_centred, dtype=np.float32))
        visual = pcl.pcl_visualization.CloudViewing()
        visual.ShowColorACloud(cloud, b'cloud')
    else:
        ptcloud = np.vstack((f.x, f.y, f.z)).transpose()
        mean_param = np.mean(ptcloud, 0)
        cloud = pcl.PointCloud()
        ptcloud_centred = ptcloud - mean_param
        cloud.from_array(np.array(ptcloud_centred, dtype=np.float32))
        visual = pcl.pcl_visualization.CloudViewing()
        visual.ShowMonochromeCloud(cloud, b'cloud')
    v = True
    while v:
        v = not visual.WasStopped()","f._header.data_format_id in (2, 3, 5)",107,pcl.IsMac(),False,1.044177559991939,N/A
"def main():
    try:
        from pylibfreenect2 import OpenCLPacketPipeline
        pipeline = OpenCLPacketPipeline()
    except:
        from pylibfreenect2 import CpuPacketPipeline
        pipeline = CpuPacketPipeline()
    logger = createConsoleLogger(LoggerLevel.Debug)
    setGlobalLogger(logger)
    fn = Freenect2()
    num_devices = fn.enumerateDevices()
<mask>:
        print('No device connected!')
        sys.exit(1)
    serial = fn.getDeviceSerialNumber(0)
    device = fn.openDevice(serial, pipeline=pipeline)
    listener = SyncMultiFrameListener(FrameType.Color | FrameType.Ir | FrameType.Depth)
    device.setColorFrameListener(listener)
    device.setIrAndDepthFrameListener(listener)
    device.start()
    registration = Registration(device.getIrCameraParams(), device.getColorCameraParams())
    undistorted = Frame(512, 424, 4)
    registered = Frame(512, 424, 4)
    need_bigdepth = False
    need_color_depth_map = False
    bigdepth = Frame(1920, 1082, 4) if need_bigdepth else None
    color_depth_map = np.zeros((424, 512), np.int32).ravel() if need_color_depth_map else None
    point = pcl.PointCloud()
    visual = pcl.pcl_visualization.CloudViewing()
    visual.ShowColorCloud(cloud)
    while True:
        frames = listener.waitForNewFrame()
        color = frames['color']
        ir = frames['ir']
        depth = frames['depth']
        registration.apply(color, depth, undistorted, registered, bigdepth=bigdepth, color_depth_map=color_depth_map)
        undistorted_arrray = undistorted.asarray(dtype=np.float32, ndim=2)
        point = pcl.PointCloud(undistorted_arrray)
        listener.release(frames)
        v = True
        while v:
            v = not visual.WasStopped()
    device.stop()
    device.close()
    sys.exit(0)",num_devices == 0,135,num_devices == 0,True,100.00000000000004,N/A
"def main():
    try:
        from pylibfreenect2 import OpenCLPacketPipeline
        pipeline = OpenCLPacketPipeline()
    except:
        from pylibfreenect2 import CpuPacketPipeline
        pipeline = CpuPacketPipeline()
    logger = createConsoleLogger(LoggerLevel.Debug)
    setGlobalLogger(logger)
    fn = Freenect2()
    num_devices = fn.enumerateDevices()
<mask>:
        print('No device connected!')
        sys.exit(1)
    serial = fn.getDeviceSerialNumber(0)
    device = fn.openDevice(serial, pipeline=pipeline)
    listener = SyncMultiFrameListener(FrameType.Color | FrameType.Ir | FrameType.Depth)
    device.setColorFrameListener(listener)
    device.setIrAndDepthFrameListener(listener)
    device.start()
    registration = Registration(device.getIrCameraParams(), device.getColorCameraParams())
    undistorted = Frame(512, 424, 4)
    registered = Frame(512, 424, 4)
    need_bigdepth = False
    need_color_depth_map = False
    bigdepth = Frame(1920, 1082, 4) if need_bigdepth else None
    color_depth_map = np.zeros((424, 512), np.int32).ravel() if need_color_depth_map else None
    point = pcl.PointCloud()
    viewer = pcl.pcl_visualization.PCLVisualizering()
    v = True
    while v:
        v = not viewer.WasStopped()
        viewer.spinOnce()
        frames = listener.waitForNewFrame()
        color = frames['color']
        ir = frames['ir']
        depth = frames['depth']
        registration.apply(color, depth, undistorted, registered, bigdepth=bigdepth, color_depth_map=color_depth_map)
        points = np.zeros((512 * 424, 3), dtype=np.float32)
        for r in range(0, 512):
            for c in range(0, 424):
                point = registration.getPointXYZ(undistorted, registered, r, c)
                points[r * 424 + c][0] = point[0]
                points[r * 424 + c][1] = point[1]
                points[r * 424 + c][2] = point[2]
        undistorted_arrray = undistorted.asarray(dtype=np.float32, ndim=2)
        point = pcl.PointCloud(undistorted_arrray)
        listener.release(frames)
    device.stop()
    device.close()
    sys.exit(0)",num_devices == 0,177,num_devices == 0,True,100.00000000000004,N/A
"def get_grouped_data(data, keys):
    """"""

    :param data: list[dict]. dict为1级
    :param keys: 依次按照keys进行group
    :return: {}nested的dict，最后的value是符合这个group的结果
    """"""
    _dict = {}
<mask>:
        grouped_data = groupBy(data, keys[0])
        for key, group in grouped_data:
            _dict[key] = list(group)
    else:
        key = keys[0]
        grouped_data = groupBy(data, key)
        for key, group in grouped_data:
            _dict[key] = get_grouped_data(list(group), keys[1:])
    return _dict",len(keys) == 1,49,"isinstance(keys[0], list)",False,10.552670315936318,N/A
"def merge(a, b, path=None):
    """"""merges b into a""""""
<mask>:
        path = []
    for key in b:
        if key in a:
            if isinstance(a[key], dict) and isinstance(b[key], dict):
                merge(a[key], b[key], path + [str(key)])
            elif a[key] == b[key]:
                pass
            else:
                raise Exception('Conflict at %s' % '.'.join(path + [str(key)]))
        else:
            a[key] = b[key]
    return a",path is None,51,path is None,True,100.00000000000004,N/A
"def maybe_skip_member(app, what, name, obj, skip, options):
<mask>:
        return True
    return obj.__doc__ is None",name.startswith('_'),14,name == '__init__',False,11.868405219520975,N/A
"def get_commit_id(file):
    """"""用户用此命令获取上一次 Git 记录的 id, 期望的使用方法如下::
        
        id = fitlog.get_commit_id(__file__)
        
    :param file: 以该路径往上寻找.fitlog所在文件夹。一般传入__file__即可:
    :return: Git 的上次记录的 commit-id 的前七位；错误时返回 `error`
    """"""
    work_dir = _committer._find_config_file(file)
    res = Committer.git_last_commit_info(work_dir)
<mask>:
        return res['msg'][0]
    else:
        return 'error'",res['status'] == 0,32,res,False,0.24787521766663595,N/A
"def get_fit_id(file):
    """"""用户用此命令获取上一次 fitlog 自动记录 commit 的 id, 期望的使用方法如下::
        
        id = fitlog.get_fit_id(__file__)
        
    
    :param file: 以该路径往上寻找.fitlog所在文件夹。一般传入__file__即可
    :return: Fitlog 的上次自动记录的 commit-id 的前七位；错误时返回 `error`
    """"""
    work_dir = _committer._find_config_file(file)
    res = Committer.fit_last_commit_info(work_dir)
<mask>:
        return res['msg'][0]
    else:
        return 'error'",res['status'] == 0,34,res,False,0.24787521766663595,N/A
"def main_cmd():
    argv = sys.argv[1:2] if len(sys.argv) > 2 else sys.argv[1:]
    args = docopt(__doc__, version='fitlog ' + version, argv=argv)
    argv = sys.argv[1:]
    cmd = args['<command>']
<mask>:
        if len(argv) > 1:
            if argv[1] in cmd_map:
                cmd_map[argv[1]](['-h'])
            else:
                print('Unknown command `{}`, only support {}.'.format(argv[1], list(cmd_map.keys())))
                print(__doc__)
        else:
            print('You have to specify a command, support {}.'.format(list(cmd_map.keys())))
            print(__doc__)
    elif cmd in cmd_map:
        cmd_map[cmd](argv)
    else:
        print('Unknown command: {}.'.format(cmd))
        print(__doc__)","cmd in ['help', None]",64,cmd is None,False,9.138402379955025,N/A
"def log_cmd(argv=None):
<mask>:
        args = docopt(__doc__, argv=argv)
    else:
        args = docopt(__doc__)
    log_dir = args['<log-dir>']
    start_port = int(args['--port'])
    log_config_name = args['--log-config-name']
    standby_hours = int(args['--standby-hours'])
    ip = args['--ip']
    token = args['--token']
    if token is False:
        token = None
    if not os.path.isabs(log_dir):
        cwd = os.getcwd()
        log_dir = os.path.join(cwd, log_dir)
    if log_config_name != None and (not log_config_name.endswith('.cfg')):
        raise RuntimeError('log_config_name has to end with .cfg.')
    if not os.path.exists(log_dir):
        raise RuntimeError('{} is not exist.'.format(log_dir))
    if not os.path.isdir(log_dir):
        raise NotADirectoryError('{} is not a directory.'.format(log_dir))
    log_dir = os.path.abspath(log_dir)
    if os.path.dirname(log_config_name) != '':
        raise ValueError('log_config_name can only be a filename.')
    start_app(log_dir, log_config_name, start_port, standby_hours, ip, token)",argv,97,argv,True,100.00000000000004,N/A
"@summary_page.route('/summary', methods=['GET', 'POST'])
def summary_index():
    ids = {}
<mask>:
        for id in request.values['ids'].split(','):
            ids[id] = 1
    return render_template('summary.html', server_uuid=all_data['uuid'], log_names=ids, settings={key.replace('_', ' '): value for key, value in all_data['settings'].items()})",request.method == 'POST',29,'ids' in request.values,False,17.491650626361256,N/A
"@summary_page.route('/summary/summary_config', methods=['POST'])
def summaries_configs():
    res = check_uuid_summary(all_data['uuid'], request.json['uuid'])
<mask>:
        return jsonify(res)
    root_log_dir = all_data['root_log_dir']
    config_names = {}
    for name in _get_config_names(root_log_dir):
        if name == all_data['log_config_name']:
            config_names[name] = 1
        else:
            config_names[name] = 0
    summary_names = {key: 0 for key in _get_all_summuries(root_log_dir)}
    summary_names['Create New Summary'] = 1
    return jsonify({'status': 'success', 'summary_names': summary_names, 'config_names': config_names})",res != None,52,res,False,4.9787068367863965,N/A
"@summary_page.route('/summary/summary_json', methods=['POST'])
def summary_json():
    res = check_uuid_summary(all_data['uuid'], request.json['uuid'])
<mask>:
        return jsonify(res)
    summary_name = request.json['summary_name']
    summary_names = _get_all_summuries(all_data['root_log_dir'])
    if summary_names.index(summary_name) == -1:
        return jsonify(status='fail', msg='There is no summary named `{}`.'.format(summary_name))
    else:
        summary = read_summary(all_data['root_log_dir'], summary_name)
        summary.pop('extra_data', None)
        return jsonify(status='success', summary=summary)",res != None,39,res,False,4.9787068367863965,N/A
"@summary_page.route('/summary/selections', methods=['POST'])
def summary_selections():
    res = check_uuid_summary(all_data['uuid'], request.json['uuid'])
<mask>:
        return jsonify(res)
    try:
        if 'config_name' in request.json:
            logs = read_logs(request.json['config_name'], all_data['root_log_dir'])
        elif 'log_names' in request.json:
            logs = read_logs(request.json['log_names'], all_data['root_log_dir'], all_data['extra_data'])
        else:
            raise ValueError('Corrupted request.')
        if isinstance(logs, dict):
            return jsonify(logs)
        if len(logs) == 0:
            return jsonify(status='fail', msg='No valid log found.')
        axises, metrics = get_summary_selection_from_logs(logs)
        if len(metrics) == 0:
            return jsonify(status='fail', msg='No valid metric.')
        if len(axises) == 0:
            return jsonify(status='fail', msg='No valid hypers or others')
        return jsonify(status='success', metrics=metrics, axises=axises)
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(e)
        return jsonify(status='fail', msg='Unknown error from the server.')",res != None,91,res,False,4.9787068367863965,N/A
"@summary_page.route('/summary/new_summary', methods=['POST'])
def new_summary():
    res = check_uuid_summary(all_data['uuid'], request.json['uuid'])
<mask>:
        return jsonify(res)
    try:
        vertical = request.json['vertical']
        horizontals = request.json['horizontals']
        method = request.json['method']
        criteria = request.json['criteria']
        results = request.json['results']
        result_maps = request.json['result_maps']
        selected_data = request.json['selected_data']
        summary_name = request.json['summary_name']
        extra_summary = []
        summary_names = _get_all_summuries(all_data['root_log_dir'])
        if summary_name in summary_names:
            request_summary = {'vertical': vertical, 'horizontals': horizontals, 'method': method, 'criteria': criteria, 'results': results, 'result_maps': result_maps}
            summary = read_summary(all_data['root_log_dir'], summary_name)
            if _summary_eq(request_summary, summary):
                extra_summary = summary.pop('extra_data', {})
        summary_table = generate_summary_table(vertical, horizontals, method, criteria, results, result_maps, selected_data, all_data['root_log_dir'], all_data['extra_data'], extra_summary)
        summary_table = stringify_dict_key(summary_table)

        def change_order_keys_to_str(_dict):
            for key, value in _dict.copy().items():
                if key == 'OrderKeys':
                    value = list(map(str, value))
                    _dict[key] = value
                if isinstance(value, dict):
                    change_order_keys_to_str(value)
        change_order_keys_to_str(summary_table)
        return jsonify(summary_table)
    except Exception as e:
        print(e)
        traceback.print_exc()
        return jsonify(status='fail', msg='Please refer to your server for exception reason.')",res != None,127,res,False,4.9787068367863965,N/A
"@line_page.route('/line', methods=['POST'])
def line_index():
    ids = request.values['ids']
    flat_logs = [all_data['data'][id].copy() for id in ids.split(',')]
    hidden_columns = all_data['hidden_columns'].copy()
    value_dict_count = defaultdict(list)
    for log in flat_logs:
        for key in log.keys():
            value_dict_count[key] += [log[key]]
    for key, _lst in list(value_dict_count.items()):
<mask>:
            for log in flat_logs:
                log.pop(key, None)
        if len(set(_lst)) == 1:
            hidden_columns[key] = 1
    logs = [expand_dict([log])[0] for log in flat_logs]
    hidden_columns['id'] = 1
    hidden_columns['memo'] = 1
    hidden_columns['meta'] = 1
    res = generate_columns(logs, hidden_columns=hidden_columns, column_order=all_data['column_order'], editable_columns={}, exclude_columns={}, ignore_unchanged_columns=False, str_max_length=20, round_to=6, num_extra_log=0)
    column_order = res['column_order']
    column_order.pop('id')
    column_order['OrderKeys'].remove('id')
    if 'metric' in column_order:
        column_order['OrderKeys'].remove('metric')
        column_order['OrderKeys'].insert(0, 'metric')
    column_dict = res['column_dict']
    column_dict.pop('id')
    hidden_columns = res['hidden_columns']
    data = res['data']
    for key, log in data.items():
        log.pop('id')
    return render_template('line.html', data=data, column_order=column_order, column_dict=column_dict, hidden_columns=hidden_columns)",len(_lst) != len(flat_logs),111,_lst,False,0.40867714384640685,N/A
"@multi_chart_page.route('/multi_chart', methods=['POST'])
def chart():
    res = check_uuid(all_data['uuid'], request.values['uuid'])
<mask>:
        return jsonify(res)
    logs = request.values['ids'].split(',')
    titles = request.values['titles'].split(',')
    root_log_dir = all_data['root_log_dir']
    has_step_logs = []
    finish_logs = []
    for log in logs:
        full_log_path = os.path.join(root_log_dir, log)
        if is_log_dir_has_step(full_log_path, ('metric.log',)):
            has_step_logs.append(log)
        if is_log_record_finish(full_log_path):
            finish_logs.append(log)
    msg = ''
    results = {}
    multi_chart_uuid = str(uuid.uuid1())
    find_titles = []
    if len(has_step_logs) > 1:
        update_every = all_data['chart_settings']['update_every']
        wait_seconds = update_every * 5
        max_no_updates = all_data['chart_settings']['max_no_updates']
        handler = MultiChartStepLogHandler(root_log_dir, has_step_logs, multi_chart_uuid, titles=titles, round_to=all_data['basic_settings']['round_to'], wait_seconds=wait_seconds, max_no_updates=max_no_updates)
        results = handler.update_logs(handler_names=('metric',))
        all_handlers[multi_chart_uuid] = handler
        if not handler_watcher._start:
            handler_watcher.start()
        for title in titles:
            if title in results:
                find_titles.append(title)
        if len(find_titles) == 0:
            msg = 'No log has step information.'
        results['update_every'] = update_every
        results['max_no_updates'] = max_no_updates
        results['multi_chart_uuid'] = multi_chart_uuid
    else:
        msg = 'Less than 2 logs have step information.'
    return render_template('multi_chart.html', data=results, message=msg, multi_chart_uuid=multi_chart_uuid, titles=','.join(find_titles), logs=request.values['ids'])",res != None,133,res,False,4.9787068367863965,N/A
"@multi_chart_page.route('/multi_chart/new_step', methods=['POST'])
def chart_new_step():
    multi_chart_uuid = request.json['multi_chart_uuid']
    try:
        results = {}
<mask>:
            handler = all_handlers[multi_chart_uuid]
            results = handler.update_logs()
        print(results)
        return jsonify(data=results, status='success')
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({'status': 'fail', 'message': f'Exception occurred in the server: {str(e)}.'})",multi_chart_uuid in all_handlers,39,multi_chart_uuid in all_handlers,True,100.00000000000004,N/A
"@chart_page.route('/chart', methods=['POST'])
def chart():
    res = check_uuid(all_data['uuid'], request.values['uuid'])
<mask>:
        return jsonify(res)
    log_dir = request.values['log_dir']
    finish = request.values['finish']
    save_log_dir = os.path.join(all_data['root_log_dir'], log_dir)
    chart_exclude_columns = all_data['chart_settings']['chart_exclude_columns']
    _uuid = str(uuid.uuid1())
    max_points = all_data['chart_settings']['max_points']
    update_every_second = all_data['chart_settings']['update_every']
    wait_seconds = update_every_second * 5
    handler = ChartStepLogHandler(save_log_dir, _uuid, round_to=all_data['basic_settings']['round_to'], max_steps=max_points, wait_seconds=wait_seconds, exclude_columns=chart_exclude_columns, max_no_updates=all_data['chart_settings']['max_no_updates'])
    only_once = is_log_record_finish(save_log_dir) or finish == 'true'
    points = handler.update_logs(only_once)
    total_steps = points.pop('total_steps', None)
    if not only_once:
        all_handlers[_uuid] = handler
        if not handler_watcher._start:
            handler_watcher.start()
    replace_nan_inf(points, round_to=all_data['basic_settings']['round_to'])
    return render_template('chart.html', log_dir=log_dir, data=points, chart_uuid=_uuid, server_uuid=all_data['uuid'], update_every=update_every_second * 1000, max_no_updates=all_data['chart_settings']['max_no_updates'], total_steps=total_steps, short_name_map={value: key for key, value in full_name_map.items()})",res != None,91,res,False,4.9787068367863965,N/A
"@chart_page.route('/chart/new_step', methods=['POST'])
def chart_new_step():
    _uuid = request.json['chart_uuid']
    max_point_per_update = 100
    points = {}
<mask>:
        handler = all_handlers[_uuid]
        points = handler.update_logs()
        for key, value in points.items():
            if isinstance(value, list):
                if len(value) < max_point_per_update:
                    continue
                total_points = len(value)
                cut_values = [v for v in value if random.random() < max_point_per_update / total_points]
                points[key] = cut_values
    else:
        points['finish'] = True
    replace_nan_inf(points, all_data['basic_settings']['round_to'])
    return jsonify(steps=points)",_uuid in all_handlers,60,_uuid in all_handlers,True,100.00000000000004,N/A
"@chart_page.route('/chart/have_trends', methods=['POST'])
def have_trends():
    try:
        res = check_uuid(all_data['uuid'], request.json['uuid'])
<mask>:
            return jsonify(res)
        log_dir = request.json['log_dir']
        save_log_dir = os.path.join(all_data['root_log_dir'], log_dir)
        if is_log_dir_has_step(save_log_dir):
            return jsonify(status='success', have_trends=True)
        else:
            return jsonify(status='success', have_trends=False, msg='There is no trend data for this log.')
    except Exception:
        print('Exception detected in have_trends()')
        return jsonify(status='fail', have_trends=False, msg='Error from the server.')",res != None,49,res,False,4.9787068367863965,N/A
"@chart_page.route('/chart/range', methods=['POST'])
def ranges():
    try:
        res = check_uuid(all_data['uuid'], request.json['uuid'])
<mask>:
            return jsonify(res)
        keys = request.json['keys']
        log_dir = request.json['log_dir']
        ranges = request.json['ranges']
        handler = ChartStepLogHandler(save_log_dir=os.path.join(all_data['root_log_dir'], log_dir), uuid=all_data['uuid'], round_to=all_data['basic_settings']['round_to'], max_steps=100000, wait_seconds=10, exclude_columns=all_data['chart_settings']['chart_exclude_columns'], max_no_updates=all_data['chart_settings']['max_no_updates'])
        filepaths = []
        for key in keys:
            filepaths.append(os.path.join(all_data['root_log_dir'], log_dir, key + '.log'))
        updates = handler.read_single_update(filepaths, ranges)
        del handler
        refined_updates = {}
        for key in keys:
            logs = updates[key]
            refined_updates[key] = _refine_logs(logs, all_data['chart_settings']['max_points'], all_data['basic_settings']['round_to'])
        return jsonify(status='success', steps=refined_updates)
    except Exception as e:
        print(e)
        import traceback
        traceback.print_exc()
        return jsonify(status='fail', msg='Some bug happens, contact developer.')",res != None,82,res,False,4.9787068367863965,N/A
"@table_page.route('/table/table')
def get_table():
    global first_time_access
<mask>:
        log_dir = all_data['root_log_dir']
        log_config_name = all_data['log_config_name']
        log_reader = all_data['log_reader']
        all_data.update(prepare_data(log_reader, log_dir, log_config_name, all_data))
    first_time_access = False
    data = all_data['data'].copy()
    replace_nan_inf(data, all_data['basic_settings']['round_to'])
    return jsonify(column_order=all_data['column_order'], column_dict=all_data['column_dict'], hidden_columns=all_data['hidden_columns'], data=data, settings={key.replace('_', ' '): value for key, value in all_data['settings'].items()}, uuid=all_data['uuid'], hidden_rows=list(all_data['hidden_rows'].keys()), unchanged_columns=all_data['unchanged_columns'], log_config_name=all_data['log_config_name'], max_compare_metrics=all_data['config']['multi_chart_settings']['max_compare_metrics'])",not first_time_access,46,first_time_access,False,81.87307530779823,N/A
"@table_page.route('/table/refresh', methods=['POST'])
def refresh_table():
    res = check_uuid(all_data['uuid'], request.json['uuid'])
<mask>:
        return jsonify(res)
    log_reader = all_data['log_reader']
    try:
        new_logs = log_reader.read_logs(all_data['deleted_rows'])
        if len(new_logs) == 0:
            return jsonify(status='success', msg='Update successfully, no update found.', new_logs=[], updated_logs=[])
        else:
            new_logs, updated_logs = prepare_incremental_data(all_data['data'], new_logs, all_data['field_columns'], all_data['filter_condition'], all_data['settings']['Ignore_filter_condition_not_exist_log'])
            if len(new_logs) == 0 and len(updated_logs) == 0:
                return jsonify(status='success', msg='Update successfully, no update found.', new_logs=[], updated_logs=[])
            replace_nan_inf(new_logs, all_data['basic_settings']['round_to'])
            replace_nan_inf(updated_logs, all_data['basic_settings']['round_to'])
            return jsonify(status='success', msg='Update successfully, {} log have updates, {} newly added.'.format(len(updated_logs), len(new_logs)), new_logs=new_logs, updated_logs=updated_logs)
    except:
        return jsonify(status='fail', msg='Unknown error from server.')",res != None,82,res,False,4.9787068367863965,N/A
"@table_page.route('/table/delete_records', methods=['POST'])
def delete_records():
    res = check_uuid(all_data['uuid'], request.json['uuid'])
<mask>:
        return jsonify(res)
    ids = request.json['ids']
    for id in ids:
        if id in all_data['data']:
            all_data['deleted_rows'][id] = 1
    return jsonify(status='success', msg='')",res != None,28,res,False,4.9787068367863965,N/A
"@table_page.route('/table/erase_records', methods=['POST'])
def erase_records():
    res = check_uuid(all_data['uuid'], request.json['uuid'])
<mask>:
        return jsonify(res)
    ids = request.json['ids']
    fail_ids = []
    for index in range(len(ids) - 1, -1, -1):
        id = ids[index]
        if id in all_data['data']:
            all_data['data'].pop(id, None)
            all_data['deleted_rows'].pop(id, None)
            try:
                record_path = os.path.join(all_data['root_log_dir'], id)
                if os.path.isdir(record_path):
                    shutil.rmtree(record_path)
            except Exception as e:
                fail_ids.append(id)
        if id in all_data['extra_data']:
            all_data['extra_data'].pop(id)
    if len(fail_ids) != 0:
        return jsonify(status='fail', msg=fail_ids)
    else:
        return jsonify(status='success', msg='')",res != None,65,res,False,4.9787068367863965,N/A
"@table_page.route('/table/edit', methods=['POST'])
def table_edit():
    try:
        res = check_uuid(all_data['uuid'], request.json['uuid'])
<mask>:
            return jsonify(res)
        id = request.json['id']
        field = request.json['field']
        new_field_value = request.json['new_field_value']
        if id in all_data['extra_data']:
            all_data['extra_data'][id][field] = new_field_value
        else:
            all_data['extra_data'][id] = {field: new_field_value}
        if id in all_data['data'] and field in all_data['data'][id]:
            all_data['data'][id][field] = new_field_value
        return jsonify(status='success', msg='')
    except Exception as e:
        print(e)
        import traceback
        traceback.print_exc()
        return jsonify(status='fail', msg='Unknown error fail to save edit results.')",res != None,64,res,False,4.9787068367863965,N/A
"@app.route('/kill', methods=['POST'])
def seriouslykill():
    time.sleep(1)
    func = request.environ.get('werkzeug.server.shutdown')
<mask>:
        raise RuntimeError('Not running with the Werkzeug Server')
    func()
    return 'stopping'",func is None,19,func is None,True,100.00000000000004,N/A
"@app.route('/arange_kill', methods=['POST'])
def arange_kill():
    res = check_uuid(all_data['uuid'], request.json['uuid'])
<mask>:
        return jsonify(res)

    def shutdown():
        req = urequest.Request('http://127.0.0.1:{}/kill'.format(all_data['port']), headers={}, data=''.encode('utf-8'))
        page = urequest.urlopen(req).read().decode('utf-8')
    print('Shutting down from the frontend...')
    Timer(1.0, shutdown).start()
    return jsonify(status='success', msg='')",res != None,31,res,False,4.9787068367863965,N/A
"def start_app(log_dir, log_config_name, start_port, standby_hours, ip='0.0.0.0', token=None):
    os.chdir(os.path.dirname(os.path.abspath(__file__)))
    all_data['root_log_dir'] = log_dir
    server_wait_seconds = int(standby_hours * 3600)
    print('This server will automatically shutdown if no api access for {} hours.'.format(standby_hours))
    all_data['log_config_name'] = log_config_name
    all_data['log_reader'] = log_reader
<mask>:
        all_data['token'] = None
    else:
        all_data['token'] = str(token)
    all_data.update(prepare_data(log_reader, all_data['root_log_dir'], all_data['log_config_name']))
    print('Finish preparing data. Found {} records in {}.'.format(len(all_data['data']), log_dir))
    all_data['uuid'] = str(uuid.uuid1())
    port = get_usage_port(start_port=start_port)
    server_watcher = ServerWatcher(LEAST_REQUEST_TIMESTAMP, port)
    server_watcher.set_server_wait_seconds(server_wait_seconds)
    server_watcher.start()
    if all_data['token'] != None:
        print(_colored_string('You specify token:{}, remember to add this token when access your table.'.format(all_data['token']), color='red'))
    all_data['port'] = port
    app.run(host=ip, port=port, debug=False, threaded=True)
    print('Shutting down server...')
    save_all_data(all_data, all_data['root_log_dir'], all_data['log_config_name'])
    handler_watcher.stop()
    server_watcher.stop()",token is None,98,token is None,True,100.00000000000004,N/A
"@folder_page.route('/folder', methods=['POST', 'GET'])
def show_folder():
<mask>:
        uuid = request.values['uuid']
        id = request.values['id'] if 'id' in request.values else ''
        subdir = request.values['subdir'] if 'subdir' in request.values else ''
    else:
        uuid = request.args.get('uuid')
        id = request.args.get('id')
        subdir = request.args.get('subdir')
    res = check_uuid(all_data['uuid'], uuid)
    if res is not None:
        return jsonify(res)
    if id:
        log_dir = all_data['root_log_dir']
        folder = os.path.join(log_dir, id)
        if os.path.relpath(folder, log_dir).startswith('.'):
            return jsonify(status='fail', msg='Permission denied.')
        if subdir == '':
            pass
        elif os.path.isfile(os.path.join(folder, subdir)):
            if os.path.splitext(subdir)[1][1:] in ('jpg', 'png', 'jpeg', 'fig'):
                return redirect(url_for('folder_page.show_image', uuid=uuid, id=id, subdir=subdir), code=301)
            resp = make_response(send_file(os.path.join(folder, subdir)))
            resp.headers['Content-type'] = 'text/plan;charset=UTF-8'
            return resp
        elif os.path.isdir(os.path.join(folder, subdir)):
            folder = os.path.join(folder, subdir)
        else:
            return jsonify(status='fail', msg='Invalid file.')
        if os.path.relpath(folder, log_dir).startswith('.'):
            return jsonify(status='fail', msg='Permission denied.')
        current_list = os.listdir(folder)
        contents = []
        for i in sorted(current_list):
            fullpath = folder + os.sep + i
            if os.path.isdir(fullpath):
                extra = os.sep
            else:
                extra = ''
            content = {}
            content['filename'] = i + extra
            content['mtime'] = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(os.stat(fullpath).st_mtime))
            content['size'] = str(round(os.path.getsize(fullpath) / 1024)) + 'k'
            content['isfile'] = os.path.isfile(fullpath)
            if extra:
                contents.insert(0, content)
            else:
                contents.append(content)
        subdir = os.path.relpath(os.path.abspath(folder), start=os.path.abspath(os.path.join(log_dir, id)))
        if subdir.startswith('.'):
            subdir = ''
        elif not subdir.endswith(os.sep):
            subdir += os.sep
        return render_template('folder.html', contents=contents, subdir=subdir, ossep=os.sep, uuid=all_data['uuid'], id=id)
    else:
        return jsonify(status='fail', msg='The request lacks id or filename.')",request.method == 'POST',200,request.method == 'GET',False,75.98356856515926,N/A
"@folder_page.route('/folder/show_image', methods=['GET'])
def show_image():
    uuid = request.args.get('uuid')
    id = request.args.get('id')
    subdir = request.args.get('subdir')
    res = check_uuid(all_data['uuid'], uuid)
<mask>:
        return jsonify(res)
    if id:
        log_dir = all_data['root_log_dir']
        folder = os.path.join(log_dir, id)
        if os.path.splitext(subdir)[1][1:] in ('jpg', 'png', 'jpeg', 'fig'):
            img_stream = ''
            with open(os.path.join(folder, subdir), 'rb') as img_f:
                img_stream = img_f.read()
                img_stream = base64.b64encode(img_stream).decode('ascii')
            try:
                width = get_image_size(os.path.join(folder, subdir))[0]
            except:
                width = -1
            if width == -1:
                width = 1000
            return render_template('folder_img.html', img_stream=img_stream, img_path=subdir, width=width)
    return jsonify(status='fail', msg=f'Fail to show {(os.path.relpath(os.path.join(folder, subdir)), log_dir)}')",res is not None,80,res,False,4.9787068367863965,N/A
"def remove_exclude(column_dict, exclude_dict, prefix=''):
    keys = list(column_dict.keys())
    for key in keys:
<mask>:
            new_prefix = str(key)
        else:
            new_prefix = prefix + '-' + str(key)
        if new_prefix in exclude_dict:
            column_dict.pop(key)
        elif isinstance(column_dict[key], dict):
            remove_exclude(column_dict[key], exclude_dict, new_prefix)",prefix == '',34,prefix == '',True,100.00000000000004,N/A
"def add_columns(prefix, key, value, depth, max_depth, column_dict, order_value, connector, exclude_columns, unselectable_columns, editable_columns, hidden_columns, new_hidden_columns, hide):
<mask>:
        prefix = prefix + connector + str(key)
    else:
        prefix = str(key)
    if prefix in exclude_columns:
        return (0, None)
    if not hide:
        if prefix in hidden_columns:
            hide = True
    item = {}
    item['title'] = key
    colspan = 1
    min_unuse_depth = max_depth - depth - 1
    column_order = {}
    if isinstance(value, dict):
        total_colspans = 0
        column_keys = [key for key in value.keys()]
        first_column_keys = []
        if isinstance(order_value, dict):
            for o_key, o_v in order_value.items():
                if o_key in value:
                    n_val = value[o_key]
                    colspan, c_col_ord = add_columns(prefix, o_key, n_val, depth + 1, max_depth, column_dict, o_v, connector, exclude_columns, unselectable_columns, editable_columns, hidden_columns, new_hidden_columns, hide)
                    total_colspans += colspan
                    min_unuse_depth = 0
                    column_keys.pop(column_keys.index(o_key))
                    if colspan != 0:
                        column_order[o_key] = c_col_ord
                        first_column_keys.append(o_key)
        for key in column_keys:
            v = value[key]
            colspan, c_col_ord = add_columns(prefix, key, v, depth + 1, max_depth, column_dict, None, connector, exclude_columns, unselectable_columns, editable_columns, hidden_columns, new_hidden_columns, hide)
            total_colspans += colspan
            min_unuse_depth = 0
            if colspan != 0:
                column_order[key] = c_col_ord
                first_column_keys.append(key)
        colspan = total_colspans
        column_order['OrderKeys'] = first_column_keys
    else:
        item['field'] = prefix
        item['sortable'] = 'true'
        if prefix not in unselectable_columns:
            if prefix in editable_columns:
                item['filterControl'] = 'input'
            else:
                item['filterControl'] = 'select'
                item['filterStrictSearch'] = True
        if prefix in editable_columns:
            item['editable'] = 'true'
        else:
            item['editable'] = 'false'
        if hide:
            new_hidden_columns[prefix] = 1
        column_order = 'EndOfOrder'
    item['rowspan'] = min_unuse_depth + 1
    item['colspan'] = colspan
    column_dict[prefix] = item
    return (colspan, column_order)",prefix != '',233,not prefix.startswith('_'),False,4.767707020457095,N/A
"def merge(a, b, path=None, use_b=True):
    """"""merges b into a""""""
<mask>:
        path = []
    for key in b:
        if key in a:
            if isinstance(a[key], dict) and isinstance(b[key], dict):
                merge(a[key], b[key], path + [str(key)])
            elif use_b:
                a[key] = b[key]
        else:
            a[key] = b[key]
    return a",path is None,43,path is None,True,100.00000000000004,N/A
"def prepare_incremental_data(logs, new_logs, field_columns, filter_condition=None, ignore_not_exist=False):
    """"""

    :param logs: {'id':dict, ...}, 之前的数据, flatten, dict.
    :param new_logs: List[dict,], 新读取到的数据, nested。包含了新增加的log以及更新的log
    :param field_columns: {field:1}, 在前端table显示的内容field. 只用抽取这些field即可
    :param filter_condition: {}, 用于过滤不需要的内容
    :param ignore_not_exist: bool, 是否删除过滤条件不存在的log
    :return: {'new_logs':[dict(), dict()...], 'update_logs':[dict(), dict()...]}
    """"""
<mask>:
        filter_condition = {}
    new_dict = {}
    log_filter = LogFilter(filter_condition)
    for log in new_logs:
        flat_dict = flatten_dict('', log, connector='-')
        _filter = log_filter._filter_this_log_or_not(flat_dict, ignore_not_exist)
        if not _filter:
            new_dict[log['id']] = flat_dict
    updated_logs = []
    keys = list(new_dict.keys())
    for key in keys:
        if key in logs:
            value = new_dict.pop(key)
            log = merge(logs[key], value, use_b=True)
            updated_logs.append(log)
        else:
            new_dict[key]['memo'] = 'Click to edit'
    new_logs = list(new_dict.values())
    for key, value in new_dict.items():
        logs[key] = value
    return (new_logs, updated_logs)",filter_condition is None,110,filter_condition is None,True,100.00000000000004,N/A
"def check_uuid_summary(gold_uuid, _uuid):
<mask>:
        return None
    else:
        return {'status': 'fail', 'msg': 'It seems like your page is out-of-date, please refresh.'}",gold_uuid == _uuid,20,gold_uuid == _uuid,True,100.00000000000004,N/A
"def read_summary(root_log_dir: str, summary_name):
    """"""

    :param root_log_dir: 存放logs的地方。
    :param summary_name: str，summary的名称
    :return: {}不包含summary_name
    """"""
    summary_fp = os.path.join(root_log_dir, 'summaries', summary_name + '.summary')
    summary = {}
<mask>:
        with open(summary_fp, 'r', encoding='utf-8') as f:
            summary = json.loads(f.readline())
    return summary",os.path.exists(summary_fp),36,os.path.exists(summary_fp),True,100.00000000000004,N/A
"def _get_all_summuries(root_log_dir: str):
    summary_dir = os.path.join(root_log_dir, 'summaries')
    summary_names = []
<mask>:
        filenames = os.listdir(summary_dir)
        for filename in filenames:
            if filename.endswith('.summary'):
                summary_names.append(os.path.splitext(filename)[0])
    return summary_names",os.path.exists(summary_dir),23,os.path.isdir(summary_dir),False,65.80370064762461,N/A
"def delete_summary(root_log_dir, summary_name):
    """"""
    删除summary
    :param root_log_dir:
    :param summary_name:
    :return:
    """"""
    summary_dir = os.path.join(root_log_dir, 'summaries')
    try:
        fp = os.path.join(summary_dir, summary_name + '.summary')
<mask>:
            os.remove(fp)
        return True
    except Exception as e:
        print(_colored_string('Error happens when delete summary {}.'.format(summary_name), 'red'))
        print(e)
        import traceback
        traceback.print_exc()
        return False",os.path.exists(fp),43,os.path.exists(fp),True,100.00000000000004,N/A
"def read_logs(log_name, root_log_dir, extra_data=None):
    log_reader = LogReader()
<mask>:
        config_names = _get_config_names(root_log_dir)
        if config_names.index(log_name) == -1:
            return {'status': 'fail', 'msg': 'There is no config named {}.'.format(log_name)}
        logs, configs, extra_data = get_log_and_extra_based_on_config(log_reader, root_log_dir, log_name)
    elif isinstance(log_name, list):
        log_names = log_name
        log_reader.set_log_dir(root_log_dir)
        logs = log_reader.read_certain_logs(log_names)
        if len(logs) != len(log_names):
            not_found_log = set(log_names) - set([log['id'] for log in logs])
            for log in not_found_log:
                if log not in extra_data:
                    print(_colored_string('The following logs are not found {}.'.format(list(not_found_log)), 'blue'))
        if extra_data == None:
            extra_data = {}
        extra_log_dict = {key: value for key, value in zip(list(extra_data.keys()), expand_dict(list(extra_data.values()), connector='-'))}
        all_logs = []
        for log in logs:
            if log['id'] in extra_log_dict:
                extra_log = extra_log_dict[log['id']]
                log = merge_use_b(log, extra_log, use_b=True)
                all_logs.append(log)
            else:
                all_logs.append(log)
        for key, value in extra_log_dict.items():
            if 'id' in value and key in log_name:
                all_logs.append(value)
        logs = all_logs
    else:
        return {'status': 'fail', 'msg': 'Unknown data source.'}
    filtered_logs = logs
    return filtered_logs","isinstance(log_name, str)",142,"isinstance(log_name, str)",True,100.00000000000004,N/A
"def read_server_config(config_path):
    """"""
    给定config的path，读取里面的config。如果config不存在，则按照默认的值创建
    :param config_path: str
    :return: dict, config的内容
    """"""
    config = ConfigParser(allow_no_value=True)
<mask>:
        config_dir = os.path.dirname(config_path)
        if not os.path.isdir(config_dir):
            os.makedirs(config_dir)
        _read_default_config(config)
        with open(config_path, 'w', encoding='utf-8') as f:
            config.write(f)
    else:
        config.read(config_path, encoding='utf-8')
        check_config(config)
    configs = {}
    settings = {}
    get_dict_from_config(config, 'frontend_settings', settings, 'bool')
    settings = {key.capitalize(): value for key, value in settings.items()}
    configs['settings'] = settings
    basic_settings = {}
    get_dict_from_config(config, 'basic_settings', basic_settings)
    for key in ['str_max_length', 'round_to']:
        basic_settings[key] = int(basic_settings[key])
    for key in ['ignore_unchanged_columns']:
        basic_settings[key] = basic_settings[key] == 'True'
    configs['basic_settings'] = basic_settings
    hidden_logs = read_list_from_config(config, 'data_settings', 'hidden_logs', ',')
    configs['hidden_rows'] = {log: 1 for log in hidden_logs}
    deleted_rows = read_list_from_config(config, 'data_settings', 'deleted_logs', ',')
    configs['deleted_rows'] = {log: 1 for log in deleted_rows}
    if len(config.get('data_settings', 'filter_condition')) != 0:
        configs['filter_condition'] = json.loads(config.get('data_settings', 'filter_condition').replace(""'"", '""'))
        for key in list(configs['filter_condition'].keys()):
            delete = False
            if not isinstance(configs['filter_condition'][key], (str, list, numbers.Number)):
                print(_colored_string('Unsupported type found in filter_condition in `{}`.'.format(key), 'red'))
                delete = True
            if isinstance(configs['filter_condition'][key], list):
                for value in configs['filter_condition'][key]:
                    if not isinstance(value, (str, numbers.Number)):
                        print(_colored_string('Unsupported type found in filter_condition in `{}`.'.format(key), 'red'))
                        delete = True
            if delete:
                configs['filter_condition'].pop(key)
    else:
        configs['filter_condition'] = {}
    hidden_columns = read_list_from_config(config, 'column_settings', 'hidden_columns', ',')
    configs['hidden_columns'] = {column: 1 for column in hidden_columns}
    exclude_columns = read_list_from_config(config, 'column_settings', 'exclude_columns', ',')
    configs['exclude_columns'] = {column: 1 for column in exclude_columns}
    editable_columns = read_list_from_config(config, 'column_settings', 'editable_columns', ',')
    configs['editable_columns'] = {column: 1 for column in editable_columns}
    column_order = config.get('column_settings', 'column_order')
    if column_order != '':
        column_order = json.loads(column_order)
    else:
        column_order = {}
    configs['column_order'] = column_order
    configs['config'] = config
    _dict = {}
    chart_exclude_columns = read_list_from_config(config, 'chart_settings', 'chart_exclude_columns', ',')
    _dict['chart_exclude_columns'] = {column: 1 for column in chart_exclude_columns}
    _dict['max_points'] = config.getint('chart_settings', 'max_points')
    _dict['update_every'] = config.getint('chart_settings', 'update_every')
    _dict['max_no_updates'] = config.getint('chart_settings', 'max_no_updates')
    configs['chart_settings'] = _dict
    _dict = {}
    if 'multi_chart_settings' in config:
        _dict['max_compare_metrics'] = config.getint('multi_chart_settings', 'max_compare_metrics')
    else:
        _dict['max_compare_metrics'] = 10
        config.add_section('multi_chart_settings')
        config.set('multi_chart_settings', 'max_compare_metrics', 10)
    configs['multi_chart_settings'] = _dict
    return configs",not os.path.exists(config_path),293,os.path.isfile(config_path),False,59.54165059120785,N/A
"def save_config(all_data, config_path):
    config = all_data['config']
<mask>:
        config_dir = os.path.dirname(config_path)
        if not os.path.isdir(config_dir):
            os.makedirs(config_dir)
    settings = all_data['settings']
    settings = {key.replace(' ', '_'): value for key, value in settings.items()}
    save_dict_to_config(config, 'frontend_settings', settings)
    basic_settings = all_data['basic_settings']
    save_dict_to_config(config, 'basic_settings', basic_settings)
    hidden_logs = all_data['hidden_rows'].keys()
    config.set('data_settings', 'hidden_logs', ','.join(hidden_logs))
    deleted_logs = all_data['deleted_rows'].keys()
    config.set('data_settings', 'deleted_logs', ','.join(deleted_logs))
    if len(all_data['filter_condition']) != 0:
        filter_condition = json.dumps(all_data['filter_condition'])
    else:
        filter_condition = ''
    config.set('data_settings', 'filter_condition', filter_condition)
    for option in ['hidden_columns', 'editable_columns']:
        save_list_to_config(config, 'column_settings', option, all_data[option])
    column_order = refine_column_order(all_data['column_order'])
    column_order = json.dumps(column_order)
    config.set('column_settings', 'column_order', column_order)
    with open(config_path, 'w', encoding='utf-8') as f:
        config.write(f)",not os.path.exists(config_path),88,config_path,False,6.948345122280157,N/A
"def refine_column_order(column_order):
    new_column_order = {}
<mask>:
        keys = column_order['OrderKeys']
        for key in keys:
            value = column_order[key]
            child = refine_column_order(value)
            if len(child) == 0:
                new_column_order[key] = 'EndOfOrder'
            else:
                new_column_order[key] = child
    return new_column_order",'OrderKeys' in column_order,32,'OrderKeys' in column_order,True,100.00000000000004,N/A
"def save_list_to_config(config, section, option, container, sep=','):
<mask>:
        config.add_section(section)
    if len(container) == 0:
        str_ = ''
    else:
        str_ = sep.join(container.keys())
    config.set(section, option, str_)",not config.has_section(section),22,not config.has_section(section),True,100.00000000000004,N/A
"def save_dict_to_config(config, section, container):
<mask>:
        config.add_section(section)
    for key, value in container.items():
        config.set(section, key, str(value))",not config.has_section(section),14,not config.has_section(section),True,100.00000000000004,N/A
"def flatten_dict(prefix, _dict, connector='-'):
    """"""
    给定一个dict, 将其展平，比如{""a"":{""v"": 1}} -> {""a-v"":1}

    :param prefix:
    :param _dict:
    :param connector:
    :return:
    """"""
    new_dict = {}
    for key, value in _dict.items():
<mask>:
            new_prefix = prefix + connector + str(key)
        else:
            new_prefix = str(key)
        if isinstance(value, dict):
            new_dict.update(flatten_dict(new_prefix, value, connector))
        else:
            new_dict[new_prefix] = value
    return new_dict",prefix != '',50,prefix,False,4.9787068367863965,N/A
"def stringify_dict_key(_dict):
    """"""
    保证_dict中所有key为str类型
    :param _dict:
    :return:
    """"""
    for key, value in _dict.copy().items():
<mask>:
            value = stringify_dict_key(value)
        if not isinstance(key, str):
            del _dict[key]
            _dict[str(key)] = value
    return _dict","isinstance(value, dict)",28,"isinstance(value, dict)",True,100.00000000000004,N/A
"def replace_nan_inf(data, round_to=6):
<mask>:
        for d in data:
            _replace_nan_inf(d, round_to=round_to)
    elif isinstance(data, dict):
        _replace_nan_inf(data, round_to=round_to)
    else:
        raise TypeError('Unsupported type.')
    return data","isinstance(data, list)",21,"isinstance(data, list)",True,100.00000000000004,N/A
"def _replace_nan_inf(d, round_to=6):
    for k, value in d.items():
<mask>:
            _replace_nan_inf(value, round_to=round_to)
        elif isinstance(value, list):
            for d in value:
                _replace_nan_inf(d, round_to=round_to)
        elif isinstance(value, float):
            d[k] = round(value, round_to)
        elif value == float('inf'):
            d[k] = 'Infinity'
        elif value == float('-inf'):
            d[k] = '-Infinity'
        elif str(value) == 'nan':
            d[k] = 'NaN'","isinstance(value, dict)",48,"isinstance(value, int)",False,53.7284965911771,N/A
"def check_uuid(gold_uuid, _uuid):
<mask>:
        return None
    else:
        return {'status': 'fail', 'msg': 'The data are out-of-date, please refresh this page. Or, you can set this page as Offline to stop sending updates to the server.'}",gold_uuid == _uuid,34,gold_uuid == _uuid,True,100.00000000000004,N/A
"def __init__(self, save_log_dir, uuid, round_to=6, max_steps=400, wait_seconds=60, exclude_columns=None, max_no_updates=30):
    self.reader = StandbyStepLogReader(save_log_dir, uuid, wait_seconds, max_no_updates)
    self.reader.start()
    self._save_log_dir = save_log_dir
    self.uuid = uuid
    self.max_steps = max_steps
    self.round_to = round_to
    self.path2path = {}
<mask>:
        exclude_columns = {}
    else:
        assert isinstance(exclude_columns, dict)
    self.exclude_columns = exclude_columns",exclude_columns is None,42,exclude_columns is None,True,100.00000000000004,N/A
"def read_single_update(self, filepaths, ranges):
    steps = self.reader.read_update_single_log(filepaths, ranges)
    data = {}
    for key, values in steps.items():
<mask>:
            path2path = self.path2path[key]
        else:
            path2path = self._add_path2path(key, values[0])
        expanded_values = defaultdict(list)
        for v in values:
            expand_v = {}
            real_v = v[key]
            if not isinstance(real_v, dict):
                real_v = {key: real_v}
            for _key in ['step', 'epoch']:
                if _key in v:
                    expand_v[_key] = v[_key]
            real_v.pop('step', None)
            real_v.pop('epoch', None)
            _flat_v = flatten_dict('', real_v)
            for i_key, i_value in _flat_v.items():
                if isinstance(i_value, str):
                    try:
                        i_value = float(i_value)
                    except:
                        continue
                if isinstance(i_value, (float, int)):
                    if i_key not in path2path:
                        path2path = self._add_path2path(key, real_v)
                    short_i_key = path2path[i_key]
                    if short_i_key in self.exclude_columns:
                        continue
                    i_value = round(i_value, self.round_to)
                    i_expand_v = expand_v.copy()
                    i_expand_v['name'] = short_i_key
                    i_expand_v['value'] = i_value
                    expanded_values[short_i_key].append(i_expand_v)
        l_expanded_values = []
        for i_key in list(expanded_values.keys()):
            i_value = expanded_values[i_key]
            l_expanded_values.extend(i_value)
        data[key] = l_expanded_values
    return data",key in self.path2path,131,key in self.path2path,True,100.00000000000004,N/A
"def update_logs(self, only_once=False):
    steps = self.reader.read_update(only_once)
    data = {}
    for key, values in steps.items():
<mask>:
            if key in self.path2path:
                path2path = self.path2path[key]
            else:
                path2path = self._add_path2path(key, values[0])
            expanded_values = defaultdict(list)
            for v in values:
                expand_v = {}
                real_v = v[key]
                if not isinstance(real_v, dict):
                    real_v = {key: real_v}
                for _key in ['step', 'epoch']:
                    if _key in v:
                        expand_v[_key] = v[_key]
                real_v.pop('step', None)
                real_v.pop('epoch', None)
                _flat_v = flatten_dict('', real_v)
                for i_key, i_value in _flat_v.items():
                    if isinstance(i_value, str):
                        try:
                            i_value = float(i_value)
                        except:
                            continue
                    if isinstance(i_value, (float, int)):
                        if i_key not in path2path:
                            path2path = self._add_path2path(key, real_v)
                        short_i_key = path2path[i_key]
                        if short_i_key in self.exclude_columns:
                            continue
                        i_value = round(i_value, self.round_to)
                        i_expand_v = expand_v.copy()
                        i_expand_v['name'] = short_i_key
                        i_expand_v['value'] = i_value
                        expanded_values[short_i_key].append(i_expand_v)
            l_expanded_values = []
            for i_key in list(expanded_values.keys()):
                i_value = expanded_values[i_key]
                if len(i_value) > self.max_steps:
                    l_expanded_values.extend(i_value[-self.max_steps:])
                else:
                    l_expanded_values.extend(i_value)
            data[key] = l_expanded_values
        else:
            data[key] = values
    return data",key != 'finish' and key != 'total_steps',143,"isinstance(values, list)",False,0.0,N/A
"def _get_dict_path(_dict, paths=None):
<mask>:
        paths = []
    else:
        paths = paths.copy()
    new_paths = []
    for key, value in _dict.items():
        if isinstance(value, dict):
            _paths = _get_dict_path(value, paths + [key])
            new_paths.extend(_paths)
        else:
            new_paths.append(paths + [key])
    return new_paths",paths == None,35,paths is None,False,24.840753130578644,N/A
"def _refine_path(paths):
    """"""
    给定list的path，将公共的部分删掉一些. 这里只处理完全一样深度的metric. 主要为了删除相同的metric_name
        [['metric', 'BMESF1MEtric', 'f1'], ['metric', 'BMESF1Metric'], ...]
    :param paths:
    :return:
    """"""
<mask>:
        path2shortpath = {'-'.join(path): '-'.join(path) for path in paths}
    elif len(paths) == 0:
        path2shortpath = {'-'.join(paths[0]): paths[0][-1]}
    else:
        delete_depths = []
        for depth in range(len(paths[0])):
            names = set()
            for path in paths:
                names.add(path[depth])
            if len(names) == 1:
                delete_depths.append(depth)
        for i in range(len(paths)):
            for d in reversed(delete_depths):
                paths[i].pop(d)
        path2shortpath = {'-'.join(path): '-'.join(path) for path in paths}
    return path2shortpath","len(set(map(len, paths))) != 1",73,"isinstance(paths[0], list)",False,3.461743640369311,N/A
"def get_usage_port(start_port):
    while start_port < 65535:
<mask>:
            start_port += 1
        else:
            return start_port",net_is_used(start_port),13,start_port in sys.argv,False,15.925177647011354,N/A
"def run(self):
    while time.time() - self.deque[0] < self.server_wait_seconds and (not self._stop_flag):
        time.sleep(1)
    print('This server is going to shut down.')
    try:
<mask>:
            req = urequest.Request('http://127.0.0.1:{}/kill'.format(int(self.port)), headers={}, data=''.encode('utf-8'))
            page = urequest.urlopen(req).read().decode('utf-8')
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise RuntimeError('Error occurred when try to automatically shut down server.')",not self._stop_flag,46,self._stop_flag,False,84.64817248906144,N/A
"def run(self):
    self._quit = False
    self._start = True
    while not self._stop_flag:
<mask>:
            for _uuid in list(self.all_handlers.keys()):
                handler = self.all_handlers[_uuid]
                if handler.reader._quit:
                    handler.reader.stop()
                    handler = self.all_handlers.pop(_uuid)
                    print(f'Delete handler {_uuid}')
                    del handler
        time.sleep(0.5)
    for _uuid in list(self.all_handlers.keys()):
        handler = self.all_handlers.pop(_uuid)
        if handler.reader._quit:
            handler.reader.stop()
            print(f'Delete handler {_uuid}')
            del handler
    self._quit = True",len(self.all_handlers) > 0,49,self._start,False,7.888842466409752,N/A
"def stop(self):
    self._stop_flag = True
    count = 0
    while not self._quit:
        time.sleep(0.6)
<mask>:
            raise RuntimeError('Some bug happens.')
        count += 1",count > 3,20,count > self._quit_flag,False,11.044795567078939,N/A
"def __init__(self, section, source=None, lineno=None):
    msg = [repr(section), ' already exists']
<mask>:
        message = ['While reading from ', repr(source)]
        if lineno is not None:
            message.append(' [line {0:2d}]'.format(lineno))
        message.append(': section ')
        message.extend(msg)
        msg = message
    else:
        msg.insert(0, 'Section ')
    Error.__init__(self, ''.join(msg))
    self.section = section
    self.source = source
    self.lineno = lineno
    self.args = (section, source, lineno)",source is not None,54,source is not None,True,100.00000000000004,N/A
"def __init__(self, section, option, source=None, lineno=None):
    msg = [repr(option), ' in section ', repr(section), ' already exists']
<mask>:
        message = ['While reading from ', repr(source)]
        if lineno is not None:
            message.append(' [line {0:2d}]'.format(lineno))
        message.append(': option ')
        message.extend(msg)
        msg = message
    else:
        msg.insert(0, 'Option ')
    Error.__init__(self, ''.join(msg))
    self.section = section
    self.option = option
    self.source = source
    self.lineno = lineno
    self.args = (section, option, source, lineno)",source is not None,64,source is not None,True,100.00000000000004,N/A
"def __init__(self, source=None, filename=None):
<mask>:
        raise ValueError(""Cannot specify both `filename' and `source'. Use `source'."")
    elif not filename and (not source):
        raise ValueError(""Required argument `source' not given."")
    elif filename:
        source = filename
    Error.__init__(self, 'Source contains parsing errors: %r' % source)
    self.source = source
    self.errors = []
    self.args = (source,)",filename and source,48,filename and source,True,100.00000000000004,N/A
"def before_set(self, parser, section, option, value):
    tmp_value = value.replace('%%', '')
    tmp_value = self._KEYCRE.sub('', tmp_value)
<mask>:
        raise ValueError('invalid interpolation syntax in %r at position %d' % (value, tmp_value.find('%')))
    return value",'%' in tmp_value,29,tmp_value.find('%') != -1,False,19.338531381761715,N/A
"def _interpolate_some(self, parser, option, accum, rest, section, map, depth):
    rawval = parser.get(section, option, raw=True, fallback=rest)
<mask>:
        raise InterpolationDepthError(option, section, rawval)
    while rest:
        p = rest.find('%')
        if p < 0:
            accum.append(rest)
            return
        if p > 0:
            accum.append(rest[:p])
            rest = rest[p:]
        c = rest[1:2]
        if c == '%':
            accum.append('%')
            rest = rest[2:]
        elif c == '(':
            m = self._KEYCRE.match(rest)
            if m is None:
                raise InterpolationSyntaxError(option, section, 'bad interpolation variable reference %r' % rest)
            var = parser.optionxform(m.group(1))
            rest = rest[m.end():]
            try:
                v = map[var]
            except KeyError:
                raise InterpolationMissingOptionError(option, section, rawval, var) from None
            if '%' in v:
                self._interpolate_some(parser, option, accum, v, section, map, depth + 1)
            else:
                accum.append(v)
        else:
            raise InterpolationSyntaxError(option, section, ""'%%' must be followed by '%%' or '(', found: %r"" % (rest,))",depth > MAX_INTERPOLATION_DEPTH,121,depth < 0,False,7.253154775624655,N/A
"def _get_config_args(conf: FitlogConfig):
    """"""
    读取FitlogConfig内的超参。
    """"""
<mask>:
        conf = conf()
    config_dict = {k: conf.__getattribute__(k) for k in dir(conf) if not k.startswith('_')}
    for k, v in config_dict.items():
        if inspect.isfunction(v):
            config_dict[k] = v.__name__
    return config_dict",inspect.isclass(conf),33,callable(conf),False,36.06452879987789,N/A
"def _check_debug(func):
    """"""
    函数闭包，只有非 debug 模式才会执行原始函数
    
    :param func: 原始函数，函数的第一个参数必须为 Logger 对象
    :return: 加上闭包后的函数
    """"""

    def wrapper(*args, **kwargs):
<mask>:
            return
        else:
            return func(*args, **kwargs)
    return wrapper",args[0]._debug,25,not _logger.debug,False,9.423716574733431,N/A
"def _check_log_dir(func):
    """"""
    函数闭包，检查原始函数执行所需的条件是否满足，只有满足才会执行
    
    1 如果没有initialize, 说明还没有设置
    
    2 如果default_log_dir不为None，设置使用default_log_dir调用set_log_dir
    
    3 否则报错
    
    :param func: 原始函数，函数的第一个参数必须为 Logger 对象
    :return: 加上闭包后的函数
    """"""

    def wrapper(*args, **kwargs):
<mask>:
            args[0].set_log_dir(args[0].default_log_dir)
        elif not args[0].initialized:
            raise RuntimeError('You have to call `fitlog.set_log_dir()` to set where to save log first.')
        return func(*args, **kwargs)
    return wrapper",not args[0].initialized and args[0].default_log_dir is not None,45,args[0].default_log_dir is not None,False,54.04329964865343,N/A
"@_check_log_dir
def get_log_dir(self, absolute=False):
    """"""
        返回的是存放所有log的文件夹。例如logs/，需要调用set_log_dir()先设置log的记录文件夹

        :param bool absolute: 是否返回绝对路径
        :return:
        """"""
    log_dir = self._log_dir
<mask>:
        if log_dir:
            log_dir = os.path.abspath(log_dir)
    elif log_dir:
        log_dir = os.path.basename(log_dir)
    return log_dir",absolute,28,absolute,True,100.00000000000004,N/A
"@_check_log_dir
def get_log_folder(self, absolute=False):
    """"""
        返回实际保存log的文件夹，类似log_20200406_055218/这种

        :param bool absolute: 是否返回绝对路径
        :return:
        """"""
    log_dir = self._save_log_dir
<mask>:
        if log_dir:
            log_dir = os.path.abspath(log_dir)
    elif log_dir:
        log_dir = os.path.basename(log_dir)
    return log_dir",absolute,28,absolute,True,100.00000000000004,N/A
"def set_log_dir(self, log_dir: str):
    """"""
        设置 log 的存放位置
        """"""
<mask>:
        raise RuntimeError('`{}` is not a valid directory.'.format(log_dir))
    empty = True
    for _dir in os.listdir(log_dir):
        if is_dirname_log_record(os.path.join(log_dir, _dir)):
            empty = False
    if empty:
        raise RuntimeError('`{}` has no valid logs. You should run your program first.'.format(log_dir))
    self._log_dir = log_dir
    self._line_counter.clear()",not os.path.isdir(log_dir),48,not os.path.isdir(log_dir),True,100.00000000000004,N/A
"def read_logs(self, ignore_log_names: dict=None) -> List[dict]:
    """"""
        从日志存放路径读取日志. 只会读取有更新的log

        :param ignore_log_names: 如果包含在这个里面，就不会读取该log
        :return: 如果有内容或者有更新的内容，则返回一个 list，里面每个元素都是nested的dict.

            .. code::

                [{
                    'id':
                    'metric': {nested dict},
                    'meta': {},
                    ...
                },{
                }]
        """"""
    assert self._log_dir is not None, 'You have to set log_dir first.'
<mask>:
        ignore_log_names = {}
    dirs = os.listdir(self._log_dir)
    logs = []
    for _dir in dirs:
        if _dir in ignore_log_names:
            continue
        dir_path = os.path.join(self._log_dir, _dir)
        if is_dirname_log_record(dir_path):
            _dict, file_stats = _read_save_log(dir_path, self._ignore_null_loss_or_metric, self._line_counter[_dir])
            if len(_dict) != 0:
                logs.append({'id': _dir, **_dict})
                self._line_counter[_dir] = file_stats
    return logs",ignore_log_names is None,82,ignore_log_names is None,True,100.00000000000004,N/A
"def read_certain_logs(self, log_dir_names):
    """"""
        给定log的名称，只读取对应的log

        :param log_dir_names list[str]: log的名称
        :return: [{}, {}], nested的log
        """"""
    assert self._log_dir is not None, 'You have to set log_dir first.'
    logs = []
    for _dir in log_dir_names:
        dir_path = os.path.join(self._log_dir, _dir)
<mask>:
            _dict, file_stats = _read_save_log(dir_path, self._ignore_null_loss_or_metric, self._line_counter[_dir])
            if len(_dict) != 0:
                logs.append({'id': _dir, **_dict})
    return logs",is_dirname_log_record(dir_path),52,not os.path.exists(dir_path),False,32.03558799120807,N/A
"def _read_save_log(_save_log_dir: str, ignore_null_loss_or_metric: bool=True, file_stats: dict=None):
    """"""
    给定一个包含metric.log, hyper.log, meta.log以及other.log的文件夹，返回一个包含数据的dict. 如果为null则返回空字典
    不读取loss.log, 因为里面的内容对table无意义。
    
    :param _save_log_dir: 日志存放的目录， 已经最后一级了，即该目录下应该包含metric.log等了
    :param ignore_null_loss_or_metric: 是否忽略metric和loss都为空的文件
    :param file_stats::
    
            {
                'meta.log': [current_line, last_modified_time],
                'hyper.log':[], 'metric.log':[], 'other.log':[]
            }
            
    :return:
        _dict: {'metric': {nested dict}, 'loss': {} }
        file_stats: {'meta.log': [current_line, last_modified_time],
                     'metric.log': [, ]} # 只包含有更新的文件的内容
    """"""
    try:
        filenames = ['meta.log', 'hyper.log', 'best_metric.log', 'other.log']
<mask>:
            file_stats = {}
        for filename in filenames:
            if filename not in file_stats:
                file_stats[filename] = [-1, -1]
        _dict = {}

        def _is_file_empty(fn):
            empty = True
            fp = os.path.join(_save_log_dir, fn)
            if os.path.exists(fp):
                with open(fp, 'r', encoding='utf-8') as f:
                    for line in f:
                        if len(line.strip()) != 0:
                            empty = False
                            break
            return empty
        if os.path.exists(os.path.join(_save_log_dir, 'metric.log')) and (not os.path.exists(os.path.join(_save_log_dir, 'best_metric.log'))):
            with open(os.path.join(_save_log_dir, 'metric.log'), 'r', encoding='utf-8') as f, open(os.path.join(_save_log_dir, 'best_metric.log'), 'w', encoding='utf-8') as f2:
                for line in f:
                    if not line.startswith('S'):
                        best_line = line
                        f2.write(best_line)
        empty = _is_file_empty('best_metric.log') and _is_file_empty('loss.log') and _is_file_empty('metric.log')
        if empty and ignore_null_loss_or_metric:
            return (_dict, file_stats)
        for filename in filenames:
            filepath = os.path.join(_save_log_dir, filename)
            last_modified_time = os.path.getmtime(filepath)
            if file_stats[filename][1] == last_modified_time:
                continue
            file_stats[filename][1] = last_modified_time
            start_line = file_stats[filename][0]
            __dict, end_line = _read_nonstep_log_file(filepath, start_line)
            file_stats[filename][0] = end_line
            _dict = merge(_dict, __dict, use_b=False)
    except Exception as e:
        print('Exception raised when read {}'.format(os.path.abspath(_save_log_dir)))
        print(repr(e))
        raise e
    return (_dict, file_stats)",file_stats is None,202,ignore_null_loss_or_metric,False,4.767707020457095,N/A
"def is_log_dir_has_step(_save_log_dir: str, check_files=('metric.log', 'loss.log')) -> bool:
    """"""
    给定log_dir, 判断是否有step数据
    
    :param _save_log_dir 日志存放的目录
    :param check_files: 检查file是否含有step
    :return: 是否有step数据
    """"""
<mask>:
        return False
    try:
        filenames = check_files
        for filename in filenames:
            filepath = os.path.join(_save_log_dir, filename)
            if not os.path.exists(filepath):
                continue
            with open(filepath, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.startswith('S'):
                        return True
    except Exception as e:
        traceback.print_exc()
        print('Exception raised when read {}'.format(os.path.abspath(filepath)))
    return False",not is_dirname_log_record(_save_log_dir),64,not check_files,False,0.9457497807469653,N/A
"def _colored_string(string: str, color: str or int) -> str:
    """"""在终端中显示一串有颜色的文字

    :param string: 在终端中显示的文字
    :param color: 文字的颜色
    :return:
    """"""
<mask>:
        color = {'black': 30, 'red': 31, 'green': 32, 'yellow': 33, 'blue': 34, 'purple': 35, 'cyan': 36, 'white': 37}[color]
    return '\x1b[%dm%s\x1b[0m' % (color, string)","isinstance(color, str)",42,"isinstance(color, str)",True,100.00000000000004,N/A
"def _find_config_file(self, run_file_path: str=None, cli: bool=True) -> str:
    """"""

        :param run_file_path: 执行 commit 操作文件的目录
        :param cli: 是否在命令行内执行。如果在命令行中执行，则对用户进行提示
        :return: 返回 work_dir ，同时在 self.config_file_path 中存储配置文件路径
        """"""
    config_file_name = '.fitconfig'
<mask>:
        path = os.getcwd()
    else:
        path = os.path.abspath(run_file_path)
    home_path = os.path.expanduser('~')
    root_path = os.path.abspath('/')
    depth_cnt = 0
    max_depth = 8
    while 1:
        depth_cnt += 1
        if max_depth == depth_cnt:
            if cli:
                print(_colored_string('Folder depth out of limitation (max_depth=8).', 'red'))
                print(_colored_string(""Can not find the config file '{}'"".format(config_file_name), 'red'))
            return 'Error'
        if os.path.isfile(os.path.join(path, config_file_name)):
            self.config_file_path = os.path.join(path, config_file_name)
            self.work_dir = path
            return path
        if os.path.isfile(os.path.join(path, '.fitlog', config_file_name)):
            self.config_file_path = os.path.join(path, '.fitlog', config_file_name)
            self.work_dir = path
            return path
        if path == home_path or path == root_path:
            if cli:
                print(_colored_string('Reach the root directory or home directory.', 'red'))
                print(_colored_string(""Can not find the config file '{}'"".format(config_file_name), 'red'))
            return 'Error'
        path = os.path.dirname(path)",run_file_path is None,132,run_file_path is None,True,100.00000000000004,N/A
"def _read_config(self):
    """"""在获取配置文件路径后，读取其中的配置。采取保守策略，遇到错误自动跳过。

        :return:
        """"""
    config = configparser.ConfigParser()
    config.read(self.config_file_path)
    self.config = config
<mask>:
        if 'watched_rules' in config['fit_settings']:
            tmp = config['fit_settings']['watched_rules']
            self.watched_rules = [each.strip() for each in tmp.split(',') if len(each) > 1]",'fit_settings' in config,31,'fit_settings' in config,True,100.00000000000004,N/A
"@staticmethod
def _switch_to_fast_git(work_dir: str):
    """"""将工作目录从通常的 git 模式切换成 fastgit 模式

        :param work_dir: 工作目录的绝对路径
        """"""
    fitlog_path, git_path, git_backup_path, gitignore_path, gitignore_backup_path = Committer._get_path_names(work_dir)
<mask>:
        shutil.move(git_path, git_backup_path)
    if os.path.isfile(gitignore_path):
        shutil.move(gitignore_path, gitignore_backup_path)
    if os.path.exists(fitlog_path):
        shutil.move(fitlog_path, git_path)
    if os.path.isfile(os.path.join(git_path, '.gitignore')):
        shutil.move(os.path.join(git_path, '.gitignore'), work_dir)",os.path.exists(git_path),37,os.path.exists(git_path),True,100.00000000000004,N/A
"@staticmethod
def _switch_to_standard_git(work_dir: str):
    """"""将工作目录从 fastgit 模式切换成通常的 git 模式

        :param work_dir: 工作目录的绝对路径
        """"""
    fitlog_path, git_path, git_backup_path, gitignore_path, gitignore_backup_path = Committer._get_path_names(work_dir)
<mask>:
        shutil.move(git_path, fitlog_path)
    if os.path.exists(gitignore_path):
        shutil.move(gitignore_path, os.path.join(fitlog_path, ''))
    if os.path.exists(git_backup_path):
        shutil.move(git_backup_path, git_path)
    if os.path.isfile(gitignore_backup_path):
        shutil.move(gitignore_backup_path, gitignore_path)",os.path.exists(git_path),36,os.path.exists(git_path),True,100.00000000000004,N/A
"def __init__(self, language=None):
    """"""Constructor for RawTextReader.

        Args:
            language (str): language of text to process.
        """"""
    self.language = language
<mask>:
        self.language = 'en'
    if len(self.language) != 2:
        raise ValueError(""`language` is '{}', but should be an iso2 language code ('en' instead of 'english')"".format(self.language))",language is None,41,self.language == 'english',False,8.116697886877475,N/A
"def read(self, text, spacy_model=None):
    """"""Read the input file and use spacy to pre-process.

        Spacy model selection: By default this function will load the spacy
        model that is closest to the `language` parameter ('fr' language will
        load the spacy model linked to 'fr' or any 'fr_core_web_*' available
        model). In order to select the model that will be used please provide a
        preloaded model via the `spacy_model` parameter, or link the model you
        wish to use to the corresponding language code
        `python3 -m spacy link spacy_model lang_code`.

        Args:
            text (str): raw text to pre-process.
            spacy_model (model): an already loaded spacy model.
        """"""
    nlp = spacy_model
<mask>:
        installed_models = [m for m in spacy.util.get_installed_models() if m[:2] == self.language]
        if len(installed_models):
            nlp = spacy.load(installed_models[0], disable=['ner', 'textcat', 'parser'])
        else:
            excp_msg = ""No downloaded spacy model for '{}' language."".format(self.language)
            excp_msg += '\nA list of downloadable spacy models is available at https://spacy.io/models.'
            excp_msg += '\nAlternatively, preprocess your document as a list of sentence tuple (word, pos), such as:'
            excp_msg += ""\n\t[[('The', 'DET'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('.', 'PUNCT')]]""
            raise Exception(excp_msg)
        nlp.add_pipe('sentencizer')
    nlp.tokenizer.infix_finditer = infix_re.finditer
    spacy_doc = nlp(text)
    sentences = []
    for sentence_id, sentence in enumerate(spacy_doc.sents):
        sentences.append(Sentence(words=[token.text for token in sentence], pos=[token.pos_ or token.tag_ for token in sentence], meta={'lemmas': [token.lemma_ for token in sentence], 'char_offsets': [(token.idx, token.idx + len(token.text)) for token in sentence]}))
    return sentences",nlp is None,217,nlp is None,True,100.00000000000004,N/A
"def compute_document_frequency(documents, output_file, language='en', stoplist=None, normalization='stemming', delimiter='\t', n=3):
    """"""Compute the n-gram document frequencies from a set of input documents.
    An extra row is added to the output file for specifying the number of
    documents from which the document frequencies were computed
    (--NB_DOC-- tab XXX). The output file is compressed using gzip.

    Args:
        documents (list): list of pke-readable documents.
        output_file (str): the output file.
        language (str): language of the input documents (used for computing the
            n-stem or n-lemma forms), defaults to 'en' (english).
        stoplist (list): the stop words for filtering n-grams, default to
            pke.lang.stopwords[language].
        normalization (str): word normalization method, defaults to
            'stemming'. Other possible value is 'none' for using word surface
            forms instead of stems/lemmas.
        delimiter (str): the delimiter between n-grams and document
            frequencies, defaults to tabulation (	).
        n (int): the size of the n-grams, defaults to 3.
    """"""
    frequencies = defaultdict(int)
    nb_documents = 0
    for document in documents:
        doc = LoadFile()
        doc.load_document(input=document, language=language, stoplist=stoplist, normalization=normalization)
        doc.ngram_selection(n=n)
        doc.candidate_filtering()
        for lexical_form in doc.candidates:
            frequencies[lexical_form] += 1
        nb_documents += 1
<mask>:
            logging.info('{} docs, memory used: {} mb'.format(nb_documents, sys.getsizeof(frequencies) / 1024 / 1024))
    if os.path.dirname(output_file):
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with gzip.open(output_file, 'wt', encoding='utf-8') as f:
        first_line = '--NB_DOC--' + delimiter + str(nb_documents)
        f.write(first_line + '\n')
        for ngram in frequencies:
            line = ngram + delimiter + str(frequencies[ngram])
            f.write(line + '\n')",nb_documents % 1000 == 0,214,len(frequencies) > 1024,False,0.0,N/A
"def train_supervised_model(documents, references, model_file, language='en', stoplist=None, normalization='stemming', df=None, model=None, leave_one_out=False):
    """"""Build a supervised keyphrase extraction model from a set of documents
    and reference keywords.

    Args:
        documents (list): list of tuple (id, pke-readable documents). `id`s
            should match the one in reference.
        references (dict): reference keywords.
        model_file (str): the model output file.
        language (str): language of the input documents (used for computing the
            n-stem or n-lemma forms), defaults to 'en' (english).
        stoplist (list): the stop words for filtering n-grams, default to
            pke.lang.stopwords[language].
        normalization (str): word normalization method, defaults to 'stemming'.
            Other possible values are 'lemmatization' or 'None' for using word
            surface forms instead of stems/lemmas.
        df (dict): df weights dictionary.
        model (object): the supervised model to train, defaults to Kea.
        leave_one_out (bool): whether to use a leave-one-out procedure for
            training, creating one model per input, defaults to False.
    """"""
    training_instances = []
    training_classes = []
    masks = {}
    for doc_id, document in documents:
        model.__init__()
        model.load_document(input=document, language=language, stoplist=stoplist, normalization=normalization)
        model.candidate_selection()
<mask>:
            continue
        model.feature_extraction(df=df, training=True)
        masks[doc_id] = [len(training_classes)]
        for candidate in model.instances:
            if candidate in references[doc_id]:
                training_classes.append(1)
            else:
                training_classes.append(0)
            training_instances.append(model.instances[candidate])
        masks[doc_id].append(len(training_classes))
    if not leave_one_out:
        logging.info('writing model to {}'.format(model_file))
        model.train(training_instances=training_instances, training_classes=training_classes, model_file=model_file)
    else:
        logging.info('leave-one-out training procedure')
        for doc_id in masks:
            logging.info('writing model to {}'.format(doc_id))
            ind = masks[doc_id]
            fold = training_instances[:ind[0]] + training_instances[ind[1]:]
            gold = training_classes[:ind[0]] + training_classes[ind[1]:]
            model.train(training_instances=fold, training_classes=gold, model_file='{}.{}.pickle'.format(model_file, doc_id))",not len(model.candidates),216,model is None,False,7.253154775624655,N/A
"def load_references(input_file, sep_doc_id=':', sep_ref_keyphrases=',', normalize_reference=False, language='en', encoding=None, excluded_file=None):
    """"""Load a reference file. Reference file can be either in json format or in
    the SemEval-2010 official format.

    Args:
        input_file (str): path to the reference file.
        sep_doc_id (str): the separator used for doc_id in reference file,
            defaults to ':'.
        sep_ref_keyphrases (str): the separator used for keyphrases in
            reference file, defaults to ','.
        normalize_reference (bool): whether to normalize the reference
            keyphrases using stemming, default to False.
        language (str): language of the input documents (used for computing the
            stems), defaults to 'en' (english).
        encoding (str): file encoding, default to None.
        excluded_file (str): file to exclude (for leave-one-out
            cross-validation), defaults to None.
    """"""
    logging.info('loading reference keyphrases from {}'.format(input_file))
    references = defaultdict(list)
    with codecs.open(input_file, 'r', encoding) as f:
<mask>:
            references = json.load(f)
            for doc_id in references:
                references[doc_id] = [keyphrase for variants in references[doc_id] for keyphrase in variants]
        else:
            for line in f:
                cols = line.strip().split(sep_doc_id)
                doc_id = cols[0].strip()
                keyphrases = cols[1].strip().split(sep_ref_keyphrases)
                for v in keyphrases:
                    if '+' in v:
                        for s in v.split('+'):
                            references[doc_id].append(s)
                    else:
                        references[doc_id].append(v)
        if normalize_reference:
            langcode = langcodes.get(language.replace('en', 'xx'), 'porter')
            stemmer = SnowballStemmer(langcode)
            for doc_id in references:
                for i, keyphrase in enumerate(references[doc_id]):
                    stems = [stemmer.stem(w) for w in keyphrase.split()]
                    references[doc_id][i] = ' '.join(stems)
    if excluded_file is not None:
        if excluded_file not in references:
            logging.warning('{} is not in references'.format(excluded_file))
        else:
            logging.info('{} removed from references'.format(excluded_file))
            del references[excluded_file]
    return references",input_file.endswith('.json'),224,"isinstance(f, str)",False,4.955725306405571,N/A
"def compute_lda_model(documents, output_file, n_topics=500, language='en', stoplist=None, normalization='stemming'):
    """"""Compute a LDA model from a collection of documents. Latent Dirichlet
    Allocation is computed using sklearn module.

    Args:
        documents (str): list fo pke-readable documents.
        output_file (str): the output file.
        n_topics (int): number of topics for the LDA model, defaults to 500.
        language (str): language of the input documents, used for stop_words
            in sklearn CountVectorizer, defaults to 'en'.
        stoplist (list): the stop words for filtering words, default to
            pke.lang.stopwords[language].
        normalization (str): word normalization method, defaults to
            'stemming'. Other possible value is 'none'
            for using word surface forms instead of stems/lemmas.
    """"""
    texts = []
    for document in documents:
        doc = LoadFile()
        doc.load_document(input=document, language=language, normalization=normalization)
        text = []
        for sentence in doc.sentences:
            text.extend([sentence.stems[i] for i in range(sentence.length) if sentence.pos[i] != 'PUNCT' and sentence.pos[i].isalpha()])
        texts.append(' '.join(text))
<mask>:
        stoplist = list(stopwords.get(language))
    tf_vectorizer = CountVectorizer(stop_words=stoplist)
    tf = tf_vectorizer.fit_transform(texts)
    vocabulary = list(tf_vectorizer.get_feature_names_out())
    lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=0, learning_method='batch')
    lda_model.fit(tf)
    saved_model = (vocabulary, lda_model.components_, lda_model.exp_dirichlet_component_, lda_model.doc_topic_prior_)
    logging.info('writing LDA model to {}'.format(output_file))
    if os.path.dirname(output_file):
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with gzip.open(output_file, 'wb') as fp:
        pickle.dump(saved_model, fp)",stoplist is None,171,stoplist is None,True,100.00000000000004,N/A
"def load_document(self, input, language=None, stoplist=None, normalization='stemming', spacy_model=None):
    """"""Loads the content of a document/string/stream in a given language.

        Args:
            input (str): input.
            language (str): language of the input, defaults to 'en'.
            stoplist (list): custom list of stopwords, defaults to
                pke.lang.stopwords[language].
            normalization (str): word normalization method, defaults to
                'stemming'. Other possible value is 'none'
                for using word surface forms instead of stems/lemmas.
            spacy_model (spacy.lang): preloaded spacy model when input is a
                string.
        """"""
    self.__init__()
<mask>:
        language = 'en'
    self.language = language
    self.normalization = normalization
    if stoplist is not None:
        self.stoplist = stoplist
    else:
        try:
            self.stoplist = stopwords[self.language]
        except KeyError:
            logging.warning(""No stoplist available in pke for '{}' language."".format(self.language))
            self.stoplist = []
    if isinstance(input, spacy.tokens.doc.Doc):
        parser = SpacyDocReader()
        sents = parser.read(spacy_doc=input)
    elif isinstance(input, str):
        parser = RawTextReader(language=self.language)
        sents = parser.read(text=input, spacy_model=spacy_model)
    elif isinstance(input, list) and all((isinstance(item, list) for item in input)):
        parser = PreprocessedReader()
        sents = parser.read(list_of_sentence_tuples=input)
    else:
        raise TypeError('Cannot process input. It is neither a spacy doc, a string or a list of list of tuple: {}'.format(type(input)))
    self.sentences = sents
    if self.normalization == 'stemming':
        try:
            langcode = langcodes.get(self.language)
            if langcode == 'english':
                langcode = 'porter'
            stemmer = SnowballStemmer(langcode)
        except ValueError:
            logging.warning(""No stemmer available in pke for '{}' language -> falling back to porter stemmer."".format(self.language))
            stemmer = SnowballStemmer('porter')
        for i, sentence in enumerate(self.sentences):
            self.sentences[i].stems = [stemmer.stem(w).lower() for w in sentence.words]
    else:
        for i, sentence in enumerate(self.sentences):
            self.sentences[i].stems = [w.lower() for w in sentence.words]",language is None,229,language is None,True,100.00000000000004,N/A
"def is_redundant(self, candidate, prev, minimum_length=1):
    """"""Test if one candidate is redundant with respect to a list of already
        selected candidates. A candidate is considered redundant if it is
        included in another candidate that is ranked higher in the list.

        Args:
            candidate (str): the lexical form of the candidate.
            prev (list): the list of already selected candidates (lexical
                forms).
            minimum_length (int): minimum length (in words) of the candidate
                to be considered, defaults to 1.
        """"""
    candidate = self.candidates[candidate].lexical_form
<mask>:
        return False
    prev = [self.candidates[u].lexical_form for u in prev]
    for prev_candidate in prev:
        for i in range(len(prev_candidate) - len(candidate) + 1):
            if candidate == prev_candidate[i:i + len(candidate)]:
                return True
    return False",len(candidate) < minimum_length,109,candidate == prev,False,5.876350803261633,N/A
"def get_n_best(self, n=10, redundancy_removal=False, stemming=False):
    """"""Returns the n-best candidates given the weights.

        Args:
            n (int): the number of candidates, defaults to 10.
            redundancy_removal (bool): whether redundant keyphrases are
                filtered out from the n-best list, defaults to False.
            stemming (bool): whether to extract stems or surface forms
                (lowercased, first occurring form of candidate), default to
                False.
        """"""
    best = sorted(self.weights, key=self.weights.get, reverse=True)
<mask>:
        non_redundant_best = []
        for candidate in best:
            if self.is_redundant(candidate, non_redundant_best):
                continue
            non_redundant_best.append(candidate)
            if len(non_redundant_best) >= n:
                break
        best = non_redundant_best
    n_best = [(u, self.weights[u]) for u in best[:min(n, len(best))]]
    if not stemming:
        n_best = [(' '.join(self.candidates[u].surface_forms[0]).lower(), self.weights[u]) for u in best[:min(n, len(best))]]
    return n_best",redundancy_removal,106,redundancy_removal,True,100.00000000000004,N/A
"def longest_sequence_selection(self, key, valid_values):
    """"""Select the longest sequences of given POS tags as candidates.

        Args:
            key (func) : function that given a sentence return an iterable
            valid_values (set): the set of valid values, defaults to None.
        """"""
    self.candidates.clear()
    for i, sentence in enumerate(self.sentences):
        shift = sum([s.length for s in self.sentences[0:i]])
        seq = []
        for j, value in enumerate(key(self.sentences[i])):
<mask>:
                seq.append(j)
                if j < sentence.length - 1:
                    continue
            if seq:
                self.add_candidate(words=sentence.words[seq[0]:seq[-1] + 1], stems=sentence.stems[seq[0]:seq[-1] + 1], pos=sentence.pos[seq[0]:seq[-1] + 1], offset=shift + seq[0], sentence_id=i)
            seq = []",value in valid_values,85,value in valid_values,True,100.00000000000004,N/A
"def grammar_selection(self, grammar=None):
    """"""Select candidates using nltk RegexpParser with a grammar defining
        noun phrases (NP).

        Args:
            grammar (str): grammar defining POS patterns of NPs.
        """"""
    self.candidates.clear()
<mask>:
        grammar = '\n                NBAR:\n                    {<NOUN|PROPN|ADJ>*<NOUN|PROPN>} \n                    \n                NP:\n                    {<NBAR>}\n                    {<NBAR><ADP><NBAR>}\n            '
    chunker = RegexpParser(grammar)
    for i, sentence in enumerate(self.sentences):
        shift = sum([s.length for s in self.sentences[0:i]])
        tuples = [(str(j), sentence.pos[j]) for j in range(sentence.length)]
        tree = chunker.parse(tuples)
        for subtree in tree.subtrees():
            if subtree.label() == 'NP':
                leaves = subtree.leaves()
                first = int(leaves[0][0])
                last = int(leaves[-1][0])
                self.add_candidate(words=sentence.words[first:last + 1], stems=sentence.stems[first:last + 1], pos=sentence.pos[first:last + 1], offset=shift + first, sentence_id=i)",grammar is None,94,grammar is None,True,100.00000000000004,N/A
"def topic_clustering(self, threshold=0.74, method='average'):
    """""" Clustering candidates into topics.

            Args:
                threshold (float): the minimum similarity for clustering,
                    defaults to 0.74, i.e. more than 1/4 of stem overlap
                    similarity.
                method (str): the linkage method, defaults to average.
        """"""
<mask>:
        candidate = list(self.candidates)[0]
        self.topics.append([candidate])
        self.topic_identifiers[candidate] = 0
        return
    candidates, X = self.vectorize_candidates()
    Y = pdist(X, 'jaccard')
    Y = np.nan_to_num(Y)
    Z = linkage(Y, method=method)
    clusters = fcluster(Z, t=threshold, criterion='distance')
    for cluster_id in range(1, max(clusters) + 1):
        self.topics.append([candidates[j] for j in range(len(clusters)) if clusters[j] == cluster_id])
    for i, cluster_id in enumerate(clusters):
        self.topic_identifiers[candidates[i]] = cluster_id - 1",len(self.candidates) == 1,92,method == 'average',False,9.153013214364877,N/A
"def build_topic_graph(self):
    """""" Build the Multipartite graph. """"""
    self.graph.add_nodes_from(self.candidates.keys())
    for node_i, node_j in combinations(self.candidates.keys(), 2):
<mask>:
            continue
        weights = []
        for p_i in self.candidates[node_i].offsets:
            for p_j in self.candidates[node_j].offsets:
                len_i = len(self.candidates[node_i].lexical_form)
                len_j = len(self.candidates[node_j].lexical_form)
                gap = self.compute_gap(p_i, p_j, len_i, len_j)
                weights.append(1.0 / gap)
        if weights:
            self.graph.add_edge(node_i, node_j, weight=sum(weights))
            self.graph.add_edge(node_j, node_i, weight=sum(weights))",self.topic_identifiers[node_i] == self.topic_identifiers[node_j],51,node_i == node_j,False,6.826095093209217,N/A
"def weight_adjustment(self, alpha=1.1):
    """""" Adjust edge weights for boosting some candidates.

            Args:
                alpha (float): hyper-parameter that controls the strength of
                    the weight adjustment, defaults to 1.1.
        """"""
    weighted_edges = {}
    norm = sum([s.length for s in self.sentences])
    for variants in self.topics:
<mask>:
            continue
        offsets = [self.candidates[v].offsets[0] for v in variants]
        first = variants[offsets.index(min(offsets))]
        for start, end in self.graph.edges(first):
            boosters = []
            for v in variants:
                if v != first and self.graph.has_edge(v, end):
                    boosters.append(self.graph[v][end]['weight'])
            if boosters:
                weighted_edges[start, end] = np.sum(boosters)
    for nodes, boosters in weighted_edges.items():
        node_i, node_j = nodes
        position_i = 1.0 / (1 + self.candidates[node_i].offsets[0])
        position_i = math.exp(position_i)
        self.graph[node_j][node_i]['weight'] += boosters * alpha * position_i",len(variants) == 1,105,len(variants) < 2,False,43.01250851313264,N/A
"def candidate_weighting(self, threshold=0.74, method='average', alpha=1.1):
    """""" Candidate weight calculation using random walk.

            Args:
                threshold (float): the minimum similarity for clustering,
                    defaults to 0.25.
                method (str): the linkage method, defaults to average.
                alpha (float): hyper-parameter that controls the strength of
                    the weight adjustment, defaults to 1.1.
        """"""
<mask>:
        return
    self.topic_clustering(threshold=threshold, method=method)
    self.build_topic_graph()
    if alpha > 0.0:
        self.weight_adjustment(alpha)
    self.weights = nx.pagerank(self.graph)",not self.candidates,59,self.weight_adjustment is None,False,13.134549472120788,N/A
"def candidate_selection(self, pos=None):
    """"""Candidate selection using longest sequences of PoS.

        Args:
            pos (set): set of valid POS tags, defaults to ('NOUN', 'PROPN',
                'ADJ').
        """"""
<mask>:
        pos = {'NOUN', 'PROPN', 'ADJ'}
    self.longest_pos_sequence_selection(valid_pos=pos)",pos is None,31,pos is None,True,100.00000000000004,N/A
"def build_word_graph(self, window=2, pos=None):
    """"""Build a graph representation of the document in which nodes/vertices
        are words and edges represent co-occurrence relation. Syntactic filters
        can be applied to select only words of certain Part-of-Speech.
        Co-occurrence relations can be controlled using the distance between
        word occurrences in the document.

        As the original paper does not give precise details on how the word
        graph is constructed, we make the following assumptions from the example
        given in Figure 2: 1) sentence boundaries **are not** taken into account
        and, 2) stopwords and punctuation marks **are** considered as words when
        computing the window.

        Args:
            window (int): the window for connecting two words in the graph,
                defaults to 2.
            pos (set): the set of valid pos for words to be considered as nodes
                in the graph, defaults to ('NOUN', 'PROPN', 'ADJ').
        """"""
<mask>:
        pos = {'NOUN', 'PROPN', 'ADJ'}
    text = [(word, sentence.pos[i] in pos) for sentence in self.sentences for i, word in enumerate(sentence.stems)]
    self.graph.add_nodes_from([word for word, valid in text if valid])
    for i, (node1, is_in_graph1) in enumerate(text):
        if not is_in_graph1:
            continue
        for j in range(i + 1, min(i + window, len(text))):
            node2, is_in_graph2 = text[j]
            if is_in_graph2 and node1 != node2:
                self.graph.add_edge(node1, node2)",pos is None,196,pos is None,True,100.00000000000004,N/A
"def candidate_weighting(self, window=2, pos=None, top_percent=None, normalized=False):
    """"""Tailored candidate ranking method for TextRank. Keyphrase candidates
        are either composed from the T-percent highest-ranked words as in the
        original paper or extracted using the `candidate_selection()` method.
        Candidates are ranked using the sum of their (normalized?) words.

        Args:
            window (int): the window for connecting two words in the graph,
                defaults to 2.
            pos (set): the set of valid pos for words to be considered as nodes
                in the graph, defaults to ('NOUN', 'PROPN', 'ADJ').
            top_percent (float): percentage of top vertices to keep for phrase
                generation.
            normalized (False): normalize keyphrase score by their length,
                defaults to False.
        """"""
<mask>:
        pos = {'NOUN', 'PROPN', 'ADJ'}
    self.build_word_graph(window=window, pos=pos)
    w = nx.pagerank(self.graph, alpha=0.85, tol=0.0001, weight=None)
    if top_percent is not None:
        logging.warning('Candidates are generated using {}-top'.format(top_percent))
        nb_nodes = self.graph.number_of_nodes()
        to_keep = min(math.floor(nb_nodes * top_percent), nb_nodes)
        top_words = sorted(w, key=w.get, reverse=True)
        self.longest_keyword_sequence_selection(top_words[:int(to_keep)])
    for k in self.candidates.keys():
        tokens = self.candidates[k].lexical_form
        self.weights[k] = sum([w[t] for t in tokens])
        if normalized:
            self.weights[k] /= len(tokens)
        self.weights[k] += self.candidates[k].offsets[0] * 1e-08",pos is None,166,pos is None,True,100.00000000000004,N/A
"def candidate_selection(self, pos=None):
    """"""Selects longest sequences of nouns and adjectives as keyphrase
        candidates.

        Args:
            pos (set): the set of valid POS tags, defaults to ('NOUN',
                'PROPN', 'ADJ').
        """"""
<mask>:
        pos = {'NOUN', 'PROPN', 'ADJ'}
    self.longest_pos_sequence_selection(valid_pos=pos)
    self.candidate_filtering()",pos is None,36,pos is None,True,100.00000000000004,N/A
"def topic_clustering(self, threshold=0.74, method='average'):
    """"""Clustering candidates into topics.

        Args:
            threshold (float): the minimum similarity for clustering, defaults
                to 0.74, i.e. more than 1/4 of stem overlap similarity.
            method (str): the linkage method, defaults to average.

        """"""
<mask>:
        self.topics.append([list(self.candidates)[0]])
        return
    candidates, X = self.vectorize_candidates()
    Y = pdist(X, 'jaccard')
    Z = linkage(Y, method=method)
    clusters = fcluster(Z, t=threshold, criterion='distance')
    for cluster_id in range(1, max(clusters) + 1):
        self.topics.append([candidates[j] for j in range(len(clusters)) if clusters[j] == cluster_id])",len(self.candidates) == 1,72,method == 'average',False,9.153013214364877,N/A
"def compute_gap(self, p_i, p_j, len_i, len_j):
    gap = abs(p_i - p_j)
<mask>:
        gap -= len_i - 1
    elif p_i > p_j:
        gap -= len_j - 1
    if gap == 0:
        gap = 1
    return gap",p_i < p_j,35,p_i < p_j,True,100.00000000000004,N/A
"def candidate_weighting(self, threshold=0.74, method='average', heuristic=None):
    """"""Candidate ranking using random walk.

        Args:
            threshold (float): the minimum similarity for clustering, defaults
                to 0.74.
            method (str): the linkage method, defaults to average.
            heuristic (str): the heuristic for selecting the best candidate for
                each topic, defaults to first occurring candidate. Other
                options are 'frequent' (most frequent candidate, position is
                used for ties).

        """"""
<mask>:
        return
    self.topic_clustering(threshold=threshold, method=method)
    self.build_topic_graph()
    self._w = nx.pagerank(self.graph, alpha=0.85, weight='weight')
    for i, topic in enumerate(self.topics):
        offsets = [self.candidates[t].offsets[0] for t in topic]
        if heuristic == 'frequent':
            freq = [len(self.candidates[t].surface_forms) for t in topic]
            indexes = [j for j, f in enumerate(freq) if f == max(freq)]
            indexes_offsets = [offsets[j] for j in indexes]
            most_frequent = offsets.index(min(indexes_offsets))
            self.weights[topic[most_frequent]] = self._w[i]
        else:
            first = offsets.index(min(offsets))
            self.weights[topic[first]] = self._w[i]",not self.candidates,124,self.candidate_weighting is None,False,13.134549472120788,N/A
"def candidate_selection(self, grammar=None):
    """"""Candidate selection heuristic.

        Here we select noun phrases that match the regular expression
        (adjective)*(noun)+, which represents zero or more adjectives followed
        by one or more nouns (Liu et al., 2010).

        Note that there is no details on this in the Single TPR paper, and these
        are the only information that can be found:

            ... a set of expressions or noun phrases ...

            ... Adjectives and nouns are then merged into keyphrases and
            corresponding scores are summed and ranked. ...

        Args:
            grammar (str): grammar defining POS patterns of NPs, defaults to
                ""NP: {<ADJ>*<NOUN|PROPN>+}"".
        """"""
<mask>:
        grammar = 'NP:{<ADJ>*<NOUN|PROPN>+}'
    self.grammar_selection(grammar=grammar)",grammar is None,101,grammar is None,True,100.00000000000004,N/A
"def candidate_weighting(self, window=10, pos=None, lda_model=None, normalized=False):
    """"""Candidate weight calculation using a biased PageRank towards LDA
        topic distributions.

        Args:
            window (int): the window within the sentence for connecting two
                words in the graph, defaults to 10.
            pos (set): the set of valid pos for words to be considered as
                nodes in the graph, defaults to ('NOUN', 'PROPN', 'ADJ').
            lda_model (pickle.gz): an LDA model produced by sklearn in
                pickle compressed (.gz) format
            normalized (False): normalize keyphrase score by their length,
                defaults to False.
        """"""
<mask>:
        return
    if pos is None:
        pos = {'NOUN', 'PROPN', 'ADJ'}
    self.build_word_graph(window=window, pos=pos)
    if lda_model is None:
        lda_model = os.path.join(self._models, 'lda-1000-semeval2010.py3.pickle.gz')
        logging.warning('LDA model is hard coded to {}'.format(lda_model))
    if isinstance(lda_model, str):
        dictionary, model = pke.utils.load_lda_model(lda_model)
    else:
        dictionary, model = lda_model
    doc = []
    for s in self.sentences:
        doc.extend([s.stems[i] for i in range(s.length)])
    tf_vectorizer = CountVectorizer(stop_words=list(self.stoplist), vocabulary=dictionary)
    tf = tf_vectorizer.fit_transform([' '.join(doc)])
    distribution_topic_document = model.transform(tf)[0]
    distributions = model.components_ / model.components_.sum(axis=1)[:, np.newaxis]
    K = len(distribution_topic_document)
    W = {}
    for word in self.graph.nodes():
        if word in dictionary:
            index = dictionary.index(word)
            distribution_word_topic = [distributions[k][index] for k in range(K)]
            W[word] = 1 - cosine(distribution_word_topic, distribution_topic_document)
    default_similarity = min(W.values())
    for word in self.graph.nodes():
        if word not in W:
            W[word] = default_similarity
    norm = sum(W.values())
    for word in W:
        W[word] /= norm
    w = nx.pagerank(self.graph, personalization=W, alpha=0.85, tol=0.0001, weight='weight')
    for k in self.candidates.keys():
        tokens = self.candidates[k].lexical_form
        self.weights[k] = sum([w[t] for t in tokens])
        if normalized:
            self.weights[k] /= len(tokens)",not self.candidates,232,self.weighting is None,False,21.3643503198117,N/A
"def build_word_graph(self, window=10, pos=None):
    """"""Build a graph representation of the document in which nodes/vertices
        are words and edges represent co-occurrence relation. Syntactic filters
        can be applied to select only words of certain Part-of-Speech.
        Co-occurrence relations can be controlled using the distance (window)
        between word occurrences in the document.

        The number of times two words co-occur in a window is encoded as *edge
        weights*. Sentence boundaries **are not** taken into account in the
        window.

        Args:
            window (int): the window for connecting two words in the graph,
                defaults to 10.
            pos (set): the set of valid pos for words to be considered as nodes
                in the graph, defaults to ('NOUN', 'PROPN', 'ADJ').
        """"""
<mask>:
        pos = {'NOUN', 'PROPN', 'ADJ'}
    text = [(word, sentence.pos[i] in pos) for sentence in self.sentences for i, word in enumerate(sentence.stems)]
    self.graph.add_nodes_from([word for word, valid in text if valid])
    for i, (node1, is_in_graph1) in enumerate(text):
        if not is_in_graph1:
            continue
        for j in range(i + 1, min(i + window, len(text))):
            node2, is_in_graph2 = text[j]
            if is_in_graph2 and node1 != node2:
                if not self.graph.has_edge(node1, node2):
                    self.graph.add_edge(node1, node2, weight=0.0)
                self.graph[node1][node2]['weight'] += 1.0",pos is None,180,pos is None,True,100.00000000000004,N/A
"def candidate_weighting(self, window=10, pos=None, normalized=False):
    """"""Keyphrase candidate ranking using the weighted variant of the
        TextRank formulae. Candidates are scored by the sum of the scores of
        their words.

        Args:
            window (int): the window within the sentence for connecting two
                words in the graph, defaults to 10.
            pos (set): the set of valid pos for words to be considered as nodes
                in the graph, defaults to ('NOUN', 'PROPN', 'ADJ').
            normalized (False): normalize keyphrase score by their length,
                defaults to False.
        """"""
<mask>:
        pos = {'NOUN', 'PROPN', 'ADJ'}
    self.build_word_graph(window=window, pos=pos)
    w = nx.pagerank(self.graph, alpha=0.85, tol=0.0001, weight='weight')
    for k in self.candidates.keys():
        tokens = self.candidates[k].lexical_form
        self.weights[k] = sum([w[t] for t in tokens])
        if normalized:
            self.weights[k] /= len(tokens)
        self.weights[k] += self.candidates[k].offsets[0] * 1e-08",pos is None,118,pos is None,True,100.00000000000004,N/A
"def candidate_selection(self, grammar=None, maximum_word_number=3):
    """"""Candidate selection heuristic using a syntactic PoS pattern for
        noun phrase extraction.

        Keyphrase candidates are noun phrases that match the regular expression
        (adjective)*(noun)+, of length up to three.

        Args:
            grammar (str): grammar defining POS patterns of NPs, defaults to 
                ""NP: {<ADJ>*<NOUN|PROPN>+}"".
            maximum_word_number (int): the maximum number of words allowed for
                keyphrase candidates, defaults to 3.
        """"""
<mask>:
        grammar = 'NP:{<ADJ>*<NOUN|PROPN>+}'
    self.grammar_selection(grammar=grammar)
    for k in list(self.candidates):
        v = self.candidates[k]
        if len(v.lexical_form) > maximum_word_number:
            del self.candidates[k]",grammar is None,78,grammar is None,True,100.00000000000004,N/A
"def build_word_graph(self, window=10, pos=None):
    """"""Build the graph representation of the document.

        In the graph, nodes are words that passes a Part-of-Speech filter. Two
        nodes are connected if the words corresponding to these nodes co-occur
        within a `window` of contiguous tokens. The weight of an edge is
        computed based on the co-occurrence count of the two words within a
        `window` of successive tokens.

        Args:
            window (int): the window within the sentence for connecting two
                words in the graph, defaults to 10.
            pos (set): the set of valid pos for words to be considered as nodes
                in the graph, defaults to ('NOUN', 'PROPN', 'ADJ').
        """"""
<mask>:
        pos = {'NOUN', 'PROPN', 'ADJ'}
    text = []
    for i, sentence in enumerate(self.sentences):
        shift = sum([s.length for s in self.sentences[0:i]])
        for j, word in enumerate(sentence.stems):
            if sentence.pos[j] in pos:
                text.append((word, shift + j))
    self.graph.add_nodes_from([word for word, position in text])
    for i, (node1, position1) in enumerate(text):
        j = i + 1
        while j < len(text) and text[j][1] - position1 < window:
            node2, position2 = text[j]
            if node1 != node2:
                if not self.graph.has_edge(node1, node2):
                    self.graph.add_edge(node1, node2, weight=0)
                self.graph[node1][node2]['weight'] += 1
            j = j + 1
    for word, position in text:
        self.positions[word] += 1 / (position + 1)",pos is None,199,pos is None,True,100.00000000000004,N/A
"def candidate_weighting(self, window=10, pos=None, normalized=False):
    """"""Candidate weight calculation using a biased PageRank.

        Args:
            window (int): the window within the sentence for connecting two
                words in the graph, defaults to 10.
            pos (set): the set of valid pos for words to be considered as nodes
                in the graph, defaults to ('NOUN', 'PROPN', 'ADJ').
            normalized (False): normalize keyphrase score by their length,
                defaults to False.
        """"""
<mask>:
        pos = {'NOUN', 'PROPN', 'ADJ'}
    self.build_word_graph(window=window, pos=pos)
    norm = sum(self.positions.values())
    for word in self.positions:
        self.positions[word] /= norm
    w = nx.pagerank(self.graph, alpha=0.85, tol=0.0001, personalization=self.positions, weight='weight')
    for k in self.candidates.keys():
        tokens = self.candidates[k].lexical_form
        self.weights[k] = sum([w.get(t, 0.0) for t in tokens])
        if normalized:
            self.weights[k] /= len(tokens)",pos is None,109,pos is None,True,100.00000000000004,N/A
"def candidate_selection(self, n=3):
    """"""Select 1-3 grams as keyphrase candidates. Candidates beginning or
        ending with a stopword are filtered out. Words that do not contain
        at least one alpha-numeric character are not allowed.

        Args:
            n (int): the n-gram length, defaults to 3.
        """"""
    self.ngram_selection(n=n)
    self.candidate_filtering()
    for k in list(self.candidates):
        v = self.candidates[k]
<mask>:
            del self.candidates[k]",v.surface_forms[0][0].lower() in self.stoplist or v.surface_forms[0][-1].lower() in self.stoplist or len(v.surface_forms[0][0]) < 3 or (len(v.surface_forms[0][-1]) < 3),54,v.alpha == alpha_numeric_character(n),False,0.069005767245054,N/A
"def _vocabulary_building(self, use_stems=False):
    """"""Build the vocabulary that will be used to weight candidates. Only
        words containing at least one alpha-numeric character are kept.

        Args:
            use_stems (bool): whether to use stems instead of lowercase words
                for weighting, defaults to False.
        """"""
    for i, sentence in enumerate(self.sentences):
        shift = sum([s.length for s in self.sentences[0:i]])
        for j, word in enumerate(sentence.words):
            index = word.lower()
<mask>:
                index = sentence.stems[j]
            self.words[index].add((shift + j, shift, i, word))",use_stems,70,use_stems,True,100.00000000000004,N/A
"def _contexts_building(self, use_stems=False, window=2):
    """"""Build the contexts of the words for computing the relatedness
        feature. Words that occur within a window of n words are considered as
        context words. Only words co-occurring in a block (sequence of words
        that appear in the vocabulary) are considered.

        Args:
            use_stems (bool): whether to use stems instead of lowercase words
                for weighting, defaults to False.
            window (int): the size in words of the window used for computing
                co-occurrence counts, defaults to 2.
        """"""
    for i, sentence in enumerate(self.sentences):
        words = [w.lower() for w in sentence.words]
<mask>:
            words = sentence.stems
        block = []
        for j, word in enumerate(words):
            if word not in self.words:
                block = []
                continue
            self.contexts[word][0].extend([w for w in block[max(0, len(block) - window):len(block)]])
            for w in block[max(0, len(block) - window):len(block)]:
                self.contexts[w][1].append(word)
            block.append(word)",use_stems,129,use_stems,True,100.00000000000004,N/A
"def candidate_weighting(self, window=2, use_stems=False):
    """"""Candidate weight calculation as described in the YAKE paper.

        Args:
            use_stems (bool): whether to use stems instead of lowercase words
                for weighting, defaults to False.
            window (int): the size in words of the window used for computing
                co-occurrence counts, defaults to 2.
        """"""
<mask>:
        return
    self._vocabulary_building(use_stems=use_stems)
    self._contexts_building(use_stems=use_stems, window=window)
    self._feature_extraction()
    for k, v in self.candidates.items():
        if use_stems:
            weights = [self.features[t]['weight'] for t in v.lexical_form]
            self.weights[k] = numpy.prod(weights)
            self.weights[k] /= len(v.offsets) * (1 + sum(weights))
        else:
            lowercase_forms = [' '.join(t).lower() for t in v.surface_forms]
            for i, candidate in enumerate(lowercase_forms):
                TF = lowercase_forms.count(candidate)
                tokens = [t.lower() for t in v.surface_forms[i]]
                prod_ = 1.0
                sum_ = 0.0
                for j, token in enumerate(tokens):
                    if self.features[token]['isstop']:
                        term_stop = token
                        prob_t1 = prob_t2 = 0
                        if j - 1 >= 0:
                            term_left = tokens[j - 1]
                            prob_t1 = self.contexts[term_left][1].count(term_stop) / self.features[term_left]['TF']
                        if j + 1 < len(tokens):
                            term_right = tokens[j + 1]
                            prob_t2 = self.contexts[term_stop][0].count(term_right) / self.features[term_right]['TF']
                        prob = prob_t1 * prob_t2
                        prod_ *= 1 + (1 - prob)
                        sum_ -= 1 - prob
                    else:
                        prod_ *= self.features[token]['weight']
                        sum_ += self.features[token]['weight']
                if sum_ == -1:
                    sum_ = -0.99999999999
                self.weights[candidate] = prod_
                self.weights[candidate] /= TF * (1 + sum_)
                self.surface_to_lexical[candidate] = k",not self.candidates,198,not self.initialized,False,59.460355750136046,N/A
"def candidate_selection(self, lasf=3, cutoff=400):
    """"""The candidate selection as described in the KP-Miner paper.

        Args:
            lasf (int): least allowable seen frequency, defaults to 3.
            cutoff (int): the number of words after which candidates are
                filtered out, defaults to 400.
            stoplist (list): the stoplist for filtering candidates, defaults
                to the nltk stoplist. Words that are punctuation marks from
                string.punctuation are not allowed.
        """"""
    self.ngram_selection(n=5)
    self.candidate_filtering()
    for k in list(self.candidates):
        v = self.candidates[k]
<mask>:
            del self.candidates[k]
        elif len(v.surface_forms) < lasf:
            del self.candidates[k]",v.offsets[0] > cutoff,79,len(v.surface_forms) < cutoff,False,9.287528999566801,N/A
"def candidate_weighting(self, df=None, sigma=3.0, alpha=2.3):
    """"""Candidate weight calculation as described in the KP-Miner paper.

        Note:
            w = tf * idf * B * P_f
            with

              * B = N_d / (P_d * alpha) and B = min(sigma, B)
              * N_d = the number of all candidate terms
              * P_d = number of candidates whose length exceeds one
              * P_f = 1

        Args:
            df (dict): document frequencies, the number of documents should
                be specified using the ""--NB_DOC--"" key.
            sigma (int): parameter for boosting factor, defaults to 3.0.
            alpha (int): parameter for boosting factor, defaults to 2.3.
        """"""
<mask>:
        logging.warning('LoadFile._df_counts is hard coded to {}'.format(self._df_counts))
        df = load_document_frequency_file(self._df_counts, delimiter='\t')
    N = 1 + df.get('--NB_DOC--', 0)
    P_d = sum([len(v.surface_forms) for v in self.candidates.values() if len(v.lexical_form) > 1])
    P_d = max(1, P_d)
    N_d = sum([len(v.surface_forms) for v in self.candidates.values()])
    B = min(N_d / (P_d * alpha), sigma)
    for k, v in self.candidates.items():
        candidate_df = 1
        if len(v.lexical_form) == 1:
            candidate_df += df.get(k, 0)
        idf = math.log(N / candidate_df, 2)
        if len(v.lexical_form) == 1:
            self.weights[k] = len(v.surface_forms) * idf
        else:
            self.weights[k] = len(v.surface_forms) * B * idf",df is None,182,df is None,True,100.00000000000004,N/A
"def candidate_weighting(self, df=None):
    """"""Candidate weighting function using document frequencies.

        Args:
            df (dict): document frequencies, the number of documents should be
                specified using the ""--NB_DOC--"" key.
        """"""
<mask>:
        logging.warning('LoadFile._df_counts is hard coded to {}'.format(self._df_counts))
        df = load_document_frequency_file(self._df_counts, delimiter='\t')
    N = 1 + df.get('--NB_DOC--', 0)
    for k, v in self.candidates.items():
        candidate_df = 1 + df.get(k, 0)
        idf = math.log(N / candidate_df, 2)
        self.weights[k] = len(v.surface_forms) * idf",df is None,65,df is None,True,100.00000000000004,N/A
"def classify_candidates(self, model=None):
    """""" Classify the candidates as keyphrase or not keyphrase.

            Args:
                model (str): the path to load the model in pickle format,
                    default to None.
        """"""
<mask>:
        instance = self.__class__.__name__
        model = os.path.join(self._models, instance + '-semeval2010.py3.pickle')
    clf = load_model(model)
    candidates = self.instances.keys()
    X = [self.instances[u] for u in candidates]
    y = clf.predict_proba(X)
    for i, candidate in enumerate(candidates):
        self.weights[candidate] = y[i][1]",model is None,62,model is None,True,100.00000000000004,N/A
"def candidate_weighting(self):
    """""" Extract features and classify candidates with default parameters.""""""
<mask>:
        return
    self.feature_extraction()
    self.classify_candidates()",not self.candidates,15,not self.candidate_weighting,False,30.213753973567677,N/A
"def candidate_selection(self):
    """"""Select 1-3 grams of `normalized` words as keyphrase candidates.
        Candidates that start or end with a stopword are discarded. Candidates
        that contain punctuation marks (from `string.punctuation`) as words are
        filtered out.
        """"""
    self.ngram_selection(n=3)
    self.candidate_filtering()
    for k in list(self.candidates):
        v = self.candidates[k]
        words = [u.lower() for u in v.surface_forms[0]]
<mask>:
            del self.candidates[k]",words[0] in self.stoplist or words[-1] in self.stoplist,53,len(words) == 1 and words[0] in self.punctuation_marks,False,33.81307292971254,N/A
"def feature_extraction(self, df=None, training=False):
    """"""Extract features for each keyphrase candidate. Features are the
        tf*idf of the candidate and its first occurrence relative to the
        document.

        Args:
            df (dict): document frequencies, the number of documents should be
                specified using the ""--NB_DOC--"" key.
            training (bool): indicates whether features are computed for the
                training set for computing IDF weights, defaults to false.
        """"""
<mask>:
        logging.warning('LoadFile._df_counts is hard coded to {}'.format(self._df_counts))
        df = load_document_frequency_file(self._df_counts, delimiter='\t')
    N = df.get('--NB_DOC--', 0) + 1
    if training:
        N -= 1
    maximum_offset = float(sum([s.length for s in self.sentences]))
    for k, v in self.candidates.items():
        candidate_df = 1 + df.get(k, 0)
        if training and candidate_df > 1:
            candidate_df -= 1
        idf = math.log(N / candidate_df, 2)
        self.instances[k] = np.array([len(v.surface_forms) * idf, v.offsets[0] / maximum_offset])
    self.feature_scaling()",df is None,124,df is None,True,100.00000000000004,N/A
"def candidate_weighting(self, model_file=None, df=None):
    """"""Extract features and classify candidates.

        Args:
            model_file (str): path to the model file.
            df (dict): document frequencies, the number of documents should
                    be specified using the ""--NB_DOC--"" key.
        """"""
<mask>:
        return
    self.feature_extraction(df=df)
    self.classify_candidates(model=model_file)",not self.candidates,37,not model_file,False,15.97357760615681,N/A
"def candidate_selection(self, grammar=None):
    """"""Select noun phrases (NP) and NP containing a pre-propositional phrase
        (NP IN NP) as keyphrase candidates.

        Args:
            grammar (str): grammar defining POS patterns of NPs.
        """"""
<mask>:
        grammar = '\n                NBAR:\n                    {<NOUN|PROPN|ADJ>{,2}<NOUN|PROPN>} \n                    \n                NP:\n                    {<NBAR>}\n                    {<NBAR><ADP><NBAR>}\n            '
    self.grammar_selection(grammar)",grammar is None,42,grammar is None,True,100.00000000000004,N/A
"def feature_extraction(self, df=None, training=False, features_set=None):
    """"""Extract features for each candidate.

        Args:
            df (dict): document frequencies, the number of documents should be
                specified using the ""--NB_DOC--"" key.
            training (bool): indicates whether features are computed for the
                training set for computing IDF weights, defaults to false.
            features_set (list): the set of features to use, defaults to
                [1, 4, 6].

        """"""
<mask>:
        features_set = [1, 4, 6]
    if df is None:
        logging.warning('LoadFile._df_counts is hard coded to {}'.format(self._df_counts))
        df = load_document_frequency_file(self._df_counts, delimiter='\t')
    N = df.get('--NB_DOC--', 0) + 1
    if training:
        N -= 1
    maximum_offset = float(sum([s.length for s in self.sentences]))
    for k, v in self.candidates.items():
        feature_array = []
        candidate_df = 1 + df.get(k, 0)
        if training and candidate_df > 1:
            candidate_df -= 1
        idf = math.log(N / candidate_df, 2)
        feature_array.append(len(v.surface_forms) * idf)
        feature_array.append(len(v.surface_forms))
        tf_of_substrings = 0
        for i in range(len(v.lexical_form)):
            for j in range(i, min(len(v.lexical_form), i + 3)):
                sub_words = v.lexical_form[i:j + 1]
                sub_string = ' '.join(sub_words)
                if sub_string == ' '.join(v.lexical_form):
                    continue
                if set(sub_words).intersection(self.stoplist):
                    continue
                if sub_string in self.candidates:
                    for offset_1 in self.candidates[sub_string].offsets:
                        is_included = False
                        for offset_2 in v.offsets:
                            if offset_2 <= offset_1 <= offset_2 + len(v.lexical_form):
                                is_included = True
                        if not is_included:
                            tf_of_substrings += 1
        feature_array.append(tf_of_substrings)
        feature_array.append(v.offsets[0] / maximum_offset)
        feature_array.append(v.offsets[-1] / maximum_offset)
        feature_array.append(len(v.lexical_form))
        feature_array.append(0)
        meta = [self.sentences[sid].meta for sid in v.sentence_ids]
        sections = [u['section'] for u in meta if 'section' in u]
        types = [u['type'] for u in meta if 'type' in u]
        feature_array.append('title' in sections)
        feature_array.append(0)
        feature_array.append('sectionHeader' in types or 'subsectionHeader' in types or 'subsubsectionHeader' in types)
        feature_array.append('abstract' in sections)
        feature_array.append('introduction' in sections)
        feature_array.append('related work' in sections)
        feature_array.append('conclusions' in sections)
        feature_array.append(types.count('sectionHeader') + types.count('subsectionHeader') + types.count('subsubsectionHeader'))
        feature_array.append(sections.count('abstract'))
        feature_array.append(sections.count('introduction'))
        feature_array.append(sections.count('related work'))
        feature_array.append(sections.count('conclusions'))
        self.instances[k] = np.array([feature_array[i - 1] for i in features_set])
    self.feature_scaling()",features_set is None,280,features_set is None,True,100.00000000000004,N/A
"def retrieve_google_dorks(save_json_response_to_file=False, save_all_dorks_to_file=False, save_individual_categories_to_files=False):
    """"""Retrieves all google dorks from https://www.exploit-db.com/google-hacking-database and optionally, writes the
    entire json response to a .json file, all the dorks to a file, and/or the individual dork categories to a file.  A
    dictionary is returned containing the total_dorks, a list of extracteddorks, and a category dictionary.
    """"""
    url = 'https://www.exploit-db.com/google-hacking-database'
    headers = {'Accept': 'application/json, text/javascript, */*; q=0.01', 'Accept-Encoding': 'deflate, gzip, br', 'Accept-Language': 'en-US', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:60.0) Gecko/20100101 Firefox/60.0', 'X-Requested-With': 'XMLHttpRequest'}
    print(f'[+] Requesting URL: {url}')
    try:
        response = requests.get(url, headers=headers, timeout=10)
    except requests.exceptions.SSLError:
        requests.packages.urllib3.disable_warnings()
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        response = requests.get(url, headers=headers, timeout=10, verify=False)
<mask>:
        print(f'[-] Error retrieving google dorks from: {url}')
        return
    json_response = response.json()
    total_dorks = json_response['recordsTotal']
    json_dorks = json_response['data']
    extracted_dorks = []
    category_dict = {}
    for dork in json_dorks:
        soup = BeautifulSoup(dork['url_title'], 'html.parser')
        extracted_dork = soup.find('a').contents[0].strip()
        extracted_dorks.append(extracted_dork)
        numeric_category_id = int(dork['category']['cat_id'])
        category_name = dork['category']['cat_title']
        if numeric_category_id not in category_dict:
            category_dict[numeric_category_id] = {'category_name': category_name, 'dorks': []}
        dork['url_title'] = dork['url_title'].replace('\t', '')
        category_dict[numeric_category_id]['dorks'].append(dork)
    if save_individual_categories_to_files:
        category_dict = dict(sorted(category_dict.items()))
        for key, value in category_dict.items():
            print(f""[*] Category {key} ('{value['category_name']}') has {len(value['dorks'])} dorks"")
            dork_file_name = value['category_name'].lower().replace(' ', '_')
            full_dork_file_name = f'dorks/{dork_file_name}.dorks'
            print(f""[*] Writing dork category '{value['category_name']}' to file: {full_dork_file_name}"")
            with open(f'{full_dork_file_name}', 'w', encoding='utf-8') as fh:
                for dork in value['dorks']:
                    soup = BeautifulSoup(dork['url_title'], 'html.parser')
                    extracted_dork = soup.find('a').contents[0].strip()
                    fh.write(f'{extracted_dork}\n')
    if save_json_response_to_file:
        google_dork_json_file = 'all_google_dorks.json'
        print(f'[*] Writing all dorks to JSON file: {google_dork_json_file}')
        with open(f'dorks/{google_dork_json_file}', 'w', encoding='utf-8') as json_file:
            json.dump(json_dorks, json_file)
    if save_all_dorks_to_file:
        google_dork_file = 'all_google_dorks.txt'
        print(f'[*] Writing all dorks to txt file: dorks/{google_dork_file}')
        with open(f'dorks/{google_dork_file}', 'w', encoding='utf-8') as fh:
            for dork in extracted_dorks:
                fh.write(f'{dork}\n')
    print(f'[*] Total Google dorks retrieved: {total_dorks}')
    ghdb_dict = {'total_dorks': total_dorks, 'extracted_dorks': extracted_dorks, 'category_dict': category_dict}
    return ghdb_dict",response.status_code != 200,268,response.status_code != 200,True,100.00000000000004,N/A
"def __init__(self, google_dorks_file, domain='', max_search_result_urls_to_return_per_dork=100, save_pagodo_results_to_json_file=None, proxies='', save_urls_to_file=None, minimum_delay_between_dork_searches_in_seconds=37, maximum_delay_between_dork_searches_in_seconds=60, disable_verify_ssl=False, verbosity=4, specific_log_file_name='pagodo.py.log'):
    """"""Initialize Pagodo class object.""""""
    self.log = logging.getLogger('pagodo')
    log_formatter = logging.Formatter('%(asctime)s [%(threadName)-12.12s] [%(levelname)s] %(message)s')
    log_file_handler = logging.FileHandler(specific_log_file_name)
    log_file_handler.setFormatter(log_formatter)
    self.log.addHandler(log_file_handler)
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(log_formatter)
    self.log.addHandler(console_handler)
    self.verbosity = verbosity
    self.log.setLevel((6 - self.verbosity) * 10)
<mask>:
        print('Specify a valid file containing Google dorks with -g')
        sys.exit(0)
    if minimum_delay_between_dork_searches_in_seconds < 0:
        print('Minimum delay between dork searches (-i) must be greater than 0')
        sys.exit(0)
    if maximum_delay_between_dork_searches_in_seconds < 0:
        print('maximum_delay_between_dork_searches_in_seconds (-x) must be greater than 0')
        sys.exit(0)
    if maximum_delay_between_dork_searches_in_seconds <= minimum_delay_between_dork_searches_in_seconds:
        print('maximum_delay_between_dork_searches_in_seconds (-x) must be greater than minimum_delay_between_dork_searches_in_seconds (-i)')
        sys.exit(0)
    if max_search_result_urls_to_return_per_dork < 0:
        print('max_search_result_urls_to_return_per_dork (-m) must be greater than 0')
        sys.exit(0)
    self.google_dorks_file = google_dorks_file
    self.google_dorks = []
    with open(google_dorks_file, 'r', encoding='utf-8') as fh:
        for line in fh.read().splitlines():
            if line.strip():
                self.google_dorks.append(line)
    self.domain = domain
    self.max_search_result_urls_to_return_per_dork = max_search_result_urls_to_return_per_dork
    self.save_pagodo_results_to_json_file = save_pagodo_results_to_json_file
    self.proxies = proxies.strip().strip(',').split(',')
    self.save_urls_to_file = save_urls_to_file
    self.minimum_delay_between_dork_searches_in_seconds = minimum_delay_between_dork_searches_in_seconds
    self.maximum_delay_between_dork_searches_in_seconds = maximum_delay_between_dork_searches_in_seconds
    self.disable_verify_ssl = disable_verify_ssl
    '\n        1) Generate a random list of values between minimum_delay_between_dork_searches_in_seconds and\n           maximum_delay_between_dork_searches_in_seconds\n        2) Round those values to the tenths place\n        3) Re-cast as a list\n        4) Sort the list\n        '
    self.delay_between_dork_searches_list = sorted(list(map(lambda x: round(x, 1), [random.uniform(minimum_delay_between_dork_searches_in_seconds, maximum_delay_between_dork_searches_in_seconds) for _ in range(20)])))
    self.base_file_name = f""pagodo_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}""
    self.total_urls_found = 0
    self.proxy_rotation_index = 0
    if self.save_pagodo_results_to_json_file is None:
        self.save_pagodo_results_to_json_file = f'{self.base_file_name}.json'
    if self.save_urls_to_file is None:
        self.save_urls_to_file = f'{self.base_file_name}.txt'",not os.path.exists(google_dorks_file),216,not google_dorks_file,False,24.764986882297123,N/A
"def _split_lines(self, text, width):
<mask>:
        return text[2:].splitlines()
    return argparse.HelpFormatter._split_lines(self, text, width)",text.startswith('R|'),11,text.startswith('\n'),False,38.260294162784454,N/A
"def _generate_formatted_text(list_of_dicts):
    result = []
    for section in list_of_dicts:
<mask>:
            text = ''
            longest = max((len(key) for key in section))
            for key, value in section.items():
                text += f'{key.ljust(longest)}: {value}\n'
            result.append(text)
    return '\n'.join(result)",section,32,"isinstance(section, dict)",False,8.116697886877475,N/A
"def _get_package_version(pkg_name):
    """"""Return the version of a given package""""""
<mask>:
        pkg_name = 'sklearn'
    try:
        imported = importlib.import_module(pkg_name)
    except ImportError:
        version = 'not installed'
    else:
        try:
            version = importlib_metadata.version(pkg_name)
        except importlib_metadata.PackageNotFoundError:
            try:
                version = imported.__version__
            except AttributeError:
                try:
                    version = imported.version
                except AttributeError:
                    try:
                        version = imported.version_info
                    except AttributeError:
                        version = 'unknown'
    return version",pkg_name == 'scikit-learn',53,pkg_name == 'sklearn',False,75.98356856515926,N/A
"def _get_all_import_versions(vars):
    to_print = {}
    imported_modules = {val for val in list(vars.values()) if isinstance(val, types.ModuleType)}
    imported_modules.update({inspect.getmodule(val) for val in list(vars.values()) if inspect.isclass(val) or inspect.isfunction(val)})
    imported_pkgs = {module.__name__.split('.')[0] for module in imported_modules}
    imported_pkgs.discard('builtins')
    for pkg_name in imported_pkgs:
        pkg_version = _get_package_version(pkg_name)
<mask>:
            to_print[pkg_name] = pkg_version
    return to_print","pkg_version not in ('not installed', 'unknown')",45,pkg_version,False,6.948345122280157,N/A
"def _get_gpu_info():
<mask>:
        return {'GPU Info': 'Install the gpu extra (pip install ""watermark[gpu]"") to display GPU information for NVIDIA chipsets'}
    try:
        gpu_info = ['']
        py3nvml.nvmlInit()
        num_gpus = py3nvml.nvmlDeviceGetCount()
        for i in range(num_gpus):
            handle = py3nvml.nvmlDeviceGetHandleByIndex(i)
            gpu_name = py3nvml.nvmlDeviceGetName(handle)
            gpu_info.append(f'GPU {i}: {gpu_name}')
        py3nvml.nvmlShutdown()
        return {'GPU Info': '\n  '.join(gpu_info)}
    except py3nvml.NVMLError_LibraryNotFound:
        return {'GPU Info': 'NVIDIA drivers do not appear to be installed on this machine.'}
    except:
        return {'GPU Info': 'GPU information is not available for this machine.'}",py3nvml is None,75,py3nvml.nvmlGetVersion() == py3nvml.NVML_GPU_EXTRA,False,2.8398387225677895,N/A
"def test_gpu_info():
    a = watermark.watermark(gpu=True)
    txt = a.split('\n')
    clean_txt = []
    for t in txt:
        t = t.strip()
<mask>:
            t = t.split(':')[0]
            clean_txt.append(t.strip())
    clean_txt = set(clean_txt)
    expected = ['GPU Info']
    for i in expected:
        assert i in clean_txt, print(f'{i} not in {clean_txt}')",t,42,t.startswith('GPU'),False,8.116697886877475,N/A
"def test_defaults():
    a = watermark.watermark()
    txt = a.split('\n')
    clean_txt = []
    for t in txt:
        t = t.strip()
<mask>:
            t = t.split(':')[0]
            clean_txt.append(t.strip())
    clean_txt = set(clean_txt)
    expected = ['Last updated', 'Python implementation', 'Python version', 'IPython version', 'Compiler', 'OS', 'Release', 'Machine', 'Processor', 'CPU cores', 'Architecture']
    for i in expected:
        assert i in clean_txt, print(f'{i} not in {clean_txt}')",t,56,t.startswith('#'),False,5.522397783539471,N/A
"def __init__(self, nameservers: Optional[Sequence[str]]=None, loop: Optional[asyncio.AbstractEventLoop]=None, **kwargs: Any) -> None:
    self.loop = loop or asyncio.get_event_loop()
    assert self.loop is not None
<mask>:
        if not isinstance(self.loop, asyncio.SelectorEventLoop):
            try:
                import winloop
                if not isinstance(self.loop, winloop.Loop):
                    raise RuntimeError('aiodns needs a SelectorEventLoop on Windows. See more: https://github.com/saghul/aiodns/issues/86')
            except ModuleNotFoundError:
                raise RuntimeError('aiodns needs a SelectorEventLoop on Windows. See more: https://github.com/saghul/aiodns/issues/86')
    kwargs.pop('sock_state_cb', None)
    timeout = kwargs.pop('timeout', None)
    self._timeout = timeout
    self._channel = pycares.Channel(sock_state_cb=self._sock_state_cb, timeout=timeout, **kwargs)
    if nameservers:
        self.nameservers = nameservers
    self._read_fds = set()
    self._write_fds = set()
    self._timer = None",sys.platform == 'win32',82,self.loop is not None,False,8.116697886877475,N/A
"@staticmethod
def _callback(fut: asyncio.Future, result: Any, errorno: int) -> None:
<mask>:
        return
    if errorno is not None:
        fut.set_exception(error.DNSError(errorno, pycares.errno.strerror(errorno)))
    else:
        fut.set_result(result)",fut.cancelled(),21,result is None,False,0.0,N/A
"def query(self, host: str, qtype: str, qclass: Optional[str]=None) -> asyncio.Future:
    try:
        qtype = query_type_map[qtype]
    except KeyError:
        raise ValueError('invalid query type: {}'.format(qtype))
<mask>:
        try:
            qclass = query_class_map[qclass]
        except KeyError:
            raise ValueError('invalid query class: {}'.format(qclass))
    fut = asyncio.Future(loop=self.loop)
    cb = functools.partial(self._callback, fut)
    self._channel.query(host, qtype, cb, query_class=qclass)
    return fut",qclass is not None,46,qclass is not None,True,100.00000000000004,N/A
"def _sock_state_cb(self, fd: int, readable: bool, writable: bool) -> None:
<mask>:
        if readable:
            self.loop.add_reader(fd, self._handle_event, fd, READ)
            self._read_fds.add(fd)
        if writable:
            self.loop.add_writer(fd, self._handle_event, fd, WRITE)
            self._write_fds.add(fd)
        if self._timer is None:
            self._start_timer()
    else:
        if fd in self._read_fds:
            self._read_fds.discard(fd)
            self.loop.remove_reader(fd)
        if fd in self._write_fds:
            self._write_fds.discard(fd)
            self.loop.remove_writer(fd)
        if not self._read_fds and (not self._write_fds) and (self._timer is not None):
            self._timer.cancel()
            self._timer = None",readable or writable,58,readable or writable,True,100.00000000000004,N/A
"def _handle_event(self, fd: int, event: Any) -> None:
    read_fd = pycares.ARES_SOCKET_BAD
    write_fd = pycares.ARES_SOCKET_BAD
<mask>:
        read_fd = fd
    elif event == WRITE:
        write_fd = fd
    self._channel.process_fd(read_fd, write_fd)",event == READ,27,event == READ,True,100.00000000000004,N/A
"def setUp(self):
<mask>:
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    self.loop = asyncio.new_event_loop()
    self.addCleanup(self.loop.close)
    self.resolver = aiodns.DNSResolver(loop=self.loop, timeout=5.0)
    self.resolver.nameservers = ['8.8.8.8']",sys.platform == 'win32',15,sys.platform == 'win32',True,100.00000000000004,N/A
"def render(self, data, media_type=None, renderer_context={}, writer_opts=None):
    """"""
        Renders serialized *data* into CSV. For a dictionary:
        """"""
<mask>:
        return ''
    if not isinstance(data, list):
        data = [data]
    if writer_opts is not None:
        log.warning('The writer_opts argument is deprecated. Set the writer_opts on the renderer class, instance, or pass writer_opts into the renderer_context instead.')
    writer_opts = renderer_context.get('writer_opts', writer_opts or self.writer_opts or {})
    header = renderer_context.get('header', self.header)
    labels = renderer_context.get('labels', self.labels)
    encoding = renderer_context.get('encoding', settings.DEFAULT_CHARSET)
    table = self.tablize(data, header=header, labels=labels)
    csv_buffer = StringIO()
    csv_writer = csv.writer(csv_buffer, **writer_opts)
    for row in table:
        csv_writer.writerow(row)
    return csv_buffer.getvalue().encode(encoding)",data is None,90,not data,False,30.326532985631665,N/A
"def tablize(self, data, header=None, labels=None):
    """"""
        Convert a list of data into a table.

        If there is a header provided to tablize it will efficiently yield each
        row as needed. If no header is provided, tablize will need to process
        each row in the data in order to construct a complete header. Thus, if
        you have a lot of data and want to stream it, you should probably
        provide a header to the renderer (using the `header` attribute, or via
        the `renderer_context`).
        """"""
<mask>:
        header = data.header
    if data:
        data = self.flatten_data(data)
        if not header:
            data = tuple(data)
            header_fields = set()
            for item in data:
                header_fields.update(list(item.keys()))
            header = sorted(header_fields)
        if labels:
            yield [labels.get(x, x) for x in header]
        else:
            yield header
        for item in data:
            row = [item.get(key, None) for key in header]
            yield row
    elif header:
        if labels:
            yield [labels.get(x, x) for x in header]
        else:
            yield header
    else:
        pass","not header and hasattr(data, 'header')",151,header is None,False,3.7238938287986976,N/A
"def flatten_item(self, item):
<mask>:
        flat_item = self.flatten_list(item)
    elif isinstance(item, dict):
        flat_item = self.flatten_dict(item)
    else:
        flat_item = {'': item}
    return flat_item","isinstance(item, list)",20,"isinstance(item, list)",True,100.00000000000004,N/A
"def render(self, data, media_type=None, renderer_context={}):
    """"""
        Renders serialized *data* into CSV to be used with Django
        StreamingHttpResponse. We need to return a generator here, so Django
        can iterate over it, rendering and returning each line.

        >>> renderer = CSVStreamingRenderer()
        >>> renderer.header = ['a', 'b']
        >>> data = [{'a': 1, 'b': 2}]
        >>> from django.http import StreamingHttpResponse
        >>> response = StreamingHttpResponse(renderer.render(data),
                                             content_type='text/csv')
        >>> response['Content-Disposition'] = 'attachment; filename=""f.csv""'
        >>> # return response

        """"""
<mask>:
        yield ''
    self.labels = renderer_context.get('labels', self.labels)
    if not isinstance(data, GeneratorType) and (not isinstance(data, list)):
        data = [data]
    writer_opts = renderer_context.get('writer_opts', self.writer_opts or {})
    header = renderer_context.get('header', self.header)
    labels = renderer_context.get('labels', self.labels)
    encoding = renderer_context.get('encoding', settings.DEFAULT_CHARSET)
    bom = renderer_context.get('bom', False)
    if bom and encoding == settings.DEFAULT_CHARSET:
        yield codecs.BOM_UTF8
    table = self.tablize(data, header=header, labels=labels)
    csv_buffer = Echo()
    csv_writer = csv.writer(csv_buffer, **writer_opts)
    for row in table:
        yield csv_writer.writerow(row).encode(encoding)",data is None,137,not data,False,30.326532985631665,N/A
"def render(self, data, *args, **kwargs):
<mask>:
        data = data.get(self.results_field, [])
    return super(PaginatedCSVRenderer, self).render(data, *args, **kwargs)","not isinstance(data, list)",15,self.results_field,False,0.0,N/A
"def __init__(self, x: Iterable[float], y: Iterable[float], S: float=1.0, curve: str='concave', direction: str='increasing', interp_method: str='interp1d', online: bool=False, polynomial_degree: int=7):
    self.x = np.array(x)
    self.y = np.array(y)
    self.curve = curve
    self.direction = direction
    self.N = len(self.x)
    self.S = S
    self.all_knees = set()
    self.all_norm_knees = set()
    self.all_knees_y = []
    self.all_norm_knees_y = []
    self.online = online
    self.polynomial_degree = polynomial_degree
    valid_curve = self.curve in VALID_CURVE
    valid_direction = self.direction in VALID_DIRECTION
<mask>:
        raise ValueError('Please check that the curve and direction arguments are valid.')
    if interp_method == 'interp1d':
        uspline = interpolate.interp1d(self.x, self.y)
        self.Ds_y = uspline(self.x)
    elif interp_method == 'polynomial':
        p = np.poly1d(np.polyfit(x, y, self.polynomial_degree))
        self.Ds_y = p(x)
    else:
        raise ValueError(""{} is an invalid interp_method parameter, use either 'interp1d' or 'polynomial'"".format(interp_method))
    self.x_normalized = self.__normalize(self.x)
    self.y_normalized = self.__normalize(self.Ds_y)
    self.y_normalized = self.transform_y(self.y_normalized, self.direction, self.curve)
    self.y_difference = self.y_normalized - self.x_normalized
    self.x_difference = self.x_normalized.copy()
    self.maxima_indices = argrelextrema(self.y_difference, np.greater_equal)[0]
    self.x_difference_maxima = self.x_difference[self.maxima_indices]
    self.y_difference_maxima = self.y_difference[self.maxima_indices]
    self.minima_indices = argrelextrema(self.y_difference, np.less_equal)[0]
    self.x_difference_minima = self.x_difference[self.minima_indices]
    self.y_difference_minima = self.y_difference[self.minima_indices]
    self.Tmx = self.y_difference_maxima - self.S * np.abs(np.diff(self.x_normalized).mean())
    self.knee, self.norm_knee = self.find_knee()
    self.knee_y = self.norm_knee_y = None
    if self.knee:
        self.knee_y = self.y[self.x == self.knee][0]
        self.norm_knee_y = self.y_normalized[self.x_normalized == self.norm_knee][0]","not all((valid_curve, valid_direction))",179,not valid_curve and (not valid_direction),False,30.125864171112873,N/A
"@staticmethod
def transform_y(y: Iterable[float], direction: str, curve: str) -> float:
    """"""transform y to concave, increasing based on given direction and curve""""""
<mask>:
        if curve == 'concave':
            y = np.flip(y)
        elif curve == 'convex':
            y = y.max() - y
    elif direction == 'increasing' and curve == 'convex':
        y = np.flip(y.max() - y)
    return y",direction == 'decreasing',53,direction == 'increasing' and curve != 'convex',False,17.747405280050266,N/A
"def find_knee(self):
    """"""This function is called when KneeLocator is instantiated. It identifies the knee value and sets the instance attributes.""""""
<mask>:
        return (None, None)
    maxima_threshold_index = 0
    minima_threshold_index = 0
    traversed_maxima = False
    for i, x in enumerate(self.x_difference):
        if i < self.maxima_indices[0]:
            continue
        j = i + 1
        if i == len(self.x_difference) - 1:
            break
        if (self.maxima_indices == i).any():
            threshold = self.Tmx[maxima_threshold_index]
            threshold_index = i
            maxima_threshold_index += 1
        if (self.minima_indices == i).any():
            threshold = 0.0
            minima_threshold_index += 1
        if self.y_difference[j] < threshold:
            if self.curve == 'convex':
                if self.direction == 'decreasing':
                    knee = self.x[threshold_index]
                    norm_knee = self.x_normalized[threshold_index]
                else:
                    knee = self.x[-(threshold_index + 1)]
                    norm_knee = self.x_normalized[threshold_index]
            elif self.curve == 'concave':
                if self.direction == 'decreasing':
                    knee = self.x[-(threshold_index + 1)]
                    norm_knee = self.x_normalized[threshold_index]
                else:
                    knee = self.x[threshold_index]
                    norm_knee = self.x_normalized[threshold_index]
            y_at_knee = self.y[self.x == knee][0]
            y_norm_at_knee = self.y_normalized[self.x_normalized == norm_knee][0]
            if knee not in self.all_knees:
                self.all_knees_y.append(y_at_knee)
                self.all_norm_knees_y.append(y_norm_at_knee)
            self.all_knees.add(knee)
            self.all_norm_knees.add(norm_knee)
            if self.online is False:
                return (knee, norm_knee)
    if self.all_knees == set():
        return (None, None)
    return (knee, norm_knee)",not self.maxima_indices.size,164,not self.x_difference,False,23.263472697663296,N/A
"def plot_knee_normalized(self, figsize: Optional[Tuple[int, int]]=None, title: str='Normalized Knee Point', xlabel: Optional[str]=None, ylabel: Optional[str]=None):
    """"""Plot the normalized curve, the difference curve (x_difference, y_normalized) and the knee, if it exists.

        :param figsize: Optional[Tuple[int, int]
            The figure size of the plot. Example (12, 8)
        :param title: str
            Title of the visualization, defaults to ""Normalized Knee Point""
        :param xlabel: Optional[str]
            X-axis label
        :param ylabel: Optional[str]
            y-axis label
        :return: NoReturn
        """"""
<mask>:
        raise _matplotlib_not_found_err
    if figsize is None:
        figsize = (6, 6)
    plt.figure(figsize=figsize)
    plt.title(title)
    if xlabel:
        plt.xlabel(xlabel)
    if ylabel:
        plt.ylabel(ylabel)
    plt.plot(self.x_normalized, self.y_normalized, 'b', label='normalized curve')
    plt.plot(self.x_difference, self.y_difference, 'r', label='difference curve')
    plt.xticks(np.arange(self.x_normalized.min(), self.x_normalized.max() + 0.1, 0.1))
    plt.yticks(np.arange(self.y_difference.min(), self.y_normalized.max() + 0.1, 0.1))
    plt.vlines(self.norm_knee, plt.ylim()[0], plt.ylim()[1], linestyles='--', label='knee/elbow')
    plt.legend(loc='best')",not _has_matplotlib,111,not HAS_MPL,False,14.794015674776452,N/A
"def plot_knee(self, figsize: Optional[Tuple[int, int]]=None, title: str='Knee Point', xlabel: Optional[str]=None, ylabel: Optional[str]=None):
    """"""
        Plot the curve and the knee, if it exists

        :param figsize: Optional[Tuple[int, int]
            The figure size of the plot. Example (12, 8)
        :param title: str
            Title of the visualization, defaults to ""Knee Point""
        :param xlabel: Optional[str]
            X-axis label
        :param ylabel: Optional[str]
            y-axis label
        :return: NoReturn
        """"""
<mask>:
        raise _matplotlib_not_found_err
    if figsize is None:
        figsize = (6, 6)
    plt.figure(figsize=figsize)
    plt.title(title)
    if xlabel:
        plt.xlabel(xlabel)
    if ylabel:
        plt.ylabel(ylabel)
    plt.plot(self.x, self.y, 'b', label='data')
    plt.vlines(self.knee, plt.ylim()[0], plt.ylim()[1], linestyles='--', label='knee/elbow')
    plt.legend(loc='best')",not _has_matplotlib,88,not HAS_MPL,False,14.794015674776452,N/A
"def find_shape(x, y):
    """"""
    Detect the direction and curve type of the line.

    :return: direction(""increasing"" or ""decreasing"") and curve type(""concave"" or ""convex"")
    """"""
    p = np.polyfit(x, y, deg=1)
    x1, x2 = (int(len(x) * 0.2), int(len(x) * 0.8))
    q = np.mean(y[x1:x2]) - np.mean(x[x1:x2] * p[0] + p[1])
<mask>:
        return ('increasing', 'concave')
    if p[0] > 0 and q <= 0:
        return ('increasing', 'convex')
    if p[0] <= 0 and q > 0:
        return ('decreasing', 'concave')
    return ('decreasing', 'convex')",p[0] > 0 and q > 0,75,q <= 0,False,4.238556455648295,N/A
"def get_command_parser():
<mask>:
        __version__ = '<version>'
    parser = argparse.ArgumentParser(description='The RDK is a command-line utility for authoring, deploying, and testing custom AWS Config rules.')
    parser.add_argument('-p', '--profile', help='[optional] indicate which Profile to use.')
    parser.add_argument('-k', '--access-key-id', help='[optional] Access Key ID to use.')
    parser.add_argument('-s', '--secret-access-key', help='[optional] Secret Access Key to use.')
    parser.add_argument('-r', '--region', help='Select the region to run the command in.')
    parser.add_argument('-f', '--region-file', help='[optional] File to specify which regions to run the command in parallel. Supported for init, deploy, and undeploy.')
    parser.add_argument('--region-set', help=""[optional] Set of regions within the region file with which to run the command in parallel. Looks for a 'default' region set if not specified."")
    rdk_commands = sorted(['clean', 'create', 'create-rule-template', 'deploy', 'deploy-organization', 'init', 'logs', 'modify', 'rulesets', 'sample-ci', 'test-local', 'undeploy', 'undeploy-organization', 'export', 'create-region-set'])
    parser.add_argument('command', metavar='<command>', help=f'Command to run.  Refer to the usage instructions for each command for more details. Commands are: {rdk_commands}', choices=rdk_commands)
    parser.add_argument('command_args', metavar='<command arguments>', nargs=argparse.REMAINDER, help='Run `rdk <command> --help` to see command-specific arguments.')
    parser.add_argument('-v', '--version', help='Display the version of this tool', action='version', version='%(prog)s ' + MY_VERSION)
    return parser",'__version__' not in globals() and '__version__' not in locals(),167,not __version__,False,3.3515765140666773,N/A
"def parse_region_file(args):
    region_set = 'default'
<mask>:
        region_set = args.region_set
    try:
        region_text = yaml.safe_load(open(args.region_file, 'r'))
        return region_text[region_set]
    except Exception:
        raise SyntaxError(f'Error reading regions: {region_set} in file: {args.region_file}')",args.region_set,26,args.region_set,True,100.00000000000004,N/A
"def main():
    my_parser = rdk.get_command_parser()
    args = my_parser.parse_args()
    my_rdk = rdk.rdk(args)
<mask>:
        if args.command in ['init', 'deploy', 'undeploy', 'deploy-organization', 'undeploy-organization']:
            regions = rdk.parse_region_file(args)
            print(f'{args.command.capitalize()}ing rules in the following regions: {regions}.')
            if args.command in ['undeploy', 'undeploy-organization'] and '--force' not in args.command_args:
                my_input = input('Delete specified Rules and Lambda Functions from your AWS Account? (y/N): ')
                while my_input.lower() not in ['y', 'n']:
                    my_input = input(f""Invalid input: {my_input}. Please enter either 'y' or 'n': "")
                if my_input.lower() == 'y':
                    vars(args)['command_args'].append('--force')
                elif my_input.lower() == 'n' or my_input == '':
                    exit(0)
            args_list = []
            for region in regions:
                vars(args)['region'] = region
                args_list.append(copy.copy(args))
            data = []
            with concurrent.futures.ProcessPoolExecutor(max_workers=16) as executor:
                future_run_multi_region = {executor.submit(rdk.run_multi_region, args): args for args in args_list}
                for future in concurrent.futures.as_completed(future_run_multi_region):
                    data.append(future.result())
            exit(0)
        else:
            my_parser.error(""Command must be 'init', 'deploy', or 'undeploy' when --region-file argument is provided."")
    return_val = my_rdk.process_command()
    exit(return_val)",args.region_file,136,args.command,False,28.254432923044853,N/A
"@staticmethod
def client(client_name, *args, **kwargs):
<mask>:
        return CONFIG_CLIENT_MOCK
    if client_name == 'sts':
        return STS_CLIENT_MOCK
    raise Exception('Attempting to create an unknown client')",client_name == 'config',21,client_name == 'config',True,100.00000000000004,N/A
"def build_lambda_configurationchange_event(invoking_event, rule_parameters=None):
    event_to_return = {'configRuleName': 'myrule', 'executionRoleArn': 'roleArn', 'eventLeftScope': False, 'invokingEvent': invoking_event, 'accountId': '123456789012', 'configRuleArn': 'arn:aws:config:us-east-1:123456789012:config-rule/config-rule-8fngan', 'resultToken': 'token'}
<mask>:
        event_to_return['ruleParameters'] = rule_parameters
    return event_to_return",rule_parameters,25,rule_parameters,True,100.00000000000004,N/A
"def build_lambda_scheduled_event(rule_parameters=None):
    invoking_event = '{""messageType"":""ScheduledNotification"",""notificationCreationTime"":""2017-12-23T22:11:18.158Z""}'
    event_to_return = {'configRuleName': 'myrule', 'executionRoleArn': 'roleArn', 'eventLeftScope': False, 'invokingEvent': invoking_event, 'accountId': '123456789012', 'configRuleArn': 'arn:aws:config:us-east-1:123456789012:config-rule/config-rule-8fngan', 'resultToken': 'token'}
<mask>:
        event_to_return['ruleParameters'] = rule_parameters
    return event_to_return",rule_parameters,27,rule_parameters,True,100.00000000000004,N/A
"def build_expected_response(compliance_type, compliance_resource_id, compliance_resource_type=DEFAULT_RESOURCE_TYPE, annotation=None):
<mask>:
        return {'ComplianceType': compliance_type, 'ComplianceResourceId': compliance_resource_id, 'ComplianceResourceType': compliance_resource_type}
    return {'ComplianceType': compliance_type, 'ComplianceResourceId': compliance_resource_id, 'ComplianceResourceType': compliance_resource_type, 'Annotation': annotation}",not annotation,22,annotation is None,False,27.516060407455225,N/A
"def assert_successful_evaluation(test_class, response, resp_expected, evaluations_count=1):
<mask>:
        test_class.assertEquals(resp_expected['ComplianceResourceType'], response['ComplianceResourceType'])
        test_class.assertEquals(resp_expected['ComplianceResourceId'], response['ComplianceResourceId'])
        test_class.assertEquals(resp_expected['ComplianceType'], response['ComplianceType'])
        test_class.assertTrue(response['OrderingTimestamp'])
        if 'Annotation' in resp_expected or 'Annotation' in response:
            test_class.assertEquals(resp_expected['Annotation'], response['Annotation'])
    elif isinstance(response, list):
        test_class.assertEquals(evaluations_count, len(response))
        for i, response_expected in enumerate(resp_expected):
            test_class.assertEquals(response_expected['ComplianceResourceType'], response[i]['ComplianceResourceType'])
            test_class.assertEquals(response_expected['ComplianceResourceId'], response[i]['ComplianceResourceId'])
            test_class.assertEquals(response_expected['ComplianceType'], response[i]['ComplianceType'])
            test_class.assertTrue(response[i]['OrderingTimestamp'])
            if 'Annotation' in response_expected or 'Annotation' in response[i]:
                test_class.assertEquals(response_expected['Annotation'], response[i]['Annotation'])","isinstance(response, dict)",50,"isinstance(response, dict)",True,100.00000000000004,N/A
"def get_client(service, event, region=None):
    """"""Return the service boto client. It should be used instead of directly calling the client.

    Keyword arguments:
    service -- the service name used for calling the boto.client()
    event -- the event variable given in the lambda handler
    region -- the region where the client is called (default: None)
    """"""
<mask>:
        return boto3.client(service, region)
    credentials = get_assume_role_credentials(get_execution_role_arn(event), region)
    return boto3.client(service, aws_access_key_id=credentials['AccessKeyId'], aws_secret_access_key=credentials['SecretAccessKey'], aws_session_token=credentials['SessionToken'], region_name=region)",not ASSUME_ROLE_MODE,67,event == 'created',False,0.0,N/A
"def build_evaluation(resource_id, compliance_type, event, resource_type=DEFAULT_RESOURCE_TYPE, annotation=None):
    """"""Form an evaluation as a dictionary. Usually suited to report on scheduled rules.

    Keyword arguments:
    resource_id -- the unique id of the resource to report
    compliance_type -- either COMPLIANT, NON_COMPLIANT or NOT_APPLICABLE
    event -- the event variable given in the lambda handler
    resource_type -- the CloudFormation resource type (or AWS::::Account) to report on the rule (default DEFAULT_RESOURCE_TYPE)
    annotation -- an annotation to be added to the evaluation (default None). It will be truncated to 255 if longer.
    """"""
    eval_cc = {}
<mask>:
        eval_cc['Annotation'] = build_annotation(annotation)
    eval_cc['ComplianceResourceType'] = resource_type
    eval_cc['ComplianceResourceId'] = resource_id
    eval_cc['ComplianceType'] = compliance_type
    eval_cc['OrderingTimestamp'] = str(json.loads(event['invokingEvent'])['notificationCreationTime'])
    return eval_cc",annotation,105,annotation is not None,False,15.97357760615681,N/A
"def build_evaluation_from_config_item(configuration_item, compliance_type, annotation=None):
    """"""Form an evaluation as a dictionary. Usually suited to report on configuration change rules.

    Keyword arguments:
    configuration_item -- the configurationItem dictionary in the invokingEvent
    compliance_type -- either COMPLIANT, NON_COMPLIANT or NOT_APPLICABLE
    annotation -- an annotation to be added to the evaluation (default None). It will be truncated to 255 if longer.
    """"""
    eval_ci = {}
<mask>:
        eval_ci['Annotation'] = build_annotation(annotation)
    eval_ci['ComplianceResourceType'] = configuration_item['resourceType']
    eval_ci['ComplianceResourceId'] = configuration_item['resourceId']
    eval_ci['ComplianceType'] = compliance_type
    eval_ci['OrderingTimestamp'] = configuration_item['configurationItemCaptureTime']
    return eval_ci",annotation,77,annotation is not None,False,15.97357760615681,N/A
"def get_execution_role_arn(event):
    role_arn = None
<mask>:
        rule_params = json.loads(event['ruleParameters'])
        role_name = rule_params.get('ExecutionRoleName')
        if role_name:
            execution_role_prefix = event['executionRoleArn'].split('/')[0]
            role_arn = '{}/{}'.format(execution_role_prefix, role_name)
    if not role_arn:
        role_arn = event['executionRoleArn']
    return role_arn",'ruleParameters' in event,29,'ruleParameters' in event,True,100.00000000000004,N/A
"def build_annotation(annotation_string):
<mask>:
        return annotation_string[:244] + ' [truncated]'
    return annotation_string",len(annotation_string) > 256,10,annotation_string.startswith('['),False,17.747405280050266,N/A
"def create_file(self, name, content=u'', folder=None):
    """"""
            Easy way to create a file.

            :param unicode name: the name of the file to create
            :param unicode content: content of the file (optional)
            :param unicode folder: the folder where the file should be
                created, defaults to the temporary directory

            :returns: the absolute path of the newly created file
            :rtype: unicode
        """"""
<mask>:
        folder = self.rootdir
    path = os.path.join(folder, name)
    if not os.path.exists(os.path.dirname(path)):
        os.makedirs(os.path.dirname(path))
    with open(path, 'w', encoding='utf-8') as fhd:
        fhd.write(content)
    return path",folder is None,79,folder is None,True,100.00000000000004,N/A
"def tearDown(self):
    """"""
            Will remove the root directory and all contents if one
            exists.
        """"""
<mask>:
        shutil.rmtree(self.rootdir)",self.rootdir and os.path.exists(self.rootdir),17,os.path.exists(self.rootdir),False,67.03200460356396,N/A
"def wikilink(text, url_formatter=None):
    """"""
        Processes Wikilink syntax ""[[Link]]"" within the html body.
        This is intended to be run after content has been processed
        by markdown and is already HTML.

        :param str text: the html to highlight wiki links in.
        :param function url_formatter: which URL formatter to use,
            will by default use the flask url formatter

        Syntax:
            This accepts Wikilink syntax in the form of [[WikiLink]] or
            [[url/location|LinkName]]. Everything is referenced from the
            base location ""/"", therefore sub-pages need to use the
            [[page/subpage|Subpage]].

        :returns: the processed html
        :rtype: str
    """"""
<mask>:
        url_formatter = url_for
    link_regex = re.compile('((?<!\\<code\\>)\\[\\[([^<].+?) \\s*([|] \\s* (.+?) \\s*)?]])', re.X | re.U)
    for i in link_regex.findall(text):
        title = [i[-1] if i[-1] else i[1]][0]
        url = clean_url(i[1])
        html_url = u""<a href='{0}'>{1}</a>"".format(url_formatter('wiki.display', url=url), title)
        text = re.sub(link_regex, html_url, text, count=1)
    return text",url_formatter is None,131,url_formatter is None,True,100.00000000000004,N/A
"def __init__(self, path, url, new=False):
    self.path = path
    self.url = url
    self._meta = OrderedDict()
<mask>:
        self.load()
        self.render()",not new,17,new,False,36.78794411714425,N/A
"def save(self, update=True):
    folder = os.path.dirname(self.path)
<mask>:
        os.makedirs(folder)
    with open(self.path, 'w', encoding='utf-8') as f:
        for key, value in self._meta.items():
            line = u'%s: %s\n' % (key, value)
            f.write(line)
        f.write(u'\n')
        f.write(self.body.replace(u'\r\n', u'\n'))
    if update:
        self.load()
        self.render()",not os.path.exists(folder),34,not os.path.exists(folder),True,100.00000000000004,N/A
"def get(self, url):
    path = os.path.join(self.root, url + '.md')
<mask>:
        return Page(path, url)
    return None",self.exists(url),15,os.path.exists(path),False,22.089591134157878,N/A
"def get_or_404(self, url):
    page = self.get(url)
<mask>:
        return page
    abort(404)",page,10,page,True,100.00000000000004,N/A
"@click.group()
@click.option('--directory', type=click.Path(exists=True), default=None)
@click.pass_context
def main(ctx, directory):
    """"""
        Base setup for all the following commands.

        \x08
        :param str directory: the directory to run wiki in, optional.
            If no directory is provided the current directory will be
            used.
    """"""
<mask>:
        directory = os.getcwd()
    ctx.meta['directory'] = os.path.abspath(click.format_filename(directory))",not directory,46,directory is None,False,27.516060407455225,N/A
"def read(self):
<mask>:
        return {}
    with open(self.file) as f:
        data = json.loads(f.read())
    return data",not os.path.exists(self.file),14,not os.path.exists(self.file),True,100.00000000000004,N/A
"def add_user(self, name, password, active=True, roles=[], authentication_method=None):
    users = self.read()
<mask>:
        return False
    if authentication_method is None:
        authentication_method = get_default_authentication_method()
    new_user = {'active': active, 'roles': roles, 'authentication_method': authentication_method, 'authenticated': False}
    if authentication_method == 'hash':
        new_user['hash'] = make_salted_hash(password)
    elif authentication_method == 'cleartext':
        new_user['password'] = password
    else:
        raise NotImplementedError(authentication_method)
    users[name] = new_user
    self.write(users)
    userdata = users.get(name)
    return User(self, name, userdata)",users.get(name),58,name not in users,False,11.521590992286539,N/A
"def get_user(self, name):
    users = self.read()
    userdata = users.get(name)
<mask>:
        return None
    return User(self, name, userdata)",not userdata,16,userdata is None,False,27.516060407455225,N/A
"def delete_user(self, name):
    users = self.read()
<mask>:
        return False
    self.write(users)
    return True","not users.pop(name, False)",12,not users,False,3.0197383422318516,N/A
"def check_password(self, password):
    """"""Return True, return False, or raise NotImplementedError if the
        authentication_method is missing or unknown.""""""
    authentication_method = self.data.get('authentication_method', None)
<mask>:
        authentication_method = get_default_authentication_method()
    if authentication_method == 'hash':
        result = check_hashed_password(password, self.get('hash'))
    elif authentication_method == 'cleartext':
        result = self.get('password') == password
    else:
        raise NotImplementedError(authentication_method)
    return result",authentication_method is None,47,authentication_method is None,True,100.00000000000004,N/A
"def get_wiki():
    wiki = getattr(g, '_wiki', None)
<mask>:
        wiki = g._wiki = Wiki(current_app.config['CONTENT_DIR'])
    return wiki",wiki is None,15,wiki is None,True,100.00000000000004,N/A
"def get_users():
    users = getattr(g, '_users', None)
<mask>:
        users = g._users = UserManager(current_app.config['CONTENT_DIR'])
    return users",users is None,15,users is None,True,100.00000000000004,N/A
"def validate_url(form, field):
<mask>:
        raise ValidationError('The URL ""%s"" exists already.' % field.data)",current_wiki.exists(field.data),12,field.data in url_cache,False,17.112717058426785,N/A
"def validate_name(form, field):
    user = current_users.get_user(field.data)
<mask>:
        raise ValidationError('This username does not exist.')",not user,13,not user,True,100.00000000000004,N/A
"def validate_password(form, field):
    user = current_users.get_user(form.name.data)
<mask>:
        return
    if not user.check_password(field.data):
        raise ValidationError('Username and password do not match.')",not user,18,not user,True,100.00000000000004,N/A
"@bp.route('/')
@protect
def home():
    page = current_wiki.get('home')
<mask>:
        return display('home')
    return render_template('home.html')",page,12,page,True,100.00000000000004,N/A
"@bp.route('/create/', methods=['GET', 'POST'])
@protect
def create():
    form = URLForm()
<mask>:
        return redirect(url_for('wiki.edit', url=form.clean_url(form.url.data)))
    return render_template('create.html', form=form)",form.validate_on_submit(),16,form.validate_on_submit(),True,100.00000000000004,N/A
"@bp.route('/edit/<path:url>/', methods=['GET', 'POST'])
@protect
def edit(url):
    page = current_wiki.get(url)
    form = EditorForm(obj=page)
<mask>:
        if not page:
            page = current_wiki.get_bare(url)
        form.populate_obj(page)
        page.save()
        flash('""%s"" was saved.' % page.title, 'success')
        return redirect(url_for('wiki.display', url=url))
    return render_template('editor.html', form=form, page=page)",form.validate_on_submit(),34,form.validate_on_submit(),True,100.00000000000004,N/A
"@bp.route('/move/<path:url>/', methods=['GET', 'POST'])
@protect
def move(url):
    page = current_wiki.get_or_404(url)
    form = URLForm(obj=page)
<mask>:
        newurl = form.url.data
        renamed = current_wiki.move(url, newurl)
        return redirect(url_for('wiki.display', url=renamed))
    return render_template('move.html', form=form, page=page)",form.validate_on_submit(),27,form.validate_on_submit(),True,100.00000000000004,N/A
"@bp.route('/search/', methods=['GET', 'POST'])
@protect
def search():
    form = SearchForm()
<mask>:
        results = current_wiki.search(form.term.data, form.ignore_case.data)
        return render_template('search.html', form=form, results=results, search=form.term.data)
    return render_template('search.html', form=form, search=None)",form.validate_on_submit(),23,form.validate_on_submit(),True,100.00000000000004,N/A
"def get_device(cuda_id: int) -> torch.device:
<mask>:
        return torch.device(f'cuda:{cuda_id}')
    return torch.device('cpu')",cuda_id >= 0 and torch.cuda.is_available(),10,cuda_id in torch.cuda.available_cpus(),False,26.020472177453954,N/A
"def get_device(cuda_id: int) -> torch.device:
    """"""Gets the torch device based on CUDA availability and ID.""""""
<mask>:
        return torch.device(f'cuda:{cuda_id}')
    else:
        return torch.device('cpu')",cuda_id >= 0 and torch.cuda.is_available(),21,cuda_id,False,1.3123728736940974,N/A
"def get_device(cuda_id: int) -> torch.device:
    """"""Gets the torch device based on CUDA availability and ID.""""""
<mask>:
        return torch.device(f'cuda:{cuda_id}')
    return torch.device('cpu')",cuda_id >= 0 and torch.cuda.is_available(),20,cuda_id,False,1.3123728736940974,N/A
"def __init__(self, input_size: int, mem_size: int=512, cell_size: int=32, independent_linears: bool=True, read_heads: int=4, sparse_reads: int=4, temporal_reads: int=4, num_lists: int | None=None, index_checks: int=32, device: torch.device | None=None):
    """"""Initialize SparseTemporalMemory.

        Args:
            input_size: Input size.
            mem_size: Memory size.
            cell_size: Size of each memory cell.
            independent_linears: Whether to use independent linear layers.
            read_heads: Number of read heads.
            sparse_reads: Number of sparse reads.
            temporal_reads: Number of temporal reads.
            num_lists: Number of lists for indexing.
            index_checks: Number of index checks.
            device: PyTorch device

        """"""
    super(SparseTemporalMemory, self).__init__()
    self.mem_size = mem_size
    self.cell_size = cell_size
    self.device = device
    self.input_size = input_size
    self.independent_linears = independent_linears
    self.K = sparse_reads if self.mem_size > sparse_reads else self.mem_size
    self.KL = temporal_reads if self.mem_size > temporal_reads else self.mem_size
    self.read_heads = read_heads
    self.num_lists = num_lists if num_lists is not None else int(self.mem_size / 100)
    self.index_checks = index_checks
    m = self.mem_size
    w = self.cell_size
    r = self.read_heads
    self.c = r * self.K + self.KL * 2 + 1
<mask>:
        self.read_query_transform = nn.Linear(self.input_size, w * r)
        self.write_vector_transform = nn.Linear(self.input_size, w)
        self.interpolation_gate_transform = nn.Linear(self.input_size, self.c)
        self.write_gate_transform = nn.Linear(self.input_size, 1)
        torch.nn.init.kaiming_uniform_(self.read_query_transform.weight)
        torch.nn.init.kaiming_uniform_(self.write_vector_transform.weight)
        torch.nn.init.kaiming_uniform_(self.interpolation_gate_transform.weight)
        torch.nn.init.kaiming_uniform_(self.write_gate_transform.weight)
    else:
        self.interface_size = r * w + w + self.c + 1
        self.interface_weights = nn.Linear(self.input_size, self.interface_size)
        torch.nn.init.kaiming_uniform_(self.interface_weights.weight)
    self.I = cuda(1 - torch.eye(self.c).unsqueeze(0), device=self.device)
    self.δ = 0.005
    self.timestep = 0
    self.mem_limit_reached = False
    if self.device is not None and self.device.type == 'cuda':
        self.to(self.device)",self.independent_linears,216,self.independent_linears,True,100.00000000000004,N/A
"def rebuild_indexes(self, hidden: dict, erase: bool=False) -> dict:
    """"""Rebuilds the indexes for sparse memory access.

        Args:
            hidden: Hidden state dictionary.
            erase: Whether to erase the existing memory content.

        Returns:
             Updated hidden state dictionary.
        """"""
    b = hidden['memory'].size(0)
<mask>:
        for x in hidden['indexes']:
            x.reset()
    else:
        try:
            from .faiss_index import FAISSIndex
            hidden['indexes'] = [FAISSIndex(cell_size=self.cell_size, nr_cells=self.mem_size, K=self.K, num_lists=self.num_lists, probes=self.index_checks, device=self.device) for _ in range(b)]
        except ImportError:
            print('FAISS not found, please install FAISS, consult https://github.com/facebookresearch/faiss/blob/main/INSTALL.md')
            raise
    pos = hidden['read_positions'].squeeze().detach().cpu().numpy()
    if not erase:
        for n, i in enumerate(hidden['indexes']):
            i.reset()
            i.add(hidden['memory'][n], last=pos[n][-1])
    else:
        self.timestep = 0
        self.mem_limit_reached = False
    return hidden",'indexes' in hidden,95,erase,False,0.0,N/A
"def reset(self, batch_size: int=1, hidden: dict | None=None, erase: bool=True) -> dict:
    """"""Resets the memory and hidden state.

        Args:
            batch_size: Batch size.
            hidden:  Hidden state dictionary.
            erase: Whether to erase the existing memory content

        Returns:
            Reset hidden state dictionary.
        """"""
    m = self.mem_size
    w = self.cell_size
    b = batch_size
    r = self.read_heads
    c = self.c
<mask>:
        hidden = {'memory': cuda(torch.zeros(b, m, w).fill_(self.δ), device=self.device), 'visible_memory': cuda(torch.zeros(b, c, w).fill_(self.δ), device=self.device), 'link_matrix': cuda(torch.zeros(b, m, self.KL * 2), device=self.device), 'rev_link_matrix': cuda(torch.zeros(b, m, self.KL * 2), device=self.device), 'precedence': cuda(torch.zeros(b, self.KL * 2).fill_(self.δ), device=self.device), 'read_weights': cuda(torch.zeros(b, m).fill_(self.δ), device=self.device), 'write_weights': cuda(torch.zeros(b, m).fill_(self.δ), device=self.device), 'read_vectors': cuda(torch.zeros(b, r, w).fill_(self.δ), device=self.device), 'least_used_mem': cuda(torch.zeros(b, 1).fill_(c + 1), device=self.device).long(), 'usage': cuda(torch.zeros(b, m), device=self.device), 'read_positions': cuda(torch.arange(0, c).expand(b, c), device=self.device).long()}
        hidden = self.rebuild_indexes(hidden, erase=True)
    else:
        hidden['memory'] = hidden['memory'].clone()
        hidden['visible_memory'] = hidden['visible_memory'].clone()
        hidden['link_matrix'] = hidden['link_matrix'].clone()
        hidden['rev_link_matrix'] = hidden['link_matrix'].clone()
        hidden['precedence'] = hidden['precedence'].clone()
        hidden['read_weights'] = hidden['read_weights'].clone()
        hidden['write_weights'] = hidden['write_weights'].clone()
        hidden['read_vectors'] = hidden['read_vectors'].clone()
        hidden['least_used_mem'] = hidden['least_used_mem'].clone()
        hidden['usage'] = hidden['usage'].clone()
        hidden['read_positions'] = hidden['read_positions'].clone()
        hidden = self.rebuild_indexes(hidden, erase)
        if erase:
            hidden['memory'].data.fill_(self.δ)
            hidden['visible_memory'].data.fill_(self.δ)
            hidden['link_matrix'].data.zero_()
            hidden['rev_link_matrix'].data.zero_()
            hidden['precedence'].data.zero_()
            hidden['read_weights'].data.fill_(self.δ)
            hidden['write_weights'].data.fill_(self.δ)
            hidden['read_vectors'].data.fill_(self.δ)
            hidden['least_used_mem'].data.fill_(c + 1)
            hidden['usage'].data.fill_(0)
            hidden['read_positions'] = cuda(torch.arange(0, c).expand(b, c), device=self.device).long()
    return hidden",hidden is None,180,hidden is None,True,100.00000000000004,N/A
"def write(self, interpolation_gate: torch.Tensor, write_vector: torch.Tensor, write_gate: torch.Tensor, hidden: dict) -> dict:
    """"""Performs the memory write operation.

        Args:
            interpolation_gate : Interpolation gate.
            write_vector: Write vector.
            write_gate: Write gate.
            hidden: Hidden state dictionary

        Returns:
            Updated hidden state dictionary.

        """"""
    read_weights = hidden['read_weights'].gather(1, hidden['read_positions'])
<mask>:
        read_weights = read_weights + 1
    write_weights = hidden['write_weights'].gather(1, hidden['read_positions'])
    hidden['usage'], I = self.update_usage(hidden['read_positions'], read_weights, write_weights, hidden['usage'])
    x = interpolation_gate * read_weights
    y = (1 - interpolation_gate) * I
    write_weights = write_gate * (x + y)
    new_write_weights = hidden['write_weights'].clone()
    new_write_weights.scatter_(1, hidden['read_positions'], write_weights)
    hidden['write_weights'] = new_write_weights
    erase_matrix = I.unsqueeze(2).expand(hidden['visible_memory'].size())
    hidden['visible_memory'] = hidden['visible_memory'] * (1 - erase_matrix) + torch.bmm(write_weights.unsqueeze(2), write_vector)
    hidden = self.write_into_sparse_memory(hidden)
    b, _ = write_weights.size()
    temporal_read_positions = hidden['read_positions'][:, self.read_heads * self.K + 1:]
    hidden['link_matrix'], hidden['rev_link_matrix'] = self.update_link_matrices(hidden['link_matrix'], hidden['rev_link_matrix'], hidden['write_weights'], hidden['precedence'], temporal_read_positions)
    read_weights = hidden['read_weights'].gather(1, temporal_read_positions)
    hidden['precedence'] = self.update_precedence(hidden['precedence'], read_weights)
    hidden['least_used_mem'] = torch.topk(hidden['usage'], 1, dim=-1, largest=False)[1]
    return hidden",self.timestep == 1,139,self.use_linalg,False,17.491650626361256,N/A
"def forward(self, ξ: torch.Tensor, hidden: dict) -> tuple[torch.Tensor, dict]:
    """"""Forward pass through the memory.

        Args:
            ξ: Input tensor.
            hidden: Hidden state dictionary

        Returns:
            Tuple: Read vectors and updated hidden state.
        """"""
    m = self.mem_size
    w = self.cell_size
    r = self.read_heads
    c = self.c
    b = ξ.size(0)
<mask>:
        read_query = self.read_query_transform(ξ).view(b, r, w)
        write_vector = self.write_vector_transform(ξ).view(b, 1, w)
        interpolation_gate = torch.sigmoid(self.interpolation_gate_transform(ξ)).view(b, c)
        write_gate = torch.sigmoid(self.write_gate_transform(ξ).view(b, 1))
    else:
        ξ = self.interface_weights(ξ)
        read_query = ξ[:, :r * w].contiguous().view(b, r, w)
        write_vector = ξ[:, r * w:r * w + w].contiguous().view(b, 1, w)
        interpolation_gate = torch.sigmoid(ξ[:, r * w + w:r * w + w + c]).contiguous().view(b, c)
        write_gate = torch.sigmoid(ξ[:, -1].contiguous()).unsqueeze(1).view(b, 1)
    self.timestep += 1
    hidden = self.write(interpolation_gate, write_vector, write_gate, hidden)
    return self.read(read_query, hidden)",self.independent_linears,121,self.use_tuning,False,23.643540225079384,N/A
"def __init__(self, input_size: int, mem_size: int=512, cell_size: int=32, independent_linears: bool=True, read_heads: int=4, sparse_reads: int=4, num_lists: int | None=None, index_checks: int=32, device: torch.device | None=None):
    """"""Initialize SparseMemory.

        Args:
            input_size: Input size.
            mem_size: Memory size.
            cell_size: Size of each memory cell.
            independent_linears: Whether to use independent linear layers.
            read_heads: Number of read heads.
            sparse_reads: Number of sparse reads.
            num_lists: Number of lists for indexing.
            index_checks: Number of index checks.
            device: PyTorch device to use.
        """"""
    super(SparseMemory, self).__init__()
    self.mem_size = mem_size
    self.cell_size = cell_size
    self.device = device
    self.input_size = input_size
    self.independent_linears = independent_linears
    self.K = sparse_reads if self.mem_size > sparse_reads else self.mem_size
    self.read_heads = read_heads
    self.num_lists = num_lists if num_lists is not None else int(self.mem_size / 100)
    self.index_checks = index_checks
    m = self.mem_size
    w = self.cell_size
    r = self.read_heads
    self.c = r * self.K + 1
<mask>:
        self.read_query_transform = nn.Linear(self.input_size, w * r)
        self.write_vector_transform = nn.Linear(self.input_size, w)
        self.interpolation_gate_transform = nn.Linear(self.input_size, self.c)
        self.write_gate_transform = nn.Linear(self.input_size, 1)
        torch.nn.init.kaiming_uniform_(self.read_query_transform.weight)
        torch.nn.init.kaiming_uniform_(self.write_vector_transform.weight)
        torch.nn.init.kaiming_uniform_(self.interpolation_gate_transform.weight)
        torch.nn.init.kaiming_uniform_(self.write_gate_transform.weight)
    else:
        self.interface_size = r * w + w + self.c + 1
        self.interface_weights = nn.Linear(self.input_size, self.interface_size)
        torch.nn.init.kaiming_uniform_(self.interface_weights.weight)
    self.I = cuda(1 - torch.eye(self.c).unsqueeze(0), device=self.device)
    self.δ = 0.005
    self.timestep = 0
    self.mem_limit_reached = False
    if self.device is not None and self.device.type == 'cuda':
        self.to(self.device)",self.independent_linears,198,self.independent_linears,True,100.00000000000004,N/A
"def rebuild_indexes(self, hidden: dict, erase: bool=False) -> dict:
    """"""Rebuilds the indexes for sparse memory access.

        Args:
            hidden: Hidden state dictionary.
            erase: Whether to erase the existing memory content.

        Returns:
            Updated hidden state dictionary.
        """"""
    b = hidden['memory'].size(0)
<mask>:
        for x in hidden['indexes']:
            x.reset()
    else:
        try:
            from .faiss_index import FAISSIndex
            hidden['indexes'] = [FAISSIndex(cell_size=self.cell_size, nr_cells=self.mem_size, K=self.K, num_lists=self.num_lists, probes=self.index_checks, device=self.device) for _ in range(b)]
        except ImportError:
            print('FAISS not found, please install FAISS, consult https://github.com/facebookresearch/faiss/blob/main/INSTALL.md')
            raise
    pos = hidden['read_positions'].squeeze().detach().cpu().numpy()
    if not erase:
        for n, i in enumerate(hidden['indexes']):
            i.reset()
            i.add(hidden['memory'][n], last=pos[n][-1])
    else:
        self.timestep = 0
        self.mem_limit_reached = False
    return hidden",'indexes' in hidden,95,erase,False,0.0,N/A
"def reset(self, batch_size: int=1, hidden: dict | None=None, erase: bool=True) -> dict:
    """"""Resets the memory and hidden state.

        Args:
            batch_size: Batch size.
            hidden:  Hidden state dictionary.
            erase: Whether to erase the existing memory content.
        Returns:
            Reset hidden state dictionary.

        """"""
    m = self.mem_size
    w = self.cell_size
    b = batch_size
    r = self.read_heads
    c = self.c
<mask>:
        hidden = {'memory': cuda(torch.zeros(b, m, w).fill_(self.δ), device=self.device), 'visible_memory': cuda(torch.zeros(b, c, w).fill_(self.δ), device=self.device), 'read_weights': cuda(torch.zeros(b, m).fill_(self.δ), device=self.device), 'write_weights': cuda(torch.zeros(b, m).fill_(self.δ), device=self.device), 'read_vectors': cuda(torch.zeros(b, r, w).fill_(self.δ), device=self.device), 'least_used_mem': cuda(torch.zeros(b, 1).fill_(c + 1), device=self.device).long(), 'usage': cuda(torch.zeros(b, m), device=self.device), 'read_positions': cuda(torch.arange(0, c).expand(b, c), device=self.device).long()}
        hidden = self.rebuild_indexes(hidden, erase=True)
    else:
        hidden['memory'] = hidden['memory'].clone()
        hidden['visible_memory'] = hidden['visible_memory'].clone()
        hidden['read_weights'] = hidden['read_weights'].clone()
        hidden['write_weights'] = hidden['write_weights'].clone()
        hidden['read_vectors'] = hidden['read_vectors'].clone()
        hidden['least_used_mem'] = hidden['least_used_mem'].clone()
        hidden['usage'] = hidden['usage'].clone()
        hidden['read_positions'] = hidden['read_positions'].clone()
        hidden = self.rebuild_indexes(hidden, erase)
        if erase:
            hidden['memory'].data.fill_(self.δ)
            hidden['visible_memory'].data.fill_(self.δ)
            hidden['read_weights'].data.fill_(self.δ)
            hidden['write_weights'].data.fill_(self.δ)
            hidden['read_vectors'].data.fill_(self.δ)
            hidden['least_used_mem'].data.fill_(c + 1)
            hidden['usage'].data.fill_(0)
            hidden['read_positions'] = cuda(torch.arange(0, c).expand(b, c), device=self.device).long()
    return hidden",hidden is None,148,hidden is None,True,100.00000000000004,N/A
"def write(self, interpolation_gate: torch.Tensor, write_vector: torch.Tensor, write_gate: torch.Tensor, hidden: dict) -> dict:
    """"""Performs the memory write operation.

        Args:
            interpolation_gate: Interpolation gate.
            write_vector: Write vector.
            write_gate: Write gate.
            hidden: Hidden state dictionary.

        Returns:
            Updated hidden state dictionary.

        """"""
    read_weights = hidden['read_weights'].gather(1, hidden['read_positions'])
<mask>:
        read_weights = read_weights + 1
    write_weights = hidden['write_weights'].gather(1, hidden['read_positions'])
    hidden['usage'], I = self.update_usage(hidden['read_positions'], read_weights, write_weights, hidden['usage'])
    x = interpolation_gate * read_weights
    y = (1 - interpolation_gate) * I
    write_weights = write_gate * (x + y)
    new_write_weights = hidden['write_weights'].clone()
    new_write_weights.scatter_(1, hidden['read_positions'], write_weights)
    hidden['write_weights'] = new_write_weights
    erase_matrix = I.unsqueeze(2).expand(hidden['visible_memory'].size())
    hidden['visible_memory'] = hidden['visible_memory'] * (1 - erase_matrix) + torch.bmm(write_weights.unsqueeze(2), write_vector)
    hidden = self.write_into_sparse_memory(hidden)
    hidden['least_used_mem'] = torch.topk(hidden['usage'], 1, dim=-1, largest=False)[1]
    return hidden",self.timestep == 1,110,self.use_linalg,False,17.491650626361256,N/A
"def forward(self, ξ: torch.Tensor, hidden: dict) -> tuple[torch.Tensor, dict]:
    """"""Forward pass through the memory.

        Args:
            ξ: Input tensor.
            hidden: Hidden state dictionary.

        Returns:
            Tuple: Read vectors and updated hidden state.
        """"""
    m = self.mem_size
    w = self.cell_size
    r = self.read_heads
    c = self.c
    b = ξ.size(0)
<mask>:
        read_query = self.read_query_transform(ξ).view(b, r, w)
        write_vector = self.write_vector_transform(ξ).view(b, 1, w)
        interpolation_gate = torch.sigmoid(self.interpolation_gate_transform(ξ)).view(b, c)
        write_gate = torch.sigmoid(self.write_gate_transform(ξ).view(b, 1))
    else:
        ξ = self.interface_weights(ξ)
        read_query = ξ[:, :r * w].contiguous().view(b, r, w)
        write_vector = ξ[:, r * w:r * w + w].contiguous().view(b, 1, w)
        interpolation_gate = torch.sigmoid(ξ[:, r * w + w:r * w + w + c]).contiguous().view(b, c)
        write_gate = torch.sigmoid(ξ[:, -1].contiguous()).unsqueeze(1).view(b, 1)
    self.timestep += 1
    hidden = self.write(interpolation_gate, write_vector, write_gate, hidden)
    return self.read(read_query, hidden)",self.independent_linears,121,self.use_tuning,False,23.643540225079384,N/A
"def __init__(self, input_size: int, nr_cells: int=512, cell_size: int=32, read_heads: int=4, independent_linears: bool=True, device: torch.device | None=None):
    """"""Memory module.

        Args:
            input_size: Input size
            nr_cells: Number of memory cells. Defaults to 512.
            cell_size:Size of each memory cell. Defaults to 32.
            read_heads: Number of read heads. Defaults to 4.
            independent_linears: Use independent linear modules for memory transform operators. Defaults to False.
            device: Device (cpu, cuda, cuda:0, ...)
        """"""
    super(Memory, self).__init__()
    self.nr_cells = nr_cells
    self.cell_size = cell_size
    self.read_heads = read_heads
    self.input_size = input_size
    self.independent_linears = independent_linears
    self.device = device
<mask>:
        self.read_keys_transform = nn.Linear(self.input_size, self.cell_size * self.read_heads)
        self.read_strengths_transform = nn.Linear(self.input_size, self.read_heads)
        self.write_key_transform = nn.Linear(self.input_size, self.cell_size)
        self.write_strength_transform = nn.Linear(self.input_size, 1)
        self.erase_vector_transform = nn.Linear(self.input_size, self.cell_size)
        self.write_vector_transform = nn.Linear(self.input_size, self.cell_size)
        self.free_gates_transform = nn.Linear(self.input_size, self.read_heads)
        self.allocation_gate_transform = nn.Linear(self.input_size, 1)
        self.write_gate_transform = nn.Linear(self.input_size, 1)
        self.read_modes_transform = nn.Linear(self.input_size, 3 * self.read_heads)
        torch.nn.init.kaiming_uniform_(self.read_keys_transform.weight)
        torch.nn.init.kaiming_uniform_(self.read_strengths_transform.weight)
        torch.nn.init.kaiming_uniform_(self.write_key_transform.weight)
        torch.nn.init.kaiming_uniform_(self.write_strength_transform.weight)
        torch.nn.init.kaiming_uniform_(self.erase_vector_transform.weight)
        torch.nn.init.kaiming_uniform_(self.write_vector_transform.weight)
        torch.nn.init.kaiming_uniform_(self.free_gates_transform.weight)
        torch.nn.init.kaiming_uniform_(self.allocation_gate_transform.weight)
        torch.nn.init.kaiming_uniform_(self.write_gate_transform.weight)
        torch.nn.init.kaiming_uniform_(self.read_modes_transform.weight)
    else:
        self.interface_size = self.cell_size * self.read_heads + 3 * self.cell_size + 5 * self.read_heads + 3
        self.interface_weights = nn.Linear(self.input_size, self.interface_size)
        torch.nn.init.kaiming_uniform_(self.interface_weights.weight)
    self.I = cuda(1 - torch.eye(self.nr_cells).unsqueeze(0), device=self.device)
    if self.device is not None and self.device.type == 'cuda':
        self.to(self.device)",self.independent_linears,177,self.device is not None,False,16.233395773754953,N/A
"def reset(self, batch_size: int=1, hidden: MemoryHiddenState | None=None, erase: bool=True) -> MemoryHiddenState:
    """"""Reset hidden states.

        Args:
            batch_size: Batch size. Defaults to 1.
            hidden: Dict containing hidden states. Defaults to None.
            erase: Whether to erase the states. Defaults to True.

        Returns:
            MemoryHiddenState: A dict containing hidden states of the memory module.
        """"""
<mask>:
        return self.new(batch_size)
    else:
        hidden = self.clone(hidden)
        if erase:
            hidden = self.erase(hidden)
    return hidden",hidden is None,65,hidden is None,True,100.00000000000004,N/A
"def recursiveTrace(obj: torch.Tensor | torch.nn.Module | None) -> None:
    """"""Recursively traces the computational graph of a tensor or module.

    Args:
        obj: The tensor or module to trace.
    """"""
<mask>:
        return
    print(type(obj))
    if hasattr(obj, 'grad_fn'):
        print(obj.grad_fn)
        recursiveTrace(obj.grad_fn)
    elif hasattr(obj, 'next_functions'):
        print(obj.requires_grad, len(obj.next_functions))
        for f, _ in obj.next_functions:
            recursiveTrace(f)",obj is None,47,obj is None,True,100.00000000000004,N/A
"def cuda(x: torch.Tensor, requires_grad: bool=False, device: torch.device | None=None) -> torch.Tensor:
    """"""Moves a tensor to the specified device (CPU or GPU).

    Args:
        x: The tensor to move.
        requires_grad: Whether the tensor should require gradients.
        device: The device to move the tensor to.  Defaults to CPU.

    Returns:
        The tensor on the specified device.
    """"""
<mask>:
        return x.float().requires_grad_(requires_grad)
    else:
        return x.float().to(device).requires_grad_(requires_grad)",device is None,59,device is None,True,100.00000000000004,N/A
"def register_nan_checks(model: nn.Module) -> None:
    """"""Registers backward hooks to check for NaN gradients.

    Args:
        model: The model to register hooks on.
    """"""

    def check_grad(module: nn.Module, grad_input: tuple[torch.Tensor | None, ...], grad_output: tuple[torch.Tensor | None, ...]) -> None:
<mask>:
            print(f'NaN gradient in grad_input of {type(module).__name__}')
    for module in model.modules():
        module.register_full_backward_hook(check_grad)",any((torch.isnan(gi).any() for gi in grad_input if gi is not None)),49,"not torch.is_equal(grad_input, grad_output)",False,6.843288490221234,N/A
"def apply_dict(dic: dict) -> None:
    """"""Applies gradient NaN checks to a dictionary of variables.

    Args:
        dic: The dictionary.
    """"""
    for k, v in dic.items():
        apply_var(v, k)
<mask>:
            key_list = [a for a in dir(v) if not a.startswith('__')]
            for key in key_list:
                apply_var(getattr(v, key), key)
            for pk, pv in v.named_parameters():
                apply_var(pv, pk)","isinstance(v, nn.Module)",51,"isinstance(v, dict)",False,38.49815007763549,N/A
"def apply_var(v: torch.Tensor | nn.Module | None, k: str) -> None:
    """"""Applies gradient NaN checks to a variable.

    Args:
        v: The variable.
        k: The name of the variable.
    """"""
<mask>:
        v.register_hook(check_nan_gradient(k))","isinstance(v, torch.Tensor) and v.requires_grad",31,v is not None,False,1.3111910958201225,N/A
"def __init__(self, input_size: int, hidden_size: int, rnn_type: str='lstm', num_layers: int=1, num_hidden_layers: int=2, bias: bool=True, batch_first: bool=True, dropout: float=0, bidirectional: bool=False, nr_cells: int=5000, sparse_reads: int=4, temporal_reads: int=4, read_heads: int=4, cell_size: int=10, nonlinearity: str='tanh', independent_linears: bool=False, share_memory: bool=True, debug: bool=False, clip: float=20, device: torch.device | None=None):
    """"""

        Args:
            input_size: Input size.
            hidden_size: Hidden size.
            rnn_type: Type of RNN cell (lstm, gru, rnn).
            num_layers: Number of RNN layers.
            num_hidden_layers: Number of hidden layers in each RNN.
            bias: Whether to use bias in the RNN.
            batch_first: Whether the input is batch-first.
            dropout: Dropout rate.
            bidirectional: Whether the RNN is bidirectional.
            nr_cells: Number of memory cells.
            sparse_reads: Number of sparse reads.
            temporal_reads: Number of temporal reads.
            read_heads: Number of read heads.
            cell_size: Size of each memory cell.
            nonlinearity: Nonlinearity for RNN ('tanh' or 'relu').
            independent_linears: Whether to use independent linear layers in memory.
            share_memory: Whether to share memory across layers.
            debug: Whether to enable debug mode.
            clip: Value to clip controller output.
            device: the device to use
        """"""
    super(SDNC, self).__init__(input_size=input_size, hidden_size=hidden_size, rnn_type=rnn_type, num_layers=num_layers, num_hidden_layers=num_hidden_layers, bias=bias, batch_first=batch_first, dropout=dropout, nr_cells=nr_cells, read_heads=read_heads, cell_size=cell_size, nonlinearity=nonlinearity, independent_linears=independent_linears, share_memory_between_layers=share_memory, debug=debug, clip=clip, device=device)
    self.sparse_reads = sparse_reads
    self.temporal_reads = temporal_reads
    self.device = device
    self.memories = []
    for layer in range(self.num_layers):
<mask>:
            self.memories.append(SparseTemporalMemory(input_size=self.output_size, mem_size=self.nr_cells, cell_size=self.w, sparse_reads=self.sparse_reads, read_heads=self.read_heads, temporal_reads=self.temporal_reads, device=self.device, independent_linears=self.independent_linears))
            setattr(self, 'rnn_layer_memory_' + str(layer), self.memories[layer])
    if self.share_memory_between_layers:
        self.memories.append(SparseTemporalMemory(input_size=self.output_size, mem_size=self.nr_cells, cell_size=self.w, sparse_reads=self.sparse_reads, read_heads=self.read_heads, temporal_reads=self.temporal_reads, device=self.device, independent_linears=self.independent_linears))
        setattr(self, 'rnn_layer_memory_shared', self.memories[0])",not self.share_memory_between_layers,224,device is not None,False,3.564186929405141,N/A
"def _debug(self, mhx: dict, debug_obj: dict | None) -> dict | None:
    """"""Debug function to collect memory information.

        Args:
            mhx: Memory hidden state.
            debug_obj: Debug object to store information.

        Returns:
            Updated debug object or None.
        """"""
<mask>:
        debug_obj = {'memory': [], 'visible_memory': [], 'link_matrix': [], 'rev_link_matrix': [], 'precedence': [], 'read_weights': [], 'write_weights': [], 'read_vectors': [], 'least_used_mem': [], 'usage': [], 'read_positions': []}
    debug_obj['memory'].append(mhx['memory'][0].detach().cpu().numpy())
    debug_obj['visible_memory'].append(mhx['visible_memory'][0].detach().cpu().numpy())
    debug_obj['link_matrix'].append(mhx['link_matrix'][0].detach().cpu().numpy())
    debug_obj['rev_link_matrix'].append(mhx['rev_link_matrix'][0].detach().cpu().numpy())
    debug_obj['precedence'].append(mhx['precedence'][0].unsqueeze(0).detach().cpu().numpy())
    debug_obj['read_weights'].append(mhx['read_weights'][0].unsqueeze(0).detach().cpu().numpy())
    debug_obj['write_weights'].append(mhx['write_weights'][0].unsqueeze(0).detach().cpu().numpy())
    debug_obj['read_vectors'].append(mhx['read_vectors'][0].detach().cpu().numpy())
    debug_obj['least_used_mem'].append(mhx['least_used_mem'][0].unsqueeze(0).detach().cpu().numpy())
    debug_obj['usage'].append(mhx['usage'][0].unsqueeze(0).detach().cpu().numpy())
    debug_obj['read_positions'].append(mhx['read_positions'][0].unsqueeze(0).detach().cpu().numpy())
    return debug_obj",not debug_obj,74,debug_obj is None,False,39.76353643835252,N/A
"def _init_hidden(self, hx: DNCHiddenState | None, batch_size: int, reset_experience: bool) -> DNCHiddenState:
    """"""Initializes the hidden states.

        Args:
            hx: Existing hidden state or None.
            batch_size: Batch size.
            reset_experience: Whether to reset memory experience.

        Returns:
            Initialized hidden state.
        """"""
<mask>:
        chx, mhx, last_read = hx
    else:
        chx, mhx, last_read = (None, None, None)
    if chx is None:
        h: torch.Tensor = cuda(torch.zeros(self.num_hidden_layers, batch_size, self.output_size), device=self.device)
        torch.nn.init.xavier_uniform_(h)
        chx = [(h, h) if self.rnn_type.lower() == 'lstm' else h for _ in range(self.num_layers)]
    if last_read is None:
        last_read = cuda(torch.zeros(batch_size, self.w * self.r), device=self.device)
    if mhx is None:
        if self.share_memory_between_layers:
            mhx = [self.memories[0].reset(batch_size, erase=reset_experience)]
        else:
            mhx = [m.reset(batch_size, erase=reset_experience) for m in self.memories]
    elif self.share_memory_between_layers:
        if len(mhx) == 0 or mhx[0] is None:
            mhx = [self.memories[0].reset(batch_size, erase=reset_experience)]
        else:
            mhx = [self.memories[0].reset(batch_size, mhx[0], erase=reset_experience)]
    elif len(mhx) == 0:
        mhx = [m.reset(batch_size, erase=reset_experience) for m in self.memories]
    else:
        new_mhx = []
        for i, m in enumerate(self.memories):
            if i < len(mhx) and mhx[i] is not None:
                new_mhx.append(m.reset(batch_size, mhx[i], erase=reset_experience))
            else:
                new_mhx.append(m.reset(batch_size, erase=reset_experience))
        mhx = new_mhx
    return (chx, mhx, last_read)",hx is not None,170,hx is not None,True,100.00000000000004,N/A
"def _debug(self, mhx: MemoryHiddenState, debug_obj: dict[str, list[np.ndarray]] | None) -> dict[str, list[np.ndarray]] | None:
    """"""Collects debug information.  Only returns a debug_obj if self.debug is True.

        Args:
            mhx: Memory hidden state.
            debug_obj: Debug object containing lists of numpy arrays.

        Returns:
             Debug object or None.
        """"""
<mask>:
        return None
    if not debug_obj:
        debug_obj = {'memory': [], 'link_matrix': [], 'precedence': [], 'read_weights': [], 'write_weights': [], 'usage_vector': []}
    debug_obj['memory'].append(mhx['memory'][0].detach().cpu().numpy())
    debug_obj['link_matrix'].append(mhx['link_matrix'][0][0].detach().cpu().numpy())
    debug_obj['precedence'].append(mhx['precedence'][0].detach().cpu().numpy())
    debug_obj['read_weights'].append(mhx['read_weights'][0].detach().cpu().numpy())
    debug_obj['write_weights'].append(mhx['write_weights'][0].detach().cpu().numpy())
    debug_obj['usage_vector'].append(mhx['usage_vector'][0].unsqueeze(0).detach().cpu().numpy())
    return debug_obj",not self.debug,72,not mhx.debug,False,35.35533905932737,N/A
"def _layer_forward(self, input: torch.Tensor, layer: int, hx: LayerHiddenState, pass_through_memory: bool=True) -> tuple[torch.Tensor, LayerHiddenState]:
    """"""Performs a forward pass through a single layer.

        Args:
            input : Input tensor.
            layer: Layer index.
            hx: Hidden state for the layer.
            pass_through_memory: Whether to pass the input through memory.

        Returns:
            Tuple: Output, and updated hidden state.
        """"""
    chx, mhx, _ = hx
    input, chx = self.rnns[layer](input.unsqueeze(1), chx)
    input = input.squeeze(1)
<mask>:
        output = torch.clamp(input, -self.clip, self.clip)
    else:
        output = input
    ξ = output
    if pass_through_memory:
        if self.share_memory_between_layers:
            read_vecs, mhx = self.memories[0](ξ, mhx)
        else:
            read_vecs, mhx = self.memories[layer](ξ, mhx)
        read_vectors = read_vecs.view(-1, self.w * self.r)
    else:
        read_vectors = cuda(torch.zeros(ξ.size(0), self.w * self.r), device=self.device)
    return (output, (chx, mhx, read_vectors))",self.clip != 0,111,self.clip is not None,False,30.213753973567677,N/A
"def forward(self, input_data: torch.Tensor | PackedSequence, hx: DNCHiddenState | None, reset_experience: bool=False, pass_through_memory: bool=True) -> tuple[torch.Tensor | PackedSequence, DNCHiddenState] | tuple[torch.Tensor | PackedSequence, DNCHiddenState, dict[str, Any]]:
    """"""Performs a forward pass through the DNC.

        Args:
            input_data: Input tensor or PackedSequence.
            hx:  Hidden state or None.
            reset_experience: Whether to reset memory experience.
            pass_through_memory: Whether to pass the input through memory.

        Returns:
            Tuple: Output (same type as input_data), updated hidden state, and optionally debug information.

        """"""
    max_length: int
<mask>:
        input, lengths = pad_packed_sequence(input_data, batch_first=self.batch_first)
        max_length = int(lengths.max().item())
    elif isinstance(input_data, torch.Tensor):
        input = input_data
        batch_size = input.size(0) if self.batch_first else input.size(1)
        max_length = input.size(1) if self.batch_first else input.size(0)
        lengths = torch.tensor([max_length] * batch_size, device=input.device)
    else:
        raise TypeError('input_data must be a PackedSequence or Tensor')
    if not self.batch_first:
        input = input.transpose(0, 1)
    controller_hidden, mem_hidden, last_read = self._init_hidden(hx, batch_size, reset_experience)
    inputs = [torch.cat([input[:, x, :], last_read], 1) for x in range(max_length)]
    if self.debug:
        viz: dict[str, Any] | None = None
    outs: list[torch.Tensor | None] = [None] * max_length
    read_vectors: torch.Tensor | None = None
    for time in range(max_length):
        for layer in range(self.num_layers):
            chx_layer = controller_hidden[layer]
            mem_layer = mem_hidden[0] if self.share_memory_between_layers else mem_hidden[layer]
            outs[time], (chx_layer_output, mem_layer_output, read_vectors) = self._layer_forward(inputs[time], layer, (chx_layer, mem_layer, read_vectors), pass_through_memory)
            if self.debug:
                viz = self._debug(mem_layer_output, viz)
            if self.share_memory_between_layers:
                mem_hidden[0] = mem_layer_output
            else:
                mem_hidden[layer] = mem_layer_output
            controller_hidden[layer] = chx_layer_output
            if read_vectors is not None:
                outs[time] = torch.cat([outs[time], read_vectors], 1)
            else:
                outs[time] = torch.cat([outs[time], last_read], 1)
            inputs[time] = outs[time]
    if self.debug and viz:
        viz = {k: [np.array(v) for v in vs] for k, vs in viz.items()}
        viz = {k: [v.reshape(v.shape[0], -1) for v in vs] for k, vs in viz.items()}
    inputs_tensor = torch.stack(inputs)
    outputs = self.output(inputs_tensor)
    if not self.batch_first:
        outputs = outputs.transpose(0, 1)
    if isinstance(input_data, PackedSequence):
        outputs = pack_padded_sequence(outputs, lengths.cpu(), batch_first=self.batch_first, enforce_sorted=False)
    if self.debug:
        return (outputs, (controller_hidden, mem_hidden, read_vectors), viz)
    else:
        return (outputs, (controller_hidden, mem_hidden, read_vectors))","isinstance(input_data, PackedSequence)",300,"isinstance(input_data, torch.PackedSequence)",False,58.14307369682194,N/A
"def __init__(self, input_size: int, hidden_size: int, rnn_type: str='lstm', num_layers: int=1, num_hidden_layers: int=2, bias: bool=True, batch_first: bool=True, dropout: float=0.0, bidirectional: bool=False, nr_cells: int=5000, sparse_reads: int=4, read_heads: int=4, cell_size: int=10, nonlinearity: str='tanh', independent_linears: bool=False, share_memory: bool=True, debug: bool=False, clip: float=20, device: torch.device | None=None):
    """"""Initialize SAM.

        Args:
            input_size: Input size.
            hidden_size: Hidden size.
            rnn_type: Type of RNN cell (lstm, gru, rnn).
            num_layers: Number of RNN layers.
            num_hidden_layers: Number of hidden layers in each RNN.
            bias: Whether to use bias in the RNN.
            batch_first: Whether the input is batch-first.
            dropout: Dropout rate.
            bidirectional: Whether the RNN is bidirectional.
            nr_cells: Number of memory cells.
            sparse_reads: Number of sparse reads.
                 read_heads: Number of read heads.
                 cell_size: Size of each memory cell.
                 nonlinearity: Nonlinearity for RNN ('tanh' or 'relu').
                 device: GPU ID (deprecated, use device).
                 independent_linears: Whether to use independent linear layers in memory.
                 share_memory: Whether to share memory across layers.
                 debug: Whether to enable debug mode.
                 clip: Value to clip controller output.
                 device: PyTorch device.
        """"""
    super(SAM, self).__init__(input_size=input_size, hidden_size=hidden_size, rnn_type=rnn_type, num_layers=num_layers, num_hidden_layers=num_hidden_layers, bias=bias, batch_first=batch_first, dropout=dropout, nr_cells=nr_cells, read_heads=read_heads, cell_size=cell_size, nonlinearity=nonlinearity, independent_linears=independent_linears, share_memory_between_layers=share_memory, debug=debug, clip=clip, device=device)
    self.sparse_reads = sparse_reads
    self.device = device
    self.memories = []
    for layer in range(self.num_layers):
<mask>:
            self.memories.append(SparseMemory(input_size=self.output_size, mem_size=self.nr_cells, cell_size=self.w, sparse_reads=self.sparse_reads, read_heads=self.read_heads, device=self.device, independent_linears=self.independent_linears))
            setattr(self, 'rnn_layer_memory_' + str(layer), self.memories[layer])
    if self.share_memory_between_layers:
        self.memories.append(SparseMemory(input_size=self.output_size, mem_size=self.nr_cells, cell_size=self.w, sparse_reads=self.sparse_reads, read_heads=self.read_heads, device=self.device, independent_linears=self.independent_linears))
        setattr(self, 'rnn_layer_memory_shared', self.memories[0])",not self.share_memory_between_layers,217,device is not None,False,3.564186929405141,N/A
"def _debug(self, mhx: dict, debug_obj: dict | None) -> dict | None:
    """"""Debug function to collect memory information.
        Args:
            mhx: Memory hidden state.
            debug_obj: Debug object to store information.

        Returns:
            Updated debug object or None.
        """"""
<mask>:
        debug_obj = {'memory': [], 'visible_memory': [], 'read_weights': [], 'write_weights': [], 'read_vectors': [], 'least_used_mem': [], 'usage': [], 'read_positions': []}
    debug_obj['memory'].append(mhx['memory'][0].detach().cpu().numpy())
    debug_obj['visible_memory'].append(mhx['visible_memory'][0].detach().cpu().numpy())
    debug_obj['read_weights'].append(mhx['read_weights'][0].unsqueeze(0).detach().cpu().numpy())
    debug_obj['write_weights'].append(mhx['write_weights'][0].unsqueeze(0).detach().cpu().numpy())
    debug_obj['read_vectors'].append(mhx['read_vectors'][0].detach().cpu().numpy())
    debug_obj['least_used_mem'].append(mhx['least_used_mem'][0].unsqueeze(0).detach().cpu().numpy())
    debug_obj['usage'].append(mhx['usage'][0].unsqueeze(0).detach().cpu().numpy())
    debug_obj['read_positions'].append(mhx['read_positions'][0].unsqueeze(0).detach().cpu().numpy())
    return debug_obj",not debug_obj,65,debug_obj is None,False,39.76353643835252,N/A
"def __init__(self, cell_size: int=20, nr_cells: int=1024, K: int=4, num_lists: int=32, probes: int=32, res: faiss.GpuResources | None=None, train: torch.Tensor | None=None, device: torch.device | None=None):
    """"""Initialize FAISSIndex.

        Args:
            cell_size: Size of each memory cell.
            nr_cells: Number of memory cells.
            K: Number of nearest neighbors to retrieve.
            num_lists: Number of lists for the index.
            probes: Number of probes for searching.
            res: FAISS GpuResources object.
            train: Training data.
            device: PyTorch device

        """"""
    super(FAISSIndex, self).__init__()
    self.cell_size = cell_size
    self.nr_cells = nr_cells
    self.probes = probes
    self.K = K
    self.num_lists = num_lists
    self.device = device
    self.res = res if res else faiss.StandardGpuResources()
    train_tensor = train if train is not None else torch.randn(self.nr_cells * 100, self.cell_size)
<mask>:
        self.res.setTempMemoryFraction(0.01)
        self.res.initializeForDevice(self.device.index if self.device.index is not None else 0)
        quantizer = faiss.IndexFlatL2(self.cell_size)
        self.index = faiss.GpuIndexIVFFlat(self.res, quantizer, self.cell_size, self.num_lists, faiss.METRIC_L2)
    else:
        quantizer = faiss.IndexFlatL2(self.cell_size)
        self.index = faiss.IndexIVFFlat(quantizer, self.cell_size, self.num_lists, faiss.METRIC_L2)
    self.index.nprobes = self.probes
    self.train(train_tensor)",self.device is not None and self.device.type == 'cuda',143,self.res is not None,False,8.477112911296592,N/A
"def train(self, train: torch.Tensor) -> None:
    """"""Trains the index.

        Args:
            train: Training data.
        """"""
    train = ensure_gpu(train, self.device)
<mask>:
        torch.cuda.synchronize(self.device)
    self.index.train_c(self.nr_cells, cast_float(ptr(train)))
    if self.device is not None and self.device.type == 'cuda':
        torch.cuda.synchronize(self.device)",self.device is not None and self.device.type == 'cuda',32,self.device is not None and self.device.type == 'cuda',True,100.00000000000004,N/A
"def reset(self) -> None:
    """"""Resets the index.""""""
<mask>:
        torch.cuda.synchronize(self.device)
    self.index.reset()
    if self.device is not None and self.device.type == 'cuda':
        torch.cuda.synchronize(self.device)",self.device is not None and self.device.type == 'cuda',20,self.device is not None and self.device.type == 'cuda',True,100.00000000000004,N/A
"def add(self, other: torch.Tensor, positions: torch.Tensor | None=None, last: int | None=None) -> None:
    """"""Adds vectors to the index.

        Args:
            other: Vectors to add.
            positions: Positions of the vectors.
            last: Index of the last vector to add.
        """"""
    other = ensure_gpu(other, self.device)
<mask>:
        torch.cuda.synchronize(self.device)
    if positions is not None:
        positions = ensure_gpu(positions, self.device).long()
        assert positions.size(0) == other.size(0), 'Mismatch in number of positions and vectors'
        self.index.add_with_ids_c(other.size(0), cast_float(ptr(other)), cast_long(ptr(positions + 1)))
    else:
        other = other[:last, :] if last is not None else other
        self.index.add_c(other.size(0), cast_float(ptr(other)))
    if self.device is not None and self.device.type == 'cuda':
        torch.cuda.synchronize(self.device)",self.device is not None and self.device.type == 'cuda',93,self.device is not None and self.device.type == 'cuda',True,100.00000000000004,N/A
"def search(self, query: torch.Tensor, k: int | None=None) -> tuple[torch.Tensor, torch.Tensor]:
    """"""Searches the index for nearest neighbors.

        Args:
            query: Query vectors.
            k: Number of nearest neighbors to retrieve.

        Returns:
            Tuple: Distances and labels of the nearest neighbors.
        """"""
    query = ensure_gpu(query, self.device)
    k = k if k else self.K
    b, _ = query.size()
    distances = torch.empty(b, k, device=self.device, dtype=torch.float32)
    labels = torch.empty(b, k, device=self.device, dtype=torch.int64)
<mask>:
        torch.cuda.synchronize(self.device)
    self.index.search_c(b, cast_float(ptr(query)), k, cast_float(ptr(distances)), cast_long(ptr(labels)))
    if self.device is not None and self.device.type == 'cuda':
        torch.cuda.synchronize(self.device)
    return (distances, labels - 1)",self.device is not None and self.device.type == 'cuda',87,self.device is not None and self.device.type == 'cuda',True,100.00000000000004,N/A
"def _strip_xml(xml, changes):
    doc = etree.fromstring(xml)
    for xpath, attributes in changes.items():
        for node in doc.xpath(xpath):
            for attrib in node.attrib.keys():
<mask>:
                    del node.attrib[attrib]
    return etree.tostring(doc)",attrib not in attributes,24,attrib in attributes,False,45.13864405503391,N/A
"def _test_xmlrunner(self, suite, runner=None, outdir=None):
<mask>:
        outdir = self.outdir
    stream = self.stream
    verbosity = self.verbosity
    runner_kwargs = self.runner_kwargs
    if runner is None:
        runner = xmlrunner.XMLTestRunner(stream=stream, output=outdir, verbosity=verbosity, **runner_kwargs)
    if isinstance(outdir, BytesIO):
        self.assertFalse(outdir.getvalue())
    else:
        self.assertEqual(0, len(glob(os.path.join(outdir, '*xml'))))
    runner.run(suite)
    if isinstance(outdir, BytesIO):
        self.assertTrue(outdir.getvalue())
    else:
        self.assertEqual(1, len(glob(os.path.join(outdir, '*xml'))))
    return runner",outdir is None,47,outdir is None,True,100.00000000000004,N/A
"@mock.patch('sys.argv', ['xmlrunner', '-o', 'flaf'])
@mock.patch('xmlrunner.runner.XMLTestRunner')
@mock.patch('sys.exit')
def test_xmlrunner_output(self, exiter, testrunner):
    xmlrunner.runner.XMLTestProgram()
    kwargs = dict(buffer=mock.ANY, failfast=mock.ANY, verbosity=mock.ANY, warnings=mock.ANY, output='flaf')
<mask>:
        kwargs.update(tb_locals=mock.ANY)
    testrunner.assert_called_once_with(**kwargs)
    exiter.assert_called_once_with(False)","sys.version_info[:2] > (3, 4)",22,self.use_tb,False,1.719207234832579,N/A
"@mock.patch('sys.argv', ['xmlrunner', '--output-file', 'test.xml'])
@mock.patch('xmlrunner.runner.open')
@mock.patch('xmlrunner.runner.XMLTestRunner')
@mock.patch('sys.exit')
def test_xmlrunner_output_file(self, exiter, testrunner, opener):
    xmlrunner.runner.XMLTestProgram()
    opener.assert_called_once_with('test.xml', 'wb')
    open_file = opener()
    open_file.close.assert_called_with()
    kwargs = dict(buffer=mock.ANY, failfast=mock.ANY, verbosity=mock.ANY, warnings=mock.ANY, output=open_file)
<mask>:
        kwargs.update(tb_locals=mock.ANY)
    testrunner.assert_called_once_with(**kwargs)
    exiter.assert_called_once_with(False)","sys.version_info[:2] > (3, 4)",30,sys.version_info[0] == 2,False,34.49083812326535,N/A
"@mock.patch('sys.argv', ['xmlrunner', '--outsuffix', ''])
@mock.patch('xmlrunner.runner.open')
@mock.patch('xmlrunner.runner.XMLTestRunner')
@mock.patch('sys.exit')
def test_xmlrunner_outsuffix(self, exiter, testrunner, opener):
    xmlrunner.runner.XMLTestProgram()
    kwargs = dict(buffer=mock.ANY, failfast=mock.ANY, verbosity=mock.ANY, warnings=mock.ANY, outsuffix='')
<mask>:
        kwargs.update(tb_locals=mock.ANY)
    testrunner.assert_called_once_with(**kwargs)
    exiter.assert_called_once_with(False)","sys.version_info[:2] > (3, 4)",24,self.debug,False,0.5039742260635457,N/A
"def __init__(self, output='.', outsuffix=None, elapsed_times=True, encoding=UTF8, resultclass=None, **kwargs):
    super(XMLTestRunner, self).__init__(**kwargs)
    self.output = output
    self.encoding = encoding
<mask>:
        outsuffix = time.strftime('%Y%m%d%H%M%S')
    self.outsuffix = outsuffix
    self.elapsed_times = elapsed_times
    if resultclass is None:
        self.resultclass = _XMLTestResult
    else:
        self.resultclass = resultclass",outsuffix is None,37,outsuffix is None,True,100.00000000000004,N/A
"def run(self, test):
    """"""
        Runs the given test case or test suite.
        """"""
    try:
        result = self._make_result()
        result.failfast = self.failfast
        result.buffer = self.buffer
<mask>:
            result.properties = test.properties
        self.stream.writeln()
        self.stream.writeln('Running tests...')
        self.stream.writeln(result.separator2)
        start_time = time.monotonic()
        test(result)
        stop_time = time.monotonic()
        time_taken = stop_time - start_time
        result.printErrors()
        self.stream.writeln(result.separator2)
        run = result.testsRun
        self.stream.writeln('Ran %d test%s in %.3fs' % (run, run != 1 and 's' or '', time_taken))
        self.stream.writeln()
        expectedFails = len(result.expectedFailures)
        unexpectedSuccesses = len(result.unexpectedSuccesses)
        skipped = len(result.skipped)
        infos = []
        if not result.wasSuccessful():
            self.stream.write('FAILED')
            failed, errored = map(len, (result.failures, result.errors))
            if failed:
                infos.append('failures={0}'.format(failed))
            if errored:
                infos.append('errors={0}'.format(errored))
        else:
            self.stream.write('OK')
        if skipped:
            infos.append('skipped={0}'.format(skipped))
        if expectedFails:
            infos.append('expected failures={0}'.format(expectedFails))
        if unexpectedSuccesses:
            infos.append('unexpected successes={0}'.format(unexpectedSuccesses))
        if infos:
            self.stream.writeln(' ({0})'.format(', '.join(infos)))
        else:
            self.stream.write('\n')
        self.stream.writeln()
        self.stream.writeln('Generating XML reports...')
        result.generate_reports(self)
    finally:
        pass
    return result","hasattr(test, 'properties')",121,test,False,0.673794699908547,N/A
"def _parseKnownArgs(self, kwargs):
    argv = kwargs.get('argv')
<mask>:
        argv = sys.argv
    parser = argparse.ArgumentParser(prog='xmlrunner')
    group = parser.add_mutually_exclusive_group()
    group.add_argument('-o', '--output', metavar='DIR', help=""Directory for storing XML reports ('.' default)"")
    group.add_argument('--output-file', metavar='FILENAME', help='Filename for storing XML report')
    parser.add_argument('--outsuffix', metavar='STRING', help='Output suffix (timestamp is default)')
    namespace, argv = parser.parse_known_args(argv)
    self.output = namespace.output
    self.output_file = namespace.output_file
    self.outsuffix = namespace.outsuffix
    kwargs['argv'] = argv",argv is None,56,argv is None,True,100.00000000000004,N/A
"def runTests(self):
    kwargs = dict(verbosity=self.verbosity, failfast=self.failfast, buffer=self.buffer, warnings=self.warnings)
<mask>:
        kwargs.update(tb_locals=self.tb_locals)
    output_file = None
    try:
        if self.output_file is not None:
            output_file = open(self.output_file, 'wb')
            kwargs.update(output=output_file)
        elif self.output is not None:
            kwargs.update(output=self.output)
        if self.outsuffix is not None:
            kwargs.update(outsuffix=self.outsuffix)
        self.testRunner = self.testRunner(**kwargs)
        super(XMLTestProgram, self).runTests()
    finally:
        if output_file is not None:
            output_file.close()","sys.version_info[:2] > (3, 4)",48,self.tb_locals is not None,False,2.7376474102577792,N/A
"def _set_result_counters(self):
    """"""Sets an attribute in this context's tag for each counter considering
        what's valid for each tag name.
        """"""
    tag = self.element_tag()
    for counter_name in TestXMLContext._allowed_counters:
        valid_counter_for_element = False
<mask>:
            valid_counter_for_element = tag == 'testsuite'
        else:
            valid_counter_for_element = tag in ('testsuites', 'testsuite')
        if valid_counter_for_element:
            value = str(self.counters.get(counter_name, 0))
            self.element.setAttribute(counter_name, value)",counter_name == 'skipped',51,"tag in ('testsuites', 'testsuite')",False,0.0,N/A
"def increment_counter(self, counter_name):
    """"""Increments a counter named by `counter_name`, which can be any one
        defined in `_allowed_counters`.
        """"""
<mask>:
        self.counters[counter_name] = self.counters.get(counter_name, 0) + 1",counter_name in TestXMLContext._allowed_counters,25,counter_name not in self.counters,False,19.03868163669696,N/A
"def append(self, tag, content, **kwargs):
    """"""Apends a tag in the format <tag attr='val' attr2='val2'>CDATA</tag>
        into the tag represented by the current context. Returns the created
        tag.
        """"""
    element = self._xml_doc.createElement(tag)
    for key, value in kwargs.items():
        filtered_value = replace_nontext(str(value))
        element.setAttribute(key, filtered_value)
<mask>:
        element.appendChild(self._create_cdata_section(content))
    self._append_child(element)
    return element",content,45,content,True,100.00000000000004,N/A
"def _append_child(self, element):
    """"""Appends a tag object represented by `element` into the tag
        represented by the current context.
        """"""
<mask>:
        self._current_context.element.appendChild(element)
    else:
        self._xml_doc.appendChild(element)",self._current_context,23,self._current_context,True,100.00000000000004,N/A
"def end_context(self):
    """"""Ends the current context and sets the current context as being the
        previous one (if it exists). Also, when a context ends, its tag is
        appended in the proper place inside the document.
        """"""
<mask>:
        return False
    element = self._current_context.end()
    self._current_context = self._current_context.parent
    self._append_child(element)
    return True",not self._current_context,48,self._current_context is None,False,68.037493331712,N/A
"def testcase_name(test_method):
    testcase = type(test_method)
    module = testcase.__module__ + '.'
<mask>:
        module = ''
    result = module + testcase.__name__
    return result",module == '__main__.',21,not module,False,0.5554498269121152,N/A
"def write(self, b):
<mask>:
        wrote = self._first.write(b)
        if wrote is not None:
            self._second.write(b[:wrote])
        return wrote
    else:
        self._first.write(b)
        self._second.write(b)
        return len(b)","isinstance(self._first, io.TextIOBase)",20,self._second_bytes,False,13.13084334918613,N/A
"def __init__(self, test_result, test_method, outcome=SUCCESS, err=None, subTest=None, filename=None, lineno=None, doc=None):
    self.test_result = test_result
    self.outcome = outcome
    self.elapsed_time = 0
    self.timestamp = datetime.datetime.min.replace(microsecond=0).isoformat()
<mask>:
        if self.outcome != _TestInfo.SKIP:
            self.test_exception_name = safe_unicode(err[0].__name__)
            self.test_exception_message = safe_unicode(err[1])
        else:
            self.test_exception_message = safe_unicode(err)
    self.stdout = test_result._stdout_data
    self.stderr = test_result._stderr_data
    self.test_description = self.test_result.getDescription(test_method)
    self.test_exception_info = '' if outcome in (self.SUCCESS, self.SKIP) else self.test_result._exc_info_to_string(err, test_method)
    self.test_name = testcase_name(test_method)
    self.test_id = test_method.id()
    if subTest:
        self.test_id = subTest.id()
        self.test_description = self.test_result.getDescription(subTest)
    self.filename = filename
    self.lineno = lineno
    self.doc = doc",err is not None,80,err,False,4.9787068367863965,N/A
"def __init__(self, stream=sys.stderr, descriptions=1, verbosity=1, elapsed_times=True, properties=None, infoclass=None):
    TextTestResult.__init__(self, stream, descriptions, verbosity)
    self._stdout_data = None
    self._stderr_data = None
    self._stdout_capture = StringIO()
    self.__stdout_saved = None
    self._stderr_capture = StringIO()
    self.__stderr_saved = None
    self.successes = []
    self.callback = None
    self.elapsed_times = elapsed_times
    self.properties = properties
    self.filename = None
    self.lineno = None
    self.doc = None
<mask>:
        self.infoclass = _TestInfo
    else:
        self.infoclass = infoclass",infoclass is None,59,infoclass is None,True,100.00000000000004,N/A
"def _prepare_callback(self, test_info, target_list, verbose_str, short_str):
    """"""
        Appends a `infoclass` to the given target list and sets a callback
        method to be called by stopTest method.
        """"""
    test_info.filename = self.filename
    test_info.lineno = self.lineno
    test_info.doc = self.doc
    target_list.append(test_info)

    def callback():
        """"""Prints the test method outcome to the stream, as well as
            the elapsed time.
            """"""
        test_info.test_finished()
<mask>:
            self.start_time = self.stop_time = 0
        if self.showAll:
            self.stream.writeln('%s (%.3fs)' % (verbose_str, test_info.elapsed_time))
        elif self.dots:
            self.stream.write(short_str)
        self.stream.flush()
    self.callback = callback",not self.elapsed_times,75,self.stop_time,False,19.3576934939088,N/A
"def get_test_runner_kwargs(self):
    verbosity = getattr(settings, 'TEST_OUTPUT_VERBOSE', 1)
<mask>:
        verbosity = (1, 2)[verbosity]
    verbosity = verbosity
    output_dir = getattr(settings, 'TEST_OUTPUT_DIR', '.')
    single_file = getattr(settings, 'TEST_OUTPUT_FILE_NAME', None)
    if single_file is None:
        output = output_dir
    else:
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        file_path = os.path.join(output_dir, single_file)
        output = open(file_path, 'wb')
    return dict(verbosity=verbosity, descriptions=getattr(settings, 'TEST_OUTPUT_DESCRIPTIONS', False), failfast=self.failfast, resultclass=self.get_resultclass(), output=output)","isinstance(verbosity, bool)",53,verbosity < 1,False,10.122592925934278,N/A
"def run_suite(self, suite, **kwargs):
    runner_kwargs = self.get_test_runner_kwargs()
    runner = self.test_runner(**runner_kwargs)
    results = runner.run(suite)
<mask>:
        runner_kwargs['output'].close()
    return results","hasattr(runner_kwargs['output'], 'close')",17,'output' in runner_kwargs,False,12.869637315183779,N/A
"def is_truthy(arg):
    """"""Convert ""truthy"" strings into Booleans.

    Args:
        arg (str): Truthy string (True values are y, yes, t, true, on and 1; false values are n, no,
        f, false, off and 0. Raises ValueError if val is anything else.

    Examples:
        >>> is_truthy('yes')
        True
    """"""
<mask>:
        return arg
    return bool(strtobool(arg))","isinstance(arg, bool)",49,"isinstance(arg, str)",False,53.7284965911771,N/A
"def run_cmd(context, exec_cmd, local=INVOKE_LOCAL):
    """"""Wrapper to run the invoke task commands.

    Args:
        context ([invoke.task]): Invoke task object.
        exec_cmd ([str]): Command to run.
        local (bool): Define as `True` to execute locally

    Returns:
        result (obj): Contains Invoke result from running task.
    """"""
<mask>:
        print(f'LOCAL - Running command {exec_cmd}')
        result = context.run(exec_cmd, pty=True)
    else:
        print(f'DOCKER - Running command: {exec_cmd} container: {IMAGE_NAME}:{IMAGE_VER}')
        result = context.run(f""docker run -it -v {PWD}:/local {IMAGE_NAME}:{IMAGE_VER} sh -c '{exec_cmd}'"", pty=True)
    return result",is_truthy(local),72,local,False,0.673794699908547,N/A
"@task
def build(context, nocache=False, forcerm=False, hide=False):
    """"""Build a Docker image.

    Args:
        context (obj): Used to run specific commands
        nocache (bool): Do not use cache when building the image
        forcerm (bool): Always remove intermediate containers
        hide (bool): Hide output of Docker image build
    """"""
    print(f'Building image {IMAGE_NAME}:{IMAGE_VER}')
    command = f'docker build --tag {IMAGE_NAME}:{IMAGE_VER} --build-arg PYTHON_VER={PYTHON_VER} -f Dockerfile .'
<mask>:
        command += ' --no-cache'
    if forcerm:
        command += ' --force-rm'
    result = context.run(command, hide=hide)
    if result.exited != 0:
        print(f'Failed to build image {IMAGE_NAME}:{IMAGE_VER}\nError: {result.stderr}')",nocache,82,nocache,True,100.00000000000004,N/A
"def main(args):
    """"""Module function.""""""
<mask>:
        raise AnsibleError(""Invalid arguments, 'evaluate_args' not found."")
    check_type = args.get('check_type')
    evaluate_args = args.get('evaluate_args')
    if not isinstance(evaluate_args, dict):
        raise AnsibleError(f""'evaluate_args' invalid type, expected <class 'dict'>, got {type(evaluate_args)}"")
    if 'value_to_compare' not in evaluate_args:
        raise AnsibleError(""Key 'value_to_compare' missing in 'evaluate_arguments'."")
    reference_data = evaluate_args.get('reference_data')
    value = evaluate_args['value_to_compare']
    jpath = args.get('jmespath', '*')
    exclude = args.get('exclude')
    try:
        check = CheckType.create(check_type)
        evaluate_args['value_to_compare'] = extract_data_from_json(value, jpath, exclude)
        if reference_data:
            evaluate_args['reference_data'] = extract_data_from_json(reference_data, jpath, exclude)
        eval_results, passed = check.evaluate(**evaluate_args)
    except NotImplementedError:
        raise AnsibleError(f""CheckType '{check_type}' not supported by jdiff"")
    except Exception as e:
        raise AnsibleError(f'Exception in backend jdiff library: {e}')
    return dict(success=passed, fail_details=eval_results)",'evaluate_args' not in args,97,'evaluate_args' not in args,True,100.00000000000004,N/A
"def run(self, tmp=None, task_vars=None):
    """"""Run of action plugin for interacting with jdiff.

        Args:
            tmp ([type], optional): [description]. Defaults to None.
            task_vars ([type], optional): [description]. Defaults to None.
        """"""
<mask>:
        raise_from(AnsibleError('jdiff library must be installed to use this plugin'), JDIFF_IMPORT_ERROR)
    self._supports_check_mode = True
    self._supports_async = False
    result = super(ActionModule, self).run(tmp, task_vars)
    del tmp
    if result.get('skipped'):
        return None
    if result.get('invocation', {}).get('module_args'):
        del result['invocation']['module_args']
    args = self._task.args
    return main(args=args)",JDIFF_IMPORT_ERROR,66,not is_installed(),False,8.116697886877475,N/A
"def main():
    """"""Main execution.""""""
    base_argument_spec = dict(global_delay_factor=dict(default=1, required=False, type='int'), delay_factor=dict(default=1, required=False, type='int'), local_file=dict(required=False), remote_file=dict(required=False), file_system=dict(required=False))
    argument_spec = base_argument_spec
    argument_spec.update(CONNECTION_ARGUMENT_SPEC)
    argument_spec['provider'] = dict(required=False, type='dict', options=CONNECTION_ARGUMENT_SPEC)
    module = AnsibleModule(argument_spec=argument_spec, mutually_exclusive=MUTUALLY_EXCLUSIVE, required_one_of=[REQUIRED_ONE_OF], supports_check_mode=True)
    provider = module.params['provider'] or {}
    for param, pvalue in provider.items():
<mask>:
            module.params[param] = module.params.get(param) or pvalue
    if not HAS_PYNTC:
        module.fail_json(msg='pyntc is required for this module.')
    platform = module.params['platform']
    host = module.params['host']
    username = module.params['username']
    password = module.params['password']
    ntc_host = module.params['ntc_host']
    ntc_conf_file = module.params['ntc_conf_file']
    transport = module.params['transport']
    port = module.params['port']
    global_delay_factor = int(module.params['global_delay_factor'])
    delay_factor = int(module.params['delay_factor'])
    secret = module.params['secret']
    if ntc_host is not None:
        device = ntc_device_by_name(ntc_host, ntc_conf_file)
    else:
        kwargs = {}
        if transport is not None:
            kwargs['transport'] = transport
        if port is not None:
            kwargs['port'] = port
        if secret is not None:
            kwargs['secret'] = secret
        if global_delay_factor is not None:
            kwargs['global_delay_factor'] = global_delay_factor
        if delay_factor is not None:
            kwargs['delay_factor'] = delay_factor
        device_type = platform
        device = ntc_device(device_type, host, username, password, **kwargs)
    local_file = module.params['local_file']
    remote_file = module.params['remote_file']
    file_system = module.params['file_system']
    argument_check = {'host': host, 'username': username, 'platform': platform, 'password': password, 'local_file': local_file}
    for key, val in argument_check.items():
        if val is None:
            module.fail_json(msg=str(key) + ' is required')
    device.open()
    changed = False
    transfer_status = 'No Transfer'
    file_exists = True
    if not os.path.isfile(local_file):
        module.fail_json(msg='Local file {0} not found'.format(local_file))
    if file_system:
        remote_exists = device.file_copy_remote_exists(local_file, remote_file, file_system=file_system)
    else:
        remote_exists = device.file_copy_remote_exists(local_file, remote_file)
    if not remote_exists:
        changed = True
        file_exists = False
    if not module.check_mode and (not file_exists):
        try:
            if file_system:
                device.file_copy(local_file, remote_file, file_system=file_system)
            else:
                device.file_copy(local_file, remote_file)
            transfer_status = 'Sent'
        except Exception as err:
            module.fail_json(msg=str(err))
    try:
        device.close()
        atomic = True
    except:
        atomic = False
    if remote_file is None:
        remote_file = os.path.basename(local_file)
    module.exit_json(changed=changed, transfer_status=transfer_status, local_file=local_file, remote_file=remote_file, file_system=file_system, atomic=atomic)",module.params.get(param) is not False,271,"isinstance(pvalue, dict)",False,4.194930905450255,N/A
"def main():
    """"""Main execution.""""""
    base_argument_spec = dict(commands=dict(required=False, type='list'), commands_file=dict(required=False, default=None, type='str'))
    argument_spec = base_argument_spec
    argument_spec.update(CONNECTION_ARGUMENT_SPEC)
    argument_spec['provider'] = dict(required=False, type='dict', options=CONNECTION_ARGUMENT_SPEC)
    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, mutually_exclusive=MUTUALLY_EXCLUSIVE, required_one_of=[REQUIRED_ONE_OF])
<mask>:
        module.fail_json(msg='pyntc is required for this module.')
    if not any([module.params['commands'], module.params['commands_file']]):
        module.fail_json(msg='One of `commands` or `commands_file` argument is required.')
    if module.params['commands'] and module.params['commands_file']:
        module.fail_json(msg='The use of both `commands` and `commands_file` in the same task is not currently supported.')
    elif module.params['commands_file']:
        with open(module.params['commands_file'], 'r') as cmds:
            commands = [cmd.rstrip() for cmd in cmds]
    elif module.params['commands']:
        commands = module.params['commands']
    else:
        module.fail_json(msg='The combination of params used is not supported.')
    provider = module.params['provider'] or {}
    for param, pvalue in provider.items():
        if module.params.get(param) is not False:
            module.params[param] = module.params.get(param) or pvalue
    platform = module.params['platform']
    host = module.params['host']
    username = module.params['username']
    password = module.params['password']
    ntc_host = module.params['ntc_host']
    ntc_conf_file = module.params['ntc_conf_file']
    transport = module.params['transport']
    port = module.params['port']
    secret = module.params['secret']
    argument_check = {'host': host, 'username': username, 'platform': platform, 'password': password}
    for key, val in argument_check.items():
        if val is None:
            module.fail_json(msg=str(key) + ' is required')
    if ntc_host is not None:
        device = ntc_device_by_name(ntc_host, ntc_conf_file)
    else:
        kwargs = {}
        if transport is not None:
            kwargs['transport'] = transport
        if port is not None:
            kwargs['port'] = port
        if secret is not None:
            kwargs['secret'] = secret
        device_type = platform
        device = ntc_device(device_type, host, username, password, **kwargs)
    device.open()
    result = device.show(commands)
    device.close()
    module.exit_json(changed=False, results=result)",not HAS_PYNTC,217,"not any([ntc_host, ntc_conf_file])",False,3.1251907639724417,N/A
"def already_set(boot_options, system_image_file, kickstart_image_file, **kwargs):
    """"""Checks if set.""""""
    volume = kwargs.get('volume')
    device = kwargs.get('device')
<mask>:
        return device.image_installed(image_name=system_image_file, volume=volume)
    return boot_options.get('sys') == system_image_file and boot_options.get('kick') == kickstart_image_file",device and volume,26,device,False,13.533528323661276,N/A
"def main():
    """"""Main execution.""""""
    base_argument_spec = dict(global_delay_factor=dict(default=1, required=False, type='int'), delay_factor=dict(default=1, required=False, type='int'), remote_file=dict(required=False, type='str'), local_file=dict(required=False, type='str'))
    argument_spec = base_argument_spec
    argument_spec.update(CONNECTION_ARGUMENT_SPEC)
    argument_spec['provider'] = dict(required=False, type='dict', options=CONNECTION_ARGUMENT_SPEC)
    module = AnsibleModule(argument_spec=argument_spec, mutually_exclusive=MUTUALLY_EXCLUSIVE, required_one_of=[REQUIRED_ONE_OF], supports_check_mode=False)
<mask>:
        module.fail_json(msg='pyntc is required for this module.')
    provider = module.params['provider'] or {}
    for param, pvalue in provider.items():
        if module.params.get(param) is not False:
            module.params[param] = module.params.get(param) or pvalue
    platform = module.params['platform']
    host = module.params['host']
    username = module.params['username']
    password = module.params['password']
    ntc_host = module.params['ntc_host']
    ntc_conf_file = module.params['ntc_conf_file']
    transport = module.params['transport']
    port = module.params['port']
    global_delay_factor = int(module.params['global_delay_factor'])
    delay_factor = int(module.params['delay_factor'])
    secret = module.params['secret']
    if ntc_host is not None:
        device = ntc_device_by_name(ntc_host, ntc_conf_file)
    else:
        kwargs = {}
        if transport is not None:
            kwargs['transport'] = transport
        if port is not None:
            kwargs['port'] = port
        if secret is not None:
            kwargs['secret'] = secret
        if platform in NETMIKO_BACKEND:
            if global_delay_factor is not None:
                kwargs['global_delay_factor'] = global_delay_factor
            if delay_factor is not None:
                kwargs['delay_factor'] = delay_factor
        device_type = platform
        device = ntc_device(device_type, host, username, password, **kwargs)
    remote_file = module.params['remote_file']
    local_file = module.params['local_file']
    argument_check = {'host': host, 'username': username, 'platform': platform, 'password': password}
    for key, val in argument_check.items():
        if val is None:
            module.fail_json(msg=str(key) + ' is required')
    device.open()
    if remote_file:
        remote_save_successful = device.save(remote_file)
    else:
        remote_save_successful = device.save()
    changed = remote_save_successful
    if local_file:
        device.backup_running_config(local_file)
        changed = True
    device.close()
    remote_file = remote_file or '(Startup Config)'
    module.exit_json(changed=changed, remote_save_successful=remote_save_successful, remote_file=remote_file, local_file=local_file)",not HAS_PYNTC,218,not AnsibleModule.is_installed(),False,6.567274736060395,N/A
"def main():
    """"""Main execution.""""""
    argument_spec = dict(schema=dict(required=True, type='dict'), data=dict(required=True, type='dict'), scope=dict(required=True, type='list'))
    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False)
<mask>:
        module.fail_json(msg='jsonschema is required for this module.')
    schema = module.params['schema']
    data = module.params['data']
    scope = module.params['scope']
    for item in scope:
        try:
            feature = item['name']
        except KeyError:
            module.fail_json(msg='Malformed list item {0}, should be in format similar to {""name"": ""feature"", ""required"": True}'.format(item))
        required = item.get('required')
        entry_data = data.get(feature)
        if entry_data is not None:
            entry_schema = schema.get(feature)
            if entry_schema is not None:
                status, msg = validate_schema(entry_schema, entry_data)
            else:
                status = False
                msg = 'Schema was not defined for feature {0}. Schema key must match data key'.format(feature)
        elif required:
            status = False
            msg = 'Feature {0} required, but not found'.format(feature)
        else:
            status = True
        if not status:
            resp = {'data': entry_data, 'schema': entry_schema, 'feature': feature, 'msg': msg}
            module.fail_json(**resp)
    module.exit_json(changed=False)",not HAS_LIB,131,schema is None and data is None and scope is None,False,0.0,N/A
"def error_params(platform, command_output):
    """"""Checks for typical cisco command error outputs.""""""
<mask>:
        if ""Invalid input detected at '^' marker"" in command_output:
            return True
        if 'Ambiguous command' in command_output:
            return True
    return False",'cisco_ios' in platform,31,'^' in command_output,False,7.809849842300637,N/A
"def main():
    """"""Main execution.""""""
    base_argument_spec = dict(commands=dict(required=False, type='list'), commands_file=dict(required=False, default=None, type='str'))
    argument_spec = base_argument_spec
    argument_spec.update(CONNECTION_ARGUMENT_SPEC)
    argument_spec['provider'] = dict(required=False, type='dict', options=CONNECTION_ARGUMENT_SPEC)
    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False, mutually_exclusive=MUTUALLY_EXCLUSIVE, required_one_of=[REQUIRED_ONE_OF])
<mask>:
        module.fail_json(msg='pyntc is required for this module.')
    if not any([module.params['commands'], module.params['commands_file']]):
        module.fail_json(msg='One of `commands` or `commands_file` argument is required.')
    if module.params['commands'] and module.params['commands_file']:
        module.fail_json(msg='The use of both `commands` and `commands_file` in the same task is not currently supported.')
    elif module.params['commands_file']:
        with open(module.params['commands_file'], 'r') as cmds:
            commands = [cmd.rstrip() for cmd in cmds]
    elif module.params['commands']:
        commands = list(module.params['commands'])
    else:
        module.fail_json(msg='The combination of params used is not supported.')
    provider = module.params['provider'] or {}
    for param, pvalue in provider.items():
        if module.params.get(param) is not False:
            module.params[param] = module.params.get(param) or pvalue
    platform = module.params['platform']
    host = module.params['host']
    username = module.params['username']
    password = module.params['password']
    ntc_host = module.params['ntc_host']
    ntc_conf_file = module.params['ntc_conf_file']
    transport = module.params['transport']
    port = module.params['port']
    secret = module.params['secret']
    argument_check = {'host': host, 'username': username, 'platform': platform, 'password': password}
    for key, val in argument_check.items():
        if val is None:
            module.fail_json(msg=str(key) + ' is required')
    if ntc_host is not None:
        device = ntc_device_by_name(ntc_host, ntc_conf_file)
    else:
        kwargs = {}
        if transport is not None:
            kwargs['transport'] = transport
        if port is not None:
            kwargs['port'] = port
        if secret is not None:
            kwargs['secret'] = secret
        device_type = platform
        device = ntc_device(device_type, host, username, password, **kwargs)
    device.open()
    changed = False
    failed = False
    try:
        result = device.config(commands)
        changed = True
    except CommandListError as err:
        changed = False
        module.fail_json(msg=err)
    finally:
        device.close()
    module.exit_json(changed=changed, failed=failed, results=result)",not HAS_PYNTC,237,"not any([ntc_host, ntc_conf_file])",False,3.1251907639724417,N/A
"def check_device(module, username, password, host, timeout, kwargs):
    """"""Simple check of the device.""""""
    success = False
    attempts = timeout / 30
    counter = 0
    atomic = False
    while counter < attempts and (not success):
        try:
<mask>:
                device = ntc_device_by_name(module.params['ntc_host'], module.params['ntc_conf_file'])
            else:
                device_type = module.params['platform']
                device = ntc_device(device_type, host, username, password, **kwargs)
            success = True
            atomic = True
            try:
                device.close()
            except:
                atomic = False
        except:
            time.sleep(30)
            counter += 1
    return (success, atomic)",module.params['ntc_host'] is not None,70,module.params['ntc_host'] and module.params['ntc_conf_file'],False,36.65882729601238,N/A
"def main():
    """"""Main execution.""""""
    base_argument_spec = dict(confirm=dict(required=False, default=True, type='bool'), timer=dict(requred=False, type='int'), timeout=dict(required=False, type='int', default=240), volume=dict(required=False, type='str'))
    argument_spec = base_argument_spec
    argument_spec.update(CONNECTION_ARGUMENT_SPEC)
    argument_spec['provider'] = dict(required=False, type='dict', options=CONNECTION_ARGUMENT_SPEC)
    module = AnsibleModule(argument_spec=argument_spec, mutually_exclusive=MUTUALLY_EXCLUSIVE, required_one_of=[REQUIRED_ONE_OF], required_if=[['platform', PLATFORM_F5, ['volume']]], supports_check_mode=False)
<mask>:
        module.fail_json(msg='pyntc is required for this module.')
    provider = module.params['provider'] or {}
    for param, pvalue in provider.items():
        if module.params.get(param) is not False:
            module.params[param] = module.params.get(param) or pvalue
    platform = module.params['platform']
    host = module.params['host']
    username = module.params['username']
    password = module.params['password']
    ntc_host = module.params['ntc_host']
    ntc_conf_file = module.params['ntc_conf_file']
    transport = module.params['transport']
    port = module.params['port']
    secret = module.params['secret']
    argument_check = {'host': host, 'username': username, 'platform': platform, 'password': password}
    for key, val in argument_check.items():
        if val is None:
            module.fail_json(msg=str(key) + ' is required')
    kwargs = {}
    if ntc_host is not None:
        device = ntc_device_by_name(ntc_host, ntc_conf_file)
    else:
        if transport is not None:
            kwargs['transport'] = transport
        if port is not None:
            kwargs['port'] = port
        if secret is not None:
            kwargs['secret'] = secret
        device_type = platform
        device = ntc_device(device_type, host, username, password, **kwargs)
    confirm = module.params['confirm']
    timer = module.params['timer']
    timeout = module.params['timeout']
    volume = module.params['volume']
    if not confirm:
        module.fail_json(msg='confirm must be set to true for this module to work.')
    supported_timer_platforms = [PLATFORM_IOS, PLATFORM_JUNOS]
    if timer is not None and device.device_type not in supported_timer_platforms:
        module.fail_json(msg=f'Timer parameter not supported on platform {platform}.')
    argument_check = {'host': host, 'username': username, 'platform': platform, 'password': password}
    for key, val in argument_check.items():
        if val is None:
            module.fail_json(msg=str(key) + ' is required')
    device.open()
    if volume:
        device.reboot(confirm=True, volume=volume)
    elif timer is not None:
        device.reboot(confirm=True, timer=timer)
    else:
        device.reboot(confirm=True)
    time.sleep(10)
    reachable, atomic = check_device(module, username, password, host, timeout, kwargs)
    changed = True
    rebooted = True
    module.exit_json(changed=changed, rebooted=rebooted, reachable=reachable, atomic=atomic)",not HAS_PYNTC,264,not AnsibleModule.is_installed(),False,6.567274736060395,N/A
"def main():
    """"""Main execution.""""""
    base_argument_spec = dict(checkpoint_file=dict(required=False, type='str'), rollback_to=dict(required=False, type='str'))
    argument_spec = base_argument_spec
    argument_spec.update(CONNECTION_ARGUMENT_SPEC)
    argument_spec['provider'] = dict(required=False, type='dict', options=CONNECTION_ARGUMENT_SPEC)
    module = AnsibleModule(argument_spec=argument_spec, mutually_exclusive=MUTUALLY_EXCLUSIVE, required_one_of=[REQUIRED_ONE_OF], supports_check_mode=False)
<mask>:
        module.fail_json(msg='pyntc is required for this module.')
    provider = module.params['provider'] or {}
    for param, pvalue in provider.items():
        if module.params.get(param) is not False:
            module.params[param] = module.params.get(param) or pvalue
    platform = module.params['platform']
    host = module.params['host']
    username = module.params['username']
    password = module.params['password']
    ntc_host = module.params['ntc_host']
    ntc_conf_file = module.params['ntc_conf_file']
    transport = module.params['transport']
    port = module.params['port']
    secret = module.params['secret']
    if platform in UNSUPPORTED_PLATFORMS:
        module.fail_json(msg=f'ntc_rollback is not implemented for this platform type {platform}.')
    if ntc_host is not None:
        device = ntc_device_by_name(ntc_host, ntc_conf_file)
    else:
        kwargs = {}
        if transport is not None:
            kwargs['transport'] = transport
        if port is not None:
            kwargs['port'] = port
        if secret is not None:
            kwargs['secret'] = secret
        device_type = platform
        device = ntc_device(device_type, host, username, password, **kwargs)
    checkpoint_file = module.params['checkpoint_file']
    rollback_to = module.params['rollback_to']
    argument_check = {'host': host, 'username': username, 'platform': platform, 'password': password}
    for key, val in argument_check.items():
        if val is None:
            module.fail_json(msg=str(key) + ' is required')
    device.open()
    status = None
    filename = None
    changed = False
    try:
        if checkpoint_file:
            device.checkpoint(checkpoint_file)
            status = 'checkpoint file created'
        elif rollback_to:
            device.rollback(rollback_to)
            status = 'rollback executed'
        changed = True
        filename = rollback_to or checkpoint_file
    except Exception as e:
        module.fail_json(msg=str(e))
    device.close()
    module.exit_json(changed=changed, status=status, filename=filename)",not HAS_PYNTC,212,not pyntc,False,18.393972058572114,N/A
"@contextmanager
def _run_server(self, *args: Any, **kwargs: Any) -> Generator[TestClient, None, None]:
    with run_server(*args, **kwargs) as (loop, site):
        assert site._server is not None
        assert isinstance(site._server, Server)
        assert site._server.sockets is not None
        host, port = parse_sockname(site._server.sockets[0].getsockname())

        async def create_session() -> aiohttp.ClientSession:
<mask>:
                connector: aiohttp.BaseConnector = aiohttp.UnixConnector(path=port)
            else:
                connector = aiohttp.TCPConnector()
            return aiohttp.ClientSession(connector=connector)
        session = loop.run_until_complete(create_session())
        try:
            yield TestClient(self, loop, host, port, session)
        finally:
            loop.run_until_complete(session.close())",host == 'unix',62,host.endswith('unix'),False,9.652434877402245,N/A
"def parse_sockname(sockname: Union[Tuple, str]) -> Tuple[str, str]:
<mask>:
        return (sockname[0], str(sockname[1]))
    return ('unix', sockname)","isinstance(sockname, tuple)",14,"isinstance(sockname, tuple)",True,100.00000000000004,N/A
"def add_argument(name: str, *aliases: str, **kwargs: Any) -> None:
    varname = name.strip('-').replace('-', '_')
    kwargs.setdefault('help', HELP.get(varname, '').replace('``', ''))
    assert kwargs['help']
    kwargs.setdefault('action', 'store')
<mask>:
        kwargs['help'] += ' Can be specified multiple times.'
    if kwargs['action'] == 'count':
        kwargs.setdefault('default', 0)
    if kwargs['action'] in ('append', 'store'):
        kwargs.setdefault('default', DEFAULTS.get(varname))
        kwargs.setdefault('type', type(kwargs['default']))
        assert not isinstance(None, kwargs['type'])
    parser.add_argument(name, *aliases, **kwargs)","kwargs['action'] in ('append', 'count')",52,"kwargs['action'] in ('append', 'store')",False,78.25422900366438,N/A
"def _run_application(application: WSGIApplication, environ: WSGIEnviron) -> Response:
    response_status: Optional[int] = None
    response_reason: Optional[str] = None
    response_headers: Optional[WSGIHeaders] = None
    response_body: List[bytes] = []

    def start_response(status: str, headers: WSGIHeaders, exc_info: Optional[Exception]=None) -> WSGIAppendResponse:
        nonlocal response_status, response_reason, response_headers, response_body
        status_code, reason = status.split(None, 1)
        status_code = int(status_code)
<mask>:
            for header_name, header_value in headers:
                assert not is_hop_by_hop(header_name), f'hop-by-hop headers are forbidden: {header_name}'
        response_status = status_code
        response_reason = reason
        response_headers = headers
        del response_body[:]
        return response_body.append
    body_iterable = application(environ, start_response)
    try:
        response_body.extend(body_iterable)
        assert response_status is not None and response_reason is not None and (response_headers is not None), 'application did not call start_response()'
        return Response(status=response_status, reason=response_reason, headers=CIMultiDict(response_headers), body=b''.join(response_body))
    finally:
        if hasattr(body_iterable, 'close'):
            body_iterable.close()",__debug__,108,headers,False,0.0,N/A
"def __init__(self, application: WSGIApplication, *, url_scheme: Optional[str]=None, stderr: Optional[IO[bytes]]=None, inbuf_overflow: int=524288, max_request_body_size: int=1073741824, executor: Optional[Executor]=None):
    assert callable(application), 'application should be callable'
    self._application = application
    self._url_scheme = url_scheme
    self._stderr = stderr or sys.stderr
    assert isinstance(inbuf_overflow, int), 'inbuf_overflow should be int'
    assert inbuf_overflow >= 0, 'inbuf_overflow should be >= 0'
    assert isinstance(max_request_body_size, int), 'max_request_body_size should be int'
    assert max_request_body_size >= 0, 'max_request_body_size should be >= 0'
<mask>:
        self._body_io: Callable[[], IO[bytes]] = partial(SpooledTemporaryFile, max_size=inbuf_overflow)
    else:
        self._body_io = BytesIO
    self._max_request_body_size = max_request_body_size
    self._executor = executor",inbuf_overflow < max_request_body_size,81,"isinstance(inbuf_overflow, int)",False,14.128386352314104,N/A
"def _get_environ(self, request: Request, body: IO[bytes], content_length: int) -> WSGIEnviron:
    path_info = request.match_info['path_info']
    script_name = request.rel_url.path[:len(request.rel_url.path) - len(path_info)]
<mask>:
        script_name = script_name[:-1]
        path_info = '/' + path_info
    assert request.transport is not None
    server_name, server_port = parse_sockname(request.transport.get_extra_info('sockname'))
    remote_addr, remote_port = parse_sockname(request.transport.get_extra_info('peername'))
    url_scheme = self._url_scheme
    if url_scheme is None:
        url_scheme = 'http' if request.transport.get_extra_info('sslcontext') is None else 'https'
    environ = {'REQUEST_METHOD': request.method, 'SCRIPT_NAME': script_name, 'PATH_INFO': path_info, 'RAW_URI': request.raw_path, 'REQUEST_URI': request.raw_path, 'QUERY_STRING': request.rel_url.raw_query_string, 'CONTENT_TYPE': request.headers.get('Content-Type', ''), 'CONTENT_LENGTH': str(content_length), 'SERVER_NAME': server_name, 'SERVER_PORT': server_port, 'REMOTE_ADDR': remote_addr, 'REMOTE_HOST': remote_addr, 'REMOTE_PORT': remote_port, 'SERVER_PROTOCOL': 'HTTP/{}.{}'.format(*request.version), 'wsgi.version': (1, 0), 'wsgi.url_scheme': url_scheme, 'wsgi.input': body, 'wsgi.errors': self._stderr, 'wsgi.multithread': True, 'wsgi.multiprocess': False, 'wsgi.run_once': False, 'asyncio.executor': self._executor, 'aiohttp.request': request}
    for header_name in request.headers:
        header_name = header_name.upper()
        if not is_hop_by_hop(header_name) and header_name not in ('CONTENT-LENGTH', 'CONTENT-TYPE'):
            header_value = ','.join(request.headers.getall(header_name))
            environ['HTTP_' + header_name.replace('-', '_')] = header_value
    return environ",script_name.endswith('/'),133,script_name[-1] == '/',False,23.462350320527996,N/A
"def format_path(path: str) -> str:
    assert not path.endswith('/'), f'{path!r} name should not end with /'
<mask>:
        path = '/'
    assert path.startswith('/'), f'{path!r} name should start with /'
    return path",path == '',29,path.startswith('/'),False,5.522397783539471,N/A
"def static_cors_middleware(*, static: Iterable[Tuple[str, str]], static_cors: str) -> Middleware:

    @middleware
    async def do_static_cors_middleware(request: Request, handler: Handler) -> StreamResponse:
        response = await handler(request)
        for path, _ in static:
<mask>:
                response.headers['Access-Control-Allow-Origin'] = static_cors
                break
        return response
    return do_static_cors_middleware",request.path.startswith(path),36,path.startswith(static_cors),False,36.55552228545123,N/A
"def __init__(self, path: str | os.PathLike[str], chunksize: int=100000) -> None:
    self.path = str(path)
<mask>:
        Path(path).mkdir(parents=True)
    self.info = self._loadinfo(chunksize)
    self.chunksize = self.info['chunksize']
    self.headf = self._openchunk(self.info['head'][0], 'ab+')
    self.tailf = self._openchunk(self.info['tail'][0])
    os.lseek(self.tailf.fileno(), self.info['tail'][2], os.SEEK_SET)",not Path(path).exists(),31,not Path(path).exists(),True,100.00000000000004,N/A
"def push(self, string: bytes) -> None:
<mask>:
        raise TypeError(f'Unsupported type: {type(string).__name__}')
    hnum, hpos = self.info['head']
    hpos += 1
    szhdr = struct.pack(self.szhdr_format, len(string))
    os.write(self.headf.fileno(), szhdr + string)
    if hpos == self.chunksize:
        hpos = 0
        hnum += 1
        self.headf.close()
        self.headf = self._openchunk(hnum, 'ab+')
    self.info['size'] += 1
    self.info['head'] = [hnum, hpos]","not isinstance(string, bytes)",48,"not isinstance(string, bytes)",True,100.00000000000004,N/A
"def pop(self) -> bytes | None:
    tnum, tcnt, toffset = self.info['tail']
<mask>:
        return None
    tfd = self.tailf.fileno()
    szhdr = os.read(tfd, self.szhdr_size)
    if not szhdr:
        return None
    size, = struct.unpack(self.szhdr_format, szhdr)
    data = os.read(tfd, size)
    tcnt += 1
    toffset += self.szhdr_size + size
    if tcnt == self.chunksize and tnum <= self.info['head'][0]:
        tcnt = toffset = 0
        tnum += 1
        self.tailf.close()
        Path(self.tailf.name).unlink()
        self.tailf = self._openchunk(tnum)
    self.info['size'] -= 1
    self.info['tail'] = [tnum, tcnt, toffset]
    return data","[tnum, tcnt] >= self.info['head']",73,self.info['size'] <= 0,False,21.1792418047206,N/A
"def peek(self) -> bytes | None:
    tnum, tcnt, _ = self.info['tail']
<mask>:
        return None
    tfd = self.tailf.fileno()
    tfd_initial_pos = os.lseek(tfd, 0, os.SEEK_CUR)
    szhdr = os.read(tfd, self.szhdr_size)
    if not szhdr:
        return None
    size, = struct.unpack(self.szhdr_format, szhdr)
    data = os.read(tfd, size)
    os.lseek(tfd, tfd_initial_pos, os.SEEK_SET)
    return data","[tnum, tcnt] >= self.info['head']",44,tnum != tcnt,False,2.215745752614824,N/A
"def push(self, obj: Any, priority: int=0) -> None:
<mask>:
        self.queues[priority] = self.qfactory(priority)
    q = self.queues[priority]
    q.push(obj)
    if self.curprio is None or priority < self.curprio:
        self.curprio = priority",priority not in self.queues,27,priority not in self.queues,True,100.00000000000004,N/A
"def pop(self) -> Any | None:
<mask>:
        return None
    q = self.queues[self.curprio]
    m = q.pop()
    if len(q) == 0:
        del self.queues[self.curprio]
        q.close()
        prios = [p for p, q in self.queues.items() if len(q) > 0]
        self.curprio = min(prios) if prios else None
    return m",self.curprio is None,43,self.curprio not in self.queues,False,20.556680845025987,N/A
"def peek(self) -> Any | None:
<mask>:
        return None
    return self.queues[self.curprio].peek()",self.curprio is None,11,self.curprio not in self.queues,False,20.556680845025987,N/A
"def close(self) -> list[int]:
    active = []
    for p, q in self.queues.items():
<mask>:
            active.append(p)
        q.close()
    return active",len(q),17,q.active,False,19.716118825581447,N/A
"def push(self, obj: Any, key: Hashable) -> None:
<mask>:
        self.queues[key] = self.qfactory(key)
        self.key_queue.appendleft(key)
    q = self.queues[key]
    q.push(obj)",key not in self.key_queue,17,key not in self.queues,False,54.44460596606694,N/A
"def pop(self) -> Any | None:
    while True:
        try:
            key = self.key_queue.pop()
        except IndexError:
            return None
        q = self.queues[key]
        m = q.pop()
<mask>:
            del self.queues[key]
            q.close()
        else:
            self.key_queue.appendleft(key)
        if m:
            return m",len(q) == 0,32,q,False,0.24787521766663595,N/A
"def close(self) -> list[Hashable]:
    active = []
    for k, q in self.queues.items():
<mask>:
            active.append(k)
        q.close()
    return active",len(q),17,q.active,False,19.716118825581447,N/A
"def copy_other(opts, flacdir, outdir):
<mask>:
        print('COPYING other files')
    for dirpath, dirs, files in os.walk(flacdir, topdown=False, followlinks=True):
        for name in files:
            if opts.nolog and fnmatch(name.lower(), '*.log'):
                continue
            if opts.nocue and fnmatch(name.lower(), '*.cue'):
                continue
            if opts.nodots and fnmatch(name.lower(), '^.'):
                continue
            if not fnmatch(name.lower(), '*.flac') and (not fnmatch(name.lower(), '*.m3u')):
                d = re.sub(re.escape(flacdir), outdir, dirpath)
                if os.path.exists(os.path.join(d, name)) and (not opts.overwrite):
                    continue
                if not os.path.exists(d):
                    os.makedirs(d)
                shutil.copy(os.path.join(dirpath, name), d)",opts.verbose,65,opts.verbose,True,100.00000000000004,N/A
"def make_torrent(opts, target):
<mask>:
        print('MAKE: %s.torrent' % os.path.relpath(target))
    torrent_cmd = ""mktorrent -p -a '%s' -o '%s.torrent' '%s' 2>&1"" % (opts.tracker, escape_quote(os.path.join(opts.torrent_dir, os.path.basename(target))), escape_quote(target))
    if opts.additional:
        torrent_cmd += ' ' + opts.additional
    if opts.nodate:
        torrent_cmd += ' -d'
    if not opts.verbose:
        torrent_cmd += ' >/dev/null'
    if opts.verbose:
        print(torrent_cmd)
    r = system(torrent_cmd)
    if r:
        failure(r, torrent_cmd)",opts.verbose,54,opts.verbose,True,100.00000000000004,N/A
"def replaygain(opts, codec, outdir):
<mask>:
        print('APPLYING replaygain')
        print(encoders[enc_opts[codec]['enc']]['regain'] % outdir)
    r = system(encoders[enc_opts[codec]['enc']]['regain'] % escape_quote(outdir))
    if r:
        failure(r, 'replaygain')
    for dirpath, dirs, files in os.walk(outdir, topdown=False, followlinks=True):
        for name in dirs:
            r = system(encoders[enc_opts[codec]['enc']]['regain'] % os.path.join(dirpath, name))
            if r:
                failure(r, 'replaygain')",opts.verbose,41,opts['verbose'],False,15.97357760615681,N/A
"def transcode(f, flacdir, mp3_dir, codec, opts, lock):
    tags = {}
    for tag in copy_tags:
        tagcmd = ""metaflac --show-tag='"" + escape_quote(tag) + ""' '"" + escape_quote(f) + ""'""
        t = re.sub('\\S.+?=', '', os.popen(tagcmd).read().rstrip(), count=1)
<mask>:
            tags.update({tag: escape_quote(t)})
        del t
    if opts.zeropad and 'TRACKNUMBER' in tags and (len(tags['TRACKNUMBER']) == 1):
        tags['TRACKNUMBER'] = '0' + tags['TRACKNUMBER']
    if opts.skipgenre and 'GENRE' in tags:
        del tags['GENRE']
    outname = re.sub(re.escape(flacdir), mp3_dir, f)
    outname = re.sub(re.compile('\\.flac$', re.IGNORECASE), '', outname)
    with lock:
        if not os.path.exists(os.path.dirname(outname)):
            os.makedirs(os.path.dirname(outname))
    outname += enc_opts[codec]['ext']
    if os.path.exists(outname) and (not opts.overwrite):
        print('WARN: file %s already exists' % os.path.relpath(outname), file=sys.stderr)
        return 1
    flac_cmd = encoders[enc_opts[codec]['enc']]['enc']
    tagline = ''
    for tag in tags:
        tagline = tagline + ' ' + encoders[enc_opts[codec]['enc']][tag]
    tagline = tagline % tags
    if opts.dither:
        flac_cmd = dither_cmd + ' | ' + flac_cmd
    flac_cmd = ""flac -sdc -- '"" + escape_percent(escape_quote(f)) + ""' | "" + flac_cmd
    flac_cmd = flac_cmd % {'opts': enc_opts[codec]['opts'], 'filename': escape_quote(outname), 'tags': tagline}
    outname = os.path.basename(outname)
    if not opts.silent:
        print('encoding %s' % outname)
    if opts.verbose:
        print(flac_cmd)
    r = system(flac_cmd)
    if r:
        failure(r, 'error encoding %s' % outname)
        system(""touch '%s/FAILURE'"" % mp3_dir)
    return 0",t,184,t,True,100.00000000000004,N/A
"def main():
    parser = setup_parser()
    opts = parser.parse_args()
<mask>:
        opts.output += '/'
    if len(codecs) == 0 and (not opts.original):
        parser.error('you must provide at least one format to transcode to')
        exit()
    for flacdir in opts.flacdirs:
        flacdir = os.path.abspath(flacdir)
        flacfiles = []
        if not os.path.exists(opts.torrent_dir):
            os.makedirs(opts.torrent_dir)
        for dirpath, dirs, files in os.walk(flacdir, topdown=False, followlinks=True):
            for name in files:
                if fnmatch(name.lower(), '*.flac'):
                    flacfiles.append(os.path.join(dirpath, name))
        flacfiles.sort()
        if opts.ignore and (not flacfiles):
            if not opts.silent:
                print('SKIP (no flacs in): %s' % os.path.relpath(flacdir))
            continue
        if opts.original:
            if not opts.silent:
                print('BEGIN ORIGINAL FLAC')
            if opts.output and opts.tracker and (not opts.notorrent):
                make_torrent(opts, flacdir)
            if not opts.silent:
                print('END ORIGINAL FLAC')
        for codec in codecs:
            outdir = os.path.basename(flacdir)
            flacre = re.compile('FLAC', re.IGNORECASE)
            if flacre.search(outdir):
                outdir = flacre.sub(codec, outdir)
            else:
                outdir = outdir + ' (' + codec + ')'
            outdir = opts.output + outdir
            if not os.path.exists(outdir):
                os.makedirs(outdir)
            if not opts.silent:
                print('BEGIN ' + codec + ': %s' % os.path.relpath(flacdir))
            threads = []
            cv = threading.Condition()
            lock = threading.Lock()
            for f in flacfiles:
                with cv:
                    while threading.active_count() == max(1, opts.max_threads) + 1:
                        cv.wait()
                    t = Transcode(f, flacdir, outdir, codec, opts, lock, cv)
                t.start()
                threads.append(t)
            for t in threads:
                t.join()
            if not opts.nocopyother:
                copy_other(opts, flacdir, outdir)
            if opts.replaygain:
                replaygain(opts, codec, outdir)
            if opts.output and opts.tracker and (not opts.notorrent):
                make_torrent(opts, outdir)
            if not opts.silent:
                print('END ' + codec + ': %s' % os.path.relpath(flacdir))
        if opts.verbose:
            print('ALL DONE: ' + os.path.relpath(flacdir))
    return 0",not opts.output.endswith('/'),230,opts.output and (not opts.output.endswith('/')),False,61.02169202557915,N/A
"def django_url_fetcher(url, *args, **kwargs):
<mask>:
        log.debug('Attempt to fetch from %s', url)
        mime_type, encoding = mimetypes.guess_type(url)
        url_path = urlparse(url).path
        data = {'mime_type': mime_type, 'encoding': encoding, 'filename': Path(url_path).name}
        default_media_url = settings.MEDIA_URL in ('', get_script_prefix())
        if not default_media_url and url_path.startswith(settings.MEDIA_URL):
            log.debug('URL contains MEDIA_URL (%s)', settings.MEDIA_URL)
            cleaned_media_root = str(settings.MEDIA_ROOT)
            if not cleaned_media_root.endswith('/'):
                cleaned_media_root += '/'
            absolute_path = url_path.replace(settings.MEDIA_URL, cleaned_media_root, 1)
            log.debug('Cleaned path: %s', absolute_path)
            data['file_obj'] = default_storage.open(absolute_path, 'rb')
            data['redirected_url'] = 'file://' + absolute_path
            return data
        elif settings.STATIC_URL and url_path.startswith(settings.STATIC_URL):
            log.debug('URL contains STATIC_URL (%s)', settings.STATIC_URL)
            relative_path = url_path.replace(settings.STATIC_URL, '', 1)
            if not settings.DEBUG and hasattr(staticfiles_storage, 'hashed_files'):
                log.debug('Hashed static files storage detected')
                relative_path = get_reversed_hashed_files()[relative_path]
                data['filename'] = Path(relative_path).name
            log.debug('Cleaned path: %s', relative_path)
            absolute_path = find(relative_path)
            log.debug('Static file finder returned: %s', absolute_path)
            if absolute_path:
                log.debug('Loading static file: %s', absolute_path)
                data['file_obj'] = open(absolute_path, 'rb')
                data['redirected_url'] = 'file://' + absolute_path
                return data
    log.debug('Forwarding to weasyprint.default_url_fetcher: %s', url)
    return weasyprint.default_url_fetcher(url, *args, **kwargs)",url.startswith('file:'),142,settings.MEDIA_URL,False,5.862502026550896,N/A
"def __init__(self, request, template, context=None, content_type=None, status=None, charset=None, using=None, headers=None, filename=None, attachment=True, stylesheets=None, options=None):
    """"""
        An HTTP response class with template and context rendered to a PDF document.

        Django TemplateResponse arguments:

        :param request: the request object
        :param template: template to use to render the response
        :param context: context to use to render the response
        :param content_type: content type of the response (default: 'application/pdf')
        :param status: status code of the response (default: 200)
        :param charset: character set of the response (default: settings.DEFAULT_CHARSET)
        :param using: template engine to use (default: 'django')
        :param headers: dictionary of headers to use in the response

        WeasyPrint specific arguments:

        :param filename: set `Content-Disposition` to use this filename
        :param attachment: set `Content-Disposition` 'attachment';
            A `filename` must be given to enable this even if set to `True`.
            (default: `True`)
        :param stylesheets: list of additional stylesheets
        :param options: dictionary of options passed to WeasyPrint
        """"""
    self._stylesheets = stylesheets or []
    self._options = options.copy() if options else {}
    kwargs = dict(context=context, content_type=content_type or WeasyTemplateResponseMixin.content_type, status=status, charset=charset, using=using, headers=headers)
    super().__init__(request, template, **kwargs)
<mask>:
        display = 'attachment' if attachment else 'inline'
        self['Content-Disposition'] = f'{display};filename=""{filename}""'",filename,181,filename,True,100.00000000000004,N/A
"def test_valid_video_track(self) -> None:
    for track in self.media_info.tracks:
<mask>:
            self.assertEqual('DV', track.codec)
            self.assertEqual('Interlaced', track.scan_type)
            break",track.track_type == 'Video',14,track.type == 'video',False,27.22230298303347,N/A
"def test_track_integer_attributes(self) -> None:
    for track in self.media_info.tracks:
<mask>:
            self.assertTrue(isinstance(track.duration, int))
            self.assertTrue(isinstance(track.bit_rate, int))
            self.assertTrue(isinstance(track.sampling_rate, int))
            break",track.track_type == 'Audio',16,track.type == 'integer',False,27.22230298303347,N/A
"def test_parse_no_cover_data(self) -> None:
    lib_version_str, lib_version = _get_library_version()
<mask>:
        pytest.skip('The Cover_Data option is not supported by this library version (v{} detected, v18.03 required)'.format(lib_version_str))
    self.assertEqual(self.no_cover_mi.tracks[0].cover_data, None)","lib_version < (18, 3)",25,lib_version != '18.03',False,18.325568129983203,N/A
"def setUp(self) -> None:
    lib_version_str, lib_version = _get_library_version()
<mask>:
        pytest.skip('The Reset option is not supported by this library version (v{} detected, v19.09 required)'.format(lib_version_str))
    self.raw_language_mi = MediaInfo.parse(os.path.join(data_dir, 'sample.mkv'), mediainfo_options={'Language': 'raw'})
    self.normal_mi = MediaInfo.parse(os.path.join(data_dir, 'sample.mkv'))","lib_version < (19, 9)",33,lib_version_str != '19.09',False,18.141207173155518,N/A
"@pytest.mark.parametrize('test_file', test_media_files)
def test_thread_safety(test_file: str) -> None:
    lib_version_str, lib_version = _get_library_version()
<mask>:
        pytest.skip('This version of the library is not thread-safe (v{} detected, v20.03 required)'.format(lib_version_str))
    expected_result = MediaInfo.parse(os.path.join(data_dir, test_file))
    results = []
    lock = threading.Lock()

    def target() -> None:
        try:
            result = MediaInfo.parse(os.path.join(data_dir, test_file))
            with lock:
                results.append(result)
        except Exception:
            pass
    threads = []
    thread_count = 100
    for _ in range(thread_count):
        thread = threading.Thread(target=target)
        thread.start()
        threads.append(thread)
    for thread in threads:
        thread.join()
    assert len(results) == thread_count
    for res in results:
        assert res.to_data() == expected_result.to_data()
        assert res == expected_result","lib_version < (20, 3)",85,"lib_version_str not in (20.03, 20.03)",False,14.991106946711685,N/A
"def __post_init__(self) -> None:
    """"""Check that the combination of platform and arch is allowed.""""""
    allowed_arch = None
<mask>:
        allowed_arch = ['x86_64', 'arm64']
    elif self.platform == 'win32':
        allowed_arch = ['x86_64', 'i386']
    else:
        raise ValueError(f'platform not recognized: {self.platform}')
    if allowed_arch is not None and self.arch not in allowed_arch:
        raise ValueError(f'arch {self.arch} is not allowed for platform {self.platform}; must be one of {allowed_arch}')","self.platform in ('linux', 'darwin')",60,self.platform == 'windows',False,18.325568129983203,N/A
"def get_compressed_file_name(self) -> str:
    """"""Get the compressed file name.""""""
<mask>:
        suffix = f'Lambda_{self.arch}.zip'
    elif self.platform == 'darwin':
        suffix = 'Mac_x86_64+arm64.tar.bz2'
    elif self.platform == 'win32':
        win_arch = 'x64' if self.arch == 'x86_64' else self.arch
        suffix = f'Windows_{win_arch}_WithoutInstaller.zip'
    else:
        raise ValueError(f'platform not recognized: {self.platform}')
    return f'MediaInfo_DLL_{MEDIAINFO_VERSION}_{suffix}'",self.platform == 'linux',44,self.platform == 'linux',True,100.00000000000004,N/A
"def compare_hash(self, h: str) -> bool:
    """"""Compare downloaded hash with expected.""""""
    key = (self.platform, self.arch)
    expected = MEDIAINFO_HASHES.get(key)
<mask>:
        raise ValueError(f'{key}, expected hash not found.')
    if expected != h:
        raise ValueError(f'hash mismatch for {key}: expected {expected}, got {h}')
    return True",expected is None,40,expected is None,True,100.00000000000004,N/A
"def unpack(self, file: os.PathLike | str, folder: os.PathLike | str) -> dict[str, str]:
    """"""Extract compressed files.""""""
    file = Path(file)
    folder = Path(folder)
    compressed_file = self.get_compressed_file_name()
<mask>:
        raise ValueError(f'compressed file not found: {file.name!r}')
    tmp_dir = file.parent
    license_file: Path | None = None
    lib_file: Path | None = None
    if compressed_file.endswith('.zip') and self.platform == 'linux':
        with ZipFile(file) as fd:
            license_file = folder / 'LICENSE'
            fd.extract('LICENSE', tmp_dir)
            shutil.move(os.fspath(tmp_dir / 'LICENSE'), os.fspath(license_file))
            lib_file = folder / 'libmediainfo.so.0'
            fd.extract('lib/libmediainfo.so.0.0.0', tmp_dir)
            shutil.move(os.fspath(tmp_dir / 'lib/libmediainfo.so.0.0.0'), os.fspath(lib_file))
    elif compressed_file.endswith('.tar.bz2') and self.platform == 'darwin':
        with tarfile.open(file) as fd:
            kwargs: dict[str, Any] = {}
            if sys.version_info >= (3, 12):
                kwargs = {'filter': 'data'}
            license_file = folder / 'License.html'
            fd.extract('MediaInfoLib/License.html', tmp_dir, **kwargs)
            shutil.move(os.fspath(tmp_dir / 'MediaInfoLib/License.html'), os.fspath(license_file))
            lib_file = folder / 'libmediainfo.0.dylib'
            fd.extract('MediaInfoLib/libmediainfo.0.dylib', tmp_dir, **kwargs)
            shutil.move(os.fspath(tmp_dir / 'MediaInfoLib/libmediainfo.0.dylib'), os.fspath(lib_file))
    elif compressed_file.endswith('.zip') and self.platform == 'win32':
        with ZipFile(file) as fd:
            license_file = folder / 'License.html'
            fd.extract('Developers/License.html', tmp_dir)
            shutil.move(os.fspath(tmp_dir / 'Developers/License.html'), os.fspath(license_file))
            lib_file = folder / 'MediaInfo.dll'
            fd.extract('MediaInfo.dll', tmp_dir)
            shutil.move(os.fspath(tmp_dir / 'MediaInfo.dll'), os.fspath(lib_file))
    files = {}
    if license_file is not None and license_file.is_file():
        files['license'] = os.fspath(license_file.relative_to(folder))
    if lib_file is not None and lib_file.is_file():
        files['lib'] = os.fspath(lib_file.relative_to(folder))
    return files",not file.is_file(),184,not compressed_file,False,13.006502375572222,N/A
"def download(self, folder: os.PathLike | str, *, timeout: int=20, verbose: bool=True) -> dict[str, str]:
    """"""Download the library and license files.""""""
    folder = Path(folder)
    url = self.get_url()
    compressed_file = self.get_compressed_file_name()
    extracted_files = {}
    with TemporaryDirectory() as tmp_dir:
        outpath = Path(tmp_dir) / compressed_file
<mask>:
            print(f'Downloading MediaInfo library from {url}')
        self.download_upstream(url, outpath, timeout=timeout, verbose=verbose)
        if verbose:
            print(f'Extracting {compressed_file}')
        extracted_files = self.unpack(outpath, folder)
        if verbose:
            print(f'Extracted files: {extracted_files}')
    return extracted_files",verbose,66,verbose,True,100.00000000000004,N/A
"def process(media_file: str) -> None:
    print(f'Processing {media_file}')
    media_info = MediaInfo.parse(media_file)
    for track in media_info.tracks:
<mask>:
            print(f'The file format is {track.format}')
            print('General information dump:')
            pprint(track.to_data())
        elif track.track_type == 'Video':
            print(f'Video track {track.track_id} has a resolution of {track.width}×{track.height}', f'and a bit rate of {track.bit_rate} bits/s')
        elif track.track_type == 'Audio':
            if track.duration is not None:
                print(f'Audio track {track.track_id} has a duration of {track.duration / 1000} seconds')",track.track_type == 'General',63,track.format is not None,False,11.631736348831648,N/A
"def __eq__(self, other: object) -> bool:
<mask>:
        return False
    return self.__dict__ == other.__dict__","not isinstance(other, Track)",13,"not isinstance(other, self.__class__)",False,30.26643726685862,N/A
"def __init__(self, xml_dom_fragment: ET.Element) -> None:
    self.track_type = xml_dom_fragment.attrib['type']
    repeated_attributes = []
    for elem in xml_dom_fragment:
        node_name = elem.tag.lower().strip().strip('_')
<mask>:
            node_name = 'track_id'
        node_value = elem.text
        if getattr(self, node_name) is None:
            setattr(self, node_name, node_value)
        else:
            other_node_name = f'other_{node_name}'
            repeated_attributes.append((node_name, other_node_name))
            if getattr(self, other_node_name) is None:
                setattr(self, other_node_name, [node_value])
            else:
                getattr(self, other_node_name).append(node_value)
    for primary_key, other_key in repeated_attributes:
        try:
            setattr(self, primary_key, int(getattr(self, primary_key)))
        except ValueError:
            for other_value in getattr(self, other_key):
                try:
                    current = getattr(self, primary_key)
                    setattr(self, primary_key, int(other_value))
                    getattr(self, other_key).append(current)
                    break
                except ValueError:
                    pass",node_name == 'id',82,node_name == 'track',False,75.98356856515926,N/A
"def __eq__(self, other: object) -> bool:
<mask>:
        return False
    return self.tracks == other.tracks","not isinstance(other, MediaInfo)",13,"not isinstance(other, Track)",False,64.34588841607616,N/A
"def __init__(self, xml: str, encoding_errors: str='strict') -> None:
    xml_dom = ET.fromstring(xml.encode('utf-8', encoding_errors))
    self.tracks = []
<mask>:
        xpath = 'track'
    else:
        xpath = 'File/track'
    for xml_track in xml_dom.iterfind(xpath):
        self.tracks.append(Track(xml_track))",xml_dom.tag == 'File',28,sys.version_info[0] == 2,False,8.913765521398126,N/A
"@staticmethod
def _normalize_filename(filename: Any) -> Any:
<mask>:
        return os.fspath(filename)
    if pathlib is not None and isinstance(filename, pathlib.PurePath):
        return str(filename)
    return filename","hasattr(os, 'PathLike') and isinstance(filename, os.PathLike)",21,"isinstance(filename, str)",False,11.988448048923711,N/A
"def __init__(self, topic, payload, qos=0, retain=False, **kwargs):
    self.topic = topic.encode('utf-8', errors='replace') if isinstance(topic, str) else topic
    self.qos = qos
    self.retain = retain
    self.dup = False
    self.properties = kwargs
<mask>:
        payload = json.dumps(payload, ensure_ascii=False)
    if isinstance(payload, (int, float)):
        self.payload = str(payload).encode('ascii')
    elif isinstance(payload, str):
        self.payload = payload.encode('utf-8', errors='replace')
    elif payload is None:
        self.payload = b''
    else:
        self.payload = payload
    self.payload_size = len(self.payload)
    if self.payload_size > 268435455:
        raise ValueError('Payload too large.')","isinstance(payload, (list, tuple, dict))",69,"isinstance(payload, dict)",False,26.013004751144457,N/A
"def update_subscriptions_with_subscription_or_topic(self, subscription_or_topic, qos, no_local, retain_as_published, retain_handling_options, kwargs):
    sentinel = object()
    subscription_identifier = kwargs.get('subscription_identifier', sentinel)
<mask>:
        if subscription_identifier is not sentinel:
            subscription_or_topic.subscription_identifier = subscription_identifier
        subscriptions = [subscription_or_topic]
    elif isinstance(subscription_or_topic, (tuple, list)):
        if subscription_identifier is not sentinel:
            for sub in subscription_or_topic:
                sub.subscription_identifier = subscription_identifier
        subscriptions = subscription_or_topic
    elif isinstance(subscription_or_topic, str):
        if subscription_identifier is sentinel:
            subscription_identifier = None
        subscriptions = [Subscription(subscription_or_topic, qos=qos, no_local=no_local, retain_as_published=retain_as_published, retain_handling_options=retain_handling_options, subscription_identifier=subscription_identifier)]
    else:
        raise ValueError('Bad subscription: must be string or Subscription or list of Subscriptions')
    self.subscriptions.extend(subscriptions)
    return subscriptions","isinstance(subscription_or_topic, Subscription)",80,"isinstance(subscription_or_topic, Subscription)",True,100.00000000000004,N/A
"def _remove_subscriptions(self, topic: Union[str, Sequence[str]]):
<mask>:
        self.subscriptions = [s for s in self.subscriptions if s.topic != topic]
    else:
        self.subscriptions = [s for s in self.subscriptions if s.topic not in topic]","isinstance(topic, str)",30,"isinstance(topic, str)",True,100.00000000000004,N/A
"def resubscribe(self, subscription: Subscription, **kwargs):
<mask>:
        subscription.subscription_identifier = kwargs['subscription_identifier']
    elif subscription.subscription_identifier is not None:
        kwargs['subscription_identifier'] = subscription.subscription_identifier
    return self._connection.subscribe([subscription], **kwargs)",'subscription_identifier' in kwargs,20,'subscription_identifier' in kwargs,True,100.00000000000004,N/A
"def set_auth_credentials(self, username, password=None):
    self._username = username.encode()
    self._password = password
<mask>:
        self._password = password.encode()","isinstance(self._password, str)",14,password is not None,False,4.576506607182439,N/A
"@classmethod
def _pack_str16(cls, packet, data):
<mask>:
        data = data.encode('utf-8')
    packet.extend(struct.pack('!H', len(data)))
    packet.extend(data)","isinstance(data, str)",12,"isinstance(data, str)",True,100.00000000000004,N/A
"@classmethod
def _build_properties_data(cls, properties_dict, protocol_version):
<mask>:
        return bytearray()
    data = bytearray()
    for property_name, property_value in properties_dict.items():
        property = Property.factory(name=property_name)
        if property is None:
            logger.warning('[GMQTT] property {} is not supported, it was ignored'.format(property_name))
            continue
        property_bytes = property.dumps(property_value)
        data.extend(property_bytes)
    result = pack_variable_byte_integer(len(data))
    result.extend(data)
    return result",protocol_version < MQTTv50,43,not properties_dict,False,12.44023474812678,N/A
"@classmethod
def build_package(cls, client_id, username, password, clean_session, keepalive, protocol, will_message=None, **kwargs):
    remaining_length = 2 + len(protocol.proto_name) + 1 + 1 + 2 + 2 + len(client_id)
    connect_flags = 0
<mask>:
        connect_flags |= 2
    if will_message:
        will_prop_bytes = cls._build_properties_data(will_message.properties, protocol.proto_ver)
        remaining_length += 2 + len(will_message.topic) + 2 + len(will_message.payload) + len(will_prop_bytes)
        connect_flags |= 4 | (will_message.qos & 3) << 3 | (will_message.retain & 1) << 5
    if username is not None:
        remaining_length += 2 + len(username)
        connect_flags |= 128
        if password is not None:
            connect_flags |= 64
            remaining_length += 2 + len(password)
    command = MQTTCommands.CONNECT
    packet = bytearray()
    packet.append(command)
    prop_bytes = cls._build_properties_data(kwargs, protocol.proto_ver)
    remaining_length += len(prop_bytes)
    packet.extend(pack_variable_byte_integer(remaining_length))
    packet.extend(struct.pack('!H' + str(len(protocol.proto_name)) + 'sBBH', len(protocol.proto_name), protocol.proto_name, protocol.proto_ver, connect_flags, keepalive))
    packet.extend(prop_bytes)
    cls._pack_str16(packet, client_id)
    if will_message:
        packet += will_prop_bytes
        cls._pack_str16(packet, will_message.topic)
        cls._pack_str16(packet, will_message.payload)
    if username is not None:
        cls._pack_str16(packet, username)
        if password is not None:
            cls._pack_str16(packet, password)
    return packet",clean_session,144,clean_session,True,100.00000000000004,N/A
"@classmethod
def build_package(cls, topic, protocol, **kwargs) -> Tuple[int, bytes]:
    remaining_length = 2
<mask>:
        topics = [topic]
    else:
        topics = topic
    for t in topics:
        remaining_length += 2 + len(t)
    properties = cls._build_properties_data(kwargs, protocol.proto_ver)
    remaining_length += len(properties)
    command = MQTTCommands.UNSUBSCRIBE | 2
    packet = bytearray()
    packet.append(command)
    packet.extend(pack_variable_byte_integer(remaining_length))
    local_mid = cls.id_generator.next_id()
    packet.extend(struct.pack('!H', local_mid))
    packet.extend(properties)
    for t in topics:
        cls._pack_str16(packet, t)
    logger.info('[SEND UNSUB] %s', topics)
    return (local_mid, packet)","not isinstance(topic, (list, tuple))",65,"isinstance(topic, list)",False,24.439253249722206,N/A
"@classmethod
def build_package(cls, subscriptions, protocol, **kwargs) -> Tuple[int, bytes]:
    remaining_length = 2
    topics = []
    subscription_identifier = kwargs.get('subscription_identifier', cls.sentinel)
    for s in subscriptions:
        topic = s.topic
<mask>:
            topic = topic.encode()
        remaining_length += 2 + len(topic) + 1
        topics.append(topic)
        if subscription_identifier is cls.sentinel:
            subscription_identifier = s.subscription_identifier
    if subscription_identifier is not cls.sentinel:
        kwargs['subscription_identifier'] = subscription_identifier
    if subscription_identifier is None:
        kwargs.pop('subscription_identifier', None)
    properties = cls._build_properties_data(kwargs, protocol.proto_ver)
    remaining_length += len(properties)
    command = MQTTCommands.SUBSCRIBE | False << 3 | 2
    packet = bytearray()
    packet.append(command)
    packet.extend(pack_variable_byte_integer(remaining_length))
    local_mid = cls.id_generator.next_id()
    packet.extend(struct.pack('!H', local_mid))
    packet.extend(properties)
    for s in subscriptions:
        cls._pack_str16(packet, s.topic)
        subscribe_options = s.retain_handling_options << 4 | s.retain_as_published << 3 | s.no_local << 2 | s.qos
        packet.append(subscribe_options)
    logger.info('[SEND SUB] %s %s', local_mid, topics)
    return (local_mid, packet)","isinstance(topic, str)",117,"isinstance(topic, bytes)",False,53.7284965911771,N/A
"@on_subscribe.setter
def on_subscribe(self, cb):
<mask>:
        raise ValueError
    self._on_subscribe_callback = cb",not callable(cb),10,cb is None,False,14.127216461522432,N/A
"@on_connect.setter
def on_connect(self, cb):
<mask>:
        raise ValueError
    self._on_connected_callback = cb",not callable(cb),10,cb is None,False,14.127216461522432,N/A
"@on_message.setter
def on_message(self, cb):
<mask>:
        raise ValueError
    self._on_message_callback = cb",not callable(cb),10,cb is None,False,14.127216461522432,N/A
"@on_disconnect.setter
def on_disconnect(self, cb):
<mask>:
        raise ValueError
    self._on_disconnected_callback = cb",not callable(cb),10,cb is None,False,14.127216461522432,N/A
"@on_unsubscribe.setter
def on_unsubscribe(self, cb):
<mask>:
        raise ValueError
    self._on_unsubscribe_callback = cb",not callable(cb),10,cb is None,False,14.127216461522432,N/A
"def loads(self, bytes_array):
<mask>:
        value, left_str = unpack_utf8(bytes_array)
    elif self.bytes_struct == 'u8x2':
        value1, left_str = unpack_utf8(bytes_array)
        value2, left_str = unpack_utf8(left_str)
        value = (value1, value2)
    elif self.bytes_struct == 'b':
        str_len, = struct.unpack('!H', bytes_array[:2])
        value = bytes_array[2:2 + str_len]
        left_str = bytes_array[2 + str_len:]
    elif self.bytes_struct == 'vbi':
        value, left_str = unpack_variable_byte_integer(bytes_array)
    else:
        value, left_str = self.unpack_helper(self.bytes_struct, bytes_array)
    return ({self.name: value}, left_str)",self.bytes_struct == 'u8',60,self.bytes_struct == 'u8x1',False,84.08964152537145,N/A
"def unpack_helper(self, fmt, data):
    size = struct.calcsize(fmt)
    value = struct.unpack(fmt, data[:size])
    left_str = data[size:]
<mask>:
        value = value[0]
    return (value, left_str)",len(value) == 1,21,len(value) == 1,True,100.00000000000004,N/A
"def dumps(self, data):
    packet = bytearray()
<mask>:
        packet.extend(struct.pack('!B', self.id))
        packet.extend(pack_utf8(data))
        return packet
    elif self.bytes_struct == 'u8x2':
        if isinstance(data[0], str):
            self._dump_user_property(data, packet)
        else:
            for kv_pair in data:
                self._dump_user_property(kv_pair, packet)
        return packet
    elif self.bytes_struct == 'b':
        packet.extend(struct.pack('!B', self.id))
        packet.extend(struct.pack('!H', len(data)))
        packet.extend(data)
        return packet
    elif self.bytes_struct == 'vbi':
        packet.extend(struct.pack('!B', self.id))
        packet.extend(pack_variable_byte_integer(data))
        return packet
    packet.extend(struct.pack('!B', self.id))
    packet.extend(struct.pack(self.bytes_struct, data))
    return packet",self.bytes_struct == 'u8',56,self.bytes_struct == 'utf8',False,84.08964152537145,N/A
"@classmethod
def factory(cls, id_=None, name=None):
<mask>:
        raise ValueError('Either id or name should be not None')
    if name is not None:
        return PROPERTIES_BY_NAME.get(name)
    else:
        return PROPERTIES_BY_ID.get(id_)",name is None and id_ is None or (name is not None and id_ is not None),25,id_ is None and name is None,False,11.44905004316212,N/A
"def __init__(self, *args, **kwargs):
<mask>:
        self._closed = asyncio.get_event_loop().create_future()
    super(_StreamReaderProtocolCompatibilityMixin, self).__init__(*args, **kwargs)","sys.version_info < (3, 7)",11,not self._closed,False,3.8261660656802645,N/A
"def connection_lost(self, exc):
    super(_StreamReaderProtocolCompatibilityMixin, self).connection_lost(exc)
<mask>:
        return
    if not self._closed.done():
        if exc is None:
            self._closed.set_result(None)
        else:
            self._closed.set_exception(exc)","sys.version_info[:2] >= (3, 7)",17,self._closed is None,False,1.8231094563196564,N/A
"def __init__(self, buffer_size=2 ** 16, loop=None):
<mask>:
        loop = asyncio.get_event_loop()
    self._connection = None
    self._transport = None
    self._connected = asyncio.Event()
    reader = asyncio.StreamReader(limit=buffer_size, loop=loop)
    self._hard_reader = reader
    super(BaseMQTTProtocol, self).__init__(reader, loop=loop)",not loop,29,loop is None,False,27.516060407455225,N/A
"def write_data(self, data: bytes):
    self._connection._last_data_out = time.monotonic()
<mask>:
        self._transport.write(data)
    else:
        logger.warning('[TRYING WRITE TO CLOSED SOCKET]')",self._transport and (not self._transport.is_closing()),15,self._connection.is_closed(),False,13.411327816072216,N/A
"def connection_lost(self, exc):
    self._connected.clear()
    super(BaseMQTTProtocol, self).connection_lost(exc)
<mask>:
        logger.warning('[EXC: CONN LOST]', exc_info=exc)
    else:
        logger.info('[CONN CLOSE NORMALLY]')",exc,15,exc,True,100.00000000000004,N/A
"def _keep_connection(self):
<mask>:
        return
    time_ = time.monotonic()
    if time_ - self._last_data_in >= 2 * self._keepalive:
        self._logger.warning('[LOST HEARTBEAT FOR %s SECONDS, GOING TO CLOSE CONNECTION]', 2 * self._keepalive)
        asyncio.ensure_future(self.close())
        return
    if time_ - self._last_data_out >= 0.8 * self._keepalive or time_ - self._last_data_in >= 0.8 * self._keepalive:
        self._send_ping_request()
    self._keep_connection_callback = asyncio.get_event_loop().call_later(self._keepalive / 2, self._keep_connection)",self.is_closing() or not self._keepalive,52,self._keepalive <= 0,False,17.447394295753057,N/A
"def send_package(self, package):
    self._last_data_out = time.monotonic()
<mask>:
        package = package
    else:
        package = package.encode()
    self._transport.write(package)","isinstance(package, (bytes, bytearray))",15,"isinstance(package, str)",False,27.585129929794586,N/A
"@keepalive.setter
def keepalive(self, value):
<mask>:
        return
    self._keepalive = value
    if self._keep_connection_callback:
        self._keep_connection_callback.cancel()
    self._keep_connection_callback = asyncio.get_event_loop().call_later(self._keepalive / 2, self._keep_connection)",self._keepalive == value,18,self._keepalive is None,False,43.01250851313264,N/A
"def _mid_generate(self):
    done = False
    while not done:
<mask>:
            raise OverflowError('All ids has already used. May be your QoS query is full.')
        self._last_used_id += 1
        if self._last_used_id in self._used_ids:
            continue
        if self._last_used_id == self._max:
            self._last_used_id = 0
            continue
        done = True
    self._used_ids.add(self._last_used_id)
    return self._last_used_id",len(self._used_ids) >= self._max - 1,44,self._last_used_id in self._used_ids,False,36.06750062031238,N/A
"def free_id(self, id):
    logger.debug('FREE MID: %s', id)
<mask>:
        return
    self._used_ids.remove(id)",id not in self._used_ids,10,id not in self._used_ids,True,100.00000000000004,N/A
"def pack_variable_byte_integer(value):
    remaining_bytes = bytearray()
    while True:
        value, b = divmod(value, 128)
<mask>:
            b |= 128
        remaining_bytes.extend(struct.pack('!B', b))
        if value <= 0:
            break
    return remaining_bytes",value > 0,25,value > 0,True,100.00000000000004,N/A
"def unpack_variable_byte_integer(bts):
    multiplier = 1
    value = 0
    i = 0
    while i < 4:
        b = bts[i]
        value += (b & 127) * multiplier
<mask>:
            raise ValueError('Malformed Variable Byte Integer')
        multiplier *= 128
        if b & 128 == 0:
            break
        i += 1
    return (value, bts[i + 1:])",multiplier > 2097152,49,value & 128 == 0,False,0.0,N/A
"def on_subscribe(client, mid, qos, properties):
    subscriptions = client.get_subscriptions_by_mid(mid)
    for subscription, granted_qos in zip(subscriptions, qos):
<mask>:
            logging.warning('[RETRYING SUB {}] mid {}, reason code: {}, properties {}'.format(client._client_id, mid, granted_qos, properties))
            client.resubscribe(subscription)
        logging.info('[SUBSCRIBED {}] mid {}, QOS: {}, properties {}'.format(client._client_id, mid, granted_qos, properties))",granted_qos >= gmqtt.constants.SubAckReasonCode.UNSPECIFIED_ERROR.value,40,"client.retry_subscription(subscription, mid, granted_qos, properties)",False,10.343603005129705,N/A
"def __init__(self, message, errors=None):
    Exception.__init__(self, message)
<mask>:
        errors = {}
    self.message = message
    self.errors = errors",errors is None,16,errors is None,True,100.00000000000004,N/A
"def __init__(self, message, URL, statusCode='', errors=None):
<mask>:
        errors = {}
    mes = '%s. URL: %s, status: %s' % (message, URL, statusCode)
    pyArangoException.__init__(self, mes, errors)",errors is None,24,errors is None,True,100.00000000000004,N/A
"def __init__(self, message, errors=None):
<mask>:
        errors = {}
    pyArangoException.__init__(self, message, errors)",errors is None,11,errors is None,True,100.00000000000004,N/A
"def __init__(self, message, errors=None):
<mask>:
        errors = {}
    CreationError.__init__(self, message, errors)",errors is None,11,errors is None,True,100.00000000000004,N/A
"def fetch(self, task_id=None):
    """"""Fetch the task for given task_id. If task_id is None return all tasks """"""
<mask>:
        url = '{tasks_url}/{task_id}'.format(tasks_url=self.URL, task_id=task_id)
    else:
        url = self.URL
    response = self.database.action.get(url)
    response.raise_for_status()
    return response.json()",task_id is not None,32,task_id,False,36.78794411714425,N/A
"def create(self, name, command, params=None, period=None, offset=None, task_id=None):
    """"""Create a task with given command and its parameters.""""""
    task = {'name': name, 'command': command, 'params': params}
<mask>:
        task['period'] = period
        if offset is not None:
            task['offset'] = offset
    if task_id is not None:
        task['id'] = task_id
        url = '{tasks_url}/{task_id}'.format(tasks_url=self.URL, task_id=task_id)
    else:
        url = self.URL
    response = self.database.action.post(url, json=task)
    response.raise_for_status()
    return response.json()",period is not None,60,period is not None,True,100.00000000000004,N/A
"def __init__(self, username, password, urls, use_jwt_authentication=False, use_lock_for_reseting_jwt=True, max_retries=5, verify=None):
    self.max_retries = max_retries
    self.use_jwt_authentication = use_jwt_authentication
<mask>:
        if self.use_jwt_authentication:
            self.auth = JWTAuth(username, password, urls, use_lock_for_reseting_jwt, max_retries)
        else:
            self.auth = (username, password)
            if verify is not None and (not isinstance(verify, bool)) and (not isinstance(verify, CA_Certificate)) and (not isinstance(verify, str)):
                raise ValueError(""'verify' argument can only be of type: bool, CA_Certificate or str or None"")
            self.verify = verify
    else:
        self.auth = None",username,68,username is not None and password is not None,False,4.767707020457095,N/A
"def __reset_auth(self):
<mask>:
        return
    if self.auth.lock_for_reseting_jwt is not None:
        self.auth.lock_for_reseting_jwt.acquire()
    self.auth.reset_token()
    if self.auth.lock_for_reseting_jwt is not None:
        self.auth.lock_for_reseting_jwt.release()",not self.use_jwt_authentication,17,self.auth is None,False,11.725004053101795,N/A
"def _run(self, req):
    """"""Run the request.""""""
<mask>:
        if isinstance(self.verify, CA_Certificate):
            req.kwargs['verify'] = self.verify.get_file_path()
        else:
            req.kwargs['verify'] = self.verify
    for _ in range(self.max_retries):
        gevent.joinall([gevent.spawn(req.send)])
        if self.use_jwt_authentication:
            if hasattr(req, 'exception'):
                logging.critical('%s is raised, will try to reset the auth and request again.', req.exception)
                self.__reset_auth()
            elif req.response.status_code == 401:
                logging.critical('Invalid authentication token provided, will try to reset the auth and request again.')
                self.__reset_auth()
            else:
                return req.response
        elif hasattr(req, 'exception'):
            logging.critical('%s is raised, will try to request again', req.exception)
        elif req.response.status_code == 401:
            logging.critical('Unauthorized access, you must supply a (username, password) with the correct credentials')
        else:
            return req.response
    logging.critical('Tried to send the request max number of times.')
    return req.response",not self.use_jwt_authentication and self.verify is not None,104,self.verify,False,1.8315638888734187,N/A
"def post(self, url, data=None, json=None, **kwargs):
    """"""HTTP POST Method.""""""
<mask>:
        kwargs['data'] = data
    if json is not None:
        kwargs['json'] = json
    kwargs['auth'] = self.auth
    req = grequests.post(url, **kwargs)
    return self._run(req)",data is not None,30,data is not None,True,100.00000000000004,N/A
"def put(self, url, data=None, **kwargs):
    """"""HTTP PUT Method.""""""
<mask>:
        kwargs['data'] = data
    kwargs['auth'] = self.auth
    req = grequests.put(url, **kwargs)
    return self._run(req)",data is not None,21,data,False,4.9787068367863965,N/A
"def __next__(self):
    """"""returns the next batch""""""
    r = self.connection.session.put(self.getURL())
    data = r.json()
<mask>:
        raise CursorError(data['errorMessage'], self.id, data)
    return r.json()","r.status_code in [400, 404]",19,data['errorMessage'],False,3.300991086751251,N/A
"def __init__(self, request, database, rawResults):
    """"""If rawResults = True, the results will be returned as dictionaries instead of Document objects.""""""
    self.rawResults = rawResults
    self.response = request.json()
<mask>:
        raise QueryError(self.response['errorMessage'], self.response)
    self.request = request
    self.database = database
    self.connection = self.database.connection
    self.currI = 0
    if request.status_code == 201 or request.status_code == 200 or request.status_code == 202:
        self.batchNumber = 1
        try:
            self.response = {'result': [self.response['document']], 'hasMore': False}
            del self.response['document']
        except KeyError:
            pass
        if 'hasMore' in self.response and self.response['hasMore']:
            cursor_id = self.response.get('id', '')
            self.cursor = RawCursor(self.database, cursor_id)
        else:
            self.cursor = None
    elif request.status_code == 404:
        self.batchNumber = 0
        self.result = []
    else:
        self._raiseInitFailed(request)",self.response.get('error') and self.response['errorMessage'] != 'no match',99,self.response['errorMessage'],False,11.455884399268777,N/A
"def _developDoc(self, i):
    """"""private function that transforms a json returned by ArangoDB into a pyArango Document or Edge""""""
    docJson = self.result[i]
    try:
        collection = self.database[docJson['_id'].split('/')[0]]
    except KeyError:
        raise CreationError('result %d is not a valid Document. Try setting rawResults to True' % i)
<mask>:
        self.result[i] = Edge(collection, docJson)
    else:
        self.result[i] = Document(collection, docJson)",collection.type == CONST.COLLECTION_EDGE_TYPE,52,self.rawResults,False,1.3699439807202476,N/A
"def nextBatch(self):
    """"""become the next batch. raises a StopIteration if there is None""""""
    self.batchNumber += 1
    self.currI = 0
    try:
<mask>:
            raise StopIteration('That was the last batch')
    except KeyError:
        raise AQLQueryError(self.response['errorMessage'], self.query, self.response)
    self.response = next(self.cursor)",not self.response['hasMore'] or self.cursor is None,36,self.batchNumber >= self.maxNumberOfRequests,False,8.3606227662173,N/A
"def __getitem__(self, i):
    """"""returns a ith result of the query. Raises IndexError if we reached the end of the current batch.""""""
<mask>:
        self._developDoc(i)
    return self.result[i]","not self.rawResults and (not isinstance(self.result[i], (Edge, Document)))",25,self.result is None,False,1.0864878331643486,N/A
"def __init__(self, collection, infos=None, creationData=None):
    self.collection = collection
    self.connection = self.collection.database.connection
    self.infos = None
    self.active = False
<mask>:
        self.infos = infos
    elif creationData:
        self._create(creationData)",infos,24,infos,True,100.00000000000004,N/A
"def getURL(self):
<mask>:
        return '%s/%s' % (self.getIndexesURL(), self.infos['id'])
    return None",self.infos,10,self.infos,True,100.00000000000004,N/A
"def _create(self, postData, force=False):
    """"""Creates an index of any type according to postData""""""
<mask>:
        r = self.connection.session.post(self.getIndexesURL(), params={'collection': self.collection.name}, data=json.dumps(postData, default=str))
        data = r.json()
        if r.status_code >= 400 or data['error']:
            raise CreationError(data['errorMessage'], data)
        self.infos = data
        self.active = True",self.infos is None or not self.active or force,39,not force and self.collection.name in self.infos,False,17.242221289766636,N/A
"def delete(self):
    """"""Delete the index""""""
    r = self.connection.session.delete(self.getURL())
    data = r.json()
<mask>:
        raise DeletionError(data['errorMessage'], data)
    self.active = False",r.status_code != 200 and r.status_code != 202 or data['error'],18,data['success'],False,0.3927623392184508,N/A
"def __init__(self, users, jsonData=None):
<mask>:
        jsonData = {}
    self._store = {}
    self.users = users
    self.connection = self.users.connection
    self._store = {'username': None, 'active': True, 'extra': None, 'changePassword': None, 'password': None}
    self.isSet = False
    if len(jsonData) > 0:
        self._set(jsonData)",jsonData is None,37,jsonData is None,True,100.00000000000004,N/A
"def save(self):
    """"""Save/updates the user""""""
    import json
    payload = {}
    payload.update(self._store)
    payload['user'] = payload['username']
    payload['passwd'] = payload['password']
    del payload['username']
    del payload['password']
    payload = json.dumps(payload, default=str)
<mask>:
        if 'username' not in self._store or 'password' not in self._store:
            raise KeyError(""You must define self['name'] and self['password'] to be able to create a new user"")
        r = self.connection.session.post(self.users.getURL(), data=payload)
        data = r.json()
        if r.status_code == 201:
            self._set(data)
        else:
            raise CreationError('Unable to create new user', data)
    else:
        r = self.connection.session.put(self.getURL(), data=payload)
        data = r.json()
        if r.status_code == 200:
            self._set(data)
        else:
            raise UpdateError('Unable to update user, status: %s' % r.status_code, data)",not self.isSet,95,self.is_authenticated(),False,13.134549472120788,N/A
"def setPermissions(self, dbName, access):
    """"""Grant revoke rights on a database, 'access' is supposed to be boolean. ArangoDB grants/revokes both read and write rights at the same time""""""
    import json
<mask>:
        raise CreationError('Please save user first', None, None)
    rights = []
    if access:
        rights.append('rw')
    rights = ''.join(rights)
    if not self.connection.hasDatabase(dbName):
        raise KeyError('Unknown database: %s' % dbName)
    url = '%s/database/%s' % (self.getURL(), dbName)
    r = self.connection.session.put(url, data=json.dumps({'grant': rights}, default=str))
    if r.status_code < 200 or r.status_code > 202:
        raise CreationError('Unable to grant rights', r.content)",not self.isSet,81,not self.connection.isSession(),False,20.556680845025987,N/A
"def delete(self):
    """"""Permanently remove the user""""""
<mask>:
        raise CreationError('Please save user first', None, None)
    r = self.connection.session.delete(self.getURL())
    if r.status_code < 200 or r.status_code > 202:
        raise DeletionError('Unable to delete user, url: %s, status: %s' % (r.url, r.status_code), r.content)
    self.isSet = False",not self.isSet,41,not self.isSet,True,100.00000000000004,N/A
"def __setitem__(self, k, v):
<mask>:
        raise KeyError('The only keys available for user are: %s' % list(self._store.keys()))
    self._store[k] = v",k not in list(self._store.keys()),19,k not in self._store.keys(),False,61.91566827062977,N/A
"def reloadCollections(self):
    """"""reloads the collection list.""""""
    r = self.connection.session.get(self.getCollectionsURL())
    data = r.json()
<mask>:
        self.collections = {}
        for colData in data['result']:
            colName = colData['name']
            if colData['isSystem']:
                colObj = COL.SystemCollection(self, colData)
            else:
                try:
                    colClass = COL.getCollectionClass(colName)
                    colObj = colClass(self, colData)
                except KeyError:
                    if colData['type'] == CONST.COLLECTION_EDGE_TYPE:
                        colObj = COL.Edges(self, colData)
                    elif colData['type'] == CONST.COLLECTION_DOCUMENT_TYPE:
                        colObj = COL.Collection(self, colData)
                    else:
                        print('Warning!! Collection of unknown type: %d, trying to load it as Collection nonetheless.' % colData['type'])
                        colObj = COL.Collection(self, colData)
            self.collections[colName] = colObj
    else:
        raise UpdateError(data['errorMessage'], data)",r.status_code == 200,83,data['success'],False,0.0,N/A
"def reloadGraphs(self):
    """"""reloads the graph list""""""
    r = self.connection.session.get(self.getGraphsURL())
    data = r.json()
<mask>:
        self.graphs = {}
        for graphData in data['graphs']:
            try:
                self.graphs[graphData['_key']] = GR.getGraphClass(graphData['_key'])(self, graphData)
            except KeyError:
                self.graphs[graphData['_key']] = Graph(self, graphData)
    else:
        raise UpdateError(data['errorMessage'], data)",r.status_code == 200,35,'graphs' in data,False,0.0,N/A
"def createCollection(self, className='Collection', **colProperties):
    """"""Creates a collection and returns it.
        ClassName the name of a class inheriting from Collection or Egdes, it can also be set to 'Collection' or 'Edges' in order to create untyped collections of documents or edges.
        Use colProperties to put things such as 'waitForSync = True' (see ArangoDB's doc
        for a full list of possible arugments). If a '_properties' dictionary is defined in the collection schema, arguments to this function overide it""""""
    colClass = COL.getCollectionClass(className)
<mask>:
        colProperties = dict(colProperties)
    else:
        try:
            colProperties = dict(colClass._properties)
        except AttributeError:
            colProperties = {}
    if className != 'Collection' and className != 'Edges' and ('name' not in colProperties):
        colProperties['name'] = className
    elif 'name' not in colProperties:
        raise ValueError(""a 'name' argument mush be supplied if you want to create a generic collection"")
    if colProperties['name'] in self.collections:
        raise CreationError('Database %s already has a collection named %s' % (self.name, colProperties['name']))
    if issubclass(colClass, COL.Edges) or colClass.__class__ is COL.Edges:
        colProperties['type'] = CONST.COLLECTION_EDGE_TYPE
    else:
        colProperties['type'] = CONST.COLLECTION_DOCUMENT_TYPE
    payload = json.dumps(colProperties, default=str)
    req = self.connection.session.post(self.getCollectionsURL(), data=payload)
    data = req.json()
    if req.status_code == 200 and (not data['error']):
        col = colClass(self, data)
        self.collections[col.name] = col
        return self.collections[col.name]
    else:
        raise CreationError(data['errorMessage'], data)",len(colProperties) > 0,190,"isinstance(colClass, COL.Properties)",False,6.567274736060395,N/A
"def createGraph(self, name, createCollections=True, isSmart=False, numberOfShards=None, smartGraphAttribute=None, replicationFactor=None, writeConcern=None):
    """"""Creates a graph and returns it. 'name' must be the name of a class inheriting from Graph.
        Checks will be performed to make sure that every collection mentionned in the edges definition exist. Raises a ValueError in case of
        a non-existing collection.""""""

    def _checkCollectionList(lst):
        for colName in lst:
<mask>:
                raise ValueError(""'%s' is not a defined Collection"" % colName)
    graphClass = GR.getGraphClass(name)
    ed = []
    for e in graphClass._edgeDefinitions:
        if not COL.isEdgeCollection(e.edgesCollection):
            raise ValueError(""'%s' is not a defined Edge Collection"" % e.edgesCollection)
        _checkCollectionList(e.fromCollections)
        _checkCollectionList(e.toCollections)
        ed.append(e.toJson())
    _checkCollectionList(graphClass._orphanedCollections)
    options = {}
    if numberOfShards:
        options['numberOfShards'] = numberOfShards
    if smartGraphAttribute:
        options['smartGraphAttribute'] = smartGraphAttribute
    if replicationFactor:
        options['replicationFactor'] = replicationFactor
    if writeConcern:
        options['writeConcern'] = writeConcern
    payload = {'name': name, 'edgeDefinitions': ed, 'orphanCollections': graphClass._orphanedCollections}
    if isSmart:
        payload['isSmart'] = isSmart
    if options:
        payload['options'] = options
    payload = json.dumps(payload)
    r = self.connection.session.post(self.getGraphsURL(), data=payload)
    data = r.json()
    if r.status_code == 201 or r.status_code == 202:
        self.graphs[name] = graphClass(self, data['graph'])
    else:
        raise CreationError(data['errorMessage'], data)
    return self.graphs[name]",not COL.isCollection(colName),163,not COL.isCollection(colName),True,100.00000000000004,N/A
"def dropAllCollections(self):
    """"""drops all public collections (graphs included) from the database""""""
    for graph_name in self.graphs:
        self.graphs[graph_name].delete()
    for collection_name in self.collections:
<mask>:
            self[collection_name].delete()
    return",not collection_name.startswith('_'),23,collection_name in self,False,11.976547020391715,N/A
"def __new__(cls, name, bases, attrs):
    clsObj = type.__new__(cls, name, bases, attrs)
<mask>:
        try:
            if len(attrs['_edgeDefinitions']) < 1:
                raise CreationError(""Graph class '%s' has no edge definition"" % name)
        except KeyError:
            raise CreationError(""Graph class '%s' has no field _edgeDefinition"" % name)
    if name != 'Graph':
        Graph_metaclass.graphClasses[name] = clsObj
    return clsObj",name != 'Graph',48,'_edgeDefinitions' in attrs,False,0.0,N/A
"def __init__(self, database, jsonInit):
    self.database = database
    self.connection = self.database.connection
    try:
        self._key = jsonInit['_key']
    except KeyError:
        self._key = jsonInit['name']
    except KeyError:
        raise KeyError(""'jsonInit' must have a field '_key' or a field 'name'"")
    self.name = self._key
    self._rev = jsonInit['_rev']
    self._id = jsonInit['_id']
    orfs = set(self._orphanedCollections)
    for o in jsonInit['orphanCollections']:
<mask>:
            self._orphanedCollections.append(o)
            if self.connection.verbose:
                print('Orphan collection %s is not in graph definition. Added it' % o)
    self.definitions = {}
    edNames = set()
    for ed in self._edgeDefinitions:
        self.definitions[ed.edgesCollection] = ed.edgesCollection
    for ed in jsonInit['edgeDefinitions']:
        if ed['collection'] not in self.definitions:
            self.definitions[ed['collection']] = EdgeDefinition(ed['collection'], fromCollections=ed['from'], toCollections=ed['to'])
            if self.connection.verbose:
                print('Edge definition %s is not in graph definition. Added it' % ed)
    for de in self._edgeDefinitions:
        if de.edgesCollection not in self.database.collections and (not COL.isEdgeCollection(de.edgesCollection)):
            raise KeyError(""'%s' is not a valid edge collection"" % de.edgesCollection)
        self.definitions[de.edgesCollection] = de",o not in orfs,130,o not in orfs,True,100.00000000000004,N/A
"def createVertex(self, collectionName, docAttributes, waitForSync=False):
    """"""adds a vertex to the graph and returns it""""""
    url = '%s/vertex/%s' % (self.getURL(), collectionName)
    store = DOC.DocumentStore(self.database[collectionName], validators=self.database[collectionName]._fields, initDct=docAttributes)
    store.validate()
    r = self.connection.session.post(url, data=json.dumps(docAttributes, default=str), params={'waitForSync': waitForSync})
    data = r.json()
<mask>:
        return self.database[collectionName][data['vertex']['_key']]
    raise CreationError('Unable to create vertice, %s' % data['errorMessage'], data)",r.status_code == 201 or r.status_code == 202,48,data['vertex']['type'] == 'vertex',False,4.167457990973867,N/A
"def deleteVertex(self, document, waitForSync=False):
    """"""deletes a vertex from the graph as well as al linked edges""""""
    url = '%s/vertex/%s' % (self.getURL(), document._id)
    r = self.connection.session.delete(url, params={'waitForSync': waitForSync})
    data = r.json()
<mask>:
        return True
    raise DeletionError('Unable to delete vertice, %s' % document._id, data)",r.status_code == 200 or r.status_code == 202,42,'Vertex' in data,False,0.0,N/A
"def createEdge(self, collectionName, _fromId, _toId, edgeAttributes, waitForSync=False):
    """"""creates an edge between two documents""""""
<mask>:
        raise ValueError('Invalid _fromId: %s' % _fromId)
    if not _toId:
        raise ValueError('Invalid _toId: %s' % _toId)
    if collectionName not in self.definitions:
        raise KeyError(""'%s' is not among the edge definitions"" % collectionName)
    url = '%s/edge/%s' % (self.getURL(), collectionName)
    self.database[collectionName].validatePrivate('_from', _fromId)
    self.database[collectionName].validatePrivate('_to', _toId)
    ed = self.database[collectionName].createEdge()
    ed.set(edgeAttributes)
    ed.validate()
    payload = ed.getStore()
    payload.update({'_from': _fromId, '_to': _toId})
    r = self.connection.session.post(url, data=json.dumps(payload, default=str), params={'waitForSync': waitForSync})
    data = r.json()
    if r.status_code == 201 or r.status_code == 202:
        return self.database[collectionName][data['edge']['_key']]
    raise CreationError('Unable to create edge, %s' % r.json()['errorMessage'], data)",not _fromId,95,not _fromId,True,100.00000000000004,N/A
"def __get_auth_token(self):
    request_data = '{""username"":""%s"",""password"":""%s""}' % (self.username, self.password)
    for connection_url in self.urls:
        try:
            response = self.session.post('%s/_open/auth' % connection_url, data=request_data)
<mask>:
                json_data = response.content
                if json_data:
                    data_dict = json_mod.loads(json_data.decode('utf-8'))
                    return data_dict.get('jwt')
        except requests_exceptions.ConnectionError:
            if connection_url is not self.urls[-1]:
                logging.critical('Unable to connect to %s trying another', connection_url)
            else:
                logging.critical('Unable to connect to any of the urls: %s', self.urls)
                raise",response.ok,57,response.status_code == 200,False,11.044795567078939,N/A
"def __call__(self, req):
<mask>:
        if self.lock_for_reseting_jwt is not None:
            self.lock_for_reseting_jwt.acquire()
        if self.is_token_expired():
            self.reset_token()
        if self.lock_for_reseting_jwt is not None:
            self.lock_for_reseting_jwt.release()
    req.headers['Authorization'] = 'Bearer %s' % self.token
    return req",self.is_token_expired(),27,self.token is None,False,11.415938068117505,N/A
"def __init__(self, certificate, encoded):
    super(CA_Certificate, self).__init__()
    self.certificate = certificate
<mask>:
        self.certificate = base64.b64decode(self.certificate)
    self.tmp_file = None",encoded,16,encoded,True,100.00000000000004,N/A
"def get_file_path(self):
    """"""saves the cetificate into a tmp file and returns the file path""""""
<mask>:
        return self.tmp_file
    _, self.tmp_file = tempfile.mkstemp(text=True)
    f = open(self.tmp_file, 'wb')
    f.write(self.certificate)
    f.close()
    return self.tmp_file",self.tmp_file is not None,29,self.tmp_file,False,54.88116360940266,N/A
"def clean(self):
    """"""erases the tmp_file containing the certificate""""""
<mask>:
        os.remove(self.tmp_file)
        self.tmp_file = None",self.tmp_file is not None,13,self.tmp_file,False,54.88116360940266,N/A
"def __init__(self, fct, auth, max_conflict_retries=5, verify=True, timeout=30):
    self.fct = fct
    self.auth = auth
    self.max_conflict_retries = max_conflict_retries
<mask>:
        raise ValueError(""'verify' argument can only be of type: bool, CA_Certificate or str "")
    self.verify = verify
    self.timeout = timeout","not isinstance(verify, bool) and (not isinstance(verify, CA_Certificate)) and (not not isinstance(verify, str))",36,"not isinstance(verify, (bool, CA_Certificate))",False,18.902123761897066,N/A
"def __call__(self, *args, **kwargs):
<mask>:
        kwargs['auth'] = self.auth
    if isinstance(self.verify, CA_Certificate):
        kwargs['verify'] = self.verify.get_file_path()
    else:
        kwargs['verify'] = self.verify
    kwargs['timeout'] = self.timeout
    try:
        do_retry = True
        retry = 0
        while do_retry and retry < self.max_conflict_retries:
            ret = self.fct(*args, **kwargs)
            do_retry = ret.status_code == 1200
            try:
                data = ret.json()
                do_retry = do_retry or ('errorNum' in data and data['errorNum'] == 1200)
            except JSONDecodeError:
                pass
            retry += 1
    except:
        print('===\nUnable to establish connection, perhaps arango is not running.\n===')
        raise
    if len(ret.content) < 1:
        raise ConnectionError('Empty server response', ret.url, ret.status_code, ret.content)
    elif ret.status_code == 401:
        raise ConnectionError('Unauthorized access, you must supply a (username, password) with the correct credentials', ret.url, ret.status_code, ret.content)
    ret.json = JsonHook(ret)
    return ret",self.auth,111,"isinstance(self.auth, CA_Auth)",False,15.619699684601283,N/A
"def __init__(self, username, password, verify=True, cert=None, max_conflict_retries=5, max_retries=5, single_session=True, log_requests=False, pool_maxsize=10, timeout=30):
<mask>:
        self.auth = (username, password)
    else:
        self.auth = None
    self.pool_maxsize = pool_maxsize
    self.verify = verify
    self.cert = cert
    self.max_retries = max_retries
    self.log_requests = log_requests
    self.max_conflict_retries = max_conflict_retries
    self.timeout = timeout
    self.session = None
    if single_session:
        self.session = self._make_session()
    if log_requests:
        self.log = {}
        self.log['nb_request'] = 0
        self.log['requests'] = {}",username,61,username and password,False,27.516060407455225,N/A
"def _make_session(self):
    session = requests.Session()
    kwargs = {'max_retries': self.max_retries, 'pool_connections': self.pool_maxsize, 'pool_maxsize': self.pool_maxsize}
    http = requests.adapters.HTTPAdapter(**kwargs)
    https = requests.adapters.HTTPAdapter(**kwargs)
    session.mount('http://', http)
    session.mount('https://', https)
<mask>:
        session.cert = self.cert
    return session",self.cert,29,self.cert,True,100.00000000000004,N/A
"def __getattr__(self, request_function_name):
<mask>:
        session = self.session
    else:
        session = self._make_session()
    try:
        request_function = getattr(session, request_function_name)
    except AttributeError:
        raise AttributeError(""Attribute '%s' not found (no Aikido move available)"" % request_function_name)
    auth = object.__getattribute__(self, 'auth')
    verify = object.__getattribute__(self, 'verify')
    timeout = object.__getattribute__(self, 'timeout')
    if self.log_requests:
        log = object.__getattribute__(self, 'log')
        log['nb_request'] += 1
        log['requests'][request_function.__name__] += 1
    return AikidoSession.Holder(request_function, auth, max_conflict_retries=self.max_conflict_retries, verify=verify, timeout=timeout)",self.session is not None,59,self.session,False,36.78794411714425,N/A
"def status(self):
    """""" fetches the server status.""""""
    url = '%s/_admin/status' % self.connection.getEndpointURL()
    result = self.connection.session.get(url)
<mask>:
        return result.json()
    raise ArangoError(result.json()['errorMessage'], result.json())",result.status_code < 400,21,result.status_code == 200,False,51.697315395717055,N/A
"def __init__(self, collection, validators=None, initDct=None, patch=False, subStore=False, validateInit=False):
<mask>:
        validators = {}
    if initDct is None:
        initDct = {}
    self.store = {}
    self.patchStore = {}
    self.collection = collection
    self.validators = validators
    self.validateInit = validateInit
    self.isSubStore = subStore
    self.subStores = {}
    self.patching = patch
    if not self.validateInit:
        self.mustValidate = False
        self.set(initDct)
    for v in self.collection._validation.values():
        if v:
            self.mustValidate = True
            break
    if self.validateInit:
        self.set(initDct)
    self.patching = True",validators is None,66,validators is None,True,100.00000000000004,N/A
"def getPatches(self):
    """"""get patches as a dictionary""""""
<mask>:
        return self.getStore()
    res = {}
    res.update(self.patchStore)
    for k, v in self.subStores.items():
        res[k] = v.getPatches()
    return res",not self.mustValidate,24,not self.subStores,False,59.460355750136046,N/A
"def validateField(self, field):
    """"""Validatie a field""""""
<mask>:
        raise SchemaViolation(self.collection.__class__, field)
    if field in self.store:
        if isinstance(self.store[field], DocumentStore):
            return self[field].validate()
        if field in self.patchStore:
            try:
                return self.validators[field].validate(self.patchStore[field])
            except ValidationError as e:
                raise ValidationError(""'%s' -> %s"" % (field, str(e)))
        else:
            try:
                return self.validators[field].validate(self.store[field])
            except ValidationError as e:
                raise ValidationError(""'%s' -> %s"" % (field, str(e)))
            except AttributeError:
                if isinstance(self.validators[field], dict) and (not isinstance(self.store[field], dict)):
                    raise ValueError(""Validator expected a sub document for field '%s', got '%s' instead"" % (field, self.store[field]))
                else:
                    raise
    return True",field not in self.validators and (not self.collection._validation['allow_foreign_fields']),80,field not in self.collection.__bases__,False,24.51997003960852,N/A
"def validate(self):
    """"""Validate the whole document""""""
<mask>:
        return True
    res = {}
    for field in self.validators.keys():
        try:
            if isinstance(self.validators[field], dict) and field not in self.store:
                self.store[field] = DocumentStore(self.collection, validators=self.validators[field], initDct={}, subStore=True, validateInit=self.validateInit)
            self.validateField(field)
        except InvalidDocument as e:
            res.update(e.errors)
        except (ValidationError, SchemaViolation) as e:
            res[field] = str(e)
    if len(res) > 0:
        raise InvalidDocument(res)
    return True",not self.mustValidate,54,not self.collection,False,59.460355750136046,N/A
"def set(self, dct):
    """"""Set the values to a dict. Any missing value will be filled by it's default""""""
    for field, value in dct.items():
<mask>:
            if isinstance(value, dict):
                if field in self.validators and isinstance(self.validators[field], dict):
                    vals = self.validators[field]
                else:
                    vals = {}
                self[field] = DocumentStore(self.collection, validators=vals, initDct=value, patch=self.patching, subStore=True, validateInit=self.validateInit)
                self.subStores[field] = self.store[field]
            else:
                self[field] = value",field not in self.collection.arangoPrivates,56,field not in self,False,36.78794411714425,N/A
"def cache(self, doc):
<mask>:
        ret = self.cacheStore[doc._key]
        if ret.prev is not None:
            ret.prev.nextDoc = ret.nextDoc
            self.head.prev = ret
            ret.nextDoc = self.head
            self.head = ret
        return self.head
    elif len(self.cacheStore) == 0:
        ret = CachedDoc(doc, prev=None, nextDoc=None)
        self.head = ret
        self.tail = self.head
        self.cacheStore[doc._key] = ret
    else:
        if len(self.cacheStore) >= self.cacheSize:
            del self.cacheStore[self.tail._key]
            self.tail = self.tail.prev
            self.tail.nextDoc = None
        ret = CachedDoc(doc, prev=None, nextDoc=self.head)
        self.head.prev = ret
        self.head = self.head.prev
        self.cacheStore[doc._key] = ret",doc._key in self.cacheStore,71,doc._key in self.cacheStore,True,100.00000000000004,N/A
"def __init__(self, validators=None, default=None):
    """"""Validators must be a list of validators.

        'default' can also be a callable.""""""
<mask>:
        validators = []
    self.validators = validators
    self.default = default",not validators,27,validators is None,False,27.516060407455225,N/A
"def __new__(cls, name, bases, attrs):

    def check_set_ConfigDict(dictName):
        defaultDict = getattr(cls, '%sDefault' % dictName)
<mask>:
            attrs[dictName] = defaultDict
        else:
            for k, v in attrs[dictName].items():
                if k not in defaultDict:
                    raise KeyError(""Unknown validation parameter '%s' for class '%s'"" % (k, name))
                if type(v) is not type(defaultDict[k]):
                    raise ValueError(""'%s' parameter '%s' for class '%s' is of type '%s' instead of '%s'"" % (dictName, k, name, type(v), type(defaultDict[k])))
            for k, v in defaultDict.items():
                if k not in attrs[dictName]:
                    attrs[dictName][k] = v
    check_set_ConfigDict('_validation')
    clsObj = type.__new__(cls, name, bases, attrs)
    Collection_metaclass.collectionClasses[name] = clsObj
    return clsObj",dictName not in attrs,89,dictName not in attrs,True,100.00000000000004,N/A
"def getDefaultDocument(self, fields=None, dct=None):
<mask>:
        dct = {}
    if fields is None:
        fields = self._fields
    for k, v in fields.items():
        if isinstance(v, dict):
            dct[k] = self.getDefaultDocument(fields[k], None)
        elif isinstance(v, list) or isinstance(v, tuple):
            dct[k] = []
        elif isinstance(v, Field):
            if callable(v.default):
                dct[k] = v.default()
            else:
                dct[k] = v.default
        else:
            raise ValueError(""Field '%s' is of invalid type '%s'"" % (k, type(v)))
    return dct",dct is None,62,dct is None,True,100.00000000000004,N/A
"def getIndexes(self):
    """"""Fill 'self.indexes' with all the indexes associated with the collection and return it.""""""
    self.indexes_by_name = {}
    url = '%s/index' % self.database.getURL()
    r = self.connection.session.get(url, params={'collection': self.name})
    data = r.json()
    for ind in data['indexes']:
        index = Index(collection=self, infos=ind)
        self.indexes[ind['type']][ind['id']] = index
<mask>:
            self.indexes_by_name[ind['name']] = index
    return self.indexes",'name' in ind,48,ind['name'] not in self.indexes_by_name,False,4.065425428798724,N/A
"def validate(self, value):
<mask>:
        raise ValidationError(""Field can't have a null value, got: '%s'"" % value)
    return True",value is None or ((value == 0 and type(value) != bool) and self.reject_zero) or (value == '' and self.reject_empty_string),17,value is None,False,0.0003154543805170235,N/A
"def validate(self, value):
    import re
    pat = '^[A-z0-9._-]+@[A-z0-9.-]+\\.[A-z]{2,4}$'
<mask>:
        raise ValidationError('The email address: %s is invalid' % value)
    return True","re.match(pat, value) is None",20,"not re.search(pat, value)",False,45.93613320783059,N/A
"def validate(self, value):
<mask>:
        raise ValidationError('%s is not a valid integer' % value)
    return True","not isinstance(value, int)",15,"not isinstance(value, int)",True,100.00000000000004,N/A
"def validate(self, value):
<mask>:
        raise ValidationError('%s is not a valid boolean' % value)
    return True","not isinstance(value, bool)",15,not self.is_boolean(value),False,11.339582221952005,N/A
"def validate(self, value):
<mask>:
        raise ValidationError('%s is not a valid string' % value)
    return True","not isinstance(value, str) and (not isinstance(value, unicode))",15,"not isinstance(value, str)",False,23.965103644177596,N/A
"def service(self, mount):
    """"""Return a service so that only route after the mount.

        Parameters
        ----------
        mount : str
            mount point.

        Returns
        -------
        FoxxService
            A mounted service

        """"""
<mask>:
        self.reload()
    if mount not in self.mounts:
        raise ValueError(""Unable to find the mount: '%s'"", mount)
    return FoxxService(self.database, mount)",mount not in self.mounts,45,self.database is None,False,17.491650626361256,N/A
"def setUp(self):
<mask>:
        global ARANGODB_URL
        global ARANGODB_ROOT_USERNAME
        global ARANGODB_ROOT_PASSWORD
    else:
        ARANGODB_URL = os.getenv('ARANGODB_URL', 'http://127.0.0.1:8529')
        ARANGODB_ROOT_USERNAME = os.getenv('ARANGODB_ROOT_USERNAME', 'root')
        ARANGODB_ROOT_PASSWORD = os.getenv('ARANGODB_ROOT_PASSWORD', 'root')
    self.conn = Connection(arangoURL=ARANGODB_URL, username=ARANGODB_ROOT_USERNAME, password=ARANGODB_ROOT_PASSWORD)
    try:
        self.conn.createDatabase(name='test_db_2')
    except CreationError:
        pass
    self.db = self.conn['test_db_2']
    self.admin = Admin(self.conn)
    self.is_cluster = self.admin.is_cluster()
    self.server_version = self.conn.getVersion()
    self._reset()",__name__ == '__main__',45,os.getenv('ARANGODB_ENABLE_AUTO_LOGIN'),False,3.7726698069117846,N/A
"def _reset(self):
    self.db.reload()
    self.db.tasks.drop()
    for colName in self.db.collections:
<mask>:
            self.db[colName].delete()
    for graph in self.db.graphs.values():
        graph.delete()
    for user in self.conn.users.fetchAllUsers():
        if user['username'].find('pyArangoTest') > -1:
            user.delete()
    self.conn.disconnectSession()",not self.db[colName].isSystem,25,colName.find('pyArangoTest') > -1,False,5.795599612995366,N/A
"def test_bulk_import_exception(self):
    usersCollection = self.db.createCollection(name='users')
    nbUsers = 2
    users = []
    for i in range(nbUsers):
        user = {}
        user['_key'] = 'tesla'
        user['name'] = 'Tesla-%d' % i
        user['number'] = i
        user['species'] = 'human'
        users.append(user)
    with self.assertRaises(CreationError):
        usersCollection.importBulk(users, onDuplicate='error', complete=True)
    expectCount = 0
<mask>:
        expectCount = 1
    self.assertEqual(usersCollection.count(), expectCount)",self.is_cluster,47,sys.version_info[0] < 3,False,4.990049701936832,N/A
"def test_document_create_update_delete(self):
    collection = self.db.createCollection(name='lala')
    doc = collection.createDocument()
    doc['name'] = 'Tesla'
    self.assertTrue(doc._id is None)
    doc.save()
<mask>:
        shardID = doc.getResponsibleShard()
        self.assertTrue(shardID.startswith('s'))
    self.assertTrue(doc._id is not None)
    did = copy.copy(doc._id)
    doc['name'] = 'Tesla2'
    doc.save()
    self.assertEqual(doc._id, did)
    doc.delete()
    self.assertTrue(doc._id is None)",self.server_version['version'] >= '3.5' and self.is_cluster,37,"hasattr(doc, 'getResponsibleShard')",False,0.0,N/A
"def test_fields_on_save(self):
    import pyArango.validation as VAL
    import types

    class String_val(VAL.Validator):

        def validate(self, value):
<mask>:
                raise ValidationError('Field value must be a string')
            return True

    class Col_on_save(Collection):
        _validation = {'on_save': True, 'on_set': False, 'allow_foreign_fields': False}
        _fields = {'str': Field(validators=[String_val()]), 'nestedStr': {'str': Field(validators=[VAL.Length(1, 51)])}}
    myCol = self.db.createCollection('Col_on_save')
    doc = myCol.createDocument()
    doc['str'] = 3
    self.assertRaises(InvalidDocument, doc.save)
    doc = myCol.createDocument()
    doc['str'] = 'string'
    self.assertRaises(SchemaViolation, doc.__setitem__, 'foreigner', 'string')
    doc = myCol.createDocument()
    doc['nestedStr'] = {}
    doc['nestedStr']['str'] = 3
    doc['str'] = 'string'
    self.assertRaises(InvalidDocument, doc.save)
    doc = myCol.createDocument()
    doc['nestedStr'] = {}
    doc['nestedStr']['str'] = 'string'
    doc['str'] = 'string'
    doc.save()
    self.assertEqual(myCol[doc._key]._store.getStore(), doc._store.getStore())
    doc['nestedStr']['str'] = 'string2'
    self.assertTrue(len(doc._store.getPatches()) > 0)
    doc.patch()
    self.assertEqual(myCol[doc._key]._store.getStore(), doc._store.getStore())","not isinstance(value, bytes) and (not isinstance(value, str))",100,"not isinstance(value, types.String_val)",False,21.282440525058952,N/A
"def __init__(self):
    self.conn = Connection(username='USERNAME', password='SECRET')
    self.db = self.conn['_system']
<mask>:
        raise Exception('The social graph was already provisioned! remove it first')
    self.female = self.db.createCollection(className='Collection', name='female')
    self.male = self.db.createCollection(className='Collection', name='male')
    self.relation = self.db.createCollection(className='Edges', name='relation')
    g = self.db.createGraph('social')
    a = g.createVertex('female', {'name': 'Alice', '_key': 'alice'})
    b = g.createVertex('male', {'name': 'Bob', '_key': 'bob'})
    c = g.createVertex('male', {'name': 'Charly', '_key': 'charly'})
    d = g.createVertex('female', {'name': 'Diana', '_key': 'diana'})
    a.save()
    b.save()
    c.save()
    d.save()
    g.link('relation', a, b, {'type': 'married', '_key': 'aliceAndBob'})
    g.link('relation', a, c, {'type': 'friend', '_key': 'aliceAndCharly'})
    g.link('relation', c, d, {'type': 'married', '_key': 'charlyAndDiana'})
    g.link('relation', b, d, {'type': 'friend', '_key': 'bobAndDiana'})",self.db.hasGraph('social'),95,self.db.hasTable('social'),False,50.000000000000014,N/A
"def main():
    conn = Connection(username='', password='')
    db = conn['_system']
    name = 'pyArangoValidation'
    schema = {'rule': {'properties': {'value': {'type': 'number'}}}, 'level': 'strict', 'message': 'invalid document - schema validation failed!'}
    collection = None
<mask>:
        db[name].delete()
        db.reloadCollections()
    collection = db.createCollection(name=name, schema=schema)
    try:
        d = collection.createDocument()
        d['value'] = 'bar'
        d.save()
    except Exception as e:
        print(e)
    d = collection.createDocument()
    d['value'] = 3
    d.save()
    print(collection.fetchAll())
    return 0",db.hasCollection(name),61,name in db,False,12.753667906901528,N/A
"def getEdgeCol(name):
<mask>:
        if not db.hasCollection(name):
            edgeCols[name] = db.createCollection(name=name, className='Edges')
        else:
            edgeCols[name] = db.collections[name]
    return edgeCols[name]",not name in edgeCols,16,name not in edgeCols,False,37.99178428257963,N/A
"def DependencySetToDict(dep, hasAlternatives):
    depset = []
    for oneDep in dep.relationships:
<mask>:
            depset.append(VersionedDependencyToDict(oneDep, hasAlternatives))
        elif isinstance(oneDep, deb_pkg_tools.deps.AlternativeRelationship):
            depset.append(DependencySetToDict(oneDep, True))
        elif isinstance(oneDep, deb_pkg_tools.deps.Relationship):
            depset.append(DependencyToDict(oneDep, hasAlternatives))
        else:
            print('Unknown relationshitp: ' + repr(oneDep))
    return depset","isinstance(oneDep, deb_pkg_tools.deps.VersionedRelationship)",31,"isinstance(oneDep, deb_pkg_tools.deps.VersionedRelationship)",True,100.00000000000004,N/A
"def PackageToDict(pkg):
    ret = {}
    for attribute in pkg.keys():
<mask>:
            ret[attribute] = DependencySetToDict(pkg[attribute], False)
        else:
            ret[attribute] = pkg[attribute]
    ret['_key'] = ret['Package']
    return ret","isinstance(pkg[attribute], deb_pkg_tools.deps.RelationshipSet)",23,"isinstance(pkg[attribute], DependencySet)",False,30.85906918246342,N/A
"def saveDependencyToEdgeCol(edgeCol, dep, pname, hasAlternatives):
    for oneDep in dep.relationships:
<mask>:
            d = VersionedDependencyToDict(oneDep, hasAlternatives)
            d['_from'] = 'packages/' + pname
            d['_to'] = 'packages/' + oneDep.name
            relation = edgeCol.createDocument(d).save()
        elif isinstance(oneDep, deb_pkg_tools.deps.AlternativeRelationship):
            saveDependencyToEdgeCol(edgeCol, oneDep, pname, True)
        elif isinstance(oneDep, deb_pkg_tools.deps.Relationship):
            d = DependencyToDict(oneDep, hasAlternatives)
            d['_from'] = 'packages/' + pname
            d['_to'] = 'packages/' + oneDep.name
            relation = edgeCol.createDocument(d).save()
        else:
            print('Unknown relationshitp: ' + repr(oneDep))","isinstance(oneDep, deb_pkg_tools.deps.VersionedRelationship)",60,"isinstance(oneDep, deb_pkg_tools.deps.VersionedRelationship)",True,100.00000000000004,N/A
"def assert_ok(result: Result):
<mask>:
        raise AssertionError(f'Command failed with exit code {result.exit_code}\nStderr: {result.stderr}')",result.exit_code != 0,12,result.exit_code,False,54.88116360940266,N/A
"def filter_vods(vods: Iterable[Vod], start: Optional[int]=None, end: Optional[int]=None) -> Tuple[List[Vod], Optional[float], Optional[float]]:
    filtered_vods: List[Vod] = []
    vod_start = 0
    crop_start = None
    crop_end = None
    for vod in vods:
        vod_end = vod_start + vod.duration
<mask>:
            crop_start = start - vod_start
        if end and end > vod_start and (end < vod_end):
            crop_end = vod_end - end
        start_condition = not start or vod_end > start
        end_condition = not end or vod_start < end
        if start_condition and end_condition:
            filtered_vods.append(vod)
        vod_start = vod_end
    crop_duration = None
    if crop_end:
        total_duration = sum((v.duration for v in filtered_vods))
        crop_duration = total_duration - (crop_start or 0) - crop_end
    return (filtered_vods, crop_start, crop_duration)",start and start > vod_start and (start < vod_end),103,start and start > vod_start and (start < vod_end),True,100.00000000000004,N/A
"def make_join_playlist(playlist: m3u8.M3U8, vods: List[Vod], targets: List[Path]) -> m3u8.Playlist:
    """"""
    Make a modified playlist which references downloaded VODs
    Keep only the downloaded segments and skip the rest
    """"""
    org_segments = playlist.segments.copy()
    path_map = OrderedDict(zip([v.path for v in vods], targets))
    playlist.segments.clear()
    for segment in org_segments:
<mask>:
            segment.uri = str(path_map[segment.uri].name)
            playlist.segments.append(segment)
    return playlist",segment.uri in path_map,51,segment.uri not in path_map,False,50.000000000000014,N/A
"def select_playlist_by_name(playlists: List[Playlist], quality: str) -> Playlist:
<mask>:
        for playlist in playlists:
            if playlist.is_source:
                return playlist
        raise click.ClickException('Source quality not found, please report an issue on github.')
    for playlist in playlists:
        if playlist.name == quality or playlist.group_id == quality:
            return playlist
    available = ', '.join([p.name for p in playlists])
    msg = f""Quality '{quality}' not found. Available qualities are: {available}""
    raise click.ClickException(msg)",quality == 'source',61,not quality,False,18.393972058572114,N/A
"def select_playlist_interactive(playlists: List[Playlist]) -> Playlist:
    playlists = sorted(playlists, key=_playlist_key)
    rows = [[f'{n + 1})', bold(playlist.name), dim(playlist.group_id), dim(playlist.resolution or '')] for n, playlist in enumerate(playlists)]
    click.echo()
    print_table(rows, headers=['#', 'Name', 'Group ID', 'Resolution'])
    default = 1
    for index, playlist in enumerate(playlists):
<mask>:
            default = index + 1
    no = utils.read_int('\nChoose quality', min=1, max=len(playlists) + 1, default=default)
    playlist = playlists[no - 1]
    return playlist",playlist.is_source,61,index == len(playlists) - 1,False,0.0,N/A
"def _playlist_key(playlist: Playlist) -> int:
    """"""Attempt to sort playlists so that source quality is on top, audio only
    is on bottom and others are sorted descending by resolution.""""""
<mask>:
        return 0
    if playlist.group_id == 'audio_only':
        return MAX
    try:
        return MAX - int(playlist.name.split('p')[0])
    except Exception:
        pass
    return MAX",playlist.is_source,47,playlist.group_id == 'source_quality',False,9.287528999566801,N/A
"def clip_placeholders(clip: Clip) -> Dict[str, str]:
    date, time = clip['createdAt'].split('T')
    game = clip['game']['name'] if clip['game'] else 'Unknown'
<mask>:
        url = clip['videoQualities'][0]['sourceURL']
        _, ext = os.path.splitext(url)
        ext = ext.lstrip('.')
    else:
        ext = 'mp4'
    return {'channel': clip['broadcaster']['displayName'], 'channel_login': clip['broadcaster']['login'], 'date': date, 'datetime': clip['createdAt'], 'format': ext, 'game': game, 'game_slug': utils.slugify(game), 'id': clip['id'], 'slug': clip['slug'], 'time': time, 'title': utils.titlify(clip['title']), 'title_slug': utils.slugify(clip['title'])}",clip['videoQualities'],57,'videoQualities' in clip,False,24.840753130578644,N/A
"def download_cached(url: str, *, subdir: Optional[str]=None, filename: Optional[str]=None) -> Path:
    target_dir = get_cache_dir(subdir)
<mask>:
        filename = hashlib.sha256(url.encode()).hexdigest()
    target = target_dir / filename
    if not target.exists():
        print_status(f'Downloading {url}', dim=True)
        download_file(url, target)
    return target",not filename,32,filename is None,False,27.516060407455225,N/A
"def get_cache_dir(subdir: Optional[str]=None) -> Path:
    path = _cache_dir_path()
<mask>:
        path = path / subdir
    path.mkdir(parents=True, exist_ok=True)
    return path",subdir,18,subdir,True,100.00000000000004,N/A
"def get_cache_subdirs() -> List[Path]:
    subdirs: List[Path] = []
    for item in _cache_dir_path().iterdir():
<mask>:
            subdirs.append(item)
    return subdirs",item.is_dir(),16,item.is_dir(),True,100.00000000000004,N/A
"def _cache_dir_path() -> Path:
    """"""Returns the path to the cache directory""""""
<mask>:
        return Path(os.environ['LOCALAPPDATA'], CACHE_SUBFOLDER)
    if sys.platform == 'darwin':
        return Path.home() / 'Library' / 'Caches' / CACHE_SUBFOLDER
    if 'XDG_CACHE_HOME' in os.environ:
        return Path(os.environ['XDG_CACHE_HOME'], CACHE_SUBFOLDER)
    return Path.home() / '.cache' / CACHE_SUBFOLDER",sys.platform == 'win32' and 'LOCALAPPDATA' in os.environ,40,'LOCALAPPDATA' in os.environ,False,24.659696394160658,N/A
"def mkdir(self, path: Path):
    """"""Create a new directory recursively, save created dirs to self.dirs.
        Original: https://github.com/python/cpython/blob/3.13/Lib/pathlib/_local.py#L717""""""
    try:
        os.mkdir(path)
        self.dirs.append(path)
    except FileNotFoundError:
<mask>:
            raise
        self.mkdir(path.parent)
        self.mkdir(path)
    except OSError:
        if not path.is_dir():
            raise",path.parent == path,31,not path.is_dir(),False,11.044795567078939,N/A
"def render_chat(id: str, width: int, height: int, font_size: int, dark: bool, padding: Tuple[int, int], output: str, format: str, image_format: str, overwrite: bool, keep: bool, no_join: bool, json: bool):
    video_id = parse_video_identifier(id)
<mask>:
        raise ConsoleError('Invalid video ID')
    print_log('Looking up video...')
    video = get_video(video_id)
    if not video:
        raise ConsoleError(f'Video {video_id} not found')
    total_duration = video['lengthSeconds']
    if json:
        render_chat_json(video)
        return
    target_path = Path(video_filename(video, format, output))
    if not overwrite and target_path.exists():
        response = click.prompt('File exists. Overwrite? [Y/n]', default='Y', show_default=False)
        if response.lower().strip() != 'y':
            raise click.Abort()
        overwrite = True
    print_log('Loading video comments...')
    video_comments = get_video_comments(video_id)
    badges_by_id = {badge['id']: badge for badge in video_comments['badges']}
    fonts = load_fonts(font_size)
    foreground = '#ffffff' if dark else '#000000'
    background = '#000000' if dark else '#ffffff'
    screen = Screen(width, height, fonts, foreground, background, padding)
    frames: List[Tuple[Path, int]] = []
    cache_dir = cache.get_cache_dir(f'chats/{video_id}')
    print_log(f'Rendering frames to: {cache_dir}')
    first = True
    frame_durations: Deque[float] = deque(maxlen=100)
    for group_index, offset, duration, comments in group_comments(video_id, total_duration):
        if group_index == 0:
            frame_path = cache_dir / f'chat_{0:05d}.{image_format}'
            screen.padded_image().save(frame_path)
            frames.append((frame_path, offset))
        frame_start = time.monotonic()
        for comment in comments:
            if comment['commenter']:
                if not first:
                    screen.next_line()
                draw_comment(screen, comment, dark, badges_by_id)
            first = False
        frame_path = cache_dir / f'chat_{offset:05d}.{image_format}'
        screen.padded_image().save(frame_path)
        frames.append((frame_path, duration))
        frame_durations.append(time.monotonic() - frame_start)
        _print_progress(group_index, offset, frame_durations, total_duration)
    spec_path = cache_dir / 'concat.txt'
    with open(spec_path, 'w') as f:
        for path, duration in frames:
            f.write(f""file '{path.resolve()}'\n"")
            f.write(f'duration {duration}\n')
    if no_join:
        print_log('Skipping video generation...')
        click.echo(f'Frames rendered to:\n{blue(cache_dir)}')
        return
    print_status('Generating chat video...', dim=True)
    generate_video(spec_path, target_path, overwrite)
    if keep:
        click.echo(f'Cached files not deleted: {yellow(cache_dir)}')
    else:
        print_status('Deleting cache...', dim=True)
        shutil.rmtree(cache_dir)",not video_id,244,not video_id,True,100.00000000000004,N/A
"def load_fonts(font_size: int):
    fonts: List[Font] = []

    def print_font_info(font: Font):
        print_log(f'    Name: {font.name}')
        print_log(f'    Codepoints: {len(font.codepoints)}')
<mask>:
            print_log(f""    Variations: {', '.join(font.variations)}"")
    for url in TEXT_FONTS:
        filename = Path(urlparse(url).path).name
        path = cache.download_cached(url, filename=filename, subdir='fonts')
        print_log(f'Loading text font: {path}')
        font = load_font(path, False, font_size)
        print_font_info(font)
        fonts.append(font)
    for url, font_size in BITMAP_FONTS:
        filename = Path(urlparse(url).path).name
        path = cache.download_cached(url, filename=filename, subdir='fonts')
        print_log(f'Loading bitmap font: {path}')
        font = load_font(path, True, font_size)
        print_font_info(font)
        fonts.append(font)
    return fonts",font.variations,70,len(font.variations) > 0,False,20.556680845025987,N/A
"def draw_comment(screen: Screen, comment: Comment, dark: bool, badges_by_id: Dict[str, Badge]):
    assert comment['commenter'] is not None
    time = format_time(comment['contentOffsetSeconds'])
    screen.draw_text(time + ' ', 'gray')
    for message_badge in comment['message']['userBadges']:
<mask>:
            continue
        badge = badges_by_id.get(message_badge['id'])
        if not badge:
            print_status(f'Badge not found: {message_badge}')
            continue
        badge_path = download_badge(badge)
        if not badge_path:
            print_status(f'Failed downloading badge {message_badge}')
            continue
        badge_image = Image.open(badge_path)
        badge_image.thumbnail((screen.max_ascent, screen.max_ascent))
        screen.draw_image(badge_image)
    if comment['message']['userBadges']:
        screen.draw_text(' ')
    user_name = comment['commenter']['displayName'] if comment['commenter'] else 'UNKWNOW'
    user_color_index = int(comment['commenter']['id']) % len(USER_COLORS)
    user_color = USER_COLORS[user_color_index]
    screen.draw_text(user_name, user_color)
    screen.draw_text(': ')
    for fragment in comment['message']['fragments']:
        if fragment['emote']:
            emote_path = download_emote(fragment['emote'], dark)
            if emote_path:
                emote_image = Image.open(emote_path)
                emote_image.thumbnail((screen.line_height, screen.line_height))
                screen.draw_image(emote_image)
            else:
                print_status(f""Failed downloading emote {fragment['emote']}"")
                screen.draw_text(fragment['text'])
        else:
            screen.draw_text(fragment['text'])",message_badge['id'] == 'Ozs=',106,message_badge['id'] not in badges_by_id,False,38.05803001674947,N/A
"def draw_text(self, text: str, color: Optional[str]=None):
    for word in re.split('(?=\\s)', text):
        for fragment, font in self.group_by_font(word):
<mask>:
                for emoji in fragment:
                    self.draw_emoji(emoji, font)
            else:
                self.draw_text_fragment(fragment, font, color)",font.is_bitmap,27,"isinstance(fragment, list)",False,0.0,N/A
"def draw_text_fragment(self, fragment: str, font: Font, color: Optional[str]):
    length = font.get_text_length(fragment)
<mask>:
        self.next_line()
    y = self.y + self.max_ascent - font.ascent
    self.draw.text((self.x, y), fragment, fill=color or self.foreground, font=font.image_font)
    self.x += length",self.image.width < self.x + length,30,length < 0,False,2.4088567143060917,N/A
"def validate_positive(_ctx: click.Context, _param: click.Parameter, value: Optional[int]):
<mask>:
        raise click.BadParameter('must be greater than 0')
    return value",value is not None and value <= 0,16,value is None,False,8.525588607164655,N/A
"def validate_non_negative(_ctx: click.Context, _param: click.Parameter, value: Optional[int]):
<mask>:
        raise click.BadParameter('must be greater or equal than 0')
    return value",value is not None and value < 0,18,value is None,False,11.898417391331403,N/A
"def validate_time(_ctx: click.Context, _param: click.Parameter, value: str) -> Optional[int]:
    """"""Parse a time string (hh:mm or hh:mm:ss) to number of seconds.""""""
<mask>:
        return None
    parts = [int(p) for p in value.split(':')]
    if not 2 <= len(parts) <= 3:
        raise click.BadParameter('invalid time')
    hours = parts[0]
    minutes = parts[1]
    seconds = parts[2] if len(parts) > 2 else 0
    if hours < 0 or not 0 <= minutes <= 59 or (not 0 <= seconds <= 59):
        raise click.BadParameter('invalid time')
    return hours * 3600 + minutes * 60 + seconds",not value,86,not value,True,100.00000000000004,N/A
"def validate_rate(_ctx: click.Context, _param: click.Parameter, value: str) -> Optional[int]:
<mask>:
        return None
    match = re.search('^([0-9]+)(k|m|)$', value, flags=re.IGNORECASE)
    if not match:
        raise click.BadParameter(""must be an integer, followed by an optional 'k' or 'm'"")
    amount = int(match.group(1))
    unit = match.group(2)
    if unit == 'k':
        return amount * 1024
    if unit == 'm':
        return amount * 1024 * 1024
    return amount",not value,58,not value,True,100.00000000000004,N/A
"@click.group(context_settings=CONTEXT)
@click.option('--debug/--no-debug', default=False, help='Enable debug logging to stderr')
@click.option('--verbose/--no-verbose', default=False, help='More verbose debug logging')
@click.option('--color/--no-color', default=sys.stdout.isatty(), help='Use ANSI color in output')
@click.version_option(package_name='twitch-dl')
@click.pass_context
def cli(ctx: click.Context, color: bool, debug: bool, verbose: bool):
    """"""twitch-dl - twitch.tv downloader

    https://twitch-dl.bezdomni.net/
    """"""
    ctx.color = color
<mask>:
        logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO)
        logging.getLogger('httpx').setLevel(logging.WARN)
        logging.getLogger('httpcore').setLevel(logging.WARN)
        logging.getLogger('PIL').setLevel(logging.WARN)",debug,50,debug,True,100.00000000000004,N/A
"def _format_size(value: float, digits: int, unit: str):
<mask>:
        return f'{{:.{digits}f}}{unit}'.format(value)
    else:
        return f'{int(value)}{unit}'",digits > 0,13,value < 0,False,27.516060407455225,N/A
"def format_size(bytes_: Union[int, float], digits: int=1):
<mask>:
        return _format_size(bytes_, digits, 'B')
    kilo = bytes_ / 1024
    if kilo < 1024:
        return _format_size(kilo, digits, 'kB')
    mega = kilo / 1024
    if mega < 1024:
        return _format_size(mega, digits, 'MB')
    return _format_size(mega / 1024, digits, 'GB')",bytes_ < 1024,43,bytes_ < 1024,True,100.00000000000004,N/A
"def format_duration(total_seconds: Union[int, float]) -> str:
    total_seconds = int(total_seconds)
    hours = total_seconds // 3600
    remainder = total_seconds % 3600
    minutes = remainder // 60
    seconds = total_seconds % 60
<mask>:
        return f'{hours} h {minutes} min'
    if minutes:
        return f'{minutes} min {seconds} sec'
    return f'{seconds} sec'",hours,45,hours,True,100.00000000000004,N/A
"def format_time(total_seconds: Union[int, float], force_hours: bool=False) -> str:
    total_seconds = int(total_seconds)
    hours = total_seconds // 3600
    remainder = total_seconds % 3600
    minutes = remainder // 60
    seconds = total_seconds % 60
<mask>:
        return f'{hours:02}:{minutes:02}:{seconds:02}'
    return f'{minutes:02}:{seconds:02}'",hours or force_hours,36,force_hours,False,51.341711903259224,N/A
"def read_int(msg: str, min: int, max: int, default: Optional[int]=None) -> int:
    while True:
        try:
            val = click.prompt(msg, default=default, type=int)
<mask>:
                return default
            if min <= int(val) <= max:
                return int(val)
        except ValueError:
            pass",default and (not val),33,val is None,False,10.122592925934278,N/A
"def advance(self, size: int):
    """"""Called every time a chunk of data is downloaded.""""""
    self._refill()
<mask>:
        deficit = size - self.available
        time.sleep(deficit / self.rate)
    self.available -= size",self.available < size,26,self.available > size,False,42.72870063962342,N/A
"def start(self, task_id: int, size: int):
<mask>:
        raise ValueError(f'Task {task_id}: cannot start, already started')
    self.tasks[task_id] = Task(task_id, size)
    self.print()",task_id in self.tasks,19,task_id in self.tasks,True,100.00000000000004,N/A
"def advance(self, task_id: int, size: int):
<mask>:
        raise ValueError(f'Task {task_id}: cannot advance, not started')
    self.downloaded += size
    self.progress_bytes += size
    self.tasks[task_id].advance(size)
    self.samples.append(Sample(self.downloaded, time.time()))
    self.print()",task_id not in self.tasks,24,not task_id in self.tasks,False,51.69731539571708,N/A
"def already_downloaded(self, task_id: int, size: int):
<mask>:
        raise ValueError(f'Task {task_id}: cannot mark as downloaded, already started')
    self.tasks[task_id] = Task(task_id, size)
    self.progress_bytes += size
    self.downloaded_count += 1
    self.print()",task_id in self.tasks,27,task_id in self.tasks,True,100.00000000000004,N/A
"def abort(self, task_id: int):
<mask>:
        raise ValueError(f'Task {task_id}: cannot abort, not started')
    del self.tasks[task_id]
    self.progress_bytes = sum((t.downloaded for t in self.tasks.values()))
    self.print()",task_id not in self.tasks,22,task_id not in self.tasks,True,100.00000000000004,N/A
"def end(self, task_id: int):
<mask>:
        raise ValueError(f'Task {task_id}: cannot end, not started')
    task = self.tasks[task_id]
    if task.size != task.downloaded:
        logger.warning(f'Taks {task_id} ended with {task.downloaded}b downloaded, expected {task.size}b.')
    self.downloaded_count += 1
    self.print()",task_id not in self.tasks,31,task_id not in self.tasks,True,100.00000000000004,N/A
"def get_codepoints(path: Path) -> Set[int]:

    def gen() -> Generator[Iterable[int], None, None]:
        for font in tt_fonts(path):
            for subtable in font['cmap'].tables:
<mask>:
                    yield subtable.cmap.keys()
    empty_set: Set[int] = set()
    return empty_set.union(*gen())",subtable.isUnicode(),28,subtable.cmap is not None,False,16.233395773754953,N/A
"def get_codepoints_cached(path: Path) -> Set[int]:
    hash = hashlib.md5(str(path).encode()).hexdigest()
    filename = f'{path.name}.{hash}.json'
    codepoints_path = get_cache_dir('fonts') / filename
<mask>:
        try:
            with open(codepoints_path, 'r') as f:
                return set(json.load(f))
        except Exception:
            pass
    print_log('Extracting supported codepoints...')
    codepoints = get_codepoints(path)
    print_log(f'Saving codepoints cache to: {codepoints_path}')
    with open(codepoints_path, 'w') as f:
        json.dump(list(codepoints), f)
    return get_codepoints(path)",codepoints_path.exists(),48,os.path.exists(codepoints_path),False,37.99178428257963,N/A
"def make_group_by_font(fonts: List[Font], on_char_not_found: Callable[[str], None]) -> Callable[[str], Generator[Tuple[str, Font], None, None]]:

    @lru_cache
    def get_font(char: str) -> Optional[Font]:
        for font in fonts:
<mask>:
                return font

    def group_by_font(text: str):
        """"""Split given text into chunks which can be rendered by the same font.""""""
        if not text:
            return
        buffer = ''
        font = None
        for char in text:
            char_font = get_font(char)
            if not char_font:
                on_char_not_found(char)
                continue
            if not font:
                font = char_font
            if font == char_font:
                buffer += char
            else:
                yield (buffer, font)
                font = char_font
                buffer = char
        if buffer and font:
            yield (buffer, font)
    return group_by_font",ord(char) in font.codepoints,95,char == font.name,False,12.872632311973014,N/A
"def authenticated_post(url: str, *, json: Any=None, content: Optional[Content]=None, auth_token: Optional[str]=None):
    headers = {'Client-ID': CLIENT_ID}
<mask>:
        headers['authorization'] = f'OAuth {auth_token}'
    response = request('POST', url, content=content, json=json, headers=headers)
    if response.status_code == 400:
        data = response.json()
        raise ConsoleError(data['message'])
    response.raise_for_status()
    return response",auth_token is not None,38,auth_token,False,36.78794411714425,N/A
"def log_response(response: httpx.Response, duration_seconds: float):
    request = response.request
    duration = f'{int(1000 * duration_seconds)}ms'
    size = format_size(len(response.content))
    logger.info(f'<-- {request.method} {request.url} HTTP {response.status_code} {duration} {size}')
<mask>:
        logger.debug(f'<-- {response.content}')",response.content,26,response.content,True,100.00000000000004,N/A
"def gql_raise_on_error(response: httpx.Response):
    data = response.json()
<mask>:
        errors = [e['message'] for e in data['errors']]
        raise GQLError(errors)",'errors' in data,16,'errors' in data,True,100.00000000000004,N/A
"def get_channel_clips(channel_id: str, period: ClipsPeriod, limit: int, after: Optional[str]=None):
    """"""
    List channel clips.

    At the time of writing this:
    * filtering by game name returns an error
    * sorting by anything but VIEWS_DESC or TRENDING returns an error
    * sorting by VIEWS_DESC and TRENDING returns the same results
    * there is no totalCount
    """"""
    query = f'''\n    {{\n      user(login: ""{channel_id}"") {{\n        clips(\n          first: {limit},\n          after: ""{after or ''}"",\n          criteria: {{\n            period: {period.upper()},\n            sort: VIEWS_DESC\n          }}\n        )\n        {{\n          pageInfo {{\n            hasNextPage\n            hasPreviousPage\n          }}\n          edges {{\n            cursor\n            node {{\n              {CLIP_FIELDS}\n            }}\n          }}\n        }}\n      }}\n    }}\n    '''
    response = gql_query(query)
    user = response['data']['user']
<mask>:
        raise ConsoleError(f'Channel {channel_id} not found')
    return response['data']['user']['clips']",not user,108,not user,True,100.00000000000004,N/A
"def truncate(string: str, length: int) -> str:
<mask>:
        return string[:length - 1] + '…'
    return string",len(string) > length,16,length > 0,False,12.753667906901528,N/A
"def print_status(message: str, transient: bool=False, dim: bool=False):
    global _prev_transient
<mask>:
        cursor_previous_line()
        clear_line()
    click.secho(message, err=True, dim=dim)
    _prev_transient = transient",_prev_transient,18,transient,False,4.9787068367863965,N/A
"def print_table(data: List[List[str]], *, alignments: Mapping[int, Align]={}, headers: Optional[List[str]]=None, footers: Optional[List[str]]=None):
    all_rows = data + ([headers] if headers else []) + ([footers] if footers else [])
    widths = [[visual_len(cell) for cell in row] for row in all_rows]
    widths = [max(width) for width in zip(*widths)]
    underlines = ['-' * width for width in widths]

    def format_cell(cell: str, idx: int):
        width = widths[idx]
        align = alignments.get(idx, 'left')
<mask>:
            return rjust(cell, width)
        elif align == 'center':
            return center(cell, width)
        else:
            return ljust(cell, width)

    def print_row(row: List[str]):
        parts = (format_cell(cell, idx) for idx, cell in enumerate(row))
        click.echo('  '.join(parts).strip())
    if headers:
        print_row([bold(h) for h in headers])
        print_row(underlines)
    for row in data:
        print_row(row)
    if footers:
        print_row(underlines)
        print_row([bold(f) for f in footers])",align == 'right',115,align == 'right',True,100.00000000000004,N/A
"def print_paged(label: str, generator: Generator[T, Any, Any], print_fn: Callable[[T], None], page_size: int, total_count: Optional[int]=None):
    iterator = iter(generator)
    page = list(islice(iterator, page_size))
    first = 1
    last = first + len(page) - 1
    while True:
        click.echo('-' * 80)
        click.echo()
        for item in page:
            print_fn(item)
        last = first + len(page) - 1
        click.echo('-' * 80)
        click.echo(f""{label} {first}-{last} of {total_count or '???'}"")
        first = first + len(page)
        last = first + 1
        page = list(islice(iterator, page_size))
<mask>:
            break",not page or not prompt_continue(),74,not page,False,3.0197383422318516,N/A
"def print_video(video: Video):
    published_at = video['publishedAt'].replace('T', ' @ ').replace('Z', '')
    length = utils.format_duration(video['lengthSeconds'])
    channel = blue(video['owner']['displayName']) if video['owner'] else ''
    playing = f""playing {blue(video['game']['name'])}"" if video['game'] else ''
    url = f""https://www.twitch.tv/videos/{video['id']}""
    click.secho(f""Video {video['id']}"", bold=True)
    click.secho(video['title'], fg='green')
<mask>:
        click.echo(' '.join([channel, playing]))
    click.echo(f'Published {blue(published_at)}  Length: {blue(length)} ')
    click.secho(url, italic=True)
    if video['description']:
        click.echo(f""\nDescription:\n{video['description']}"")
    click.echo()",channel or playing,51,channel,False,13.533528323661276,N/A
"def clips(channel_name: str, *, all: bool=False, compact: bool=False, download: bool=False, json: bool=False, limit: Optional[int]=None, pager: Optional[int]=None, period: ClipsPeriod='all_time', target_dir: Path=Path(), workers: int=10):
    default_limit = 40 if compact else 10
    limit = sys.maxsize if all or pager else limit or default_limit
<mask>:
        asyncio.run(_download_clips(channel_name, period, target_dir, workers))
        return
    generator = twitch.channel_clips_generator(channel_name, period, limit)
    if json:
        return print_json(list(generator))
    print_fn = print_clip_compact if compact else print_clip
    if pager:
        return print_paged('Clips', generator, print_fn, pager)
    return _print_all(generator, print_fn, all)",download,73,download,True,100.00000000000004,N/A
"def _target_filename(clip: Clip, video_qualities: List[VideoQuality]):
    url = video_qualities[0]['sourceURL']
    _, ext = path.splitext(url)
    ext = ext.lstrip('.')
    match = re.search('^(\\d{4})-(\\d{2})-(\\d{2})T', clip['createdAt'])
<mask>:
        raise ValueError(f""Failed parsing date from: {clip['createdAt']}"")
    date = ''.join(match.groups())
    name = '_'.join([date, clip['id'], clip['broadcaster']['login'], utils.slugify(clip['title'])])
    return f'{name}.{ext}'",not match,37,not match,True,100.00000000000004,N/A
"def _get_clip_url(access_token: ClipAccessToken, quality: str) -> str:
    qualities = access_token['videoQualities']
<mask>:
        return qualities[0]['sourceURL']
    selected_quality = quality.rstrip('p')
    for q in qualities:
        if q['quality'] == selected_quality:
            return q['sourceURL']
    available = ', '.join([str(q['quality']) for q in qualities])
    msg = f""Quality '{quality}' not found. Available qualities are: {available}""
    raise ConsoleError(msg)",quality == 'source',46,len(qualities) == 1,False,13.134549472120788,N/A
"def _print_all(generator: Generator[Clip, None, None], print_fn: Callable[[Clip], None], all: bool):
    for clip in generator:
        print_fn(clip)
<mask>:
        click.secho('\nThere may be more clips. ' + 'Increase the --limit, use --all or --pager to see the rest.', dim=True)",not all,35,all,False,36.78794411714425,N/A
"def download(ids: List[str], args: DownloadOptions):
<mask>:
        print_log('No IDs to downlad given')
        return
    for video_id in ids:
        download_one(video_id, args)",not ids,18,not ids,True,100.00000000000004,N/A
"def download_one(id_or_slug: str, args: DownloadOptions):
    video_id = utils.parse_video_identifier(id_or_slug)
<mask>:
        print_log('Looking up video...')
        video = twitch.get_video(video_id)
        if video:
            _download_video(video, args)
        else:
            print_error(f""Video '{video_id}' not found"")
        return
    slug = utils.parse_clip_identifier(id_or_slug)
    if slug:
        print_log('Looking up clip...')
        clip = twitch.get_clip(slug)
        if clip:
            _download_clip(clip, args)
        else:
            print_error(f""Clip '{slug}' not found"")
        return
    print_error(f'Not a valid video ID or clip slug: {id_or_slug}')",video_id,55,video_id,True,100.00000000000004,N/A
"def _join_vods(playlist_path: Path, metadata_path: Path, target: Path, overwrite: bool, crop_start: Optional[float], crop_duration: Optional[float]):
    command: List[str] = []

    def append(*vars: Any):
        for var in vars:
            command.append(str(var))
    append('ffmpeg')
    append('-i', playlist_path)
<mask>:
        append('-ss', utils.format_time(crop_start))
    append('-i', metadata_path)
    append('-map_metadata', 1)
    if crop_duration:
        append('-t', utils.format_time(crop_duration))
    append('-c', 'copy')
    append('-stats')
    append('-loglevel', 'warning')
    append(f'file:{target}')
    if overwrite:
        append('-y')
    click.secho(f'{shlex.join(command)}', dim=True)
    result = subprocess.run(command)
    if result.returncode != 0:
        raise ConsoleError('Joining files failed')",crop_start,61,crop_start,True,100.00000000000004,N/A
"def _concat_vods(vod_paths: List[Path], target: Path):
    tool = 'type' if platform.system() == 'Windows' else 'cat'
    command = [tool] + [str(p) for p in vod_paths]
    with open(target, 'wb') as target_file:
        result = subprocess.run(command, stdout=target_file)
<mask>:
            raise ConsoleError(f'Joining files failed: {result.stderr}')",result.returncode != 0,38,result.returncode != 0,True,100.00000000000004,N/A
"def _get_clip_url(access_token: ClipAccessToken, quality: Optional[str]) -> str:
    qualities = access_token['videoQualities']
<mask>:
        if quality == 'source':
            return qualities[0]['sourceURL']
        selected_quality = quality.rstrip('p')
        for q in qualities:
            if q['quality'] == selected_quality:
                return q['sourceURL']
        available = ', '.join([str(q['quality']) for q in qualities])
        msg = f""Quality '{quality}' not found. Available qualities are: {available}""
        raise ConsoleError(msg)
    click.echo('\nAvailable qualities:')
    for n, q in enumerate(qualities):
        click.echo(f""{n + 1}) {bold(q['quality'])} [{q['frameRate']} fps]"")
    click.echo()
    no = utils.read_int('Choose quality', min=1, max=len(qualities), default=1)
    selected_quality = qualities[no - 1]
    return selected_quality['sourceURL']",quality,78,len(qualities) == 1,False,0.0,N/A
"def info(id: str, *, json: bool=False, auth_token: Optional[str]):
    video_id = utils.parse_video_identifier(id)
<mask>:
        print_log('Fetching video...')
        video = twitch.get_video(video_id)
        if not video:
            raise ConsoleError(f'Video {video_id} not found')
        print_log('Fetching access token...')
        access_token = twitch.get_access_token(video_id, auth_token)
        print_log('Fetching playlists...')
        playlists = twitch.get_playlists(video_id, access_token)
        print_log('Fetching chapters...')
        chapters = twitch.get_video_chapters(video_id)
        if json:
            video_json(video, playlists, chapters)
        else:
            video_info(video, playlists, chapters)
        return
    clip_slug = utils.parse_clip_identifier(id)
    if clip_slug:
        print_log('Fetching clip...')
        clip = twitch.get_clip(clip_slug)
        if not clip:
            raise ConsoleError(f'Clip {clip_slug} not found')
        if json:
            print_json(clip)
        else:
            clip_info(clip)
        return
    raise ConsoleError(f'Invalid input: {id}')",video_id,81,video_id,True,100.00000000000004,N/A
"def video_info(video: Video, playlists: str, chapters: List[Chapter]):
    click.echo()
    print_video(video)
    click.echo('Playlists:\n')
    playlist_data = [[f""{p.name} {dim('source')}"" if p.is_source else p.name, p.group_id, f'{p.resolution}', p.url] for p in parse_playlists(playlists)]
    print_table(playlist_data, headers=['Name', 'Group', 'Resolution', 'URL'])
<mask>:
        click.echo()
        click.echo('Chapters:')
        for chapter in chapters:
            start = utils.format_time(chapter['positionMilliseconds'] // 1000, force_hours=True)
            duration = utils.format_time(chapter['durationMilliseconds'] // 1000)
            click.echo(f""{start} {bold(chapter['description'])} ({duration})"")
    placeholders = video_placeholders(video, format='mkv')
    placeholders = [[f'{{{k}}}', v] for k, v in placeholders.items()]
    click.echo('')
    print_table(placeholders, headers=['Placeholder', 'Value'])",chapters,68,chapters,True,100.00000000000004,N/A
"def clip_info(clip: Clip):
    click.echo()
    print_clip(clip)
    click.echo()
    click.echo('Download links:')
<mask>:
        for q in clip['videoQualities']:
            click.echo(f""{bold(q['quality'])} [{q['frameRate']} fps] {q['sourceURL']}"")
    else:
        click.echo('No download URLs found')",clip['videoQualities'],22,'videoQualities' in clip,False,24.840753130578644,N/A
"def videos(channel_name: str, *, all: bool, compact: bool, games: List[str], json: bool, limit: Optional[int], pager: Optional[int], sort: twitch.VideosSort, type: twitch.VideosType):
    game_ids = get_game_ids(games)
    limit = limit or (40 if compact else 10)
    max_videos = sys.maxsize if all or pager else limit
    total_count, generator = twitch.channel_videos_generator(channel_name, max_videos, sort, type, game_ids=game_ids)
<mask>:
        videos = list(generator)
        print_json({'count': len(videos), 'totalCount': total_count, 'videos': videos})
        return
    if total_count == 0:
        click.echo('No videos found')
        return
    if pager:
        print_fn = print_video_compact if compact else print_video
        print_paged('Videos', generator, print_fn, pager, total_count)
        return
    count = 0
    for video in generator:
        if compact:
            print_video_compact(video)
        else:
            click.echo()
            print_video(video)
        count += 1
    click.echo()
    click.echo('-' * 80)
    click.echo(f'Videos 1-{count} of {total_count}')
    if total_count > count:
        click.secho('\nThere are more videos. ' + 'Increase the --limit, use --all or --pager to see the rest.', dim=True)",json,129,json,True,100.00000000000004,N/A
"def get_game_id(name: str) -> str:
    print_log(f""Looking up game '{name}'..."")
    game_id = twitch.get_game_id(name)
<mask>:
        raise ConsoleError(f""Game '{name}' not found"")
    return game_id",not game_id,20,game_id is None,False,39.76353643835252,N/A
"def detect_chromedriver(self):
    logger.info('detecting chromedriver')
    this_os = self.get_os().lower()
<mask>:
        if os.path.isfile(self.home_dir + self.binary_win):
            os.chmod(self.home_dir + self.binary_win, 755)
            return self.home_dir + self.binary_win
    elif 'linux' in this_os:
        if os.path.isfile(self.home_dir + self.binary_linux):
            os.chmod(self.home_dir + self.binary_linux, 755)
            return self.home_dir + self.binary_linux
    elif 'darwin' in this_os:
        if os.path.isfile(self.home_dir + self.binary_mac64):
            os.chmod(self.home_dir + self.binary_mac64, 755)
            return self.home_dir + self.binary_mac64
    else:
        raise Exception('\n            Platform not supported.\n            install chromedriver by your own and update the path in your config\n            ')",'windows' in this_os,71,'win' in this_os,False,66.87403049764218,N/A
"def download(self):
    logger.info('downloading chromedriver')
    this_os = self.get_os().lower()
    base_url = 'http://chromedriver.storage.googleapis.com/76.0.3809.68/'
<mask>:
        file_name = 'chromedriver_win32.zip'
        archive = 'zip'
    elif 'linux' in this_os:
        os.chmod('install_chrome.sh', 755 | stat.S_IEXEC)
        subprocess.call('install_chrome.sh')
        archive = 'zip'
        file_name = 'chromedriver_linux64.zip'
    elif 'darwin' in this_os:
        file_name = 'chromedriver_mac64.zip'
        archive = 'zip'
    else:
        raise Exception('\n            Platform not supported.\n            install chromedriver by your own and update the path in your config\n            ')
    tmp_dir = tempfile.gettempdir() + '/'
    try:
        urllib.request.urlretrieve(base_url + file_name, tmp_dir + file_name)
        self.unpack(tmp_dir + file_name, archive)
    except:
        raise Exception('Download and unpack of chromedriver failed. Check if %(tmp_dir)s exists and has write permissions' % {'tmp_dir': tmp_dir})",'windows' in this_os,96,'win32' in this_os,False,66.87403049764218,N/A
"def unpack(self, file_path, archive):
    logger.info('unpacking chromedriver')
<mask>:
        os.mkdir(self.home_dir)
    if 'zip' in archive:
        with zipfile.ZipFile(file_path, 'r') as zip_ref:
            zip_ref.extractall(self.home_dir)",os.path.isdir(self.home_dir) is False,18,not os.path.exists(self.home_dir),False,62.07106843779346,N/A
"def detect_phantomjs(self):
    logger.info('detecting phantomjs')
    this_os = self.get_os().lower()
<mask>:
        if os.path.isfile(self.home_dir + self.binary_win):
            return self.home_dir + self.binary_win
    elif 'linux' in this_os:
        if sys.maxsize > 2 ** 32:
            if os.path.isfile(self.home_dir + self.binary_linux64):
                return self.home_dir + self.binary_linux64
        elif os.path.isfile(self.home_dir + self.binary_linux32):
            return self.home_dir + self.binary_linux32
    else:
        raise Exception('\n            Platform not supported.\n            install phantomjs manualy and update the path in your config\n            ')",'windows' in this_os,59,'win' in this_os,False,66.87403049764218,N/A
"def download(self):
    logger.info('downloading phantomjs')
    this_os = self.get_os().lower()
    base_url = 'https://bitbucket.org/ariya/phantomjs/downloads/'
<mask>:
        file_name = 'phantomjs-2.1.1-windows.zip'
        archive = 'zip'
    elif 'linux' in this_os:
        archive = 'tar.bz2'
        if sys.maxsize > 2 ** 32:
            file_name = 'phantomjs-2.1.1-linux-x86_64.tar.bz2'
        else:
            file_name = 'phantomjs-2.1.1-linux-i686.tar.bz2'
    else:
        raise Exception('\n            Platform not supported.\n            install phantomjs manualy and update the path in your config\n            ')
    tmp_dir = tempfile.gettempdir() + '/'
    try:
        urllib.request.urlretrieve(base_url + file_name, tmp_dir + file_name)
        self.unpack(tmp_dir + file_name, archive)
    except:
        raise Exception('Download and unpack of phantomjs failed. Check if %(tmp_dir)s exists and has write permissions' % {'tmp_dir': tmp_dir})",'windows' in this_os,89,'windows' in this_os,True,100.00000000000004,N/A
"def unpack(self, file_path, archive):
    logger.info('unpacking phantomjs')
<mask>:
        os.mkdir(self.home_dir)
    if 'tar.bz2' in archive:
        tar = tarfile.open(file_path, 'r:bz2')
        tar.extractall(self.home_dir)
        tar.close()
    if 'zip' in archive:
        with zipfile.ZipFile(file_path, 'r') as zip_ref:
            zip_ref.extractall(self.home_dir)",os.path.isdir(self.home_dir) is False,28,not os.path.exists(self.home_dir),False,62.07106843779346,N/A
"def cli(self, args=None):
    """"""method called if executed on command line
        Args:
            args (mixed): args via commandline
        Returns:
            list: dicts of results
        """"""
    parser = argparse.ArgumentParser(prog='serpscrap')
    parser.add_argument('-k', '--keyword', help='keyword for scraping', nargs='*')
    self.args = parser.parse_args()
<mask>:
        keywords = ' '.join(self.args.keyword)
    self.init(config=None, keywords=keywords)
    return self.run()",len(self.args.keyword) > 0,43,self.args.keyword,False,36.78794411714425,N/A
"def init(self, config=None, keywords=None):
    """"""init config and serp_query
        Args:
            config (None|dict): override default config
            keywords (str|list): string or list of strings, keywords to scrape
        Raises:
            ValueError:
        """"""
<mask>:
        self.config = config
    else:
        self.config = Config().get()
    if self.config['executable_path'] == '' and self.config['sel_browser'] == 'phantomjs':
        logger.info('preparing phantomjs')
        firstrun = PhantomInstall()
        phantomjs = firstrun.detect_phantomjs()
        if phantomjs is None:
            firstrun.download()
            phantomjs = firstrun.detect_phantomjs()
            if phantomjs is None:
                raise Exception('\n                        phantomjs binary not found,\n                        provide custom path in config')
        self.config.__setitem__('executable_path', phantomjs)
        logger.info('using ' + str(phantomjs))
    elif self.config['executable_path'] == '' and self.config['sel_browser'] == 'chrome':
        logger.info('preparing chromedriver')
        firstrun = ChromeInstall()
        chromedriver = firstrun.detect_chromedriver()
        if chromedriver is None:
            firstrun.download()
            chromedriver = firstrun.detect_chromedriver()
            if chromedriver is None:
                raise Exception('\n                        chromedriver binary not found,\n                        provide custom path in config')
        self.config.__setitem__('executable_path', chromedriver)
        logger.info('using ' + str(chromedriver))
    if os.path.exists(self.config['dir_screenshot']):
        shutil.rmtree(self.config['dir_screenshot'], ignore_errors=True)
    screendir = '{}/{}'.format(self.config['dir_screenshot'], self.config['today'])
    if not os.path.exists(screendir):
        os.makedirs(screendir)
    if isinstance(keywords, str):
        self.serp_query = [keywords]
    elif isinstance(keywords, list) and len(keywords) > 0:
        self.serp_query = keywords
    else:
        raise ValueError('no keywords given')",config is not None,158,config is not None,True,100.00000000000004,N/A
"def run(self):
    """"""main method to run scrap_serps and scrap_url
        Returns:
            list: dicts with all results
        """"""
    self.results = []
<mask>:
        self.results = self.scrap_serps()
    if self.config['scrape_urls']:
        for index, result in enumerate(self.results):
            if 'serp_type' in result and 'serp_url' in result:
                logger.info('Scraping URL: ' + result['serp_url'])
                result_url = self.scrap_url(result['serp_url'])
                if 'status' in result_url:
                    self.results[index].update(result_url)
    return self.results if isinstance(self.results, list) else [self.results]",self.serp_query is not None,58,self.config['scrape_serps'],False,12.22307556087252,N/A
"def scrap_serps(self):
    """"""call scrap method and append serp results to list
        Returns
            list: dict of scrape results
        """"""
    search = self.scrap()
    self.results = []
<mask>:
        for serp in search.serps:
            self.related = []
            for related_keyword in serp.related_keywords:
                self.related.append({'keyword': related_keyword.keyword, 'rank': related_keyword.rank})
            for link in serp.links:
                self.results.append({'query_num_results_total': serp.num_results_for_query, 'query_num_results_page': serp.num_results, 'query_page_number': serp.page_number, 'query': serp.query, 'serp_rank': link.rank, 'serp_type': link.link_type, 'serp_url': link.link, 'serp_rating': link.rating, 'serp_title': link.title, 'serp_domain': link.domain, 'serp_visible_link': link.visible_link, 'serp_snippet': link.snippet, 'serp_sitelinks': link.sitelinks, 'screenshot': os.path.join('{}/{}/{}_{}-p{}.png'.format(self.config['dir_screenshot'], self.config['today'], 'google', serp.query, str(serp.page_number)))})
        return self.results
    else:
        raise Exception('No Results')",search is not None,82,search.serps,False,19.716118825581447,N/A
"@staticmethod
def adjust_encoding(data):
    """"""detect and adjust encoding of data return data decoded to utf-8""""""
    check_encoding = chardet.detect(data)
<mask>:
        try:
            data = data.decode(check_encoding['encoding']).encode('utf-8')
        except:
            pass
    try:
        data = data.decode('utf-8')
    except:
        data = data.decode('utf-8', 'ignore')
    return {'encoding': check_encoding['encoding'], 'data': data}",'utf-8' not in check_encoding['encoding'],38,check_encoding,False,13.533528323661276,N/A
"@staticmethod
def fetch_url(url, cache_file):
    result = {}
    try:
        with urllib.request.urlopen(url) as response:
            html = response.read()
            encoded = UrlScrape.adjust_encoding(data=html)
            html = encoded['data']
            for sign in ['[', ']', '(', ')']:
                html = html.replace(sign, ' ')
            for sign in ['»']:
                html = html.replace(sign, '')
            meta_robots = UrlScrape.meta_robots_pattern.findall(html)
            meta_title = UrlScrape.meta_title_pattern.findall(html)
<mask>:
                result.update({'meta_robots': meta_robots[0][0:15]})
            else:
                result.update({'meta_robots': ''})
            if len(meta_title) > 0:
                result.update({'meta_title': meta_title[0]})
            else:
                result.update({'meta_title': ''})
            result.update({'status': response.getcode()})
            result.update({'url': response.geturl()})
            result.update({'encoding': encoded['encoding']})
            headers = dict(response.getheaders())
            if 'Last-Modified' in headers.keys():
                result.update({'last_modified': headers['Last-Modified']})
            else:
                result.update({'last_modified': None})
            h = html2text.HTML2Text()
            h.ignore_links = True
            h.ignore_images = True
            txt = split_into_sentences(BeautifulSoup(h.handle(html), 'html.parser').get_text().replace('\n', ' '))
            row_count = len(txt)
            word_sum = 0
            tmp_txt = []
            for row in txt:
                row = row.replace('*', '').replace('#', '').replace('_', '').replace('\t', '').replace('   ', ' ').replace('  ', ' ')
                row = ' '.join([word for word in row.split(' ') if len(word) > 1])
                word_sum += len(row.split(' '))
                tmp_txt.append(row)
            avg_row_length = int(word_sum / row_count)
            clean_txt = ''
            for row in tmp_txt:
                word_count = len(row.split(' '))
                if 2 < word_count < avg_row_length * 3:
                    clean_txt += row + '\n'
            result.update({'text_raw': clean_txt})
            with open(cache_file, 'w') as fp:
                json.dump(result, fp)
    except:
        pass
    return result",len(meta_robots) > 0,181,len(meta_robots) > 0,True,100.00000000000004,N/A
"def is_abbreviation(dotted_word):
    clipped = dotted_word[:-1]
<mask>:
        if clipped.lower() in abbr_capped:
            return True
        else:
            return False
    elif clipped in abbr_lowercase:
        return True
    else:
        return False",clipped[0] in ascii_uppercase,24,len(clipped) > 1,False,5.815868174415823,N/A
"def is_sentence_ender(word):
<mask>:
        return False
    if word[-1] in ['?', '!', ' .', ' .']:
        return True
    if len(re.sub('[^A-Z]', '', word)) > 1:
        return True
    if word[-1] == '.' and (not is_abbreviation(word)):
        return True
    return False",word in exceptions,35,not word,False,30.326532985631665,N/A
"def get_parser_by_url(self, url):
    """"""Get the appropriate parser by an search engine url.""""""
    parser = None
<mask>:
        parser = GoogleParser
    if not parser:
        raise Exception('No parser for {}.'.format(url))
    return parser","re.search('^http[s]?://www\\.google', url)",29,url.startswith('http://'),False,6.011896349510331,N/A
"def get_parser_by_search_engine(self, search_engine):
    """"""Get the appropriate parser for the search_engine""""""
<mask>:
        return GoogleParser
    else:
        raise Exception('No such parser for ""{}""'.format(search_engine))",search_engine == 'google' or search_engine == 'googleimg',20,self.search_engine_match(search_engine),False,19.505518236282093,N/A
"def parse_serp(self, config, html=None, parser=None, scraper=None, search_engine=None, query=''):
    """"""parse and store data in the sqlalchemy session.
        Returns:
            The parsed SERP object.
        """"""
<mask>:
        parser = self.get_parser_by_search_engine(search_engine)
        parser = parser(config, query=query)
        parser.parse(html)
    serp = SearchEngineResultsPage()
    if query:
        serp.query = query
    if parser:
        serp.set_values_from_parser(parser)
    if scraper:
        serp.set_values_from_scraper(scraper)
    return serp",not parser and html,47,parser is None and search_engine,False,7.809849842300637,N/A
"def init_outfile(self, config, force_reload=False):
<mask>:
        output_file = config.get('output_filename', '')
        if output_file is None:
            self.output_format = None
        elif output_file.endswith('.json'):
            self.output_format = 'json'
        elif output_file.endswith('.csv'):
            self.output_format = 'csv'
        elif output_file is 'stdout':
            self.output_format = 'stdout'
        if self.output_format == 'json':
            self.outfile = JsonStreamWriter(output_file)
        elif self.output_format == 'csv':
            csv_fieldnames = sorted(set(Link.__table__.columns._data.keys() + SERP.__table__.columns._data.keys()) - {'id', 'serp_id'})
            self.outfile = CsvStreamWriter(output_file, csv_fieldnames)
        elif self.output_format == 'stdout':
            self.outfile = sys.stdout
        else:
            self.outfile = None",not self.outfile or force_reload,67,force_reload,False,18.887560283756194,N/A
"def store_serp_result(self, serp, config):
    """"""Store the parsed SERP page.
        When called from SearchEngineScrape, then
        a parser object is passed.
        When called from caching, a list of serp object are given.
        """"""
<mask>:
        data = self.row2dict(serp)
        data['results'] = []
        for link in serp.links:
            data['results'].append(self.row2dict(link))
        if self.output_format == 'json':
            self.outfile.write(data)
        elif self.output_format == 'csv':
            serp = self.row2dict(serp)
            self.outfile.write(data, serp)
        elif self.output_format == 'stdout':
            if config.get('print_results') == 'summarize':
                print(serp)
            elif config.get('print_results') == 'all':
                pprint.pprint(data)",self.outfile,71,serp.links,False,27.516060407455225,N/A
"def set_values_from_parser(self, parser):
    """"""Populate itself from a parser object.""""""
    self.num_results_for_query = parser.num_results_for_query
    self.num_results = parser.num_results
    self.effective_query = parser.effective_query
    self.no_results = parser.no_results
    for key, value in parser.search_results.items():
<mask>:
            for link in value:
                parsed = urlparse(link['link'])
                if link['snippet'] is not None:
                    tmp_snipped = link['snippet'].split('}')
                    if len(tmp_snipped) > 1:
                        link['snippet'] = tmp_snipped[len(tmp_snipped) - 1]
                [link.update({key: None}) for key in ('snippet', 'title', 'visible_link', 'rating', 'sitelinks') if key not in link]
                Link(link=link['link'], snippet=link['snippet'], title=link['title'], visible_link=link['visible_link'], domain=parsed.netloc, rank=link['rank'], serp=self, link_type=key, rating=link['rating'], sitelinks=link['sitelinks'])
    for key, value in parser.related_keywords.items():
        if isinstance(value, list) and len(value) > 0:
            for keyword in value:
                [keyword.update({key: None}) for key in ('keyword',) if key not in keyword]
                RelatedKeyword(keyword=keyword['keyword'], rank=keyword['rank'], serp=self)","isinstance(value, list)",106,"isinstance(value, list) and len(value) > 0",False,38.05803001674947,N/A
"def get_session(config, scoped=False, engine=None, path=None):
<mask>:
        engine = get_engine(config, path=path)
    session_factory = sessionmaker(bind=engine, autoflush=True, autocommit=False)
    if scoped:
        ScopedSession = scoped_session(session_factory)
        return ScopedSession
    else:
        return session_factory",not engine,25,engine is None,False,27.516060407455225,N/A
"def fixtures(config, session):
    """"""Add some base data.""""""
    for se in config.get('supported_search_engines', []):
<mask>:
            search_engine = session.query(SearchEngine).filter(SearchEngine.name == se).first()
            if not search_engine:
                session.add(SearchEngine(name=se))
    session.commit()",se,23,se,True,100.00000000000004,N/A
"def write(self, obj):
<mask>:
        self.file.write(',')
    json.dump(obj, self.file, indent=2, sort_keys=True)
    self.last_object = id(obj)",self.last_object,12,not self.file.is_closed,False,12.22307556087252,N/A
"def parse_proxy_file(self, fname):
    """"""Parses a proxy file
        The format should be like the following:
            socks5 XX.XXX.XX.XX:1080 username:password
            socks4 XX.XXX.XX.XX:80 username:password
            http XX.XXX.XX.XX:80
            If username and password aren't provided, we assumes
            that the proxy doesn't need auth credentials.
        Args:
            fname: The file name where to look for proxies.
        Returns:
            The parsed proxies.
        Raises:
            ValueError if no file with the path fname could be found.
        """"""
    proxies = []
    path = os.path.join(os.getcwd(), fname)
<mask>:
        with open(path, 'r') as pf:
            for line in pf.readlines():
                if not (line.strip().startswith('#') or line.strip().startswith('//')):
                    tokens = line.replace('\n', '').split(' ')
                    try:
                        proto = tokens[0]
                        host, port = tokens[1].split(':')
                    except Exception:
                        raise Exception('\n                                Invalid proxy file.\n                                Should have the following format: {}\n                                '.format(self.parse_proxy_file.__doc__))
                    if len(tokens) == 3:
                        username, password = tokens[2].split(':')
                        proxies.append(self.Proxy(proto=proto, host=host, port=port, username=username, password=password))
                    else:
                        proxies.append(self.Proxy(proto=proto, host=host, port=port, username='', password=''))
        return proxies
    else:
        raise ValueError('No such file/directory')",os.path.exists(path),139,os.path.isfile(path),False,50.000000000000014,N/A
"def add_proxies_to_db(self, proxies, session):
    """"""Adds the list of proxies to the database.
        If the proxy-ip already exists and the other data differs,
        it will be overwritten.
        Will not check the status of the proxy.
        Args:
            proxies: A list of proxies.
            session: A database session to work with.
        """"""
    for proxy in proxies:
<mask>:
            p = session.query(database.Proxy).filter(proxy.host == database.Proxy.ip).first()
            if not p:
                p = database.Proxy(ip=proxy.host)
            p.port = proxy.port
            p.username = proxy.username
            p.password = proxy.password
            p.proto = proxy.proto
            session.add(p)
            session.commit()",proxy,78,proxy.ip != database.Proxy.ip,False,4.196114906296549,N/A
"def run(self):
    while self.num_already_processed < self.num_keywords:
        e = self.queue.get()
<mask>:
            break
        self.num_already_processed += 1
        print(self.progress_fmt.format(self.num_already_processed, self.num_keywords), end='\r')
        self.queue.task_done()",e == 'done',18,e is None,False,19.716118825581447,N/A
"def main(self, return_results=False, config=None):
    """"""the main method""""""
    logger = Logger()
    logger.setup_logger(level=config.get('log_level').upper())
    self.logger = logger.get_logger()
    keywords = set(config.get('keywords', []))
    proxy_file = config.get('proxy_file', '')
    search_engines = config.get('search_engines', ['google'])
<mask>:
        if search_engines == '*':
            search_engines = config.get('supported_search_engines')
        else:
            search_engines = search_engines.split(',')
    search_engines = set(search_engines)
    num_search_engines = len(search_engines)
    num_workers = int(config.get('num_workers'))
    scrape_method = config.get('scrape_method')
    pages = int(config.get('num_pages_for_keyword', 1))
    method = config.get('scrape_method', 'selenium')
    result_writer = ResultWriter()
    result_writer.init_outfile(config, force_reload=True)
    cache_manager = CacheManager(config, self.logger, result_writer)
    scrape_jobs = {}
    if not scrape_jobs:
        scrape_jobs = ScrapeJobGenerator().get(keywords, search_engines, scrape_method, pages)
    scrape_jobs = list(scrape_jobs)
    proxies = []
    if config.get('use_own_ip'):
        proxies.append(None)
    elif proxy_file:
        proxies = Proxies().parse_proxy_file(proxy_file)
    if not proxies:
        raise Exception('No proxies available. Turning down.')
    shuffle(proxies)
    session_cls = get_session(config, scoped=True)
    session = session_cls()
    fixtures(config, session)
    Proxies().add_proxies_to_db(proxies, session)
    scraper_search = ScraperSearch(number_search_engines_used=num_search_engines, number_proxies_used=len(proxies), number_search_queries=len(keywords), started_searching=datetime.datetime.utcnow(), used_search_engines=','.join(search_engines))
    if config.get('do_caching'):
        scrape_jobs = cache_manager.filter_scrape_jobs(scrape_jobs, session, scraper_search)
    if scrape_jobs:
        db_lock = threading.Lock()
        cache_lock = threading.Lock()
        captcha_lock = threading.Lock()
        self.logger.info('\n                Going to scrape {num_keywords} keywords with {num_proxies}\n                proxies by using {num_threads} threads.'.format(num_keywords=len(list(scrape_jobs)), num_proxies=len(proxies), num_threads=num_search_engines))
        progress_thread = None
        q = queue.Queue()
        progress_thread = ShowProgressQueue(config, q, len(scrape_jobs))
        progress_thread.start()
        workers = queue.Queue()
        num_worker = 0
        for search_engine in search_engines:
            for proxy in proxies:
                for worker in range(num_workers):
                    num_worker += 1
                    workers.put(ScrapeWorkerFactory(config, cache_manager=cache_manager, mode=method, proxy=proxy, search_engine=search_engine, session=session, db_lock=db_lock, cache_lock=cache_lock, scraper_search=scraper_search, captcha_lock=captcha_lock, progress_queue=q, browser_num=num_worker))
        for job in scrape_jobs:
            while True:
                worker = workers.get()
                workers.put(worker)
                if worker.is_suitabe(job):
                    worker.add_job(job)
                    break
        threads = []
        while not workers.empty():
            worker = workers.get()
            thread = worker.get_worker()
            if thread:
                threads.append(thread)
        for t in threads:
            t.start()
        for t in threads:
            t.join()
        q.put('done')
        progress_thread.join()
    result_writer.close_outfile()
    scraper_search.stopped_searching = datetime.datetime.utcnow()
    try:
        session.add(scraper_search)
        session.commit()
    except Exception:
        pass
    if return_results:
        return scraper_search","not isinstance(search_engines, list)",255,config.get('supported_search_engines'),False,9.980099403873663,N/A
"def setup_logger(self, level=logging.INFO):
    """"""Configure global log settings""""""
<mask>:
        self.level = logging.getLevelName(level)
    self.logger = logging.getLogger()
    self.logger.setLevel(self.level)
    if not len(self.logger.handlers):
        ch = logging.StreamHandler(stream=sys.stderr)
        logformat = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        formatter = logging.Formatter(logformat)
        ch.setFormatter(formatter)
        self.logger.addHandler(ch)","isinstance(level, int)",35,level is not None,False,9.688464563433238,N/A
"def create_cache_dir(self):
<mask>:
        cd = self.config.get('cachedir', self.CACHEDIR)
        if not os.path.exists(cd):
            os.mkdir(cd)","self.config.get('do_caching', True)",11,not os.path.exists(self.CACHEDIR),False,8.606119900909883,N/A
"def clean_cache(self):
    """"""Clean the caches searches.""""""
    cachedir = self.config.get('cachedir', self.CACHEDIR)
<mask>:
        for file_name in os.listdir(cachedir):
            path = os.path.join(cachedir, file_name)
            cache_time = int(self.config.get('clean_cache_after', self.CLEAN_CACHE_AFTER))
            max_cache_time = 60 * 60 * cache_time
            if time.time() > os.path.getmtime(path) + max_cache_time:
                if os.path.isdir(path):
                    shutil.rmtree(path)
                else:
                    os.remove(os.path.join(cachedir, file_name))",os.path.exists(cachedir),42,os.path.isdir(cachedir),False,50.000000000000014,N/A
"def get_cached(self, keyword, search_engine, scrapemode, page_number):
    """"""Loads a cached result.""""""
<mask>:
        file_name = self.cached_file_name(keyword, search_engine, scrapemode, page_number)
        cache_dir = self.config.get('cachedir', self.CACHEDIR)
        if file_name in os.listdir(cache_dir):
            try:
                modtime = os.path.getmtime(os.path.join(cache_dir, file_name))
            except FileNotFoundError:
                return False
            modtime = (time.time() - modtime) / 60 / 60
            if modtime > int(self.config('clean_cache_after', 48)):
                return False
            path = os.path.join(cache_dir, file_name)
            return self.read_cached_file(path)
        else:
            return False","self.config.get('do_caching', False)",59,"self.config.get('cache_enabled', True)",False,46.17366309441024,N/A
"def read_cached_file(self, path):
    """"""Read a cache file.""""""
<mask>:
        ext = path.split('.')[-1]
        if ext == 'cache':
            with open(path, 'r') as fd:
                try:
                    return fd.read()
                except UnicodeDecodeError as e:
                    self.logger.warning(str(e))
        else:
            raise Exception('""{}"" is a invalid cache file.'.format(path))","self.config.get('do_caching', False)",36,os.path.isfile(path),False,4.736913377107212,N/A
"def cache_results(self, parser, query, search_engine, scrape_mode, page_number, db_lock=None):
    """"""Stores the parsed html in a file.
        If an db_lock is given, all action are wrapped in this lock.
        """"""
<mask>:
        if db_lock:
            db_lock.acquire()
        if self.config.get('minimize_caching_files', False):
            html = parser.cleaned_html
        else:
            html = parser.html
        file_name = self.cached_file_name(query, search_engine, scrape_mode, page_number)
        cache_dir = self.config.get('cachedir', self.CACHEDIR)
        path = os.path.join(cache_dir, file_name)
        with open(path, 'w') as fd:
            if isinstance(html, bytes):
                fd.write(html.decode())
            else:
                fd.write(html)
        if db_lock:
            db_lock.release()","self.config.get('do_caching', False)",70,parser.is_valid(),False,4.546632359631261,N/A
"def validate(self, config):
<mask>:
        raise Error('config is not a dict')
    if config.get('num_results_per_page') > 100:
        raise Error('num_results_per_page must be lower then 100')
    valid_search_types = ['normal', 'video', 'news', 'image']
    if config.get('search_type') not in valid_search_types:
        raise Error('Invalid search type!')
    if config.get('use_own_ip') != True and len(config.get('proxy_file')) == 0:
        raise Error('No proxy_file provided and using own IP is disabled.')
    if config.get('scrape_method') not in 'selenium':
        raise Error('No such scrape_method {}'.format(config.get('scrape_method')))
    if config.get('screenshot') is True and (config.get('dir_screenshot') is None or len(config.get('dir_screenshot')) < 1):
        raise Error('No config dir_screenshot found')","not isinstance(config, dict)",81,"not isinstance(config, dict)",True,100.00000000000004,N/A
"def __init__(self, config, cache_manager=None, jobs=None, scraper_search=None, session=None, db_lock=None, cache_lock=None, start_page_pos=1, search_engine=None, search_type=None, proxy=None, progress_queue=None):
    """"""Instantiate an SearchEngineScrape object.""""""
    self.config = config
    self.cache_manager = cache_manager
    jobs = jobs or {}
    self.search_engine_name = search_engine.lower()
    assert self.search_engine_name, 'You need to specify an search_engine'
<mask>:
        self.search_type = self.config.get('search_type', 'normal')
    else:
        self.search_type = search_type
    self.jobs = jobs
    self.missed_keywords = set()
    self.num_keywords = len(self.jobs)
    self.query = ''
    self.pages_per_keyword = [1]
    self.search_number = 1
    self.parser = Parsing().get_parser_by_search_engine(self.search_engine_name)(config=self.config)
    self.num_results_per_page = int(self.config.get('num_results_per_page', 10))
    if start_page_pos:
        self.start_page_pos = 1 if start_page_pos < 1 else start_page_pos
    else:
        self.start_page_pos = int(self.config.get('search_offset', 1))
    self.page_number = self.start_page_pos
    self.proxy = proxy
    if isinstance(proxy, Proxies().Proxy):
        self.set_proxy()
        self.requested_by = self.proxy.host + ':' + self.proxy.port
    else:
        self.requested_by = 'localhost'
    self.scraper_search = scraper_search
    self.scrape_method = ''
    self.startable = True
    self.db_lock = db_lock
    self.cache_lock = cache_lock
    self.progress_queue = progress_queue
    self.session = session
    self.requested_at = None
    self.name = '[{}]'.format(self.search_engine_name) + self.__class__.__name__
    self.sleeping_min = self.config.get('sleeping_min')
    self.sleeping_max = self.config.get('sleeping_max')
    self.timeout = 5
    self.status = 'successful'
    self.html = ''",not search_type,155,search_type is None,False,39.76353643835252,N/A
"def store(self):
    """"""Store the parsed data in the sqlalchemy scoped session.""""""
    assert self.session, 'No database session.'
<mask>:
        self.parser.parse(self.html)
    else:
        self.parser = None
    with self.db_lock:
        serp = Parsing().parse_serp(self.config, parser=self.parser, scraper=self, query=self.query)
        self.scraper_search.serps.append(serp)
        self.session.add(serp)
        self.session.commit()
        ResultWriter().store_serp_result(serp, self.config)
        if serp.num_results:
            return True
        else:
            return False",self.html,42,self.parser,False,55.03212081491043,N/A
"def after_search(self):
    """"""Store the results and parse em.
        Notify the progress queue if necessary.
        """"""
    self.search_number += 1
<mask>:
        logger.debug('\n            No results to store for keyword: ""{}"" in search engine: {}\n            '.format(self.query, self.search_engine_name))
    if self.progress_queue:
        self.progress_queue.put(1)
    self.cache_results()",not self.store(),37,self.search_number == len(self.search_engine_name),False,5.439330544349821,N/A
"def before_search(self):
    """"""before entering the search loop.""""""
<mask>:
        if not self.proxy_check(proxy=self.proxy):
            self.startable = False","self.config.get('check_proxies', True) and self.proxy",14,self.startable,False,0.7222266253934472,N/A
"def update_proxy_status(self, status, ipinfo=None, online=True):
    """"""Sets the proxy status with the results of ipinfo.io
        Args:
            status: A string the describes the status of the proxy.
            ipinfo: The json results from ipinfo.io
            online: Whether the proxy is usable or not.
        """"""
    ipinfo = ipinfo or {}
    with self.db_lock:
        proxy = self.session.query(db_Proxy).filter(self.proxy.host == db_Proxy.ip).first()
<mask>:
            for key in ipinfo.keys():
                setattr(proxy, key, ipinfo[key])
            proxy.checked_at = datetime.datetime.utcnow()
            proxy.status = status
            proxy.online = online
            try:
                self.session.merge(proxy, load=True)
                self.session.commit()
            except:
                pass",proxy,75,proxy,True,100.00000000000004,N/A
"def get_selenium_scraper_by_search_engine_name(config, search_engine_name, *args, **kwargs):
    """"""Get the appropriate selenium scraper for the given search engine name.

    Args:
        search_engine_name: The search engine name.
        args: The arguments for the target search engine instance creation.
        kwargs: The keyword arguments for the target search engine instance.
    Returns;
        Either a concrete SelScrape instance specific for the given
        search engine or the abstract SelScrape object.
    """"""
    class_name = search_engine_name[0].upper() + search_engine_name[1:].lower() + 'SelScrape'
    ns = globals()
<mask>:
        return ns[class_name](config, *args, **kwargs)
    return SelScrape(config, *args, **kwargs)",class_name in ns,79,class_name in ns,True,100.00000000000004,N/A
"def proxy_check(self, proxy):
    assert self.proxy and self.webdriver, 'Scraper instance needs valid\n        webdriver and proxy instance to make the proxy check'
    online = False
    status = 'Proxy check failed: {host}:{port}\n        is not used while requesting'.format(host=self.proxy.host, port=self.proxy.port)
    ipinfo = {}
    try:
        self.webdriver.get(self.config.get('proxy_info_url'))
        time.sleep(2)
        try:
            text = re.search('(\\{.*?\\})', self.webdriver.page_source, flags=re.DOTALL).group(0)
            ipinfo = json.loads(text)
        except ValueError as v:
            logger.critical(v)
    except Exception as e:
        status = str(e)
<mask>:
        online = True
        status = 'Proxy is working.'
    else:
        logger.warning(status)
    super().update_proxy_status(status, ipinfo, online)
    return online",'ip' in ipinfo and ipinfo['ip'],78,'offline' in status,False,5.197112497172873,N/A
"def _save_debug_screenshot(self):
    """"""
        Saves a debug screenshot of the browser window to figure
        out what went wrong.
        """"""
    screendir = '{}/{}'.format(self.config['dir_screenshot'], self.config['today'])
<mask>:
        os.makedirs(screendir)
    location = os.path.join(screendir, '{}_{}-p{}.png'.format(self.search_engine_name, self.query, str(self.page_number)))
    if self.config.get('sel_browser') == 'chrome' and self.config.get('chrome_headless') is True:
        self._enable_download_in_headless_chrome(self.webdriver, screendir)
        total_height = self.webdriver.execute_script('return document.body.parentNode.scrollHeight')
        self.webdriver.set_window_size('1024', total_height)
    try:
        self.webdriver.get_screenshot_as_file(location)
    except Exception as err:
        logger.error(err)",not os.path.exists(screendir),53,not os.path.exists(screendir),True,100.00000000000004,N/A
"def _get_webdriver(self):
    """"""Return a webdriver instance and set it up
        with the according profile/ proxies.
        Chrome is quite fast, but not as stealthy as PhantomJS.
        Returns:
            The appropriate webdriver mode according to self.browser_type.
            If no webdriver mode could be found, return False.
        """"""
<mask>:
        return self._get_Chrome()
    elif self.browser_type == 'firefox':
        return self._get_Firefox()
    elif self.browser_type == 'phantomjs':
        return self._get_PhantomJS()
    return False",self.browser_type == 'chrome',60,self.browser_type == 'chrome',True,100.00000000000004,N/A
"def add_job(self, job):
    query = job['query']
    page_number = job['page_number']
<mask>:
        self.jobs[query] = []
    self.jobs[query].append(page_number)",query not in self.jobs,14,query not in self.jobs,True,100.00000000000004,N/A
"def get_worker(self):
<mask>:
        if self.mode == 'selenium':
            from scrapcore.scraper.selenium import get_selenium_scraper_by_search_engine_name
            return get_selenium_scraper_by_search_engine_name(self.config, self.search_engine, cache_manager=self.cache_manager, search_engine=self.search_engine, jobs=self.jobs, session=self.session, scraper_search=self.scraper_search, cache_lock=self.cache_lock, db_lock=self.db_lock, proxy=self.proxy, progress_queue=self.progress_queue, captcha_lock=self.captcha_lock, browser_num=self.browser_num)
    return None",self.jobs,27,self.config.get('scraper_search_engine'),False,6.772997136689072,N/A
"def after_parsing(self):
    """"""Clean the urls.

        A typical scraped results looks like the following:

        '/url?q=http://www.youtube.com/user/Apple&sa=U&ei=        lntiVN7JDsTfPZCMgKAO&ved=0CFQQFjAO&usg=AFQjCNGkX65O-hKLmyq1FX9HQqbb9iYn9A'

        Clean with a short regex.
        """"""
    super().after_parsing()
<mask>:
        if self.num_results > 0:
            self.no_results = False
        elif self.num_results <= 0:
            self.no_results = True
        if 'No results found for' in self.html or 'did not match any documents' in self.html:
            self.no_results = True
        if self.no_results is True:
            for key, i in self.iter_serp_items():
                if 'snippet' in self.search_results[key][i] and self.query:
                    if self.query.replace('""', '') in self.search_results[key][i]['snippet']:
                        self.no_results = False
    if self.searchtype == 'image':
        for key, i in self.iter_serp_items():
            if self.search_results[key][i]:
                meta_dict = json.loads(self.search_results[key][i]['snippet'])
                rank = self.search_results[key][i]['rank']
                self.search_results[key][i] = {'link': meta_dict['ou'], 'snippet': meta_dict['s'], 'title': meta_dict['pt'], 'visible_link': meta_dict['isu'], 'rating': None, 'sitelinks': None, 'rank': rank}
    clean_regexes = {'normal': '/url\\?q=(?P<url>.*?)&sa=U&ei=', 'image': 'imgres\\?imgurl=(?P<url>.*?)&'}
    for key, i in self.iter_serp_items():
        result = re.search(clean_regexes[self.searchtype], self.search_results[key][i]['link'])
        if result:
            self.search_results[key][i]['link'] = unquote(result.group('url'))",self.searchtype == 'normal',131,self.searchtype == 'url',False,75.98356856515926,N/A
"def __init__(self, config={}, html='', query=''):
    """"""Create new Parser instance and parse all information.""""""
    self.config = config
    self.searchtype = self.config.get('search_type', 'normal')
    assert self.searchtype in self.search_types, 'search type ""{}"" is not supported in {}'.format(self.searchtype, self.__class__.__name__)
    self.query = query
    self.html = html
    self.dom = None
    self.search_results = {}
    self.num_results_for_query = ''
    self.num_results = 0
    self.effective_query = ''
    self.page_number = -1
    self.no_results = False
    self.related_keywords = {}
    self.search_engine = ''
    self.css_to_xpath = HTMLTranslator().css_to_xpath
<mask>:
        self.parse()",self.html,71,"self.config.get('parser', False)",False,8.392229812593097,N/A
"def parse(self, html=None):
    """"""Public function to start parsing the search engine results.

        Args:
            html: The raw html data to extract the SERP entries from.
        """"""
<mask>:
        self.html = html.encode('utf-8').decode('utf-8')
    self._parse()
    self.after_parsing()",html,31,html,True,100.00000000000004,N/A
"def _parse_lxml(self, cleaner=None):
    try:
        parser = lxml.html.HTMLParser(encoding='utf-8')
<mask>:
            self.dom = cleaner.clean_html(self.dom)
        self.dom = lxml.html.document_fromstring(self.html, parser=parser)
        self.dom.resolve_base_href()
    except Exception as e:
        logger.error(e)",cleaner,21,cleaner,True,100.00000000000004,N/A
"def _parse(self, cleaner=None):
    """"""Internal parse the dom according to the provided css selectors.
        Raises: Exception
        if no css selectors for the searchtype could be found.
        """"""
    self.num_results = 0
    self._parse_lxml(cleaner)
    attr_name = self.searchtype + '_search_selectors'
    selector_dict = getattr(self, attr_name, None)
    num_results_selector = getattr(self, 'num_results_search_selectors', None)
    self.num_results_for_query = self.first_match(num_results_selector, self.dom)
<mask>:
        logger.debug(""'{}: Cannot parse num_results from serp page\n            with selectors {}\n            "".format(self.__class__.__name__, num_results_selector))
    try:
        self.page_number = int(self.first_match(self.page_number_selectors, self.dom))
    except ValueError:
        self.page_number = -1
    self.effective_query = self.first_match(self.effective_query_selector, self.dom)
    if self.effective_query:
        logger.debug('{}: There was no search hit for the search query.\n            Search engine used {} instead.\n            '.format(self.__class__.__name__, self.effective_query))
    else:
        self.effective_query = ''
    self.no_results_text = self.first_match(self.no_results_selector, self.dom)
    if not selector_dict and (not isinstance(selector_dict, dict)):
        raise Exception('There is no such attribute: {}. No selectors found\n            '.format(attr_name))
    for result_type, selector_class in selector_dict.items():
        self.search_results[result_type] = []
        self.related_keywords[result_type] = []
        for _, selectors in selector_class.items():
            if 'result_container' in selectors and selectors['result_container']:
                css = '{container} {result_container}'.format(**selectors)
            else:
                css = selectors['container']
            results = self.dom.xpath(self.css_to_xpath(css))
            to_extract = set(selectors.keys()) - {'container', 'result_container'}
            selectors_to_use = {key: selectors[key] for key in to_extract if key in selectors.keys()}
            for index, result in enumerate(results):
                serp_result = {}
                for key, selector in selectors_to_use.items():
                    serp_result[key] = self.advanced_css(selector, result)
                serp_result['rank'] = index + 1
                if 'link' in serp_result and serp_result['link'] and (not [e for e in self.search_results[result_type] if e['link'] == serp_result['link']]):
                    self.search_results[result_type].append(serp_result)
                    self.num_results += 1
                if 'keyword' in serp_result and serp_result['keyword']:
                    self.related_keywords[result_type].append(serp_result)",not self.num_results_for_query,222,self.num_results_for_query,False,89.483931681437,N/A
"def advanced_css(self, selector, element):
    """"""Evaluate the :text and ::attr(attr-name) additionally.

        Args:
            selector: A css selector.
            element: The element on which to apply the selector.

        Returns:
            The targeted element.

        """"""
    value = None
<mask>:
        try:
            value = element.xpath(self.css_to_xpath(selector.split('::')[0]))[0].text_content()
        except IndexError:
            pass
    else:
        match = re.search('::attr\\((?P<attr>.*)\\)$', selector)
        if match:
            attr = match.group('attr')
            try:
                value = element.xpath(self.css_to_xpath(selector.split('::')[0]))[0].get(attr)
            except IndexError:
                pass
        else:
            try:
                value = element.xpath(self.css_to_xpath(selector))[0].text_content()
            except IndexError:
                pass
    return value",selector.endswith('::text'),67,self.use_xpath,False,4.79981069911921,N/A
"def get_related(config, keywords, related):
    scrap = serpscrap.SerpScrap()
    scrap.init(config=config.get(), keywords=keywords)
    scrap.run()
    results = scrap.get_related()
    for keyword in results:
<mask>:
            related.append(keyword['keyword'])
    return related",keyword['keyword'] not in related,21,keyword['id'] not in related,False,48.892302243490086,N/A
"@task
def black(c, check=False):
    print('Running black')
    cmd = f'black {SOURCES}'
<mask>:
        cmd += ' --check'
    c.run(cmd)",check,16,check,True,100.00000000000004,N/A
"@task
def isort(c, check=False):
    print('Running isort')
    cmd = f'isort {SOURCES}'
<mask>:
        cmd += ' --check'
    c.run(cmd)",check,16,check,True,100.00000000000004,N/A
"@task
def mypy(c, machine_readable=False):
    print('Running mypy')
    cmd = 'mypy'
<mask>:
        cmd += ' --no-pretty'
    else:
        cmd += ' --color-output --pretty'
    c.run(cmd)",machine_readable,21,machine_readable,True,100.00000000000004,N/A
"def __init__(self, *, actual: Optional[str], expected: Optional[str]):
    self.actual = actual
<mask>:
        if expected:
            self.message = f""Current branch: '{actual}' does not match expected branch: '{expected}'""
        else:
            self.message = f""Current branch: '{actual}' does not match empty branch""
    else:
        self.message = f""Not on any branch. Expected branch: '{expected}'""",actual,45,actual is not None,False,15.97357760615681,N/A
"def process(self, index: int, count: int, repo: Repo) -> Outcome:
    """"""Synchronize a repo given its configuration in the manifest.

        Always start by running `git fetch`, then either:

        * try resetting the repo to the given tag or sha1 (abort
          if the repo is dirty)

        * or try merging the local branch with its upstream (abort if not
          on on the correct branch, or if the merge is not fast-forward).
        """"""
    error = None
    self.info_count(index, count, 'Synchronizing', repo.dest)
    self.fetch(repo)
    summary_lines = []
    ref = None
<mask>:
        ref = repo.sha1
    elif repo.tag:
        ref = repo.tag
    if ref:
        self.info_3('Resetting to', ref)
        self.sync_repo_to_ref(repo, ref)
        summary_lines += [repo.dest, '-' * len(repo.dest)]
        summary_lines += [f'Reset to {ref}']
    else:
        error, current_branch = self.check_or_change_branch(repo)
        self.info_3('Updating branch:', current_branch)
        sync_summary = self.sync_repo_to_branch(repo, current_branch=current_branch)
        if sync_summary:
            title = f'{repo.dest} on {current_branch}'
            summary_lines += [title, '-' * len(title), sync_summary]
    if not repo.ignore_submodules:
        submodule_line = self.update_submodules(repo)
        if submodule_line:
            summary_lines.append(submodule_line)
    summary = '\n'.join(summary_lines)
    return Outcome(error=error, summary=summary)",repo.sha1,152,repo.sha1,True,100.00000000000004,N/A
"def check_or_change_branch(self, repo: Repo) -> Tuple[Optional[Error], str]:
    """"""Check that the current branch:
            * exists
            * matches the one in the manifest

        If it does, do nothing.

        If not but the repo is clean and the correct_branch flag is set,
        switch to the configured branch.""""""
    error = None
    current_branch = None
    try:
        current_branch = self.check_branch(repo)
    except IncorrectBranch as e:
        current_branch = e.actual
<mask>:
            self.checkout_branch(repo)
            current_branch = repo.branch
        else:
            error = e
        if not current_branch:
            raise
    return (error, current_branch)",self.correct_branch,77,not current_branch and repo.correct_branch,False,26.269098944241588,N/A
"def check_branch(self, repo: Repo) -> str:
    """"""Check that the current branch:
            * exists
            * matches the one in the manifest

        Return the current branch.
        """"""
    repo_path = self.workspace_path / repo.dest
    current_branch = None
    try:
        current_branch = get_current_branch(repo_path)
    except Error:
        raise IncorrectBranch(actual=None, expected=repo.branch)
<mask>:
        raise IncorrectBranch(actual=current_branch, expected=repo.branch)
    return current_branch",current_branch and current_branch != repo.branch,48,not current_branch,False,8.047084086794415,N/A
"def _pick_remotes(self, repo: Repo) -> List[Remote]:
<mask>:
        for remote in repo.remotes:
            if remote.name == self.remote_name:
                return [remote]
        message = f'Remote {self.remote_name} not found for repository {repo.dest}'
        raise Error(message)
    return repo.remotes",self.remote_name,30,self.remote_name,True,100.00000000000004,N/A
"def init(self, url: str, *, branch: Optional[str], show_output: bool=True, show_cmd: bool=True) -> None:
    parent = self.clone_path.parent
    name = self.clone_path.name
    parent.mkdir(parents=True, exist_ok=True)
    cmd = ['clone', url]
<mask>:
        cmd += ['--branch', branch]
    cmd += [name]
    run_git(self.clone_path.parent, *cmd, show_output=show_output, show_cmd=show_cmd)",branch,37,branch,True,100.00000000000004,N/A
"def update_remotes(self) -> None:
    _, out = run_git_captured(self.working_path, 'remote')
    for line in out.splitlines():
        _, url = run_git_captured(self.working_path, 'remote', 'get-url', line)
<mask>:
            tmp_r = Remote(name=line, url=url)
            self.remotes.append(tmp_r)",line and url,26,url,False,13.533528323661276,N/A
"def update_upstreamed(self) -> None:
    use_branch = self.branch
<mask>:
        if use_branch.startswith('heads/') is True:
            use_branch = use_branch[6:]
    else:
        return
    rc, _ = run_git_captured(self.working_path, 'config', '--get', f'branch.{use_branch}.remote', check=False)
    if rc == 0:
        self.upstreamed = True",use_branch,32,use_branch is not None,False,30.213753973567677,N/A
"def remote_urls_are_same(url_1: str, url_2: str) -> bool:
    """"""
    return True if provided URLs are the same
    """"""
    up_1 = urlparse(url_1)
    up_2 = urlparse(url_2)
<mask>:
        return up_1.scheme == up_2.scheme and up_1.hostname == up_2.hostname and (up_1.port == up_2.port) and (_norm_path(quote(up_1.path)) == _norm_path(quote(up_2.path)))
    elif platform.startswith('win'):
        return up_1.scheme == up_2.scheme and up_1.netloc == up_2.netloc
    else:
        return up_1.scheme == up_2.scheme and up_1.netloc == up_2.netloc and (up_1.hostname == up_2.hostname) and (_norm_path(quote(up_1.path)) == _norm_path(quote(up_2.path)))",up_1.scheme != 'file' and up_2.scheme != 'file',67,platform.startswith('win'),False,1.2976955971595585,N/A
"def _norm_path(path: str) -> str:
    ret: str = ''
<mask>:
        ret += '/'
    u_seg: List[str] = []
    path_split = path.split('/')
    for seg in path_split:
        if seg != '':
            u_seg.append(seg)
    ret += '/'.join(u_seg)
    return ret",path[0] == '/',34,path != '',False,5.4424142191183185,N/A
"def check_shallow_with_sha1(self, repo: Repo) -> None:
<mask>:
        return
    if self.shallow:
        message = textwrap.dedent(f'Cannot use --shallow with a fixed sha1 ({repo.sha1})\nConsider using a tag instead')
        raise Error(message)",not repo.sha1,26,not repo.sha1,True,100.00000000000004,N/A
"def _choose_remote(self, repo: Repo) -> Remote:
<mask>:
        for remote in repo.remotes:
            if remote.name == self.remote_name:
                return remote
        message = f""Remote '{self.remote_name}' not found for repository '{repo.dest}'""
        raise Error(message)
    return repo.remotes[0]",self.remote_name,30,self.remote_name,True,100.00000000000004,N/A
"def clone_repo(self, repo: Repo) -> str:
    """"""Clone a missing repo.""""""
    repo_path = self.workspace_path / repo.dest
    parent = repo_path.parent
    name = repo_path.name
    parent.mkdir(parents=True, exist_ok=True)
    remote = self._choose_remote(repo)
    remote_name = remote.name
    remote_url = remote.url
    clone_args = ['clone', '--origin', remote_name, remote_url]
    ref = None
<mask>:
        ref = repo.tag
    elif repo.branch:
        ref = repo.branch
    if ref:
        clone_args.extend(['--branch', ref])
    if self.shallow:
        clone_args.extend(['--depth', '1'])
    if not repo.ignore_submodules:
        clone_args.append('--recurse-submodules')
    clone_args.append(name)
    self.run_git(parent, *clone_args)
    summary = f'{repo.dest} cloned from {remote_url}'
    if ref:
        summary += f' (on {ref})'
    return summary",repo.tag,80,repo.tag,True,100.00000000000004,N/A
"def reset_repo(self, repo: Repo) -> str:
    ref = repo.sha1
<mask>:
        return ''
    else:
        self.info_2('Resetting', repo.dest, 'to', ref)
        repo_path = self.workspace_path / repo.dest
        try:
            self.run_git(repo_path, 'reset', '--hard', ref)
        except Error:
            raise Error('Resetting to', ref, 'failed')
        summary = f' and reset to {ref}'
        return summary",not ref,43,not repo.dest,False,15.97357760615681,N/A
"def __init__(self, working_path: Path, cmd: Iterable[str], *, output: Optional[str]=None, error: Optional[str]=None) -> None:
    self.cmd = cmd
    self.working_path = working_path
    self.output = output
    cmd_str = ' '.join(cmd)
    message = f'`git {cmd_str}` from {working_path} failed'
<mask>:
        message += '\n' + output
    if error:
        message += '\n' + error
    super().__init__(message)",output,47,output,True,100.00000000000004,N/A
"def update_upstream(self) -> None:
<mask>:
        rc, is_upstr = run_git_captured(self.working_path, 'rev-parse', '--symbolic-full-name', f'{self.branch}@{{u}}', check=False)
        if rc == 0:
            self.is_upstreamed = True
            return
        run_git_captured(self.working_path, 'remote', 'remove', self.remote_name, check=False)
        run_git_captured(self.working_path, 'remote', 'add', self.remote_name, self.remote_url, check=False)
        run_git_captured(self.working_path, 'fetch', '--all', '--prune', check=False)
        rc, _ = run_git_captured(self.working_path, 'branch', self.branch, '--set-upstream-to', f'{self.remote_name}/{self.branch}', check=False)
        if rc == 0:
            self.is_upstreamed = True",self.branch,53,not self.is_upstreamed,False,16.233395773754953,N/A
"def update_remote_status(self) -> None:
    rc, ahead_rev = run_git_captured(self.working_path, 'rev-list', '@{upstream}..HEAD', check=False)
<mask>:
        self.ahead = len(ahead_rev.splitlines())
    rc, behind_rev = run_git_captured(self.working_path, 'rev-list', 'HEAD..@{upstream}', check=False)
    if rc == 0:
        self.behind = len(behind_rev.splitlines())",rc == 0,29,rc == 0,True,100.00000000000004,N/A
"def process(self, index: int, count: int, repo: Repo) -> Outcome:
    summary_lines = []
    for remote in repo.remotes:
        existing_remote = self.get_remote(repo, remote.name)
<mask>:
            if remote_urls_are_same(existing_remote.url, remote.url) is False:
                self.set_remote(repo, remote)
                summary_lines.append(f""{repo.dest}: remote '{remote.name}' set to '{remote.url}'"")
        else:
            self.add_remote(repo, remote)
            summary_lines.append(f""{repo.dest}: added remote '{remote.name}' with url: '{remote.url}'"")
    return Outcome.from_lines(summary_lines)",existing_remote,47,existing_remote is not None,False,30.213753973567677,N/A
"def get_remote(self, repo: Repo, name: str) -> Optional[Remote]:
    full_path = self.workspace_path / repo.dest
    rc, url = run_git_captured(full_path, 'remote', 'get-url', name, check=False)
<mask>:
        return None
    else:
        return Remote(name=name, url=url)",rc != 0,28,rc != 0,True,100.00000000000004,N/A
"def commit_config_update(self, cfgud: ConfigUpdateData, cfguts: List[ConfigUpdateType]) -> None:
    """"""once all updates are done,
        calling commit is in order""""""
    for this_type in cfguts:
<mask>:
            self.workspace.config.manifest_branch = cfgud.manifest_branch
    self.workspace.config.save_to_file(self.workspace.cfg_path)",this_type == ConfigUpdateType.MANIFEST_BRANCH and cfgud.manifest_branch,27,this_type == ConfigUpdateType.MANIFEST,False,36.78794411714425,N/A
"def update_manifest_branch(self, new_branch: str) -> ConfigStatusReturnCode:
<mask>:
        return ConfigStatusReturnCode.CANCEL
    if self.workspace.config.manifest_branch_0 != new_branch:
        rc_is_on_remote = remote_branch_exist(self.workspace.config.manifest_url, new_branch)
        if rc_is_on_remote == 0:
            return ConfigStatusReturnCode.SUCCESS
        else:
            return ConfigStatusReturnCode.NOT_FOUND
    else:
        return ConfigStatusReturnCode.REVERT",self.workspace.config.manifest_branch == new_branch,29,self.workspace.config.manifest_branch_0 == new_branch,False,74.47819789879651,N/A
"def __init__(self, statuses: Optional[CollectedStatuses]=None, w_repos: Optional[List[Repo]]=None, repos: Optional[List[Repo]]=None) -> None:
    self.mris: Dict[str, ManifestRepoItem] = {}
<mask>:
        return
    if statuses:
        self._statuses_to_mris(statuses, w_repos)
    if repos:
        self._repos_to_mris(repos)",bool(statuses) == bool(repos),24,not w_repos or not statuses or (not repos),False,8.054496384843702,N/A
"def _repos_to_mris(self, repos: Union[List[Repo], None]) -> None:
<mask>:
        for repo in repos:
            if repo.branch or repo.tag or repo.sha1:
                self.mris[repo.dest] = self._repo_to_mri(repo)
            else:
                ui.warning(f'Skipping empty Repo: {repo.dest}')",repos,26,repos,True,100.00000000000004,N/A
"def _status_to_mri(self, status: Union[Status, Exception], w_repo: Repo) -> ManifestRepoItem:
<mask>:
        return ManifestRepoItem(branch=status.git.branch, tag=status.git.tag, sha1=status.git.sha1_full, empty=status.git.empty, ignore_submodules=w_repo.ignore_submodules, remotes=w_repo.remotes, groups_considered=True, ahead=status.git.ahead, behind=status.git.behind)
    return ManifestRepoItem()","isinstance(status, Status) and status.git.empty is False",22,"isinstance(status, Status)",False,26.359713811572682,N/A
"def _statuses_to_mris(self, statuses: Union[CollectedStatuses, None], w_repos: Union[List[Repo], None]) -> None:
<mask>:
        for repo in w_repos:
            dest = repo.dest
            if isinstance(statuses[dest], Status):
                self.mris[dest] = self._status_to_mri(statuses[dest], repo)",statuses and w_repos,25,statuses is not None and w_repos is not None,False,24.808415001701817,N/A
"def _handle_repo(self, repo_config: Any) -> None:
    dest = repo_config['dest']
    branch = orig_branch = repo_config.get('branch')
    tag = repo_config.get('tag')
    sha1 = repo_config.get('sha1')
    url = repo_config.get('url')
    ignore_submodules = repo_config.get('ignore_submodules', False)
<mask>:
        origin = Remote(name='origin', url=url)
        remotes = [origin]
    else:
        remotes = self._handle_remotes(repo_config)
    repo = Repo(dest=dest, branch=branch, orig_branch=orig_branch, sha1=sha1, tag=tag, remotes=remotes, ignore_submodules=ignore_submodules)
    self._repos.append(repo)",url,49,repo_config.get('use_origin'),False,0.0,N/A
"def _handle_remotes(self, repo_config: Any) -> List[Remote]:
    remotes_config = repo_config.get('remotes')
    res = []
<mask>:
        for remote_config in remotes_config:
            remote = Remote(name=remote_config['name'], url=remote_config['url'])
            res.append(remote)
    return res",remotes_config,24,remotes_config,True,100.00000000000004,N/A
"def _handle_copies(self, repo_config: Any) -> None:
<mask>:
        return
    to_cp = repo_config['copy']
    for item in to_cp:
        src = item['file']
        dest = item.get('dest', src)
        copy = Copy(repo_config['dest'], src, dest)
        self.file_system_operations.append(copy)",'copy' not in repo_config,28,not repo_config['copy'],False,27.77619034011791,N/A
"def _handle_links(self, repo_config: Any) -> None:
<mask>:
        return
    to_link = repo_config['symlink']
    for item in to_link:
        source = item['source']
        target = item['target']
        link = Link(repo_config['dest'], source, target)
        self.file_system_operations.append(link)",'symlink' not in repo_config,27,"not repo_config.get('symlink', None)",False,15.851165692617148,N/A
"def _handle_groups(self, groups_config: Any, ignore_on_mtod: Optional[ManifestsTypeOfData]=None) -> None:
    elements = [repo.dest for repo in self._repos]
    self.group_list = GroupList(elements=elements)
<mask>:
        return
    for name, group_config in groups_config.items():
        elements = group_config['repos']
        includes = group_config.get('includes', [])
        self.group_list.add(name, elements, includes=includes, ignore_on_mtod=ignore_on_mtod)",not groups_config,36,not groups_config,True,100.00000000000004,N/A
"def get_local_future_manifests_manifest_and_repos(workspace: Workspace, gtf: GroupsToFind, must_find_all_groups: bool=False, use_same_future_manifest: bool=False) -> Tuple[Union[Manifest, None], Union[Dict[str, Repo], None], bool, GroupsToFind, bool]:
    path = workspace.root_path / '.tsrc' / 'future_manifest'
    path_to_m_file = path / 'manifest.yml'
    report_skip_fm_update: bool = False
    clone_all_repos = False
<mask>:
        clone_all_repos = True
    lfm = LocalManifest(path)
    if path.is_dir():
        if use_same_future_manifest is False or not path_to_m_file.is_file():
            lfm.update(url=workspace.config.manifest_url, branch=workspace.config.manifest_branch, show_output=False, show_cmd=False)
        else:
            report_skip_fm_update = True
    else:
        lfm.init(url=workspace.config.manifest_url, branch=workspace.config.manifest_branch, show_output=False, show_cmd=False)
    try:
        lfmm = lfm.get_manifest_safe_mode(ManifestsTypeOfData.FUTURE)
    except LoadManifestSchemaError as lmse:
        ui.warning(lmse)
        return (None, None, must_find_all_groups, gtf, False)
    mgr = ManifestGetRepos(workspace, lfmm, True, clone_all_repos)
    repos, must_find_all_groups, gtf = mgr.by_groups(gtf, must_find_all_groups=must_find_all_groups)
    dict_repos: Dict[str, Repo] = {}
    for repo in repos:
        dict_repos[repo.dest] = repo
    return (lfmm, dict_repos, must_find_all_groups, gtf, report_skip_fm_update)",workspace.config.clone_all_repos is True,111,path_to_m_file.is_dir(),False,4.368583925857938,N/A
"def _take_care_of__save_to(self) -> None:
<mask>:
        if self.args.save_to.is_dir() is True:
            self.args.save_to = self.args.save_to / 'manifest.yml'
        elif os.path.dirname(self.args.save_to) and os.path.isdir(os.path.dirname(self.args.save_to)) is False:
            raise Exception(f""'SAVE_TO' directory structure must exists, however '{os.path.dirname(self.args.save_to)}' does not"")
        if self.args.save_to.is_file() is True:
            if self.args.use_force is False and FinalOutputModeFlag.PREVIEW not in self.dmod.final_output_mode:
                raise Exception(f""'SAVE_TO' file exist, use '--force' to overwrite existing file, or use '--update-on {self.args.save_to}' instead"")
            else:
                self.dmod.final_output_mode.append(FinalOutputModeFlag.OVERWRITE)
                self.dmod.final_output_path_list.save_to_path = self.args.save_to
        else:
            self.dmod.final_output_mode.append(FinalOutputModeFlag.NEW)
            self.dmod.final_output_path_list.save_to_path = self.args.save_to",self.args.save_to,68,self.args.save_to is not None,False,63.894310424627285,N/A
"def pre_check_change(self, cfgud: ConfigUpdateData, cfguts: List[ConfigUpdateType]) -> Tuple[List[ConfigStatusReturnCode], List[ConfigUpdateType], bool]:
    cfgrcs: List[ConfigStatusReturnCode] = []
    config_tools = ConfigTools(self.workspace)
    found_some: bool = False
    for i, this_type in enumerate(cfguts):
<mask>:
            rc = config_tools.update_manifest_branch(cfgud.manifest_branch)
            if rc == ConfigStatusReturnCode.SUCCESS:
                found_some = True
            else:
                if StatusHeaderDisplayMode.BRANCH in self.shdms:
                    self._manifest_branch_report_issue(rc, cfgud.manifest_branch)
                no_further_display: bool = False
                if rc == ConfigStatusReturnCode.REVERT:
                    rc = ConfigStatusReturnCode.SUCCESS
                    found_some = True
                    no_further_display = True
                if rc == ConfigStatusReturnCode.CANCEL:
                    cfguts[i] = ConfigUpdateType.NONE
                    no_further_display = True
                if rc == ConfigStatusReturnCode.NOT_FOUND:
                    cfguts[i] = ConfigUpdateType.NONE
                if no_further_display is True:
                    self._no_further_display(StatusHeaderDisplayMode.BRANCH)
            cfgrcs.append(rc)
    return (cfgrcs, cfguts, found_some)",this_type == ConfigUpdateType.MANIFEST_BRANCH and cfgud.manifest_branch,88,this_type == StatusHeaderDisplayMode.MANIFEST,False,21.874242445215227,N/A
"def _no_further_display(self, mode: StatusHeaderDisplayMode) -> None:
    """"""replace given Status Header Display Mode
        by 'NONE', so it will not gets displayed""""""
<mask>:
        for index, shdm in enumerate(self.shdms):
            if shdm == mode:
                self.shdms[index] = StatusHeaderDisplayMode.NONE",mode in self.shdms,33,mode != StatusHeaderDisplayMode.NONE,False,9.652434877402245,N/A
"def _manifest_branch_report_issue(self, rc: ConfigStatusReturnCode, branch: Optional[str]=None) -> None:
    """"""report only issue, success will be reported elsewhere,
        everything else will be taken care of elsewhere""""""
<mask>:
        ui.info_2(""Such Manifest's branch:"", ui.green, branch, ui.reset, 'was not found on remote,', ui.red, 'ignoring', ui.reset)
        raise Error('aborting Manifest branch change')
    if rc == ConfigStatusReturnCode.CANCEL:
        branch_0 = self.workspace.config.manifest_branch_0
        if branch == branch_0:
            ui.info_2(""No change to Manifest's branch, it will still stays on:"", ui.green, branch, ui.reset)
        else:
            ui.info_2(""No update, Manifest's branch will still change from:"", ui.green, branch_0, ui.reset, '~~>', ui.green, branch, ui.reset)
    if rc == ConfigStatusReturnCode.REVERT:
        ui.info_2(""Reverting previous update, Manifest's branch will stays on:"", ui.green, branch, ui.reset)",rc == ConfigStatusReturnCode.NOT_FOUND,99,rc == ConfigStatusReturnCode.SUCCESS,False,54.44460596606694,N/A
"def manifest_branch_change(self, branch: str, branch_0: Union[str, None]) -> None:
    """"""report successful change""""""
<mask>:
        ui.info_2(""Accepting Manifest's branch change from:"", ui.green, branch_0, ui.reset, '~~>', ui.green, branch, ui.reset)",branch_0,25,branch_0 is not None,False,30.213753973567677,N/A
"def register_change(self, cfgud: ConfigUpdateData, cfguts: List[ConfigUpdateType]) -> bool:
    """"""this function should be called only once (as only once will work)""""""
<mask>:
        self._config_update_data = cfgud
        self._config_update_type = cfguts
        cs = ConfigStatus(self.workspace, self.shdms)
        try:
            found_some = False
            self._config_status_rc, self._config_update_type, found_some = cs.pre_check_change(cfgud, cfguts)
        except Error as e:
            ui.error(e)
            return False
        else:
            if found_some is True:
                self.shdms += [StatusHeaderDisplayMode.CONFIG_CHANGE]
            cs.proceed_to_change(cfgud, self._config_update_type)
    return True",not self._config_update_data,60,self._config_status_rc is None,False,41.11336169005198,N/A
"def display(self) -> None:
<mask>:
        'always check for this flag first, as it may introduce change\n            and such change may require pre-check and if that fails,\n            report may need to be produced'
        if self._config_update_type:
            self._config_change = True
    for shdm in self.shdms:
        if StatusHeaderDisplayMode.URL in shdm:
            self._header_manifest_url(self.url)
        if StatusHeaderDisplayMode.BRANCH in shdm:
            if self._config_update_data and self._config_update_data.manifest_branch and (ConfigUpdateType.MANIFEST_BRANCH in self._config_update_type):
                self._header_manifest_branch(self._config_update_data.manifest_branch, self.branch_0)
            else:
                self._header_manifest_branch(self.branch, self.branch_0)",StatusHeaderDisplayMode.CONFIG_CHANGE in self.shdms,63,not self._config_change,False,10.923299908191149,N/A
"def report_collecting(self, cw: int, cl: int=0, cb: int=0) -> None:
    """"""
        Properly display counters of Repos of various kind,
        that will be processed together, like:
        * 'cw' - count of Workspace Repos
        * 'cl' - count of leftovers Repo (both DM and FM)
        * 'cb' - count of temporary Bare Repos (both DM and FM)
        """"""
    cw_pl = self._is_plural(cw, 's')
    cl_pl = self._is_plural(cl, 's')
    cb_pl = self._is_plural(cb, 's')
    str_cw = f'{cw} workspace repo{cw_pl}'
    str_cl = f'{cl} leftovers repo{cl_pl}'
    str_cb = f'{cb} tmp bare repo{cb_pl}'
    start_pl = ''
<mask>:
        start_pl = self._is_plural(cw, 'es')
    elif cl > 0:
        start_pl = self._is_plural(cl, 'es')
    else:
        start_pl = self._is_plural(cb, 'es')
    str_out = ''
    if cw > 0 or cl > 0 or cb > 0:
        str_out = f'Collecting status{start_pl} of '
    if cw > 0:
        str_out += str_cw
        if cl > 0 or cb > 0:
            str_out += ' + '
    if cl > 0:
        str_out += str_cl
        if cb > 0:
            str_out += ' + '
    if cb > 0:
        str_out += str_cb
    if str_out:
        ui.info_1(str_out)",cw > 0,172,cw > 0,True,100.00000000000004,N/A
"def _is_plural(self, value: int, ext: str) -> str:
    """"""return plural defined by 'ext' when value > 1""""""
<mask>:
        return ext
    return ''",value > 1,22,value > 1,True,100.00000000000004,N/A
"def _header_manifest_branch(self, branch: str, branch_0: Union[str, None]) -> None:
<mask>:
        cs = ConfigStatus(self.workspace, self.shdms)
        cs.manifest_branch_change(branch, branch_0)
    else:
        self._header_manifest_branch_nc(branch, branch_0)
        pass",self._config_change is True,20,"self.config.getboolean('config.getboolean', 'enable_manifest_branch')",False,6.150343144231885,N/A
"def __init__(self, **kwargs: Any) -> None:
    names = {f.name for f in fields(self)}
    for key, value in kwargs.items():
<mask>:
            setattr(self, key, value)",key in names,22,key not in names,False,35.35533905932737,N/A
"@classmethod
def from_file(cls, cfg_path: Path) -> 'WorkspaceConfig':
    yaml = ruamel.yaml.YAML(typ='rt')
    parsed = yaml.load(cfg_path.read_text())
<mask>:
        'compatibility fix for older version.\n            usefull when transitioning with Workspace initialized\n            by older version'
        parsed['manifest_branch_0'] = parsed.get('manifest_branch')
        parsed = OrderedDict(sorted(parsed.items()))
    return cls(**parsed)",not parsed.get('manifest_branch_0'),36,parsed.get('manifest_branch'),False,48.59869096699083,N/A
"def repo_from_pcsrepo(st_m: PCSRepo) -> Union[Repo, None]:
<mask>:
        origin = Remote(st_m.origin, st_m.url)
        remotes = []
        remotes.append(origin)
        return Repo(dest=st_m.dest, remotes=remotes, branch=st_m.branch)
    return None",st_m.url and st_m.branch,21,"isinstance(st_m, Remote)",False,14.128386352314104,N/A
"def get_deep_manifest_pcsrepo(all_repos: List[Repo], m_url: str) -> Tuple[List[Repo], Union[PCSRepo, None]]:
    """"""Gets Deep Manifest properly.
    If you call this function from 'status', than
    you can ignore 1st returned value and just use the 2nd one""""""
    repos = []
    for repo in all_repos:
        repo_remotes = repo.remotes
        is_found = False
        for remote in repo_remotes:
<mask>:
                is_found = True
                break
        if is_found is True:
            repos += [repo]
            break
    dm = None
    if repos:
        dm = PCSRepo(repos[0].dest, repos[0].branch, url=m_url)
    return (repos, dm)","remote.url and remote_urls_are_same(remote.url, m_url) is True",76,remote.url == m_url,False,6.007726408001221,N/A
"def get_workspace_manifest_pcsrepo(statuses: Dict[str, StatusOrError], m_url: str) -> Union[PCSRepo, None]:
    for dest, status in statuses.items():
<mask>:
            for remote in status.manifest.repo.remotes:
                if remote_urls_are_same(remote.url, m_url) is True:
                    branch = None
                    if isinstance(status.git.branch, str):
                        branch = status.git.branch
                    return PCSRepo(dest=dest, branch=branch, url=m_url)
    return None","isinstance(status, Status)",39,"isinstance(status, StatusOrError)",False,53.7284965911771,N/A
"def is_manifest_in_workspace(workspace: Workspace, repos: List[Repo]) -> Union[PCSRepo, None]:
    for x in repos:
        this_dest = x.dest
        this_branch = x.branch
        for y in x.remotes:
<mask>:
                return PCSRepo(this_dest, this_branch, url=workspace.config.manifest_url)
    return None","y.url and remote_urls_are_same(y.url, workspace.config.manifest_url) is True",29,y.is_manifest_in_workspace(workspace),False,3.750628042880163,N/A
"def get_mtod_str(tod: ManifestsTypeOfData) -> str:
<mask>:
        return 'Local Manifest'
    if tod == ManifestsTypeOfData.DEEP:
        return 'Deep Manifest'
    if tod == ManifestsTypeOfData.DEEP_ON_UPDATE:
        return 'Deep Manifest on UPDATE'
    if tod == ManifestsTypeOfData.DEEP_BLOCK:
        return ""Deep Manifest's block""
    if tod == ManifestsTypeOfData.FUTURE:
        return 'Future Manifest'
    if tod == ManifestsTypeOfData.SAVED:
        return 'Saved Manifest'",tod == ManifestsTypeOfData.LOCAL,47,tod == ManifestsTypeOfData.LOCAL,True,100.00000000000004,N/A
"def mtod_get_main_color(tod: ManifestsTypeOfData) -> ui.Token:
<mask>:
        return ui.reset
    if tod == ManifestsTypeOfData.DEEP:
        return ui.purple
    if tod == ManifestsTypeOfData.DEEP_BLOCK:
        return ui.brown
    if tod == ManifestsTypeOfData.FUTURE:
        return ui.cyan
    return ui.reset",tod == ManifestsTypeOfData.LOCAL,28,tod == ManifestsTypeOfData.RESET,False,75.98356856515926,N/A
"def __init__(self, group_name: str, parent_group: Optional[Group[Any]]=None) -> None:
    self.group_name = group_name
    self.parent_group = parent_group
<mask>:
        message = f""Invalid include detected for '{self.parent_group.name}':\n""
    else:
        message = ''
    message += f""No such group: '{self.group_name}'""
    super().__init__(message)",self.parent_group,33,self.parent_group,True,100.00000000000004,N/A
"def add(self, name: str, elements: List[T], includes: Optional[List[str]]=None, ignore_on_mtod: Optional[ManifestsTypeOfData]=None) -> None:
    can_add: bool = True
    ignored_elements: List[T] = []
    for element in elements:
<mask>:
            if ignore_on_mtod:
                can_add = False
                if ignore_on_mtod != ManifestsTypeOfData.DEEP_ON_UPDATE:
                    ui.warning(f""{get_mtod_str(ignore_on_mtod)}: Groups: cannot add '{element}' to '{name}'."")
                self.missing_elements.append({name: element})
                ignored_elements.append(element)
            else:
                raise UnknownGroupElement(name, element)
    if can_add is False:
        elements = list(set(elements).difference(ignored_elements))
    self.groups[name] = Group(name, elements, includes=includes)",element not in self.all_elements,60,element not in self.missing_elements,False,59.4603557501361,N/A
"def _rec_get_elements(self, res: Dict[T, bool], group_names: List[str], *, parent_group: Optional[Group[T]], ignore_if_group_not_found: bool=False) -> None:
    for group_name in group_names:
<mask>:
            return
        if group_name not in self.groups:
            if ignore_if_group_not_found is True:
                continue
            raise GroupNotFound(group_name, parent_group=parent_group)
        group = self.groups[group_name]
        self._groups_seen.append(group.name)
        self._rec_get_elements(res, group.includes, parent_group=group)
        for element in group.elements:
            res[element] = True",group_name in self._groups_seen,47,self._groups_seen,False,51.341711903259224,N/A
"def __init__(self, args: argparse.Namespace) -> None:
    self.args = args
    self.gac: GroupsAndConstraints = get_group_and_constraints_data(args)
    self.mdo: ManifestDumpersOptions
    self.any_update = args.do_update or bool(args.update_on)
<mask>:
        self.mdo = ManifestDumpersOptions(delete_repo=not args.no_repo_delete)
    else:
        self.mdo = ManifestDumpersOptions()
    self.dmod = DumpManifestOperationDetails()
    self.s_m = SourceMode(args, self.dmod)
    self.dmod, self.args = self.s_m.get_source_mode_and_path()
    self.u_s = UpdateSource(args, self.dmod)
    self.dmod = self.u_s.get_update_source_and_path()
    self.dmod.manifest_data_options = self._get_manifest_data_options()
    self.f_o = FinalOutput(args, self.dmod)
    self.dmod = self.f_o.get_final_output_modes_and_paths()
    self.dmod = self._check_default_mode_and_path()
    self._take_care_of_common_warnings()",self.any_update is True,61,self.any_update,False,67.03200460356396,N/A
"def _get_manifest_data_options(self) -> ManifestDataOptions:
    mdo = ManifestDataOptions()
<mask>:
        raise Exception(""'--sha1-on' and '--sha1-off' are mutually exclusive"")
    elif self.args.sha1_on is True:
        mdo.sha1_on = True
    elif self.args.sha1_off is True:
        mdo.sha1_off = True
    if self.args.skip_manifest is True:
        mdo.skip_manifest = True
    if self.args.only_manifest is True:
        mdo.only_manifest = True
    if self.args.skip_manifest is True and self.args.only_manifest is True:
        raise Exception(""'--skip-manifest-repo' and '--only-manifest-repo' are mutually exclusive"")
    return mdo",self.args.sha1_on is True and self.args.sha1_off is True,60,self.args.sha1_on is True and self.args.sha1_off is True,True,100.00000000000004,N/A
"def _check_default_mode_and_path(self) -> DumpManifestOperationDetails:
<mask>:
        if self.dmod.final_output_path_list.default_path.is_file() is True:
            if self.args.use_force is False:
                raise Exception(f""such file '{self.dmod.final_output_path_list.default_path}' already exists, use '--force' to overwrite it"")
            else:
                self.dmod.final_output_mode.append(FinalOutputModeFlag.OVERWRITE)
        else:
            self.dmod.final_output_mode.append(FinalOutputModeFlag.NEW)
    return self.dmod",not self.dmod.final_output_mode and self.dmod.source_mode != SourceModeEnum.RAW_DUMP,30,self.dmod.final_output_path_list.default_path is not None,False,29.17256859057499,N/A
"def _take_care_of_common_warnings(self) -> None:
<mask>:
        ui.warning(""'SAVE_TO' path will be ignored when using '--preview'"")
    if self.args.save_to and self.args.update_on:
        ui.warning(""'SAVE_TO' path will be ignored when using '--update-on'"")
    if self.any_update is True and self.args.just_preview is True:
        ui.warning('When in preview mode, no actual update will be made')",self.args.save_to and self.args.just_preview is True,43,self.any_update and self.args.preview,False,24.904151315712156,N/A
"def consider_common_path(self, common_path: List[str]) -> DumpManifestOperationDetails:
    tmp_save_file = deepcopy(common_path)
    tmp_default_file = str(self.dmod.final_output_path_list.default_path).split(os.sep)
    tmp_save_file += tmp_default_file
    tmp_save_file_path = Path(os.sep.join(tmp_save_file))
<mask>:
        grab_save_path = tmp_save_file_path
        if grab_save_path.is_file():
            if FinalOutputModeFlag.PREVIEW in self.dmod.final_output_mode:
                return self.dmod
            if self.args.use_force is True:
                self.dmod.final_output_mode.append(FinalOutputModeFlag.OVERWRITE)
            else:
                raise Exception(f""Such file '{grab_save_path}' already exists, use '--force' if you want to overwrite it"")
        elif FinalOutputModeFlag.UPDATE not in self.dmod.final_output_mode:
            self.dmod.final_output_mode.append(FinalOutputModeFlag.NEW)
        self.dmod.final_output_path_list.common_path = grab_save_path
    return self.dmod",self.args.raw_dump_path and (not self.args.save_to),61,tmp_save_file_path.exists(),False,5.562171233991619,N/A
"def __init__(self, switch_config: Any) -> None:
    self._config: Optional[Any] = None
    self._groups: Optional[List[Any]] = None
<mask>:
        self._config = switch_config.get('config')
    if self._config:
        self._groups = self._config.get('groups')",switch_config,23,switch_config,True,100.00000000000004,N/A
"def process(self, index: int, count: int, repo: Repo) -> Outcome:
    """"""
        Clean each repo so it will be ready for next 'sync'
        """"""
    self.info_count(index, count, 'Cleaning', repo.dest)
    repo_path = self.workspace_path / repo.dest
    self.run_git(repo_path, 'clean', '-f', '-d')
<mask>:
        self.run_git(repo_path, 'clean', '-f', '-X', '-d')
    return Outcome.empty()",self.do_hard_clean is True,44,not self.dryrun,False,9.153013214364877,N/A
"def get_update_source_and_path(self) -> DumpManifestOperationDetails:
    self._possible_mismatch_on_dump_path()
    self._allow_only_1_update_on_a_time()
<mask>:
        if self.dmod.source_mode == SourceModeEnum.WORKSPACE_DUMP or self.dmod.source_mode == SourceModeEnum.RAW_DUMP:
            self._get_dm_load_path()
    elif self.args.update_on:
        self._update_on_file_must_exist()
        self.dmod.update_source = UpdateSourceEnum.FILE
        self.dmod.update_source_path = self.args.update_on
        self.dmod.final_output_path_list.update_on_path = self.args.update_on
        self.dmod.final_output_mode.append(FinalOutputModeFlag.UPDATE)
    return self.dmod",self.args.do_update is True,31,self.args.dm_load,False,32.66828640925501,N/A
"def _possible_mismatch_on_dump_path(self) -> None:
<mask>:
        dump_path = self.args.raw_dump_path
        if self.args.raw_dump_path.is_absolute() is False:
            dump_path = os.getcwd() / self.args.raw_dump_path
        if self.args.workspace_path:
            if self.args.workspace_path.is_absolute() is False:
                root_path = os.getcwd() / self.args.workspace_path
            else:
                root_path = self.args.workspace_path
        else:
            root_path = os.getcwd()
        if os.path.normpath(dump_path) != os.path.normpath(root_path):
            if self.args.use_force is False:
                raise Exception(""Please consider again what you are trying to do.\nYou want to update Manifest in the Workspace by RAW dump, yet you want to start dump not from Workspace root.\nThis may lead to strange Manifest.\nIf you are still sure that this is what you want, use '--force'."")",self.args.raw_dump_path and self.args.do_update is True and (self.args.just_preview is False),91,self.args.raw_dump_path,False,8.677432947392926,N/A
"def _allow_only_1_update_on_a_time(self) -> None:
<mask>:
        raise Exception(""Use only one out of '--update' or '--update-on' at a time"")",self.args.do_update is True and self.args.update_on,17,self.update and self.update_on,False,19.778518936723607,N/A
"def _update_on_file_must_exist(self) -> None:
<mask>:
        if self.args.update_on.is_file() is False:
            raise Exception(""'UPDATE_AT' file does not exists"")",self.args.update_on,15,self.args.update_on is not None,False,63.894310424627285,N/A
"def _get_dm_load_path(self) -> None:
    dm_is_dirty: bool = False
    gtf = GroupsToFind(self.args.groups)
    dm = None
<mask>:
        dm, _ = get_deep_manifest_from_local_manifest_pcsrepo(self.dmod.workspace, gtf)
        if dm:
            self.dmod.update_source_path = self.dmod.workspace.root_path / dm.dest / 'manifest.yml'
            gits = GitStatus(self.dmod.workspace.root_path / dm.dest)
            gits.update()
            dm_is_dirty = gits.dirty
            if dm_is_dirty is True:
                _, out_stat = run_git_captured(self.dmod.workspace.root_path / dm.dest, 'status', '--porcelain=1', 'manifest.yml', check=False)
                if out_stat == '':
                    dm_is_dirty = False
    if dm_is_dirty is True and self.args.use_force is False and (not self.args.save_to) and (self.args.just_preview is False):
        raise Exception(""not updating Deep Manifest as it is dirty, use '--force' to overide or '--save-to' somewhere else"")
    if self.dmod.update_source_path:
        ui.info_2('Loading Deep Manifest from', self.dmod.update_source_path)
        self.dmod.final_output_path_list.update_on_path = self.dmod.update_source_path
        self.dmod.update_source = UpdateSourceEnum.DEEP_MANIFEST
        self.dmod.final_output_mode.append(FinalOutputModeFlag.UPDATE)
    else:
        raise Exception('Cannot obtain Deep Manifest from Workspace to update')",self.dmod.workspace,115,gtf,False,0.0,N/A
"def found_this(self, this_group: str) -> None:
    """"""mark single group as found""""""
<mask>:
        if this_group not in self.found_groups:
            self.found_groups.append(this_group)",self.groups,18,self.group_type == 'single',False,11.044795567078939,N/A
"def found_these(self, this_found_groups: List[str]) -> None:
    """"""mark entire list of groups as found""""""
<mask>:
        self.found_groups = list(set(self.found_groups + this_found_groups))
    else:
        self.found_groups = this_found_groups",self.found_groups,23,"isinstance(this_found_groups, list)",False,15.619699684601283,N/A
"def was_found(self, this_group: str) -> bool:
    """"""check only single group whether it was found""""""
<mask>:
        return True
    return False",this_group in self.found_groups,19,self.group_name == this_group,False,20.556680845025987,N/A
"def all_found(self) -> Tuple[bool, List[str]]:
    """"""checks if we have found all groups""""""
<mask>:
        missing_groups = list(set(self.groups).difference(self.found_groups))
    else:
        return (True, [])
    if missing_groups:
        return (False, missing_groups)
    return (True, [])",self.groups,28,self.found_groups,False,23.643540225079384,N/A
"def by_groups(self, gtf: GroupsToFind, must_find_all_groups: bool=False) -> Tuple[List[Repo], bool, GroupsToFind]:
    self.gtf = gtf
    self.must_find_all_groups = must_find_all_groups
    repos: List[Repo] = []
<mask>:
        repos = self._with_groups()
    else:
        repos = self._without_groups()
    return (repos, self.must_find_all_groups, self.gtf)",self.gtf.groups,32,self.gtf.groups,True,100.00000000000004,N/A
"def _with_groups(self) -> List[Repo]:
    m_group_items = []
<mask>:
        groups_for_m = list(set(self.gtf.groups).intersection(self.manifest.group_list.groups))
        self._with_groups_missing_group(groups_for_m)
        if self.must_find_all_groups is True:
            self.must_find_all_groups = False
        if groups_for_m:
            self.gtf.found_these(groups_for_m)
        m_group_items = list(self.manifest.group_list.get_elements(groups_for_m))
    else:
        for i in self.manifest.get_repos(all_=True):
            m_group_items.append(i.dest)
    if self.on_manifest_only is False:
        return self._with_groups_consider_local(m_group_items)
    repos: List[Repo] = []
    for _, item in enumerate(m_group_items):
        repos.append(self.manifest.get_repo(item))
    return repos",self.gtf.groups and self.manifest.group_list and self.manifest.group_list.groups,49,self.manifest.group_list.groups,False,21.107208779109033,N/A
"@catch_manifest_group_not_found
def _with_groups_missing_group(self, groups_for_m: List[str]) -> None:
<mask>:
        missing_groups: List[str] = []
        missing_groups = list(set(self.gtf.groups).difference(groups_for_m))
        if missing_groups and self.must_find_all_groups is True:
            for missing_group in missing_groups:
                if self.gtf.was_found(missing_group) is False:
                    raise ManifestGroupNotFound(missing_group)",self.gtf.groups,31,len(groups_for_m) > 0,False,4.196114906296549,N/A
"def _with_groups_consider_local(self, m_group_items: List[str]) -> List[Repo]:
<mask>:
        groups_for_w = list(set(self.gtf.groups).intersection(self._local_m.group_list.groups))
        w_group_items = self._local_m.group_list.get_elements(groups_for_w)
    else:
        return self._local_m.get_repos(all_=True)
    repos: List[Repo] = []
    found_items = list(set(w_group_items).intersection(m_group_items))
    found_items_wd = found_items + m_group_items
    found_items = list(set(found_items_wd))
    for item in found_items:
        repos.append(self.manifest.get_repo(item))
    return repos",self.gtf.groups and self._local_m.group_list and self._local_m.group_list.groups,38,self._local_m.group_list,False,14.956861922263514,N/A
"def _without_groups(self) -> List[Repo]:
    found_items = []
<mask>:
        return self.manifest.get_repos(all_=True)
    else:
        if self.on_manifest_only is True:
            return self.manifest.get_repos(all_=True)
        if self.workspace.config.repo_groups:
            m_group_items: List[str] = []
            if self.manifest.group_list and self.manifest.group_list.groups:
                m_group_items = self.manifest.group_list.get_elements(list(self.workspace.config.repo_groups), ignore_if_group_not_found=True)
            else:
                for repo in self.manifest.get_repos(all_=True):
                    m_group_items.append(repo.dest)
        else:
            return self.manifest.get_repos(all_=True)
        if self._local_m.group_list and self._local_m.group_list.groups:
            w_group_items = self._local_m.group_list.get_elements(self.workspace.config.repo_groups, ignore_if_group_not_found=self.gtf.ignore_missing_groups)
        else:
            return self._local_m.get_repos(all_=True)
        found_items = list(set(w_group_items).intersection(m_group_items))
        found_items_wd = found_items + m_group_items
        found_items = list(set(found_items_wd))
    repos: List[Repo] = []
    for item in found_items:
        repos.append(self.manifest.get_repo(item))
    return repos",self.clone_all_repos,73,self.on_manifest_only is False,False,11.339582221952005,N/A
"def on_update(self, y: Union[Dict, List], mris: Dict[str, ManifestRepoItem], workspace: Union[Workspace, None], mdo: ManifestDataOptions, opt: ManifestDumpersOptions, gac: GroupsAndConstraints) -> Tuple[Union[Dict, List], bool]:
    """"""
<mask>:

        * Find out if there are some Repos, that should be
        renamed (instead of del(old)+add(new))

        * Apply constraints like Groups, regexes

        Update Repo records in YAML:
        * 1st delete Repo(s) that does not exists
            * also delete Group item
        * 2nd update such Repo(s) that does exists
        * 3rd add new Repo(s) that was not updated

        * finaly return something that can be dumped as YAML
        """"""
    is_updated: bool = False
    u_m = Manifest()
    u_m.apply_config(y, ignore_on_mtod=ManifestsTypeOfData.DEEP)
    repos = resolve_repos_without_workspace(u_m, gac)
    this_m_repo: Optional[Repo] = None
    if workspace:
        repos, this_m_repo = self.filter_repos_bo_manifest(workspace, mdo.skip_manifest, mdo.only_manifest, repos)
    tmp_is_updated: bool
    tmp_is_updated, repos = self._rename_update_source_based_on_dump_source(y, mris, repos)
    is_updated |= tmp_is_updated
    repos, ignored_repos_dests = self._filter_by_groups_and_constraints(y, gac, repos)
    repos_dests: List[str] = [repo.dest for repo in repos]
    ds_rs: List[str] = list(mris.keys())
    us_rs: List[str] = []
    self._walk_yaml_get_repos_keys(y, 0, us_rs, False)
    if this_m_repo:
        if mdo.skip_manifest is True:
            if this_m_repo.dest in us_rs:
                us_rs.remove(this_m_repo.dest)
    is_constrained: bool = self._is_constrained(us_rs, repos, this_m_repo)
    a_rs: List[str]
    d_rs: List[str] = []
    u_rs: List[str]
    if is_constrained is True:
        a_rs = list(set(ds_rs).difference(us_rs).intersection(repos_dests + ignored_repos_dests))
        u_rs = list(set(ds_rs).intersection(us_rs).intersection(repos_dests))
        if opt.delete_repo is True:
            d_rs = list(set(us_rs).difference(ds_rs).intersection(repos_dests))
    else:
        a_rs = list(set(ds_rs).difference(us_rs))
        u_rs = list(set(ds_rs).intersection(us_rs))
        if opt.delete_repo is True:
            d_rs = list(set(us_rs).difference(ds_rs))
    is_updated_tmp: List[bool] = [False]
    self._walk_yaml_delete_group_items(y, 0, False, False, d_rs, is_updated_tmp)
    is_updated |= is_updated_tmp[0]
    is_updated_tmp[0] = False
    self._walk_yaml_delete_repos_items(y, 0, False, d_rs, is_updated_tmp)
    is_updated |= is_updated_tmp[0]
    is_updated_tmp[0] = False
    self._walk_yaml_update_repos_items(y, 0, mris, mdo, False, u_rs, is_updated_tmp)
    is_updated |= is_updated_tmp[0]
    is_updated_tmp[0] = False
    self._walk_yaml_add_repos_items(y, 0, mris, mdo, False, a_rs, is_updated_tmp)
    is_updated |= is_updated_tmp[0]
    return (y, is_updated)",we want to UPDATE existing manifest,264,y.get('name') == 'Repo',False,0.0,N/A
"def filter_repos_bo_manifest(self, workspace: Workspace, is_skip: bool, is_only: bool, repos: List[Repo]) -> Tuple[List[Repo], Optional[Repo]]:
    m_repos, _ = get_deep_manifest_pcsrepo(repos, workspace.config.manifest_url)
    x_repo: Optional[Repo] = None
<mask>:
        if is_skip is True:
            x_repo = deepcopy(m_repos[0])
            repos.remove(m_repos[0])
        elif is_only is True:
            repos = m_repos
    elif is_only is True:
        repos = []
    return (repos, x_repo)",m_repos and m_repos[0] in repos,48,m_repos,False,4.9787068367863965,N/A
"def _is_constrained(self, us_rs: List[str], repos: List[Repo], this_m_repo: Optional[Repo]) -> bool:
    for dest in us_rs:
        is_found: bool = False
        for repo in repos:
<mask>:
                is_found = True
                break
        if is_found is False:
            if this_m_repo and dest == this_m_repo.dest:
                continue
            return True
    return False",repo.dest == dest,42,dest == repo.dest,False,50.81327481546149,N/A
"def _filter_by_groups_and_constraints(self, y: Union[Dict, List], gac: GroupsAndConstraints, repos: List[Repo]) -> Tuple[List[Repo], List[str]]:
    ignored_repos_dests: List[str] = []
<mask>:
        o_repos: List[Repo] = []
        u_m = Manifest()
        u_m.apply_config(y, ignore_on_mtod=ManifestsTypeOfData.DEEP_ON_UPDATE)
        m_groups: List[str] = []
        if u_m.group_list:
            for gr in u_m.group_list.groups:
                if gr in gac.groups:
                    m_groups.append(gr)
            for e in u_m.group_list.get_elements(m_groups):
                o_repos.append(u_m.get_repo(e))
        repos = o_repos
        if u_m.group_list and u_m.group_list.missing_elements:
            for mi in u_m.group_list.missing_elements:
                for k_r_d, i_r_d in mi.items():
                    if gac.groups and k_r_d in gac.groups:
                        if is_match_repo_dest_on_inc_excl(gac, i_r_d) is True:
                            ignored_repos_dests.append(i_r_d)
    repos = resolve_repos_apply_constraints(repos, gac)
    return (repos, ignored_repos_dests)",gac.groups,81,"isinstance(y, dict)",False,0.0,N/A
"def _get_dump_url_dict(self, mris: Dict[str, ManifestRepoItem]) -> 'OrderedDict[str, str]':
    dump_urls_dict: OrderedDict[str, str] = OrderedDict()
    for k, v in mris.items():
<mask>:
            for remote in v.remotes:
                dump_urls_dict[remote.url] = k
    return dump_urls_dict",v.remotes,28,"isinstance(v, ManifestRepoItem)",False,8.116697886877475,N/A
"def len_of_cli_ui(ui_tokens: List[ui.Token]) -> int:
    len_: int = 0
    for i in ui_tokens:
<mask>:
            len_ += len(i) + 1
    if len_ > 0:
        len_ -= 1
    return len_","isinstance(i, str)",28,i.is_cli_ui(),False,6.27465531099474,N/A
"def align_left(l_just: int, l_str: str) -> List[ui.Token]:
    str_: str = ''
<mask>:
        str_ = ' '
    if l_just > 1:
        str_ = ' '.ljust(l_just)
    return [str_ + l_str]",l_just == 1,28,l_just < 1,False,34.98330125272253,N/A
"def process(self, index: int, count: int, repo: Repo) -> Outcome:
    repo_path: Optional[Path] = repo._grabbed_from_path
<mask>:
        if is_git_repository(repo_path) is False:
            return Outcome.empty()
        gits = GitStatus(repo_path)
        gits.update()
        gitr = GitRemote(repo_path, repo.branch)
        gitr.update()
        if not gitr.remotes:
            ui.warning(f""No remote found for: '{repo.dest}' (path: '{repo_path}')"")
        self.repos.append(Repo(dest=repo.dest, branch=gits.branch, keep_branch=True, is_default_branch=False, orig_branch=gits.branch, sha1=gits.sha1, sha1_full=gits.sha1_full, tag=gits.tag, remotes=gitr.remotes, _grabbed_ahead=gits.ahead, _grabbed_behind=gits.behind))
    return Outcome.empty()",repo_path,53,repo_path is not None,False,30.213753973567677,N/A
"def __post_init__(self) -> None:
<mask>:
        object.__setattr__(self, 'branch', 'master')
        object.__setattr__(self, 'is_default_branch', True)",not self.branch and self.keep_branch is False,11,"not hasattr(self, 'branch')",False,3.823246852690463,N/A
"def describe_to_tokens(self, ljust: int=0, mtod: ManifestsTypeOfData=ManifestsTypeOfData.LOCAL, bare_dm_status: Optional[GitBareStatus]=None) -> Tuple[List[ui.Token], List[ui.Token]]:
    """"""returns:
        1st list: is properly left-align for print
        2nd list: is NOT align. it is for 1:1 comparsion""""""
    sha1: str = ''
<mask>:
        sha1 = self.sha1[:7]
    present_dtt: List[DescribeToTokens] = []
    if self.branch and (self.is_default_branch is False or (not self.sha1 and (not self.tag))):
        present_dtt.append(DescribeToTokens.BRANCH)
    elif self.sha1:
        present_dtt.append(DescribeToTokens.SHA1)
    if self.tag:
        present_dtt.append(DescribeToTokens.TAG)
    if self.sha1:
        if DescribeToTokens.SHA1 not in present_dtt:
            present_dtt.append(DescribeToTokens.POSITION)
    if not self.remotes:
        if mtod == ManifestsTypeOfData.DEEP or mtod == ManifestsTypeOfData.FUTURE:
            present_dtt.append(DescribeToTokens.MISSING_REMOTES)
    if not present_dtt:
        present_dtt.append(DescribeToTokens.NONE)
    return self._describe_to_token_output(present_dtt, ljust, mtod, sha1, bare_dm_status)",self.sha1,90,self.sha1 and self.sha1[-7],False,15.619699684601283,N/A
"def _describe_to_token_output(self, present_dtt: List[DescribeToTokens], ljust: int, mtod: ManifestsTypeOfData, sha1: str, bare_dm_status: Optional[GitBareStatus]=None) -> Tuple[ui.Token, ui.Token]:
    cb = ui.green
    cs = ui.red
    ct = ui.brown
<mask>:
        cb = cs = mtod_get_main_color(mtod)
    res: List[ui.Token] = []
    able: List[ui.Token] = []
    last_element: DescribeToTokens = present_dtt[-1]
    for e in present_dtt:
        this_ljust: int = 0
        if e == last_element:
            this_ljust = ljust
        if e == DescribeToTokens.BRANCH and self.branch:
            res += [cb, self.branch.ljust(this_ljust), ui.reset]
            able += [ui.green, self.branch, ui.reset]
            ljust -= len(self.branch) + 1
        elif e == DescribeToTokens.SHA1:
            res += [cs, sha1.ljust(this_ljust), ui.reset]
            able += [ui.red, sha1, ui.reset]
            ljust -= len(sha1) + 1
        elif e == DescribeToTokens.TAG and self.tag:
            res += [ct, 'on', self.tag.ljust(this_ljust - 3), ui.reset]
            able += [ui.brown, 'on', self.tag, ui.reset]
            ljust -= len(self.tag) + 1 + 2 + 1
        elif e == DescribeToTokens.POSITION:
            if mtod == ManifestsTypeOfData.DEEP or mtod == ManifestsTypeOfData.FUTURE:
                if bare_dm_status:
                    tmp_res, tmp_able, ljust = bare_dm_status.describe_position(ljust, self.sha1)
                    res += tmp_res
                    able += tmp_able
                elif self.sha1:
                    res += [ui.red, f'?? {sha1}'.ljust(this_ljust), ui.reset]
                    able += [ui.red, f'?? {self.sha1}', ui.reset]
                    ljust -= 3 + 7 + 1
                else:
                    res += [ui.red, '?? commit'.ljust(this_ljust), ui.reset]
                    able += [ui.red, '?? commit', ui.reset]
                    ljust -= 9 + 1
        elif e == DescribeToTokens.MISSING_REMOTES:
            res += [ui.red, '(missing remote)'.ljust(this_ljust), ui.reset]
            able += [ui.red, '(missing remote)', ui.reset]
            ljust -= 16 + 1
        else:
            res += [' '.ljust(this_ljust)]
            able += [' ']
    return (res, able)",mtod == ManifestsTypeOfData.DEEP or mtod == ManifestsTypeOfData.FUTURE,224,mtod != ManifestsTypeOfData.NONE,False,10.110263558057797,N/A
"def get_source_mode_and_path(self) -> Tuple[DumpManifestOperationDetails, argparse.Namespace]:
    self._respect_workspace_path()
    self._decide_source_mode()
    self._get_workspace_if_needed()
<mask>:
        self._get_workspace_optionally()
    return (self.dmod, self.args)",not self.dmod.workspace,13,self.args.workspace,False,24.736929544091932,N/A
"def _decide_source_mode(self) -> None:
<mask>:
        self.dmod.source_mode = SourceModeEnum.RAW_DUMP
        self.dmod.source_path = self.args.raw_dump_path
    else:
        self.dmod.source_mode = SourceModeEnum.WORKSPACE_DUMP",self.args.raw_dump_path,15,self.args.raw_dump_path,True,100.00000000000004,N/A
"def _get_workspace_optionally(self) -> None:
<mask>:
        try:
            self.dmod.workspace = get_workspace_with_repos(self.args)
        except Exception as e:
            if isinstance(e, Error):
                pass
            else:
                raise e",self.args.raw_dump_path and (self.args.skip_manifest is True or self.args.only_manifest is True),20,not self.dmod.workspace,False,0.2785299174698855,N/A
"def __init__(self, mtod: ManifestsTypeOfData) -> None:
<mask>:
        msg = 'Failed to get Deep Manifest'
    elif mtod == ManifestsTypeOfData.FUTURE:
        msg = 'Failed to get Future Manifest'
    else:
        msg = 'Failed to get Manifest'
    super().__init__(msg)",mtod == ManifestsTypeOfData.DEEP,33,mtod == ManifestsTypeOfData.Deep,False,75.98356856515926,N/A
"def update(self, git_status: GitStatus, git_remote: Union[GitRemote, None]) -> None:
    """"""Set self.incorrect_branch if the local git status
        does not match the branch set in the manifest.
        """"""
    expected_branch = self.repo.branch
    actual_branch = git_status.branch
<mask>:
        self.incorrect_branch = (actual_branch, expected_branch)
    if git_remote:
        self.missing_upstream = not git_remote.upstreamed
        self.git_remote = git_remote",actual_branch and expected_branch and (actual_branch != expected_branch),46,actual_branch != expected_branch,False,28.650479686019022,N/A
"def test_proc_data():
    print(os.getcwd())
<mask>:
        os.makedirs('./tests/Data/Output/')
    proc_embedding(input_file='./tests/Data/Input/embedding_example.txt', output_path='./tests/Data/Output/')
    with open('./tests/Data/Output/embedding.pkl', 'rb') as f:
        embedding = pickle.load(f)
    words = list(embedding.keys())
    example_data_2000 = gen_testing_data(embedding=embedding, from_year=2000, to_year=2010, type=0, seed=1)
    example_data_2000.to_pickle('./tests/Data/Output/2000_2010.pkl')
    example_data_2011 = gen_testing_data(embedding=embedding, from_year=2011, to_year=2012, type=1, seed=2)
    example_data_2011.to_pickle('./tests/Data/Output/2011.pkl')
    proc_pd(input='./tests/Data/Output/2000_2010.pkl', create=1, seed=1, k_fold=10, output='./tests/Data/Output/database.db', embedding='./tests/Data/Output/')
    proc_pd(input='./tests/Data/Output/2011.pkl', create=0, seed=2, k_fold=10, output='./tests/Data/Output/database.db', embedding='./tests/Data/Output/')
    run_pci_model(year_target=2010, mt_target=3, i=1, gpu=-1, model='testing', root='./tests/', T=0.01, discount=0.05, bandwidth=0.2)
    run_pci_model(year_target=2010, mt_target=4, i=1, gpu=-1, model='testing', root='./tests/', T=0.01, discount=0.05, bandwidth=0.2)
    run_pci_model(year_target=2011, mt_target=1, i=2, gpu=-1, model='testing', root='./tests/', T=0.01, discount=0.05, bandwidth=0.2)
    compile_model_results('testing', root='./tests')
    create_text_output('testing', '2011_M1', gpu='-1', root='./tests/')",not os.path.exists('./tests/Data/Output/'),79,not os.path.exists('./tests/Data/Output/'),True,100.00000000000004,N/A
"def gen_candidate(x, bandwidth=0.1, type='int', min_value=None, max_value=None):
    r = random.uniform(-bandwidth, bandwidth)
    new_x = x * (1 + r)
<mask>:
        if x * bandwidth < 1:
            new_x = x + random.choice([-1, 0, 1])
        else:
            new_x = round(new_x)
    if min_value != None:
        new_x = max(new_x, min_value)
    if max_value != None:
        new_x = min(new_x, max_value)
    return new_x",type == 'int',53,type == 'int',True,100.00000000000004,N/A
"def calc_prev_month(year, month, period):
<mask>:
        if month == 1:
            return (year - 1, 12)
        else:
            return (year, month - 1)
    else:
        y, q = calc_prev_month(year, month, 1)
        return calc_prev_month(y, q, period - 1)",period == 1,33,period == 1,True,100.00000000000004,N/A
"def build_output_folder_structure(year_target, mt_target, models_path, create=True):
<mask>:
        os.makedirs(models_path)
    output_folder = models_path + str(year_target) + '_M' + str(mt_target) + '/'
    history_folder = output_folder + '/history/'
    if create:
        if not os.path.exists(output_folder):
            os.makedirs(output_folder)
        if not os.path.exists(history_folder):
            os.makedirs(history_folder)
    return (history_folder, output_folder)",not os.path.exists(models_path),36,not os.path.exists(models_path),True,100.00000000000004,N/A
"def compile_model_results(model, root='./'):
    listing = glob.glob(root + '/models/' + model + '/*/best_pars.pkl')
    dic_list = []
    for file in listing:
        tmp = hyper_parameters_load(file)
        dic_list.append(tmp.to_dictionary())
    df = pd.DataFrame(dic_list)
    df['diff'] = df.test_F1 - df.forecast_F1
    df['pci'] = abs(df.test_F1 - df.forecast_F1)
<mask>:
        os.makedirs(root + '/figures/' + model)
    df.to_csv(root + '/figures/' + model + '/results.csv', index=False)
    return df",not os.path.exists(root + '/figures/' + model),52,not os.path.exists(root + '/figures/' + model),True,100.00000000000004,N/A
"def update_weight(self):
    self.W[0] = self.y_prop[0]
<mask>:
        self.W[1] = self.y_prop[1] * self.hyper_pars.varirate['w']
    else:
        self.W[1] = self.y_prop[0]",self.hyper_pars.varirate['w'] > 0,15,self.hyper_pars.varirate,False,48.95416595569534,N/A
"def save(self, folder, file=None):
<mask>:
        file = self.fixed['mod_id'] + '.pkl'
    with open(folder + file, 'wb') as f:
        pickle.dump(self, f)",file == None,19,file is None,False,24.840753130578644,N/A
"def proc_embedding(input_file, output_path):
    print('Reading embedding file')
    embedding_raw = pd.read_csv(input_file, delim_whitespace=True, header=None, skiprows=1, quoting=3)
    dim_embedding = embedding_raw.shape[1] - 1
    embedding = {}
    for index, i in embedding_raw.iterrows():
        word = i[0]
        coefs = i[1:]
        embedding[word] = coefs
    with open(output_path + '/embedding.pkl', 'wb') as f:
        pickle.dump(embedding, f)
    print('Preparing tokenizer')
    all_text = [*embedding, 'unk']
    all_text = [i for i in all_text if type(i) is not float]
    tokenizer = Tokenizer(filters='')
    tokenizer.fit_on_texts(all_text)
    with open(output_path + '/tokenizer.pkl', 'wb') as f:
        pickle.dump(tokenizer, f)
    word_index = tokenizer.word_index
    embedding_matrix = np.zeros((len(word_index) + 1, dim_embedding))
    for word, i in word_index.items():
        embedding_vector = embedding.get(word)
<mask>:
            embedding_matrix[i] = embedding_vector
    new_vec = np.zeros((embedding_matrix.shape[0], 1))
    new_vec[word_index.get('unk')] = 1
    embedding_matrix = np.concatenate((new_vec, embedding_matrix), axis=1)
    with open(output_path + '/embedding_matrix.pkl', 'wb') as f:
        pickle.dump(embedding_matrix, f)",embedding_vector is not None,117,embedding_vector is not None,True,100.00000000000004,N/A
"def run(self):
    """"""
        Copy the required directory to the build directory and super().run()
        """"""
    self.announce('Moving scripts files', level=3)
    self.skip_build = True
    bin_dir = self.distribution.bin_dir
    scripts_dirs = [os.path.join(bin_dir, _dir) for _dir in os.listdir(bin_dir) if os.path.isdir(os.path.join(bin_dir, _dir))]
    for scripts_dir in scripts_dirs:
        dst_dir = os.path.join(self.build_dir, os.path.basename(scripts_dir))
<mask>:
            if os.path.isdir(dst_dir):
                shutil.rmtree(dst_dir)
            elif os.path.isfile(dst_dir):
                os.remove(dst_dir)
        shutil.copytree(scripts_dir, os.path.join(self.build_dir, os.path.basename(scripts_dir)))
    self.distribution.scripts = scripts_dirs
    super().run()",os.path.exists(dst_dir),57,os.path.exists(dst_dir),True,100.00000000000004,N/A
"def run(self):
    """"""
        Perform build_cmake before doing the 'normal' stuff
        """"""
    os.makedirs(str(pathlib.Path(self.build_temp).absolute()), exist_ok=True)
    for extension in self.extensions:
        extension_path = pathlib.Path(self.get_ext_fullpath(extension.name))
<mask>:
            self.announce(f'Preparing the build environment for CMake extension: ""{extension.name}""', level=3)
            os.makedirs(str(extension_path.parent.absolute()), exist_ok=True)
        if extension.name == 'bpy':
            if self.bpy_prebuilt:
                self.announce(f'Using supplied prebuilt path {self.bpy_prebuilt}', level=3)
                self.copy_bpy(self.bpy_prebuilt, extension_path)
            else:
                git_checkout_path = pathlib.Path(os.path.join(self.build_temp, 'blender'))
                build_path = pathlib.Path(os.path.join(self.build_temp, 'build'))
                os.makedirs(str(git_checkout_path), exist_ok=True)
                os.makedirs(str(build_path), exist_ok=True)
                self.build_bpy(git_checkout_path, self.build_temp, build_path)
    super().run()","isinstance(extension, CMakeExtension)",63,extension_path.parent,False,8.745825313180626,N/A
"def build_bpy(self, git_checkout_path: pathlib.Path, svn_checkout_path: pathlib.Path, build_path: pathlib.Path):
    """"""
        The steps required to build the extension
        """"""
    import bpybuild.sources
    import bpybuild.make
    self.announce('Searching for compatible Blender online (this will take a while)', level=3)
    compatible_bpy = bpybuild.sources.get_compatible_sources()
<mask>:
        raise Exception(f'{VERSION} bpy is not compatible with {SYSTEM_OS_NAME} Python {sys.version} {bpybuild.BITNESS}bit')
    self.announce(f'Found compatible Blender version {VERSION}', level=3)
    git_repo = compatible_bpy[VERSION_TUPLE][0][0]
    svn_repo = compatible_bpy[VERSION_TUPLE][1][0]
    self.announce('Cloning Blender source from git (this will take a while)', level=3)
    git_repo.checkout(git_checkout_path)
    self.announce('Cloning precompiled libs from svn (this will take a while)', level=3)
    svn_repo.checkout(svn_checkout_path)
    self.announce('Configuring cmake project and building binaries (this will take a while)', level=3)
    for command in bpybuild.make.get_make_commands(source_location=git_checkout_path, build_location=build_path, cmake_configure_args=self.cmake_configure_args):
        self.spawn(command)",not VERSION_TUPLE in compatible_bpy,102,not compatible_bpy,False,23.50540321304655,N/A
"def copy_bpy(self, source_path: pathlib.Path, dest_path: pathlib.Path):
    """"""
        Move the bpy files
        """"""
    self.announce('Searching for Blender python module', level=3)
    bpy_canidates = [os.path.join(source_path, _bpy) for _bpy in os.listdir(source_path) if os.path.isfile(os.path.join(source_path, _bpy)) and os.path.splitext(_bpy)[0].startswith('bpy') and (os.path.splitext(_bpy)[1] in ['.pyd', '.so'])]
<mask>:
        raise Exception(f'Could not find Blender python module in {source_path}')
    bpy_path = bpy_canidates[0]
    self.distribution.bin_dir = source_path
    self.announce('Moving Blender python module', level=3)
    shutil.copy(str(bpy_path), str(dest_path))",not bpy_canidates,59,not bpy_canidates,True,100.00000000000004,N/A
"def find_blender_scripts_directory(search_root: str) -> Optional[str]:
    for _dir, _dirs, _files in os.walk(search_root, followlinks=True):
<mask>:
            return _dir
    return None","re.match(BLENDER_SCRIPTS_DIR_REGEX, os.path.basename(_dir)) and all([entry in _dirs for entry in ['datafiles', 'scripts']])",17,_files[0].endswith('.blender.js'),False,0.7456229697698591,N/A
"def get_python_scripts_directory() -> str:
<mask>:
        return PYTHON_SCRIPTS_DIR_UNIX
    elif SYSTEM_NAME == 'Windows':
        if os.path.basename(str(EXECUTABLE_DIR)).casefold() == 'scripts'.casefold():
            return PYTHON_SCRIPTS_DIR_WINDOWS_VENPYTHON
        else:
            return PYTHON_SCRIPTS_DIR_WINDOWS_SYSPYTHON
    else:
        raise Exception('Cannot determine system type: ' + SYSTEM_NAME)","SYSTEM_NAME in ['Darwin', 'Linux']",29,SYSTEM_NAME == 'Windows',False,18.325568129983203,N/A
"def get_blender_scripts_install_dir() -> str:
<mask>:
        return BLENDER_SCRIPTS_INSTALL_DIR_MACOS
    elif SYSTEM_NAME == 'Linux':
        return BLENDER_SCRIPTS_INSTALL_DIR_LINUX
    elif SYSTEM_NAME == 'Windows':
        return BLENDER_SCRIPTS_INSTALL_DIR_WINDOWS
    else:
        raise Exception('Cannot determine system type: ' + SYSTEM_NAME)",SYSTEM_NAME == 'Darwin',28,SYSTEM_NAME == 'MacOS',False,75.98356856515926,N/A
"def remove_blender_scripts_dir():
    """"""Find and remove the blender scripts directory
    """"""
    blender_scripts_search_root_dir = get_blender_scripts_install_dir()
    blender_scripts_current_dir = find_blender_scripts_directory(blender_scripts_search_root_dir)
<mask>:
        print('Found blender scripts dir at ' + blender_scripts_current_dir)
        print('Removing ' + blender_scripts_current_dir)
        shutil.rmtree(blender_scripts_current_dir)
    else:
        print('Did not find Blender scripts at post_install script location')
        print('Did you forget to perform bpy_post_install after installing?')
        print('Searching original pip install location')
        blender_scripts_search_root_dir = get_python_scripts_directory()
        blender_scripts_current_dir = find_blender_scripts_directory(blender_scripts_search_root_dir)
        if blender_scripts_current_dir is not None:
            print('Found blender scripts dir at ' + blender_scripts_current_dir)
            print('Removing ' + blender_scripts_current_dir)
            shutil.rmtree(blender_scripts_current_dir)
        else:
            raise Exception('Could not find Blender scripts directory in ' + blender_scripts_search_root_dir)",blender_scripts_current_dir is not None,89,blender_scripts_current_dir is not None,True,100.00000000000004,N/A
"def install_scripts_directory():
    blender_scripts_search_root_dir = get_python_scripts_directory()
    blender_scripts_install_dir = get_blender_scripts_install_dir()
    blender_scripts_current_dir = find_blender_scripts_directory(blender_scripts_search_root_dir)
<mask>:
        print('Found Blender scripts directory at ' + blender_scripts_current_dir)
        if str(pathlib.Path(blender_scripts_current_dir).parent.absolute()).casefold() == blender_scripts_install_dir.casefold():
            print(blender_scripts_current_dir + ' already direct child of ' + blender_scripts_install_dir)
        else:
            print('Moving ' + blender_scripts_current_dir + ' to ' + blender_scripts_install_dir)
            shutil.move(blender_scripts_current_dir, os.path.join(blender_scripts_install_dir, os.path.basename(blender_scripts_current_dir)))
    else:
        raise Exception('Could not find Blender scripts directory in ' + blender_scripts_search_root_dir)",blender_scripts_current_dir is not None,60,blender_scripts_current_dir,False,65.14390575310559,N/A
"def __init__(self, hm, source):
    self.hm = hm
    self.source = source
    self.buffer = source.read(self.hm.digest_size)
<mask>:
        raise VerificationException('Source does not contain HMAC hash (too small)')",not len(self.buffer) == self.hm.digest_size,23,len(self.buffer) < self.hm.digest_size,False,67.83686168526629,N/A
"def read(self, n=None):
<mask>:
        return b''
    new_read = self.source.read(n) if n is not None else self.source.read()
    finished = n is None or len(new_read) != n
    self.buffer += new_read
    if n is not None:
        offset = min(n, len(self.buffer) - self.hm.digest_size)
    else:
        offset = len(self.buffer) - self.hm.digest_size
    rv, self.buffer = (self.buffer[:offset], self.buffer[offset:])
    self.hm.update(rv)
    if finished:
        if not self.buffer == self.hm.digest():
            raise VerificationException('HMAC verification failed.')
    return rv",b'' == self.buffer or 0 == n,64,self.hm.digest_size == 0,False,11.355085821332205,N/A
"def __new_hmac(self, key, msg=None):
<mask>:
        msg = b''
    hm = hmac.HMAC(key=key.encode('ascii') + self.__secret_key, msg=msg, digestmod=self.__hashfunc)
    return hm",not msg,17,msg is None,False,27.516060407455225,N/A
"def get(self, key):
    buf = self._dstore.get(key)
    hm = self.__new_hmac(key)
    hash = buf[-hm.digest_size:]
    buf = buf[:-hm.digest_size]
    hm.update(buf)
<mask>:
        raise VerificationException('Invalid hash on key %r' % key)
    return buf",not hm.digest() == hash,27,not hm.update(hash),False,21.8465998163823,N/A
"def get_file(self, key, file):
<mask>:
        try:
            f = open(file, 'wb')
        except (OSError, IOError) as e:
            raise IOError('Error opening %s for writing: %r' % (file, e))
        try:
            self.get_file(key, f)
        finally:
            f.close()
    else:
        source = self.open(key)
        bufsize = 1024 * 1024
        while True:
            buf = source.read(bufsize)
            file.write(buf)
            if len(buf) != bufsize:
                break","isinstance(file, str)",50,"isinstance(file, str)",True,100.00000000000004,N/A
"def _on_tree(repo, tree, components, obj):
    """"""Mounts an object on a tree, using the given path components.

    :param tree: Tree object to mount on.
    :param components: A list of strings of subpaths (i.e. ['foo', 'bar'] is
                       equivalent to '/foo/bar')
    :param obj: Object to mount. If None, removes the object found at path
                and prunes the tree downwards.
    :return: A list of new entities that need to be added to the object store,
             where the last one is the new tree.
    """"""
<mask>:
        if isinstance(obj, Blob):
            mode = 33188
        elif isinstance(obj, Tree):
            mode = 16384
        elif obj is None:
            mode = None
        else:
            raise TypeError('Can only mount Blobs or Trees')
        name = components[0]
        if mode is not None:
            tree[name] = (mode, obj.id)
            return [tree]
        if name in tree:
            del tree[name]
        return [tree]
    elif len(components) > 1:
        a, bc = (components[0], components[1:])
        if a in tree:
            a_tree = repo[tree[a][1]]
            if not isinstance(a_tree, Tree):
                a_tree = Tree()
        else:
            a_tree = Tree()
        res = _on_tree(repo, a_tree, bc, obj)
        a_tree_new = res[-1]
        if a_tree_new.items():
            tree[a] = (16384, a_tree_new.id)
            return res + [tree]
        if a in tree:
            del tree[a]
        return [tree]
    else:
        raise ValueError(""Components can't be empty."")",len(components) == 1,190,len(components) == 1,True,100.00000000000004,N/A
"def _create_top_commit(self):
    commit = Commit()
    author = self.AUTHOR.encode('utf8')
    commit.author = commit.committer = author
    commit.commit_time = commit.author_time = int(time.time())
<mask>:
        tz = self.TIMEZONE
    else:
        tz = time.timezone if time.localtime().tm_isdst else time.altzone
    commit.commit_timezone = commit.author_timezone = tz
    commit.encoding = b'UTF-8'
    return commit",self.TIMEZONE is not None,40,self.TIMEZONE,False,36.78794411714425,N/A
"def _delete(self, key):
    try:
        commit = self.repo[self._refname]
        tree = self.repo[commit.tree]
    except KeyError:
        return
    commit = self._create_top_commit()
    objects_to_add = []
    components = self._key_components(key)
<mask>:
        components = self._subdir_components + components
    res = _on_tree(self.repo, tree, components, None)
    objects_to_add.extend(res)
    tree = res[-1]
    commit.tree = tree.id
    commit.message = 'Deleted key {}'.format(self.subdir + '/' + key).encode('utf8')
    objects_to_add.append(commit)
    for obj in objects_to_add:
        self.repo.object_store.add_object(obj)
    self.repo.refs[self._refname] = commit.id",self.subdir,59,self.subdir,True,100.00000000000004,N/A
"def iter_keys(self, prefix=u''):
    try:
        commit = self.repo[self._refname]
        tree = self.repo[commit.tree]
<mask>:
            tree = self.repo[tree.lookup_path(self.repo.__getitem__, self.subdir.encode('ascii'))[1]]
    except KeyError:
        pass
    else:
        for o in self.repo.object_store.iter_tree_contents(tree.sha().hexdigest().encode('ascii')):
            if o.path.decode('ascii').startswith(prefix):
                yield o.path.decode('ascii')",self.subdir,27,self.subdir,True,100.00000000000004,N/A
"def _put(self, key, data):
    commit = self._create_top_commit()
    commit.message = 'Updated key {}'.format(self.subdir + '/' + key).encode('utf8')
    blob = Blob.from_string(data)
    try:
        parent_commit = self.repo[self._refname]
    except KeyError:
        tree = Tree()
    else:
        commit.parents = [parent_commit.id]
        tree = self.repo[parent_commit.tree]
    objects_to_add = [blob]
    components = self._key_components(key)
<mask>:
        components = self._subdir_components + components
    res = _on_tree(self.repo, tree, components, blob)
    objects_to_add.extend(res)
    commit.tree = res[-1].id
    objects_to_add.append(commit)
    for obj in objects_to_add:
        self.repo.object_store.add_object(obj)
    self.repo.refs[self._refname] = commit.id
    return key",self.subdir,68,self.subdir,True,100.00000000000004,N/A
"def put(self, key, data, *args, **kwargs):
<mask>:
        key = self._template.format(self.hashfunc(data).hexdigest())
    return self._dstore.put(key, data, *args, **kwargs)",not key,15,not key,True,100.00000000000004,N/A
"def put_file(self, key, file, *args, **kwargs):
    bufsize = 1024 * 1024
    phash = self.hashfunc()
<mask>:
        if isinstance(file, str):
            with open(file, 'rb') as source:
                while True:
                    buf = source.read(bufsize)
                    phash.update(buf)
                    if len(buf) < bufsize:
                        break
                return self._dstore.put_file(self._template.format(phash.hexdigest()), file, *args, **kwargs)
        else:
            tmpfile = tempfile.NamedTemporaryFile(delete=False)
            try:
                while True:
                    buf = file.read(bufsize)
                    phash.update(buf)
                    tmpfile.write(buf)
                    if len(buf) < bufsize:
                        break
                tmpfile.close()
                return self._dstore.put_file(self._template.format(phash.hexdigest()), tmpfile.name, *args, **kwargs)
            finally:
                try:
                    os.unlink(tmpfile.name)
                except OSError as e:
                    if 2 == e.errno:
                        pass
                    else:
                        raise
    return self._dstore.put_file(key, file, *args, **kwargs)",not key,81,file is not None,False,15.97357760615681,N/A
"def put(self, key, data, *args, **kwargs):
<mask>:
        key = text_type(getattr(uuid, self.uuidfunc)())
    return self._dstore.put(self._template.format(key), data, *args, **kwargs)",not key,16,key is None,False,27.516060407455225,N/A
"def put_file(self, key, file, *args, **kwargs):
<mask>:
        key = text_type(getattr(uuid, self.uuidfunc)())
    return self._dstore.put_file(self._template.format(key), file, *args, **kwargs)",not key,16,key is None,False,27.516060407455225,N/A
"def iter_prefixes(self, delimiter, prefix=u''):
    dlen = len(delimiter)
    plen = len(prefix)
    memory = set()
    for k in self.iter_keys(prefix):
        pos = k.find(delimiter, plen)
<mask>:
            k = k[:pos + dlen]
        if k not in memory:
            yield k
            memory.add(k)",pos >= 0,35,pos != -1,False,18.99589214128981,N/A
"def _map_key(self, key):
<mask>:
        raise ValueError('%r is not a unicode string' % key)
    quoted = quote_plus(key.encode('utf-8'))
    if isinstance(quoted, binary_type):
        quoted = quoted.decode('utf-8')
    return quoted","not isinstance(key, text_type)",24,"not isinstance(key, unicode_type)",False,59.694917920196445,N/A
"def __getattr__(self, attr):
<mask>:
        return super(ReadOnlyDecorator, self).__getattr__(attr)
    else:
        raise AttributeError","attr in ('get', 'iter_keys', 'keys', 'open', 'get_file')",10,attr in self.decorators,False,1.9381301343713229,N/A
"def get_file(self, key, file):
    """"""Write contents of key to file

        Like :meth:`.KeyValueStore.put_file`, this method allows backends to
        implement a specialized function if data needs to be written to disk or
        streamed.

        If *file* is a string, contents of *key* are written to a newly
        created file with the filename *file*. Otherwise, the data will be
        written using the *write* method of *file*.

        :param key: The key to be read
        :param file: Output filename or an object with a *write* method.

        :raises exceptions.ValueError: If the key is not valid.
        :raises exceptions.IOError: If there was a problem reading or writing
                                    data.
        :raises exceptions.KeyError: If the key was not found.
        """"""
    self._check_valid_key(key)
<mask>:
        return self._get_filename(key, file)
    else:
        return self._get_file(key, file)","isinstance(file, str)",117,"isinstance(file, str)",True,100.00000000000004,N/A
"def iter_prefixes(self, delimiter, prefix=u''):
    """"""Returns an Iterator over all prefixes currently in the store, in any order. The
        prefixes are listed up to the given delimiter.

        If the prefix contains the delimiter, the first delimiter after the prefix is used
        as a cut-off point.

        The uniqueness of the prefixes is ensured.

        The default uses an naive key iteration. Some backends may implement more efficient
        variants.

        :raises exceptions.IOError: If there was an error accessing the store.
        """"""
    dlen = len(delimiter)
    plen = len(prefix)
    memory = set()
    for k in self.iter_keys(prefix):
        pos = k.find(delimiter, plen)
<mask>:
            k = k[:pos + dlen]
        if k not in memory:
            yield k
            memory.add(k)",pos >= 0,107,pos != -1,False,18.99589214128981,N/A
"def put(self, key, data):
    """"""Store into key from file

        Stores bytestring *data* in *key*.

        :param key: The key under which the data is to be stored
        :param data: Data to be stored into key, must be `bytes`.

        :returns: The key under which data was stored

        :raises exceptions.ValueError: If the key is not valid.
        :raises exceptions.IOError: If storing failed or the file could not
                                    be read
        """"""
    self._check_valid_key(key)
<mask>:
        raise IOError('Provided data is not of type bytes')
    return self._put(key, data)","not isinstance(data, bytes)",79,"not isinstance(data, bytes)",True,100.00000000000004,N/A
"def put_file(self, key, file):
    """"""Store into key from file on disk

        Stores data from a source into key. *file* can either be a string,
        which will be interpretet as a filename, or an object with a *read()*
        method.

        If the passed object has a *fileno()* method, it may be used to speed
        up the operation.

        The file specified by *file*, if it is a filename, may be removed in
        the process, to avoid copying if possible. If you need to make a copy,
        pass the opened file instead.

        :param key: The key under which the data is to be stored
        :param file: A filename or an object with a read method. If a filename,
                     may be removed

        :returns: The key under which data was stored

        :raises exceptions.ValueError: If the key is not valid.
        :raises exceptions.IOError: If there was a problem moving the file in.
        """"""
<mask>:
        return self._put_filename(key, file)
    else:
        return self._put_file(key, file)","isinstance(file, str)",153,"isinstance(file, (str, os.Path))",False,23.462350320527996,N/A
"def _check_valid_key(self, key):
    """"""Checks if a key is valid and raises a ValueError if its not.

        When in need of checking a key for validity, always use this
        method if possible.

        :param key: The key to be checked
        """"""
<mask>:
        raise ValueError('%r is not a valid key type' % key)
    if not VALID_KEY_RE.match(key):
        raise ValueError('%r contains illegal characters' % key)","not isinstance(key, key_type)",60,"not isinstance(key, str)",False,48.35447404743731,N/A
"def _get(self, key):
    obj = self.obj_class.get_by_id(id=key)
<mask>:
        raise KeyError(key)
    return obj.v",not obj,11,obj is None,False,27.516060407455225,N/A
"def _remove_empty_parents(self, path):
    parents = os.path.relpath(path, os.path.abspath(self.root))
    while len(parents) > 0:
        absparent = os.path.join(self.root, parents)
<mask>:
            if len(os.listdir(absparent)) == 0:
                os.rmdir(absparent)
            else:
                break
        parents = os.path.dirname(parents)",os.path.isdir(absparent),26,os.path.isdir(absparent),True,100.00000000000004,N/A
"def _delete(self, key):
    try:
        targetname = self._build_filename(key)
        os.unlink(targetname)
        self._remove_empty_parents(targetname)
    except OSError as e:
<mask>:
            raise",not e.errno == 2,15,e.errno != errno.ENOENT,False,22.089591134157878,N/A
"def _fix_permissions(self, filename):
    current_umask = os.umask(0)
    os.umask(current_umask)
    perm = self.perm
<mask>:
        perm = 438 & (511 ^ current_umask)
    os.chmod(filename, perm)",self.perm is None,20,not perm,False,11.15650800742149,N/A
"def _open(self, key):
    try:
        f = open(self._build_filename(key), 'rb')
        return f
    except IOError as e:
<mask>:
            raise KeyError(key)
        else:
            raise",2 == e.errno,19,e.errno == errno.ENOENT,False,25.848657697858535,N/A
"def _copy(self, source, dest):
    try:
        source_file_name = self._build_filename(source)
        dest_file_name = self._build_filename(dest)
        self._ensure_dir_exists(os.path.dirname(dest_file_name))
        shutil.copy(source_file_name, dest_file_name)
        self._fix_permissions(dest_file_name)
        return dest
    except IOError as e:
<mask>:
            raise KeyError(source)
        else:
            raise",2 == e.errno,26,e.errno == errno.ENOENT,False,25.848657697858535,N/A
"def lazy_property(fn):
    """"""Decorator that makes a property lazy-evaluated.

    On first access, lazy properties are computed and saved
    as instance attribute with the name `'_lazy_' + method_name`
    Any subsequent property access then returns the cached value.""""""
    attr_name = LAZY_PROPERTY_ATTR_PREFIX + fn.__name__

    @property
    def _lazy_property(self):
<mask>:
            setattr(self, attr_name, fn(self))
        return getattr(self, attr_name)
    return _lazy_property","not hasattr(self, attr_name)",52,"not hasattr(self, attr_name)",True,100.00000000000004,N/A
"@contextmanager
def map_azure_exceptions(key=None, exc_pass=()):
    """"""Map Azure-specific exceptions to the simplekv-API.""""""
    from azure.common import AzureMissingResourceHttpError, AzureHttpError, AzureException
    try:
        yield
    except AzureMissingResourceHttpError as ex:
<mask>:
            s = str(ex)
            if s.startswith(u'The specified container does not exist.'):
                raise IOError(s)
            raise KeyError(key)
    except AzureHttpError as ex:
        if ex.__class__.__name__ not in exc_pass:
            raise IOError(str(ex))
    except AzureException as ex:
        if ex.__class__.__name__ not in exc_pass:
            raise IOError(str(ex))",ex.__class__.__name__ not in exc_pass,59,key is None,False,0.0,N/A
"@lazy_property
def block_blob_service(self):
    from azure.storage.blob import BlockBlobService, PublicAccess
    block_blob_service = BlockBlobService(connection_string=self.conn_string, socket_timeout=self.socket_timeout)
<mask>:
        block_blob_service.MAX_BLOCK_SIZE = self.max_block_size
    if self.max_block_size is not None:
        block_blob_service.MAX_SINGLE_PUT_SIZE = self.max_single_put_size
    if self.create_if_missing:
        block_blob_service.create_container(self.container, public_access=PublicAccess.Container if self.public else None)
    return block_blob_service",self.max_block_size is not None,34,self.max_block_size is not None,True,100.00000000000004,N/A
"def iter_keys(self, prefix=u''):
<mask>:
        prefix = None
    with map_azure_exceptions():
        blobs = self.block_blob_service.list_blob_names(self.container, prefix=prefix)
        return (blob.decode('utf-8') if isinstance(blob, binary_type) else blob for blob in blobs)",prefix == '',24,prefix is None,False,19.716118825581447,N/A
"def iter_prefixes(self, delimiter, prefix=u''):
<mask>:
        prefix = None
    with map_azure_exceptions():
        blobs = self.block_blob_service.list_blob_names(self.container, prefix=prefix, delimiter=delimiter)
        return (blob.decode('utf-8') if isinstance(blob, binary_type) else blob for blob in blobs)",prefix == '',26,prefix is None,False,19.716118825581447,N/A
"def _put(self, key, data):
    from azure.storage.blob.models import ContentSettings
<mask>:
        content_settings = ContentSettings(content_md5=_byte_buffer_md5(data))
    else:
        content_settings = ContentSettings()
    with map_azure_exceptions(key=key):
        self.block_blob_service.create_blob_from_bytes(container_name=self.container, blob_name=key, blob=data, max_connections=self.max_connections, content_settings=content_settings)
        return key",self.checksum,25,self.content_md5,False,21.3643503198117,N/A
"def _file_md5(file_, b64encode=True):
    """"""
    Compute the md5 digest of a file in base64 encoding.

    For ``b64encode``, returns the base64 encoded string; otherwise, returns the
    bytes directly.
    """"""
    md5 = hashlib.md5()
    chunk_size = 128 * md5.block_size
    for chunk in iter(lambda: file_.read(chunk_size), b''):
        md5.update(chunk)
    file_.seek(0)
    byte_digest = md5.digest()
<mask>:
        return base64.b64encode(byte_digest).decode()
    else:
        return byte_digest",b64encode,52,b64encode,True,100.00000000000004,N/A
"def _byte_buffer_md5(buffer_, b64encode=True):
    """"""
    Computes the md5 digest of a byte buffer in base64 encoding.
    """"""
    md5 = hashlib.md5(buffer_)
    byte_digest = md5.digest()
<mask>:
        return base64.b64encode(byte_digest).decode()
    else:
        return byte_digest",b64encode,28,b64encode,True,100.00000000000004,N/A
"@contextmanager
def map_boto_exceptions(key=None, exc_pass=()):
    """"""Map boto-specific exceptions to the simplekv-API.""""""
    from boto.exception import BotoClientError, BotoServerError, StorageResponseError
    try:
        yield
    except StorageResponseError as e:
<mask>:
            raise KeyError(key)
        raise IOError(str(e))
    except (BotoClientError, BotoServerError) as e:
        if e.__class__.__name__ not in exc_pass:
            raise IOError(str(e))",e.code == 'NoSuchKey',39,key is not None,False,0.0,N/A
"def __new_key(self, name):
    from boto.s3.key import Key
    k = Key(self.bucket, self.prefix + name)
<mask>:
        k.update_metadata(self.metadata)
    return k",self.metadata,17,self.metadata,True,100.00000000000004,N/A
"def __upload_args(self):
    """"""Generates a dictionary of arguments to pass to various
        set_content_from* functions. This allows us to save API calls by
        passing the necessary parameters on with the upload.""""""
    d = {'reduced_redundancy': self.reduced_redundancy}
<mask>:
        d['policy'] = 'public-read'
    return d",self.public,39,self.public_read,False,39.76353643835252,N/A
"def _delete(self, key):
    from boto.exception import StorageResponseError
    try:
        self.bucket.delete_key(self.prefix + key)
    except StorageResponseError as e:
<mask>:
            raise IOError(str(e))",e.code != 'NoSuchKey',18,e.errno != errno.ENOENT,False,15.619699684601276,N/A
"def _open(self, key):
    from boto.s3.keyfile import KeyFile

    class SimpleKeyFile(KeyFile):

        def read(self, size=-1):
<mask>:
                raise ValueError('I/O operation on closed file')
            if size < 0:
                size = self.key.size - self.location
            return KeyFile.read(self, size)

        def seekable(self):
            return False

        def readable(self):
            return True
    k = self.__new_key(key)
    with map_boto_exceptions(key=key):
        return SimpleKeyFile(k)",self.closed,46,self.key.closed,False,28.117066259517458,N/A
"@contextmanager
def map_gcloud_exceptions(key=None, error_codes_pass=()):
    """"""Map Google Cloud specific exceptions to the simplekv-API.

    This function exists so the gcstore module can be imported
    without needing to install google-cloud-storage (as we lazily
    import the google library)
    """"""
    from google.cloud.exceptions import NotFound, GoogleCloudError
    from google.api_core.exceptions import ClientError
    try:
        yield
    except NotFound:
<mask>:
            pass
        else:
            raise KeyError(key)
    except GoogleCloudError:
        if 'GoogleCloudError' in error_codes_pass:
            pass
        else:
            raise IOError",'NotFound' in error_codes_pass,63,key is None,False,0.0,N/A
"@lazy_property
def _bucket(self):
<mask>:
        return self._client.create_bucket(bucket_or_name=self.bucket_name, location=self.bucket_creation_location)
    else:
        return self._client.get_bucket(self.bucket_name)",self.create_if_missing and (not self._client.lookup_bucket(self.bucket_name)),10,self.bucket_creation_location,False,2.880123986699673,N/A
"@lazy_property
def _client(self):
    from google.cloud.storage import Client
<mask>:
        return Client.from_service_account_json(self._credentials)
    else:
        return Client(credentials=self._credentials, project=self.project_name)",type(self._credentials) == str,14,self.is_service_account_json,False,9.442944296079734,N/A
"def _open(self, key: str):
    blob = self._bucket.blob(key)
<mask>:
        raise KeyError
    return IOInterface(blob)",not blob.exists(),12,blob is None,False,10.122592925934278,N/A
"def _put(self, key: str, data: bytes):
    blob = self._bucket.blob(key)
<mask>:
        raise IOError(f""data has to be of type 'bytes', not {type(data)}"")
    blob.upload_from_string(data, content_type='application/octet-stream')
    return key",type(data) != bytes,24,"not isinstance(data, bytes)",False,15.619699684601283,N/A
"def _public_readable(grants):
    """"""Take a list of grants from an ACL and check if they allow public read access.""""""
    for grant in grants:
<mask>:
            continue
        grantee = grant['Grantee']
        if grantee.get('Type') != 'Group':
            continue
        if grantee.get('URI') != 'http://acs.amazonaws.com/groups/global/AllUsers':
            continue
        return True
    return False","grant['Permission'] not in ('READ', 'FULL_CONTROL')",41,'Grantee' not in grant,False,3.726425320974899,N/A
"@contextmanager
def map_boto3_exceptions(key=None, exc_pass=()):
    """"""Map boto3-specific exceptions to the simplekv-API.""""""
    from botocore.exceptions import ClientError
    try:
        yield
    except ClientError as ex:
        code = ex.response['Error']['Code']
<mask>:
            raise KeyError(key)
        raise IOError(str(ex))",code == '404' or code == 'NoSuchKey',28,code == 'NoSuchKey',False,28.650479686019022,N/A
"def seek(self, offset, whence=io.SEEK_SET):
<mask>:
        self.position = offset
    elif whence == io.SEEK_CUR:
        self.position += offset
    elif whence == io.SEEK_END:
        self.position = self.size + offset
    else:
        raise ValueError('invalid whence (%r, should be %d, %d, %d)' % (whence, io.SEEK_SET, io.SEEK_CUR, io.SEEK_END))
    return self.position",whence == io.SEEK_SET,41,whence == io.SEEK_SET,True,100.00000000000004,N/A
"def read(self, size=-1):
<mask>:
        range_header = 'bytes=%d-' % self.position
        self.seek(offset=0, whence=io.SEEK_END)
    else:
        new_position = self.position + size
        if new_position >= self.size:
            return self.read()
        range_header = 'bytes=%d-%d' % (self.position, new_position - 1)
        self.seek(offset=size, whence=io.SEEK_CUR)
    return self.s3_object.get(Range=range_header)['Body'].read()",size == -1,35,size == -1,True,100.00000000000004,N/A
"def __init__(self, bucket, prefix='', url_valid_time=0, reduced_redundancy=False, public=False, metadata=None):
<mask>:
        import boto3
        s3_resource = boto3.resource('s3')
        bucket = s3_resource.Bucket(bucket)
        if bucket not in s3_resource.buckets.all():
            raise ValueError('invalid s3 bucket name')
    self.bucket = bucket
    self.prefix = prefix.strip().lstrip('/')
    self.url_valid_time = url_valid_time
    self.reduced_redundancy = reduced_redundancy
    self.public = public
    self.metadata = metadata or {}","isinstance(bucket, str)",47,"not isinstance(bucket, str)",False,80.91067115702207,N/A
"@contextmanager
def map_azure_exceptions(key=None, error_codes_pass=()):
    """"""Map Azure-specific exceptions to the simplekv-API.""""""
    from azure.core.exceptions import AzureError
    try:
        yield
    except AzureError as ex:
        error_code = getattr(ex, 'error_code', None)
<mask>:
            return
        if error_code == 'BlobNotFound':
            raise KeyError(key)
        raise IOError(str(ex))",error_code is not None and error_code in error_codes_pass,35,error_code == 'NoSuchKey',False,5.706640995743375,N/A
"@lazy_property
def blob_container_client(self):
    from azure.storage.blob import BlobServiceClient
    kwargs = {}
<mask>:
        kwargs['max_single_put_size'] = self.max_single_put_size
    if self.max_block_size:
        kwargs['max_block_size'] = self.max_block_size
    service_client = BlobServiceClient.from_connection_string(self.conn_string, **kwargs)
    container_client = service_client.get_container_client(self.container)
    if self.create_if_missing:
        with map_azure_exceptions(error_codes_pass='ContainerAlreadyExists'):
            container_client.create_container(public_access='container' if self.public else None)
    return container_client",self.max_single_put_size,37,self.max_single_put_size,True,100.00000000000004,N/A
"def _put(self, key, data):
    from azure.storage.blob import ContentSettings
<mask>:
        content_settings = ContentSettings(content_md5=_byte_buffer_md5(data, b64encode=False))
    else:
        content_settings = ContentSettings()
    with map_azure_exceptions(key):
        blob_client = self.blob_container_client.get_blob_client(key)
        blob_client.upload_blob(data, overwrite=True, content_settings=content_settings, max_concurrency=self.max_connections)
    return key",self.checksum,28,self.content_settings_md5,False,13.134549472120788,N/A
"def _put_file(self, key, file):
    from azure.storage.blob import ContentSettings
<mask>:
        content_settings = ContentSettings(content_md5=_file_md5(file, b64encode=False))
    else:
        content_settings = ContentSettings()
    with map_azure_exceptions(key):
        blob_client = self.blob_container_client.get_blob_client(key)
        blob_client.upload_blob(file, overwrite=True, content_settings=content_settings, max_concurrency=self.max_connections)
    return key",self.checksum,28,self.content_settings_md5,False,13.134549472120788,N/A
"def tell(self):
    """"""Returns he current offset as int. Always >= 0.""""""
<mask>:
        raise ValueError('I/O operation on closed file')
    return self.pos",self.closed,20,self.closed,True,100.00000000000004,N/A
"def _get(self, key):
    val = self.redis.get(key)
<mask>:
        raise KeyError(key)
    return val",val is None,11,val is None,True,100.00000000000004,N/A
"def _put(self, key, value, ttl_secs):
<mask>:
        self.redis.set(key, value)
    else:
        ittl = None
        try:
            ittl = int(ttl_secs)
        except ValueError:
            pass
        if ittl == ttl_secs:
            self.redis.setex(key, ittl, value)
        else:
            self.redis.psetex(key, int(ttl_secs * 1000), value)
    return key","ttl_secs in (NOT_SET, FOREVER)",34,ttl_secs is None,False,11.976547020391715,N/A
"def _check_valid_key(self, key):
    """"""Checks if a key is valid and raises a ValueError if its not.

        When in need of checking a key for validity, always use this
        method if possible.

        :param key: The key to be checked
        """"""
<mask>:
        raise ValueError('%r is not a valid key type' % key)
    if not VALID_KEY_RE_EXTENDED.match(key) or key == u'/':
        raise ValueError('%r contains illegal characters' % key)","not isinstance(key, key_type) and key is not None",64,"not isinstance(key, str)",False,23.671529472186084,N/A
"def _get(self, key):
    rv = self.bind.execute(select([self.table.c.value], self.table.c.key == key).limit(1)).scalar()
<mask>:
        raise KeyError(key)
    return rv",not rv,14,rv is None,False,27.516060407455225,N/A
"def _copy(self, source, dest):
    con = self.bind.connect()
    with con.begin():
        data = self.bind.execute(select([self.table.c.value], self.table.c.key == source).limit(1)).scalar()
<mask>:
            raise KeyError(source)
        con.execute(self.table.delete(self.table.c.key == dest))
        con.execute(self.table.insert({'key': dest, 'value': data}))
    con.close()
    return dest",not data,28,data is None,False,27.516060407455225,N/A
"def iter_keys(self, prefix=u''):
    query = select([self.table.c.key])
<mask>:
        query = query.where(self.table.c.key.like(prefix + '%'))
    return imap(lambda v: text_type(v[0]), self.bind.execute(query))",prefix != '',17,prefix,False,4.9787068367863965,N/A
"@pytest.fixture(params=[True, False])
def store(self, request, prefix):
    base_store = DictStore()
<mask>:
        base_store.put(u'some_other_value', b'data1')
        base_store.put(u'ends_with_short_', b'data2')
        base_store.put(u'xx', b'data3')
        base_store.put(u'test', b'data4')
    return PrefixDecorator(prefix, base_store)",request.param,21,request.method == 'POST',False,16.233395773754953,N/A
"def test_put_file_generates_uuid_form(self, uuidstore):
    key = uuidstore.put_file(None, open('/dev/null', 'rb'))
    assert UUID_REGEXP.match(key)
    tmpfile = tempfile.NamedTemporaryFile(delete=False)
    try:
        tmpfile.close()
        key2 = uuidstore.put_file(None, tmpfile.name)
        assert UUID_REGEXP.match(key2)
    finally:
<mask>:
            os.unlink(tmpfile.name)",os.path.exists(tmpfile.name),24,tmpfile.name,False,9.697196786440509,N/A
"def test_put_file_generates_valid_uuid(self, uuidstore):
    key = uuidstore.put_file(None, open('/dev/null', 'rb'))
    uuid.UUID(hex=key)
    tmpfile = tempfile.NamedTemporaryFile(delete=False)
    try:
        tmpfile.close()
        key2 = uuidstore.put_file(None, tmpfile.name)
        uuid.UUID(hex=key2)
    finally:
<mask>:
            os.unlink(tmpfile.name)",os.path.exists(tmpfile.name),22,tmpfile.name,False,9.697196786440509,N/A
"def test_put_file_generates_correct_hash(self, hashstore, value_hash, value):
    tmpfile = tempfile.NamedTemporaryFile(delete=False)
    try:
        tmpfile.write(value)
        tmpfile.close()
        with open(tmpfile.name, 'rb') as f:
            key = hashstore.put_file(None, f)
        assert key == value_hash
        key2 = hashstore.put_file(None, tmpfile.name)
        assert key2 == value_hash
    finally:
<mask>:
            os.unlink(tmpfile.name)",os.path.exists(tmpfile.name),35,os.path.exists(tmpfile.name),True,100.00000000000004,N/A
"def get_azure_conn_string():
    cfg_fn = 'azure_credentials.ini'
    parser = ConfigParser({'protocol': 'https', 'endpoint': '', 'account_name': ''})
    result = parser.read(cfg_fn)
<mask>:
        pytest.skip('file {} not found'.format(cfg_fn))
    for section in parser.sections():
        account_name = parser.get(section, 'account_name')
        if not account_name:
            pytest.skip(""no 'account_name' found in file {}"".format(cfg_fn))
        account_key = parser.get(section, 'account_key')
        protocol = parser.get(section, 'protocol')
        endpoint = parser.get(section, 'endpoint')
        conn_string = 'DefaultEndpointsProtocol={};AccountName={};AccountKey={}'.format(protocol, account_name, account_key)
        if endpoint:
            conn_string += ';BlobEndpoint={}'.format(endpoint)
        return conn_string",not result,62,not result,True,100.00000000000004,N/A
"def _delete_container(conn_string, container):
    try:
        from azure.storage.blob import BlobServiceClient
        from azure.core.exceptions import AzureError
        s = BlobServiceClient.from_connection_string(conn_string)
        try:
            s.delete_container(container)
        except AzureError as ex:
<mask>:
                raise
    except ImportError:
        from azure.storage.blob import BlockBlobService
        s = BlockBlobService(connection_string=conn_string)
        s.delete_container(container)",ex.error_code != 'ContainerNotFound',33,ex.response['Error']['Code'] != 'ResourceNotFoundException',False,9.578464408619821,N/A
"@pytest.fixture
def store(self):
    azure_storage_blob_major_version = int(asb.__version__.split('.', 1)[0])
    conn_string = get_azure_conn_string()
    use_azurite = 'http://127.0.0.1:10000/devstoreaccount1' in conn_string
<mask>:
        pytest.skip('Compatibility issues with azurite and azure-storage-blob<12')
    container = str(uuid())

    class ExtendedKeysStore(ExtendedKeyspaceMixin, AzureBlockBlobStore):
        pass
    yield ExtendedKeysStore(conn_string=conn_string, container=container, public=False)
    _delete_container(conn_string, container)",use_azurite and azure_storage_blob_major_version < 12,35,use_azurite and azure_storage_blob_major_version < 12,True,100.00000000000004,N/A
"def test_azure_special_args():
    conn_string = get_azure_conn_string()
    MBS = 983645
    MSP = 756235
    abbs = AzureBlockBlobStore(conn_string=conn_string, container='container-unused', max_block_size=MBS, max_single_put_size=MSP, create_if_missing=False)
<mask>:
        cfg = abbs.blob_container_client._config
        assert cfg.max_single_put_size == MSP
        assert cfg.max_block_size == MBS","hasattr(abbs, 'blob_container_client')",30,"hasattr(abbs, 'blob_container_client')",True,100.00000000000004,N/A
"def test_wrong_endpoint(self):
    container = str(uuid())
    conn_string = get_azure_conn_string()
    conn_settings = dict([s.split('=', 1) for s in conn_string.split(';') if s])
    conn_settings['BlobEndpoint'] = 'https://host-does-not-exist/'
    conn_string = ';'.join(('{}={}'.format(key, value) for key, value in conn_settings.items()))
    store = AzureBlockBlobStore(conn_string=conn_string, container=container, create_if_missing=False)
<mask>:
        from azure.storage.common.retry import ExponentialRetry
        store.block_blob_service.retry = ExponentialRetry(max_attempts=0).retry
    else:
        store.blob_container_client._config.retry_policy.total_retries = 0
    with pytest.raises(IOError) as exc:
        store.put(u'key', b'data')
    assert u'connect' in str(exc.value)","hasattr(store, 'block_blob_service')",57,"azure.VERSION >= (1, 9)",False,5.522397783539471,N/A
"@contextmanager
def boto_bucket(access_key, secret_key, host, connect_func='connect_s3', ordinary_calling_format=False, bucket_name=None, port=None, is_secure=True):
<mask>:
        from boto.s3.connection import OrdinaryCallingFormat
        conn = getattr(boto, connect_func)(access_key, secret_key, host=host, calling_format=OrdinaryCallingFormat(), port=port, is_secure=is_secure)
    else:
        conn = getattr(boto, connect_func)(access_key, secret_key, host=host, port=port, is_secure=is_secure)
    name = bucket_name or 'testrun-bucket-{}'.format(uuid())
    bucket = conn.create_bucket(name)
    yield bucket
    for key in bucket.list():
        key.delete()
    bucket.delete()",ordinary_calling_format,49,ordinary_calling_format,True,100.00000000000004,N/A
"def load_boto_credentials():
    cfg_fn = 'boto_credentials.ini'
    parser = ConfigParser({'host': 's3.amazonaws.com', 'is_secure': 'true', 'ordinary_calling_format': 'false'})
<mask>:
        pytest.skip('file {} not found'.format(cfg_fn))
    for section in parser.sections():
        yield {'access_key': parser.get(section, 'access_key'), 'secret_key': parser.get(section, 'secret_key'), 'connect_func': parser.get(section, 'connect_func'), 'host': parser.get(section, 'host'), 'is_secure': parser.getboolean(section, 'is_secure'), 'port': parser.getint(section, 'port'), 'ordinary_calling_format': parser.getboolean(section, 'ordinary_calling_format')}",not parser.read(cfg_fn),44,not parser.read(cfg_fn),True,100.00000000000004,N/A
"def test_storage_class_put(self, store, prefix, key, value, storage_class, bucket):
    store.put(key, value)
    keyname = prefix + key
<mask>:
        pytest.xfail('boto does not support checking the storage class?')
    assert bucket.lookup(keyname).storage_class == storage_class",storage_class != 'STANDARD',28,not storage_class in S3_STORAGE_CLASSES,False,15.619699684601283,N/A
"def test_storage_class_putfile(self, store, prefix, key, value, storage_class, bucket):
    store.put_file(key, BytesIO(value))
    keyname = prefix + key
<mask>:
        pytest.xfail('boto does not support checking the storage class?')
    assert bucket.lookup(keyname).storage_class == storage_class",storage_class != 'STANDARD',28,not storage_class in S3_STORAGE_CLASSES,False,15.619699684601283,N/A
"@pytest.fixture(scope='module')
def gc_credentials():
    parser = ConfigParser()
    parser.read('google_cloud_credentials.ini')
    credentials_path = parser.get('google-cloud-tests', 'credentials_json_path', fallback=None)
    emulator_endpoint = parser.get('google-cloud-tests', 'emulator_endpoint', fallback=None)
    assert credentials_path or emulator_endpoint, 'Either set endpoint (for gc emulation) or credentials_json_path (for actual gc)'
<mask>:
        os.environ['STORAGE_EMULATOR_HOST'] = emulator_endpoint
        credentials = AnonymousCredentials()
    else:
        credentials = credentials_path
    yield credentials
    if emulator_endpoint:
        del os.environ['STORAGE_EMULATOR_HOST']",emulator_endpoint,49,emulator_endpoint,True,100.00000000000004,N/A
"@pytest.fixture(scope='module')
def dirty_store(gc_credentials):
    uuid = str(uuid4())
<mask>:
        project_name = 'testing'
    else:
        project_name = None
    store = GoogleCloudStore(credentials=gc_credentials, bucket_name=uuid, project=project_name)
    yield store
    try_delete_bucket(store._bucket)",type(gc_credentials) == AnonymousCredentials,22,"uuid in ['testing', 'busted']",False,0.0,N/A
"@pytest.fixture(scope='function')
def store(dirty_store):
    for blob in dirty_store._bucket.list_blobs():
        blob.delete()
<mask>:
        time.sleep(0.3)
    return dirty_store","not os.environ.get('STORAGE_EMULATOR_HOST', None)",12,not dirty_store._bucket.exists(blob.name),False,4.300779255833658,N/A
"@pytest.fixture(scope='class')
def dirty_store(self, gc_credentials):
    uuid = str(uuid4())
<mask>:
        project_name = 'testing'
    else:
        project_name = None

    class ExtendedKeysStore(ExtendedKeyspaceMixin, GoogleCloudStore):
        pass
    store = ExtendedKeysStore(credentials=gc_credentials, bucket_name=uuid, project=project_name)
    yield store
    try_delete_bucket(store._bucket)",type(gc_credentials) == AnonymousCredentials,27,uuid in self.project_names,False,4.935157841536379,N/A
"@pytest.fixture(scope='function')
def store(self, dirty_store):
    for blob in dirty_store._bucket.list_blobs():
        blob.delete()
<mask>:
        time.sleep(0.2)
    return dirty_store","not os.environ.get('STORAGE_EMULATOR_HOST', None)",13,len(dirty_store._bucket.list_blobs()) > 0,False,3.5792807886387674,N/A
"def test_uses_subdir(self, repo_path, store, subdir_name, branch):
    store.put(u'foo', b'bar')
    sdir = subdir_name.decode('ascii').strip('/')
    fn = 'foo'
<mask>:
        fn = sdir + '/' + fn
    repo = Repo(repo_path)
    commit = repo[repo.refs[b'refs/heads/' + branch]]
    tree = repo[commit.tree]
    _, blob_id = tree.lookup_path(repo.__getitem__, fn.encode('ascii'))
    assert repo[blob_id].data == b'bar'
    store.put(u'foo2', b'bar2')
    fn = 'foo'
    if sdir:
        fn = sdir + '/' + fn
    repo = Repo(repo_path)
    commit = repo[repo.refs[b'refs/heads/' + branch]]
    tree = repo[commit.tree]
    _, blob_id = tree.lookup_path(repo.__getitem__, fn.encode('ascii'))
    assert repo[blob_id].data == b'bar'
    fn2 = 'foo2'
    if sdir:
        fn2 = sdir + '/' + fn2
    _, blob_id = tree.lookup_path(repo.__getitem__, fn2.encode('ascii'))
    assert repo[blob_id].data == b'bar2'",sdir,97,sdir,True,100.00000000000004,N/A
"def test_reading_with_limit(self, secret_key, hashfunc, value, create_reader, chunk_sizes):
    for n in chunk_sizes:
        chunks = []
        reader = create_reader()
        while True:
            r = reader.read(n)
<mask>:
                break
            chunks.append(r)
        assert b('').join(chunks) == value",not r,29,r is None,False,27.516060407455225,N/A
"def test_manipulated_input_incremental_read(self, secret_key, bad_datas, hashfunc):
    for bad_data in bad_datas:
        reader = _HMACFileReader(hmac.HMAC(secret_key, None, hashfunc), BytesIO(bad_data))
        with pytest.raises(VerificationException):
            bitesize = 100
            while True:
<mask>:
                    break",len(reader.read(bitesize)) != bitesize,24,reader.incremental_read(bitesize) == bitesize,False,28.947421495675087,N/A
"def test_concurrent_mkdir(self, tmpdir, mocker):
    makedirs = mocker.patch('os.makedirs')
    makedirs.side_effect = OSError('Failure')
    mocker.patch('os.path.isdir')
    store = FilesystemStore(os.path.join(tmpdir, 'test'))
<mask>:
        with pytest.raises(IOError):
            store.put('test', b'test')
    else:
        with pytest.raises(FileNotFoundError):
            store.put('test', b'test')",PY2,25,sys.version_info[0] == 2,False,0.0,N/A
"def test_file_uri(self, store, value):
    tmpfile = tempfile.NamedTemporaryFile(delete=False)
    try:
        tmpfile.write(value)
        tmpfile.close()
        key = store.put_file(u'testkey', tmpfile.name)
        url = store.url_for(key)
        assert url.startswith('file://')
        parts = urlparse(url)
        ndata = open(parts.path, 'rb').read()
        assert value == ndata
    finally:
<mask>:
            os.unlink(tmpfile.name)",os.path.exists(tmpfile.name),33,tmpfile.name,False,9.697196786440509,N/A
"def test_file_permissions_on_moved_in_file_have_correct_value(self, store, perms, key, value):
    tmpfile = tempfile.NamedTemporaryFile(delete=False)
    tmpfile.write(value)
    tmpfile.close()
    os.chmod(tmpfile.name, 511)
    try:
        key = store.put_file(key, tmpfile.name)
        parts = urlparse(store.url_for(key))
        path = url_unquote(parts.path)
        mode = os.stat(path).st_mode
        mask = stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO
        assert mode & mask == perms
    finally:
<mask>:
            os.path.unlink(tmpfile.name)",os.path.exists(tmpfile.name),43,os.path.exists(tmpfile.name),True,100.00000000000004,N/A
"def test_store_and_copy(self, store, key, key2, value):
<mask>:
        pytest.skip()
    store.put(key, value)
    assert store.get(key) == value
    store.copy(key, key2)
    assert store.get(key) == value
    assert store.get(key2) == value","not isinstance(store, CopyMixin)",24,"not hasattr(store, 'get')",False,27.77619034011791,N/A
"def test_store_and_copy_overwrite(self, store, key, key2, value, value2):
<mask>:
        pytest.skip()
    store.put(key, value)
    store.put(key2, value2)
    assert store.get(key) == value
    assert store.get(key2) == value2
    store.copy(key, key2)
    assert store.get(key) == value
    assert store.get(key2) == value","not isinstance(store, CopyMixin)",31,"not hasattr(store, 'get')",False,27.77619034011791,N/A
"def test_key_error_on_nonexistant_copy(self, store, key, key2):
<mask>:
        pytest.skip()
    with pytest.raises(KeyError):
        store.copy(key, key2)","not isinstance(store, CopyMixin)",11,key == key2,False,0.0,N/A
"def test_exception_on_invalid_key_copy(self, store, invalid_key, key):
<mask>:
        pytest.skip()
    with pytest.raises(ValueError):
        store.copy(invalid_key, key)
    with pytest.raises(ValueError):
        store.copy(key, invalid_key)","not isinstance(store, CopyMixin)",15,invalid_key is None,False,0.0,N/A
"def test_put_file(self, store, key, value):
    tmp = tempfile.NamedTemporaryFile(delete=False)
    try:
        tmp.write(value)
        tmp.close()
        store.put_file(key, tmp.name)
        assert store.get(key) == value
    finally:
<mask>:
            os.unlink(tmp.name)",os.path.exists(tmp.name),20,os.path.exists(tmp.name),True,100.00000000000004,N/A
"def pytest_generate_tests(metafunc):
<mask>:
        metafunc.parametrize('python_git_url', (metafunc.config.option.python_git_url,))
    if 'python' in metafunc.fixturenames:
        metafunc.parametrize('python', (metafunc.config.option.python_ver,))
    if 'skip_binary_strip' in metafunc.fixturenames:
        metafunc.parametrize('skip_binary_strip', (metafunc.config.option.skip_binary_strip,))
    if 'use_pip_install' in metafunc.fixturenames:
        metafunc.parametrize('use_pip_install', (True, False))
    if 'remove_pycache' in metafunc.fixturenames:
        metafunc.parametrize('remove_pycache', (True, False))",'python_git_url' in metafunc.fixturenames,31,'python_git_url' in metafunc.fixturenames,True,100.00000000000004,N/A
"@pytest.fixture
def python_source_code(python_git_url, tmpdir):
    """"""Generate a source code directory and return the path.""""""
    pkg_name = python_git_url[:-4].split('/')[-1]
    cmd = 'git clone {0} {1}/{2}'.format(python_git_url, str(tmpdir), pkg_name).encode('ascii')
<mask>:
        cmd = cmd.decode('utf8')
    subprocess.check_call(shlex.split(cmd))
    return os.path.abspath(str(tmpdir.join(pkg_name)))",sys.version_info[0] > 2,31,sys.version_info[0] == 2,False,69.89307622784945,N/A
"@pytest.fixture(autouse=True)
def qa_skip_buildroot(skip_binary_strip):
<mask>:
        os.environ['QA_SKIP_BUILD_ROOT'] = '1'
    elif 'QA_SKIP_BUILD_ROOT' in os.environ:
        os.environ.pop('QA_SKIP_BUILD_ROOT')",skip_binary_strip,12,skip_binary_strip,True,100.00000000000004,N/A
"def test_python_cmd_build(request, python_source_code, python_config_file, tmpdir):
    """"""Test that a default build works without exception.""""""
<mask>:
        pytest.skip('No --python-git-url option was given')
    with pytest.raises(SystemExit) as exc_info:
        cli.main((python_config_file, '--source', python_source_code, '--destination', str(tmpdir)))
    rc = exc_info.value.code if type(exc_info.value) == SystemExit else exc_info.value
    assert rc == 0",not request.config.getvalue('python_git_url'),41,not request.args.python_git_url,False,20.225232797581672,N/A
"def generate_spec(config, extensions):
    """"""Generate a SPEC file from the given arguments mapping.""""""
    specfile = spec.Spec()
    for ext in extensions:
        specfile = ext.generate(config, specfile)
<mask>:
            sys.stderr.write('The {0} extension did not return a valid Spec object.{1}'.format(ext.name, os.linesep))
            sys.exit(1)
    return str(specfile)",not specfile,38,not specfile,True,100.00000000000004,N/A
"def main(argv=sys.argv[1:]):
    """"""Build an RPM.""""""
    args = parse_args(argv)
    config = confpy.api.parse_options(files=(args['config'],), env_prefix='RPMVENV', strict=False)
    whitelist = tuple(config.extensions.enabled)
    extensions = extensions_loader.load_extensions(whitelist=whitelist)
    config = confpy.api.parse_options(files=(args['config'],), env_prefix='RPMVENV', strict=True)
    config.core.source = args['source']
    try:
        extensions_loader.validate_extensions(extensions)
    except (extensions_loader.MissingDependency, extensions_loader.InvalidDependency) as exc:
        sys.stderr.write('{0}{1}'.format(str(exc), os.linesep))
        sys.exit(1)
    specfile = generate_spec(config, extensions)
<mask>:
        sys.stdout.write('{0}{1}'.format(specfile, os.linesep))
        sys.exit(0)
    try:
        rpm_path = generate_rpm(args['source'], args['destination'], specfile, args['verbose'])
    except rpmbuild.RpmProcessError as exc:
        sys.stderr.write('There was an error generating the RPM.{0}'.format(os.linesep))
        sys.stderr.write('The exit code was: {0}.{1}'.format(exc.returncode, os.linesep))
        sys.stderr.write('The rpmbuild command was: {0}.{1}'.format(exc.cmd, os.linesep))
        sys.stderr.write('The stderr was: {0}.{1}'.format(exc.stderr, os.linesep))
        sys.stderr.write('The stdout was: {0}.{1}'.format(exc.stdout, os.linesep))
        sys.exit(1)
    except subprocess.CalledProcessError as exc:
        sys.stderr.write('There was an error generating the RPM.{0}'.format(os.linesep))
        sys.stderr.write('The exit code was: {0}.{1}'.format(exc.returncode, os.linesep))
        sys.stderr.write('The rpmbuild command was: {0}.{1}'.format(exc.cmd, os.linesep))
        sys.exit(1)
    sys.stdout.write('RPM generated at {0}{1}.'.format(rpm_path, os.linesep))
    sys.exit(0)",args['spec'],116,args['verbose'],False,35.35533905932737,N/A
"def verbose_popen(cmd):
    """"""Run a command with streaming output.

    Args:
        cmd (str): A command to run with popen.

    Raises:
        CalledProcessError: If the returncode is not 0.
    """"""
    proc = subprocess.Popen(shlex.split(cmd))
    proc.wait()
<mask>:
        raise subprocess.CalledProcessError(returncode=proc.returncode, cmd=cmd)",proc.returncode != 0,34,proc.returncode,False,36.78794411714425,N/A
"def quiet_popen(cmd):
    """"""Run a command with captured output.

    Args:
        cmd (str): A command to run with popen.

    Raises:
        RpmProcessError: If the returncode is not 0.
    """"""
    proc = subprocess.Popen(shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    out, err = proc.communicate()
<mask>:
        raise RpmProcessError(returncode=proc.returncode, cmd=cmd, output=err, stdout=out, stderr=err)",proc.returncode != 0,42,proc.returncode != 0,True,100.00000000000004,N/A
"def build(specfile, top=None, verbose=False):
    """"""Run rpmbuild with options.

    Args:
        specfile: The absolute path to the SPEC file to build.
        top: The %_topdir to use during the build. The default is a temporary
            directory which is automatically generated.
        verbose: Whether or not to stream the rpmbuild output in real time
            or only during errors.

    Returns:
        The absolute path to the new RPM.
    """"""
    top = top or topdir()
    cmd = ""rpmbuild -ba --define='_topdir {0}' {1}"".format(top, specfile).encode('ascii')
<mask>:
        cmd = cmd.decode('utf8')
    if not verbose:
        quiet_popen(cmd)
    else:
        verbose_popen(cmd)
    return glob.glob(os.path.join(top, 'RPMS', '**', '*.rpm')).pop()",sys.version_info[0] > 2,90,"not isinstance(cmd, bytes)",False,0.0,N/A
"@staticmethod
def generate(config, spec):
    """"""Generate the core RPM package metadata.""""""
    name = config.core.name
    version = config.core.version
    release = config.core.release
    summary = config.core.summary
    group = config.core.group
    license = config.core.license
    url = config.core.url
    source = config.core.source
    buildroot = config.core.buildroot
    buildarch = config.core.buildarch
    requires = tuple(config.core.requires)
    conflicts = tuple(config.core.conflicts)
    obsoletes = tuple(config.core.obsoletes)
    provides = tuple(config.core.provides)
    spec.tags['Name'] = name
    spec.tags['Version'] = version
    spec.tags['Release'] = release
    spec.tags['BuildRoot'] = buildroot
<mask>:
        spec.tags['Requires'] = ', '.join(requires)
    if conflicts:
        spec.tags['Conflicts'] = ', '.join(conflicts)
    if obsoletes:
        spec.tags['Obsoletes'] = ', '.join(obsoletes)
    if provides:
        spec.tags['Provides'] = ', '.join(provides)
    if buildarch:
        spec.tags['BuildArch'] = buildarch
    if summary:
        spec.tags['Summary'] = summary
    if group:
        spec.tags['Group'] = group
    if license:
        spec.tags['License'] = license
    if url:
        spec.tags['Url'] = url
    if source:
        spec.tags['Source0'] = source
    spec.blocks.prep.append('rm -rf %{buildroot}/*')
    spec.blocks.clean.append('rm -rf %{buildroot}')
    return spec",requires,125,requires,True,100.00000000000004,N/A
"def validate_extensions(extensions):
    """"""Process the extension dependencies.""""""
    ext_map = dict(((ext.name, ext) for ext in extensions))
    for ext in extensions:
        for dependency, versions in ext.requirements.items():
            ext_dependency = ext_map.get(dependency, None)
<mask>:
                raise MissingDependency('{0} is required by {1} but is not loaded.'.format(ext.name, dependency))
            for version in versions:
                if not semver.match(ext.version, version):
                    raise InvalidDependency('{0}-{1} required by {2} but found {0}-{3}.'.format(dependency, version, ext.name, ext.version))
    return extensions",not ext_dependency,60,ext_dependency is None,False,39.76353643835252,N/A
"@staticmethod
def generate(config, spec):
    """"""Produce block segments from input.""""""
    for block in BLOCKS:
        lines = tuple(getattr(config.blocks, block))
<mask>:
            block_lines = None
            if block == 'desc':
                block_lines = spec.blocks.get('description')
            else:
                block_lines = spec.blocks.get(block)
            block_lines.extend(lines)
    return spec",lines,35,len(lines) > 0,False,8.116697886877475,N/A
"@staticmethod
def generate(config, spec):
    """"""Generate Python virtualenv content.""""""
    spec.macros['venv_cmd'] = '{0} {1}'.format(config.python_venv.cmd, ' '.join(config.python_venv.flags if config.python_venv.flags else ()))
<mask>:
        spec.macros['venv_cmd'] = '{0} --python={1}'.format(spec.macros['venv_cmd'], config.python_venv.python)
    spec.macros['venv_name'] = config.python_venv.name
    spec.macros['venv_install_dir'] = '{0}/%{{venv_name}}'.format(config.python_venv.path)
    spec.macros['venv_dir'] = '%{buildroot}/%{venv_install_dir}'
    spec.macros['venv_bin'] = '%{venv_dir}/bin'
    spec.macros['venv_python'] = '%{venv_bin}/python'
    spec.macros['venv_pip'] = '%{{venv_python}} %{{venv_bin}}/pip install {0}'.format(' '.join(config.python_venv.pip_flags if config.python_venv.pip_flags else ()))
    spec.macros['__prelink_undo_cmd'] = '%{nil}'
    spec.globals['__os_install_post'] = ""%(echo '%{__os_install_post}' | sed -e 's!/usr/lib[^[:space:]]*/brp-python-bytecompile[[:space:]].*$!!g')""
    spec.tags['AutoReq'] = 'No'
    spec.tags['AutoProv'] = 'No'
    spec.blocks.prep.append('mkdir -p %{buildroot}/%{venv_install_dir}')
    spec.blocks.files.append('/%{venv_install_dir}')
    spec.blocks.install.append('%{venv_cmd} %{venv_dir}')
    for requirement in config.python_venv.requirements:
        spec.blocks.install.extend(('cd %{SOURCE0}', '%{{venv_pip}} -r {0}'.format(requirement), 'cd -'))
    if config.python_venv.require_setup_py:
        spec.blocks.install.append('cd %{SOURCE0}')
        if config.python_venv.use_pip_install:
            spec.blocks.install.append('%{venv_pip} .')
        else:
            spec.blocks.install.append('%{venv_python} setup.py install')
        spec.blocks.install.append('cd -')
    if config.python_venv.remove_pycache:
        spec.blocks.install.append('find %{venv_dir} -type d -name ""__pycache__"" -print0 | xargs -0 rm -rf')
    spec.blocks.install.extend(('# RECORD files are used by wheels for checksum. They contain path names which', '# match the buildroot and must be removed or the package will fail to build.', 'find %{buildroot} -name ""RECORD"" -exec rm -rf {} \\;', '# Change the virtualenv path to the target installation direcotry.', 'venvctrl-relocate --source=%{venv_dir} --destination=/%{venv_install_dir}'))
    if config.python_venv.strip_binaries:
        spec.blocks.install.extend(('# Strip native modules as they contain buildroot paths intheir debug information', 'find %{venv_dir}/lib -type f -name ""*.so"" | xargs -r strip'))
    else:
        spec.macros['debug_package'] = 'debug_package %{nil}'
        spec.macros['__strip'] = '/bin/true'
    return spec",config.python_venv.python,197,config.python_venv.python,True,100.00000000000004,N/A
"@staticmethod
def generate(config, spec):
    """"""Produce file block segments for packaging files.""""""
    for file_ in config.file_extras.files:
        file_directive = ''
<mask>:
            file_directive = '%attr({0}, {1}, {2}) '.format(file_.file_attr['permissions'], file_.file_attr['user'], file_.file_attr['group'])
        if file_.file_type is not None:
            if file_.file_type_option is not None:
                file_directive += '%{1}({2}) /{0}'.format(file_.dest, file_.file_type, file_.file_type_option)
            else:
                file_directive += '%{1} /{0}'.format(file_.dest, file_.file_type)
        else:
            file_directive += '/{0}'.format(file_.dest)
        spec.blocks.install.append('mkdir -p ""%{{buildroot}}/%(dirname {0})""'.format(file_.dest))
        spec.blocks.install.append('cp -R %{{SOURCE0}}/{0} %{{buildroot}}/{1}'.format(file_.src, file_.dest))
        spec.blocks.files.append(file_directive)
    return spec",file_.file_attr is not None,65,file_.file_attr is not None,True,100.00000000000004,N/A
"def coerce(self, value):
    """"""Convert dict or string values into RpmFile values.

        Supports structured dicts for more advanced directives or simple
        colon delimited strings for basic files.

        Args:
            value (str or dict): The value to coerce.

        Raises:
            TypeError: If the value is not a dict or string.
            ValueError: For dicts, if the value is missing a required key.
                        For strings, if the value is missing a colon.

        Returns:
            RpmFile: The RpmFile value represented.
        """"""
<mask>:
        if 'src' not in value:
            raise ValueError('The value is missing the ""src"" key')
        if 'dest' not in value:
            raise ValueError('The value is missing the ""dest"" key')
        file_type = None
        file_type_option = None
        config_type = value.get('config', False)
        if config_type:
            file_type = 'config'
            if isinstance(config_type, basestring):
                file_type_option = config_type
        elif value.get('doc', False):
            file_type = 'doc'
        file_attr = value.get('attr', None)
        if file_attr is not None and isinstance(file_attr, MutableMapping):
            if 'permissions' not in file_attr:
                file_attr['permissions'] = '-'
            if 'user' not in file_attr:
                file_attr['user'] = '-'
            if 'group' not in file_attr:
                file_attr['group'] = '-'
        elif file_attr is not None:
            file_attr = None
        return RpmFile(src=value['src'], dest=value['dest'], file_type=file_type, file_type_option=file_type_option, file_attr=file_attr)
    elif isinstance(value, basestring):
        try:
            src, dest = value.split(':')
            return RpmFile(src=src, dest=dest, file_type=None, file_type_option=None, file_attr=None)
        except ValueError:
            raise ValueError('The value {0} is missing a :'.format(value))
    raise TypeError('Could not coerce {0} to an RpmFile'.format(value))","isinstance(value, MutableMapping)",209,"isinstance(value, dict)",False,53.7284965911771,N/A
"@staticmethod
def generate(config, spec):
    """"""Produce file block segments for setting permissions.""""""
    spec.macros['file_permissions_user'] = config.file_permissions.user
    spec.macros['file_permissions_group'] = config.file_permissions.group
    spec.blocks.files.insert(0, '%defattr(-,%{file_permissions_user},%{file_permissions_group},-)')
<mask>:
        spec.blocks.pre.append('id -u %{file_permissions_user} &>/dev/null || useradd %{file_permissions_user}')
    if config.file_permissions.create_group:
        spec.blocks.pre.append('id -g %{file_permissions_group} &>/dev/null || groupadd %{file_permissions_group}')
    return spec",config.file_permissions.create_user,38,config.file_permissions.create_user,True,100.00000000000004,N/A
"def finalize_options(self):
    part = None
    for attr in ('major', 'minor', 'patch'):
<mask>:
            if part is None:
                part = attr
            else:
                from distutils.errors import DistutilsOptionError
                raise DistutilsOptionError('version part options are mutually exclusive')
    self.part = part or 'patch'","getattr(self, attr, False)",36,self.is_mutually_exclusive(attr),False,5.934202609760488,N/A
"def finalize_options(self):
    bump_version.finalize_options(self)
    self.bump_first = any((getattr(self, a, False) for a in ('major', 'minor', 'patch')))
<mask>:
        import re
        current_version = self.distribution.metadata.get_version()
        if not re.search('\\.dev[0-9]+', current_version):
            from distutils.errors import DistutilsSetupError
            raise DistutilsSetupError(""current version (%s) has no '.devN' suffix.\n       Run 'setup.py bump_version', or use any of --major, --minor, --patch options"" % current_version)
    message = self.message
    if message is None:
        if sys.stdin.isatty():
            message = self.edit_release_notes()
        else:
            message = sys.stdin.read()
    if not message.strip():
        from distutils.errors import DistutilsSetupError
        raise DistutilsSetupError('release notes message is empty')
    self.message = 'Release {new_version}\n\n%s' % message",not self.bump_first,84,not self.bump_first,True,100.00000000000004,N/A
"def run(self):
<mask>:
        log.info(""bumping '%s' version"" % self.part)
        self.bumpversion(self.part, commit=False)
        dirty = True
    else:
        dirty = False
    log.info('stripping developmental release suffix')
    self.bumpversion('release', tag=True, message=self.message, allow_dirty=dirty)",self.bump_first,25,self.part,False,28.254432923044853,N/A
"def testMutingOptions(rootPath, cleanUp=True):
    path1, path2, path3 = makeTestFonts(rootPath)
    documentPath = os.path.join(rootPath, 'mutingTest.designspace')
    doc = DesignSpaceDocumentWriter(documentPath, verbose=True)
    doc.addSource(path1, name='master_1', location=dict(width=0), copyLib=True, copyGroups=True, copyInfo=True, copyFeatures=True, muteKerning=True)
    doc.addSource(path2, name='master_2', location=dict(width=1000), copyLib=False, copyGroups=False, copyInfo=False, copyFeatures=False, muteInfo=True, mutedGlyphNames=['glyphThree'])
    doc.startInstance(fileName=path3, familyName='TestInstance', styleName='Regular', location=dict(width=500))
    doc.writeGlyph('glyphFour', mute=True)
    doc.writeKerning()
    doc.writeInfo()
    doc.endInstance()
    doc.save()
    doc = DesignSpaceDocumentReader(documentPath, 2, roundGeometry=True, verbose=True, progressFunc=testingProgressFunc)
    doc.process(makeGlyphs=True, makeKerning=True, makeInfo=True)
    m1 = Font(path1)
    m2 = Font(path2)
    r = Font(path3)
    assert r['glyphThree'].bounds == m1['glyphThree'].bounds
    assert 'glyphFour' not in r
    assert r.info.unitsPerEm == m1.info.unitsPerEm
    assert r.kerning['glyphOne', 'glyphOne'] == m2.kerning['glyphOne', 'glyphOne']
<mask>:
        try:
            os.remove(documentPath)
            shutil.rmtree(path1)
            shutil.rmtree(path2)
            shutil.rmtree(path3)
        except:
            pass
    return True",cleanUp,91,cleanUp,True,100.00000000000004,N/A
"def stripPrefix(d):
    new = []
    for pair, value in d:
        a, b = pair
<mask>:
            a = a[13:]
        if 'public.kern' in b:
            b = b[13:]
        new.append(((a, b), value))
    return sorted(new)",'public.kern' in a,30,'public.kern' in a,True,100.00000000000004,N/A
"def testingProgressFunc(state, action, text, tick):
    """""" Progress function that gets passed to the DesignSpaceDocumentReader should
        report on the faulty kerning pairs it found.
    """"""
    failPair1 = 'invalidInstance.ufo:\nThese kerning pairs failed validation and have been removed:\nglyphOne, public.kern2.@MMK_R_two (-400) conflicts with public.kern1.@MMK_L_one, glyphThree (-250)\npublic.kern1.@MMK_L_one, glyphThree (-250) conflicts with glyphOne, public.kern2.@MMK_R_two (-400)'
<mask>:
        assert failPair1 in text",state == 'error' and action == 'kerning',54,action == 'invalidInstance',False,17.035677145427364,N/A
"def testOuroborosKerning(rootPath, cleanUp=True):
    path1, path2, path3 = makeTestFonts(rootPath)
    documentPath = os.path.join(rootPath, 'kerningTest.designspace')
    doc = DesignSpaceDocumentWriter(documentPath, verbose=True)
    doc.addSource(path1, name='master_1', location=dict(width=0), copyLib=True, copyGroups=True, copyInfo=True, copyFeatures=True)
    doc.addSource(path2, name='master_2', location=dict(width=1000), copyLib=False, copyGroups=False, copyInfo=False, copyFeatures=False)
    doc.startInstance(fileName=path3, familyName='TestInstance', styleName='Regular', location=dict(width=100))
    doc.writeKerning(location=dict(width=500))
    doc.endInstance()
    doc.save()
    doc = DesignSpaceDocumentReader(documentPath, 2, roundGeometry=True, verbose=True, progressFunc=testingProgressFunc)
    doc.process(makeGlyphs=True, makeKerning=True, makeInfo=True)
<mask>:
        shutil.rmtree(path1)
        shutil.rmtree(path2)
        shutil.rmtree(path3)
    return True

    def test1():
        """"""
        >>> import time
        >>> import os
        >>> testOuroborosKerning(os.path.join(os.getcwd(), ""testData""), cleanUp=True)
        True
        """"""",cleanUp,68,cleanUp,True,100.00000000000004,N/A
"def testGeometry(rootPath, cleanUp=True):
    path1, path2, path3, path4, path5 = makeTestFonts(rootPath)
    documentPath = os.path.join(rootPath, 'geometryTest.designspace')
    doc = DesignSpaceDocumentWriter(documentPath, verbose=True)
    doc.addSource(path1, name='master_1', location=dict(width=0), copyLib=True, copyGroups=True, copyInfo=True, copyFeatures=True)
    doc.addSource(path2, name='master_2', location=dict(width=1000), copyLib=False, copyGroups=False, copyInfo=False, copyFeatures=False)
    doc.startInstance(fileName=path3, familyName='TestInstance', styleName='Regular', location=dict(width=500))
    doc.endInstance()
    doc.startInstance(fileName=path4, familyName='TestInstance', styleName='Anisotropic1', location=dict(width=(0, 1000)))
    doc.endInstance()
    doc.startInstance(fileName=path5, familyName='TestInstance', styleName='Anisotropic2', location=dict(width=(1000, 0)))
    doc.endInstance()
    doc.save()
    doc = DesignSpaceDocumentReader(documentPath, 2, roundGeometry=True, verbose=True, progressFunc=testingProgressFunc)
    doc.process(makeGlyphs=True, makeKerning=False, makeInfo=True)
    r1 = Font(path3)
    assert r1['glyphOne'].bounds == (0, 0, 300, 300)
    r2 = Font(path4)
    assert r2['glyphOne'].bounds == (0, 0, 100, 500)
    r3 = Font(path5)
    assert r3['glyphOne'].bounds == (0, 0, 500, 100)
<mask>:
        os.remove(documentPath)
        shutil.rmtree(path1)
        shutil.rmtree(path2)
        shutil.rmtree(path3)
        shutil.rmtree(path4)
        shutil.rmtree(path5)
    return True",cleanUp,99,cleanUp,True,100.00000000000004,N/A
"def __init__(self, path, ufoVersion=1, roundGeometry=False, axes=None, verbose=False, logger=None, bendLocations=False):
    self.path = path
    self.font = self._fontClass()
    self.ufoVersion = ufoVersion
    self.roundGeometry = roundGeometry
    self.bendLocations = bendLocations
<mask>:
        self.axes = axes
    else:
        self.axes = {}
    self.sources = {}
    self.muted = dict(kerning=[], info=[], glyphs={})
    self.mutedGlyphsNames = []
    self.familyName = None
    self.styleName = None
    self.postScriptFontName = None
    self.locationObject = None
    self.unicodeValues = {}
    self.verbose = verbose
    self.logger = None
    if self.verbose:
        self.logger = logging.getLogger('mutatorMath')
    self._failed = []
    self._missingUnicodes = []",axes is not None,75,axes is not None,True,100.00000000000004,N/A
"def setGroups(self, groups, kerningGroupConversionRenameMaps=None):
    """""" Copy the groups into our font. """"""
    skipping = []
    for name, members in groups.items():
        checked = []
        for m in members:
<mask>:
                checked.append(m)
            else:
                skipping.append(m)
        if checked:
            self.font.groups[name] = checked
    if skipping:
        if self.verbose and self.logger:
            self.logger.info('\tNote: some glyphnames were removed from groups: %s (unavailable in the font)', ', '.join(skipping))
    if kerningGroupConversionRenameMaps:
        self.font.kerningGroupConversionRenameMaps = kerningGroupConversionRenameMaps",m in self.font,61,name in self.font.groups,False,41.11336169005198,N/A
"def copyFeatures(self, featureSource):
    """""" Copy the features from this source """"""
<mask>:
        src, loc = self.sources[featureSource]
        if isinstance(src.features.text, str):
            self.font.features.text = u'' + src.features.text
        elif isinstance(src.features.text, unicode):
            self.font.features.text = src.features.text",featureSource in self.sources,30,featureSource in self.sources,True,100.00000000000004,N/A
"def makeUnicodeMapFromSources(self):
    """""" Create a dict with glyphName -> unicode value pairs
            using the data in the sources. 
            If all master glyphs have the same unicode value
            this value will be used in the map.
            If master glyphs have conflicting value, a warning will be printed, no value will be used.
            If only a single master has a value, that value will be used.
        """"""
    values = {}
    for locationName, (source, loc) in self.sources.items():
        for glyph in source:
<mask>:
                if glyph.name not in values:
                    values[glyph.name] = {}
            for u in glyph.unicodes:
                values[glyph.name][u] = 1
    for name, u in values.items():
        if len(u) == 0:
            if '.' not in name:
                self._missingUnicodes.append(name)
            continue
        k = list(u.keys())
        self.unicodeValues[name] = k
    return self.unicodeValues",glyph.unicodes is not None,118,locationName == glyph.location,False,16.233395773754953,N/A
"def addInfo(self, instanceLocation=None, sources=None, copySourceName=None):
    """""" Add font info data. """"""
<mask>:
        instanceLocation = self.locationObject
    infoObject = self.font.info
    infoMasters = []
    if sources is None:
        sources = self.sources
    items = []
    for sourceName, (source, sourceLocation) in sources.items():
        if sourceName in self.muted['info']:
            continue
        items.append((sourceLocation, MathInfo(source.info)))
    try:
        bias, m = buildMutator(items, axes=self.axes)
    except:
        if self.logger:
            self.logger.exception('Error processing font info. %s', items)
        return
    instanceObject = m.makeInstance(instanceLocation, bend=self.bendLocations)
    if self.roundGeometry:
        try:
            instanceObject = instanceObject.round()
        except AttributeError:
            warnings.warn('MathInfo object missing round() method.')
    instanceObject.extractInfo(self.font.info)
    if copySourceName is not None:
        if not copySourceName in sources:
            if self.verbose and self.logger:
                self.logger.info('Copy info source %s not found, skipping.', copySourceName)
                return
        copySourceObject, loc = sources[copySourceName]
        self._copyFontInfo(self.font.info, copySourceObject.info)",instanceLocation is None,107,instanceLocation is None,True,100.00000000000004,N/A
"def build(documentPath, outputUFOFormatVersion=2, roundGeometry=True, verbose=True, logPath=None, progressFunc=None, bendLocations=False):
    """"""

		Simple builder for UFO designspaces.

	""""""
    from mutatorMath.ufo.document import DesignSpaceDocumentReader
    import os, glob
<mask>:
        todo = glob.glob(os.path.join(documentPath, '*.designspace'))
    else:
        todo = [documentPath]
    results = []
    for path in todo:
        reader = DesignSpaceDocumentReader(path, ufoVersion=outputUFOFormatVersion, roundGeometry=roundGeometry, verbose=verbose, logPath=logPath, progressFunc=progressFunc)
        reader.process(bendLocations=bendLocations)
        results.append(reader.results)
    reader = None
    return results",os.path.isdir(documentPath),53,"os.path.isdir(os.path.join(documentPath, '*.designspace'))",False,26.529518334824456,N/A
"def _indent(elem, whitespace='    ', level=0):
    i = '\n' + level * whitespace
<mask>:
        if not elem.text or not elem.text.strip():
            elem.text = i + whitespace
        if not elem.tail or not elem.tail.strip():
            elem.tail = i
        for elem in elem:
            _indent(elem, whitespace, level + 1)
        if not elem.tail or not elem.tail.strip():
            elem.tail = i
    elif level and (not elem.tail or not elem.tail.strip()):
        elem.tail = i",len(elem),62,len(elem) > 0,False,50.81327481546149,N/A
"def __init__(self, path, toolVersion=3, verbose=False):
    self.path = path
    self.toolVersion = toolVersion
    self.verbose = verbose
    self.root = ET.Element('designspace')
    self.root.attrib['format'] = '%d' % toolVersion
    self.root.append(ET.Element('axes'))
    self.root.append(ET.Element('sources'))
    self.root.append(ET.Element('instances'))
    self.logger = None
<mask>:
        self.logger = logging.getLogger('mutatorMath')
    self.currentInstance = None",verbose,35,self.verbose,False,27.516060407455225,N/A
"def save(self, pretty=True):
    """""" Save the xml. Make pretty if necessary. """"""
    self.endInstance()
<mask>:
        _indent(self.root, whitespace=self._whiteSpace)
    tree = ET.ElementTree(self.root)
    tree.write(self.path, encoding='utf-8', method='xml', xml_declaration=True)
    if self.logger:
        self.logger.info('Writing %s', self.path)",pretty,28,pretty,True,100.00000000000004,N/A
"def _makeLocationElement(self, locationObject, name=None):
    """""" Convert Location object to an locationElement.""""""
    locElement = ET.Element('location')
<mask>:
        locElement.attrib['name'] = name
    for dimensionName, dimensionValue in locationObject.items():
        dimElement = ET.Element('dimension')
        dimElement.attrib['name'] = dimensionName
        if type(dimensionValue) == tuple:
            dimElement.attrib['xvalue'] = '%f' % dimensionValue[0]
            dimElement.attrib['yvalue'] = '%f' % dimensionValue[1]
        else:
            dimElement.attrib['xvalue'] = '%f' % dimensionValue
        locElement.append(dimElement)
    return locElement",name is not None,52,name,False,4.9787068367863965,N/A
"def addSource(self, path, name, location, copyLib=False, copyGroups=False, copyInfo=False, copyFeatures=False, muteKerning=False, muteInfo=False, mutedGlyphNames=None, familyName=None, styleName=None):
    """"""
        Add a new UFO source to the document.
        *   path:           path to this UFO, will be written as a relative path to the document path.
        *   name:           reference name for this source
        *   location:       name of the location for this UFO
        *   copyLib:        copy the contents of this source to instances
        *   copyGroups:     copy the groups of this source to instances
        *   copyInfo:       copy the non-numerical fields from this source.info to instances.
        *   copyFeatures:   copy the feature text from this source to instances
        *   muteKerning:    mute the kerning data from this source
        *   muteInfo:       mute the font info data from this source
        *   familyName:     family name for this UFO (to be able to work on the names without reading the whole UFO)
        *   styleName:      style name for this UFO (to be able to work on the names without reading the whole UFO)

        Note: no separate flag for mute font: the source is just not added.
        """"""
    sourceElement = ET.Element('source')
    sourceElement.attrib['filename'] = self._posixPathRelativeToDocument(path)
    sourceElement.attrib['name'] = name
<mask>:
        libElement = ET.Element('lib')
        libElement.attrib['copy'] = '1'
        sourceElement.append(libElement)
    if copyGroups:
        groupsElement = ET.Element('groups')
        groupsElement.attrib['copy'] = '1'
        sourceElement.append(groupsElement)
    if copyFeatures:
        featuresElement = ET.Element('features')
        featuresElement.attrib['copy'] = '1'
        sourceElement.append(featuresElement)
    if copyInfo or muteInfo:
        infoElement = ET.Element('info')
        if copyInfo:
            infoElement.attrib['copy'] = '1'
        if muteInfo:
            infoElement.attrib['mute'] = '1'
        sourceElement.append(infoElement)
    if muteKerning:
        kerningElement = ET.Element('kerning')
        kerningElement.attrib['mute'] = '1'
        sourceElement.append(kerningElement)
    if mutedGlyphNames:
        for name in mutedGlyphNames:
            glyphElement = ET.Element('glyph')
            glyphElement.attrib['name'] = name
            glyphElement.attrib['mute'] = '1'
            sourceElement.append(glyphElement)
    if familyName is not None:
        sourceElement.attrib['familyname'] = familyName
    if styleName is not None:
        sourceElement.attrib['stylename'] = styleName
    locationElement = self._makeLocationElement(location)
    sourceElement.append(locationElement)
    self.root.findall('.sources')[0].append(sourceElement)",copyLib,270,copyLib,True,100.00000000000004,N/A
"def buildMutator(items, axes=None, bias=None):
    """"""
        Build a mutator with the (location, obj) pairs in items.
        Determine the bias based on the given locations.
    """"""
    from mutatorMath.objects.bender import Bender
    items = [(Location(loc), obj) for loc, obj in items]
<mask>:
        bias = Location()
    else:
        bias = Location(bias)
    m = Mutator()
    if axes is not None:
        bender = Bender(axes)
        m.setBender(bender)
    else:
        bender = noBend
    items = sorted(items)
    if not bias:
        bias = biasFromLocations([loc for loc, obj in items], True)
    m.setBias(bias)
    n = None
    ofx = []
    onx = []
    for loc, obj in items:
        nn = loc - bias
        if nn.isOrigin():
            m.setNeutral(obj)
            break
    if m.getNeutral() is None:
        raise MutatorError('Did not find a neutral for this system', items)
    for loc, obj in items:
        lb = loc - bias
        if lb.isOrigin():
            continue
        if lb.isOnAxis():
            onx.append((lb, obj - m.getNeutral()))
        else:
            ofx.append((lb, obj - m.getNeutral()))
    for loc, obj in onx:
        m.addDelta(loc, obj, punch=False, axisOnly=True)
    for loc, obj in ofx:
        m.addDelta(loc, obj, punch=True, axisOnly=True)
    return (bias, m)",bias is None,159,bias is None,True,100.00000000000004,N/A
"def addDelta(self, location, aMathObject, deltaName=None, punch=False, axisOnly=True):
    """""" Add a delta at this location.
            *   location:   a Location object
            *   mathObject: a math-sensitive object
            *   deltaName: optional string/token
            *   punch:
                *   True: add the difference with the instance value at that location and the delta
                *   False: just add the delta.
        """"""
<mask>:
        r = self.getInstance(location, axisOnly=axisOnly)
        if r is not None:
            self[location.asTuple()] = (aMathObject - r, deltaName)
        else:
            raise MutatorError('Could not get instance.')
    else:
        self[location.asTuple()] = (aMathObject, deltaName)",punch,79,aMathObject is not None,False,0.0,N/A
"def _collectAxisPoints(self):
    """"""
            Return a dictionary with all on-axis locations.
        """"""
    for l, (value, deltaName) in self.items():
        location = Location(l)
        name = location.isOnAxis()
<mask>:
            if name not in self._axes:
                self._axes[name] = []
            if l not in self._axes[name]:
                self._axes[name].append(l)
    return self._axes",name is not None and name is not False,40,name,False,0.0335462627902512,N/A
"def _collectOffAxisPoints(self):
    """"""
            Return a dictionary with all off-axis locations.
        """"""
    offAxis = {}
    for l, (value, deltaName) in self.items():
        location = Location(l)
        name = location.isOnAxis()
<mask>:
            offAxis[l] = 1
    return list(offAxis.keys())",name is None or name is False,32,name,False,0.24787521766663595,N/A
"def getInstance(self, aLocation, axisOnly=False, getFactors=False):
    """""" Calculate the delta at aLocation.
            *   aLocation:  a Location object, expected to be in bent space
            *   axisOnly:
                *   True: calculate an instance only with the on-axis masters.
                *   False: calculate an instance with on-axis and off-axis masters.
            *   getFactors:
                *   True: return a list of the calculated factors.
        """"""
    self._collectAxisPoints()
    factors = self.getFactors(aLocation, axisOnly)
    total = None
    for f, item, name in factors:
<mask>:
            total = f * item
            continue
        total += f * item
    if total is None:
        total = 0 * self._neutral
    if getFactors:
        return (total, factors)
    return total",total is None,98,total is None,True,100.00000000000004,N/A
"def numberToString(value):
<mask>:
        return 'None'
    if type(value) == tuple:
        t = []
        for v in value:
            t.append(numberToString(v))
        return '(%s)' % ','.join(t)
    if int(value) == value:
        return '%d' % value
    return '%3.3f' % value",value is None,33,value is None,True,100.00000000000004,N/A
"def __lt__(self, other):
<mask>:
        return True
    elif len(self) > len(other):
        return False
    self_keys = sorted(self.keys())
    other_keys = sorted(other.keys())
    for i, key in enumerate(self_keys):
        if key < other_keys[i]:
            return True
        elif key > other_keys[i]:
            return False
        if self[key] < other[key]:
            return True
    return False",len(self) < len(other),43,self == other,False,5.4424142191183185,N/A
"def expand(self, axisNames):
    """"""
        Expand the location with zero values for all axes in axisNames that aren't filled in the current location.
        ::

            >>> l = Location(pop=1)
            >>> l.expand(['snap', 'crackle'])
            >>> print(l)
            <Location crackle:0, pop:1, snap:0 >
        """"""
    for k in axisNames:
<mask>:
            self[k] = 0",k not in self,46,self[k] is None,False,9.652434877402245,N/A
"def getType(self, short=False):
    """"""Return a string describing the type of the location, i.e. origin, on axis, off axis etc.

        ::
            
            >>> l = Location()
            >>> l.getType()
            'origin'
            >>> l = Location(pop=1)
            >>> l.getType()
            'on-axis, pop'
            >>> l = Location(pop=1, snap=1)
            >>> l.getType()
            'off-axis, pop snap'
            >>> l = Location(pop=(1,2))
            >>> l.getType()
            'on-axis, pop, split'
        """"""
<mask>:
        return 'origin'
    t = []
    onAxis = self.isOnAxis()
    if onAxis is False:
        if short:
            t.append('off-axis')
        else:
            t.append('off-axis, ' + ' '.join(self.getActiveAxes()))
    elif short:
        t.append('on-axis')
    else:
        t.append('on-axis, %s' % onAxis)
    if self.isAmbivalent():
        t.append('split')
    return ', '.join(t)",self.isOrigin(),91,self.isOrigin(),True,100.00000000000004,N/A
"def asString(self, strict=False):
    """"""
        Return the location as a string.
        ::
        
            >>> l = Location(pop=1, snap=(-100.0, -200))
            >>> l.asString()
            'pop:1, snap:(-100.000,-200.000)'
        """"""
<mask>:
        return 'origin'
    v = []
    n = []
    try:
        for name, value in self.asTuple():
            s = ''
            if value is None:
                s = 'None'
            elif type(value) == tuple or type(value) == list:
                s = '(%.3f,%.3f)' % (value[0], value[1])
            elif int(value) == value:
                s = '%d' % int(value)
            else:
                s = '%.3f' % value
            if s != '':
                n.append('%s:%s' % (name, s))
        return ', '.join(n)
    except TypeError:
        import traceback
        print('Location value error:', name, value)
        for key, value in self.items():
            print('\t\tkey:', key)
            print('\t\tvalue:', value)
        traceback.print_exc()
        return 'error'",len(self.keys()) == 0,108,strict,False,0.0,N/A
"def __init__(self, axes):
    warpDict = {}
    self.maps = {}
    self.warps = {}
    for axisName, axisAttributes in axes.items():
        mapData = axisAttributes.get('map', [])
<mask>:
            if mapData == 0:
                self.warps[axisName] = None
            else:
                self._makeWarpFromList(axisName, mapData, axisAttributes['minimum'], axisAttributes['maximum'])
        elif hasattr(mapData, '__call__'):
            self.warps[axisName] = mapData",type(mapData) == list,40,"isinstance(mapData, list)",False,16.341219448835542,N/A
"def _makeWarpFromList(self, axisName, warpMap, minimum, maximum):
<mask>:
        warpMap = [(minimum, minimum), (maximum, maximum)]
    self.warps[axisName] = warpMap
    if not sum([a == minimum for a, b in warpMap]):
        warpMap = [(minimum, minimum)] + warpMap
    if not sum([a == maximum for a, b in warpMap]):
        warpMap.append((maximum, maximum))
    items = []
    for x, y in warpMap:
        items.append((Location(w=x), y))
    m = WarpMutator()
    items.sort()
    bias = biasFromLocations([loc for loc, obj in items], True)
    m.setBias(bias)
    n = None
    ofx = []
    onx = []
    for loc, obj in items:
        if (loc - bias).isOrigin():
            m.setNeutral(obj)
            break
    if m.getNeutral() is None:
        raise MutatorError('Did not find a neutral for this system', m)
    for loc, obj in items:
        lb = loc - bias
        if lb.isOrigin():
            continue
        if lb.isOnAxis():
            onx.append((lb, obj - m.getNeutral()))
        else:
            ofx.append((lb, obj - m.getNeutral()))
    for loc, obj in onx:
        m.addDelta(loc, obj, punch=False, axisOnly=True)
    for loc, obj in ofx:
        m.addDelta(loc, obj, punch=True, axisOnly=True)
    self.warps[axisName] = m",not warpMap,147,"not sum([a == minimum for a, b in warpMap])",False,2.908317710573757,N/A
"def __call__(self, loc):
    new = loc.copy()
    for dim, warp in self.warps.items():
<mask>:
            new[dim] = loc[dim]
            continue
        if not dim in loc:
            continue
        try:
            new[dim] = warp(loc.get(dim))
        except:
            ex_type, ex, tb = sys.exc_info()
            raise MutatorError('A warpfunction ""%s"" (for axis ""%s"") raised ""%s"" at location %s' % (str(warp), dim, ex, loc.asString()), loc)
    return new",warp is None,52,"isinstance(warp, tuple)",False,8.116697886877475,N/A
"def warpFunc_1(value):
<mask>:
        return (value[0] * 2, value[1] * 2)
    return value * 2","isinstance(value, tuple)",14,"isinstance(value, tuple)",True,100.00000000000004,N/A
"def _create_alerts(self, alerts):
<mask>:
        alerts = map_and_group(alerts, self.group_alerts_by, self.json_composer.compose, self.max_payload)
    else:
        alerts = map_prom_alerts_to_teams_alerts(alerts)
    return self.json_composer.compose_all(alerts)",self.group_alerts_by,16,self.group_alerts_by,True,100.00000000000004,N/A
"def _update_application_configuration(application, configuration):
<mask>:
        application.config['MICROSOFT_TEAMS'] = configuration['Microsoft Teams']
    if 'Microsoft Teams Client' in configuration:
        application.config['TEAMS_CLIENT_CONFIG'] = {'TIMEOUT': configuration.getint('Microsoft Teams Client', 'RequestTimeout'), 'RETRY_ENABLE': configuration.getboolean('Microsoft Teams Client', 'RetryEnable'), 'RETRY_WAIT_TIME': configuration.getint('Microsoft Teams Client', 'RetryWaitTime'), 'MAX_PAYLOAD': configuration.getint('Microsoft Teams Client', 'MaxPayload')}
    if 'Template' in configuration and 'Path' in configuration['Template']:
        application.config['TEMPLATE_PATH'] = configuration['Template']['Path']
    if 'Log' in configuration and 'Level' in configuration['Log']:
        application.config['LOG_LEVEL'] = configuration['Log']['Level']
    if 'Log' in configuration and 'Path' in configuration['Log']:
        application.config['LOG_FILE_PATH'] = configuration['Log']['Path']
    if 'Group Alerts' in configuration:
        application.config['GROUP_ALERTS_BY'] = configuration['Group Alerts']['Field']
    if 'HTTP Server' in configuration:
        if 'Host' in configuration['HTTP Server']:
            _host = configuration['HTTP Server']['Host']
            application.config['HOST'] = _host
        if 'Port' in configuration['HTTP Server']:
            _port = configuration['HTTP Server']['Port']
            application.config['PORT'] = _port
    if 'Labels' in configuration:
        application.config['LABELS_EXCLUDED'] = tuple(configuration['Labels']['Excluded'].replace(' ', '').split(','))
    if 'Annotations' in configuration:
        application.config['ANNOTATIONS_EXCLUDED'] = tuple(configuration['Annotations']['Excluded'].replace(' ', '').split(','))",'Microsoft Teams' in configuration,125,'Microsoft Teams' in configuration,True,100.00000000000004,N/A
"def _config_provided(filepath):
    config = configparser.ConfigParser()
    try:
        with open(filepath) as f_prov:
            config.read_file(f_prov)
<mask>:
            raise MissingConnectorConfigKeyException('missing connector key in provided config')
    except configparser.NoSectionError:
        raise MissingConnectorConfigKeyException('missing required Microsoft Teams / connector key in provided config')
    return config",not config.options('Microsoft Teams'),34,config.connector_key is None,False,11.386050660556927,N/A
"def setup_logging(application):
    with open(os.path.join(root, 'config/logging.yml'), 'rt') as f:
        config = yaml.safe_load(f.read())
        for logger in config['loggers']:
            config['loggers'][logger]['level'] = application.config['LOG_LEVEL']
        config['root']['level'] = application.config['LOG_LEVEL']
        config['loggers']['prom2teams_app']['level'] = 'INFO'
        environment = os.getenv('APP_ENVIRONMENT', 'None')
<mask>:
            config['handlers']['file']['filename'] = application.config['LOG_FILE_PATH']
            for logger in config['loggers']:
                config['loggers'][logger]['handlers'] = ['file']
            config['root']['handlers'] = ['file']
        else:
            del config['handlers']['file']
        logging.config.dictConfig(config)",environment == 'pro' or environment == 'pre',46,environment == 'Windows',False,17.035677145427364,N/A
"def config_app(application):
    try:
        application.config.from_object('prom2teams.config.settings')
        instance = os.path.join(os.path.join(root, os.pardir), 'instance')
        config = os.path.join(instance, 'config.py')
<mask>:
            application.config.from_pyfile(config)
        if 'APP_CONFIG_FILE' in os.environ:
            application.config['APP_CONFIG_FILE'] = os.environ.get('APP_CONFIG_FILE')
            config_provided = _config_provided(os.getenv('APP_CONFIG_FILE'))
            _update_application_configuration(application, config_provided)
        command_line_args = _config_command_line()
        if command_line_args.configpath:
            application.config['APP_CONFIG_FILE'] = command_line_args.configpath
            config_provided = _config_provided(command_line_args.configpath)
            _update_application_configuration(application, config_provided)
        if command_line_args.loglevel:
            application.config['LOG_LEVEL'] = command_line_args.loglevel
        if command_line_args.logfilepath:
            application.config['LOG_FILE_PATH'] = command_line_args.logfilepath
        if command_line_args.templatepath:
            application.config['TEMPLATE_PATH'] = command_line_args.templatepath
        if command_line_args.groupalertsby:
            application.config['GROUP_ALERTS_BY'] = command_line_args.groupalertsby
        if command_line_args.enablemetrics or os.environ.get('PROM2TEAMS_PROMETHEUS_METRICS', False):
            os.environ['DEBUG_METRICS'] = 'True'
            if application.config['ENV'] == 'werkzeug':
                from prometheus_flask_exporter import PrometheusMetrics
                metrics = PrometheusMetrics(application)
            else:
                from prometheus_flask_exporter.multiprocess import UWsgiPrometheusMetrics
                metrics = UWsgiPrometheusMetrics(application)
                metrics.start_http_server(int(os.getenv('PROMETHEUS_MULTIPROC_PORT')))
        if 'MICROSOFT_TEAMS' not in application.config:
            raise MissingConnectorConfigKeyException('missing connector key in config')
    except MissingConnectorConfigKeyException:
        sys.exit('No Microsoft Teams connector available')",os.path.isdir(instance) and os.path.exists(config),106,os.path.exists(config),False,32.46524673583499,N/A
"def error_handler(e):
    msg = 'An unhandled exception occurred. {}'.format(e)
    log.exception(msg)
<mask>:
        return (str(e), e.code)
    if isinstance(e, ValidationError):
        return (str(e), 400)
    return (str(e), 500)","isinstance(e, MicrosoftTeamsRequestException)",23,"isinstance(e, HTTPError)",False,53.7284965911771,N/A
"def __init__(self, config=None):
    self.session = requests.Session()
    self.session.headers.update({'Content-Type': 'application/json'})
<mask>:
        config = {}
    config = {**TeamsClient.DEFAULT_CONFIG, **config}
    self.timeout = config['TIMEOUT']
    self.max_payload_length = config['MAX_PAYLOAD']
    self.retry = config['RETRY_ENABLE']
    self.wait_time = config['RETRY_WAIT_TIME']",config is None,28,config is None,True,100.00000000000004,N/A
"def post(self, teams_webhook_url, message):

    @retry(wait=wait_fixed(self.wait_time), after=after_log(log, logging.WARN))
    def post_with_retry(teams_webhook_url, message):
        self._do_post(teams_webhook_url, message)

    def simple_post(teams_webhook_url, message):
        self._do_post(teams_webhook_url, message)
    log.debug('The message that will be sent is: ' + message)
<mask>:
        post_with_retry(teams_webhook_url, message)
    else:
        simple_post(teams_webhook_url, message)",self.retry,33,self.retry_enabled,False,39.76353643835252,N/A
"def _do_post(self, teams_webhook_url, message):
    response = self.session.post(teams_webhook_url, data=message, timeout=self.timeout)
<mask>:
        exception_msg = 'Error performing request to: {}.\n Returned status code: {}.\n Returned data: {}\n Sent message: {}\n'
        exception_msg = exception_msg.format(teams_webhook_url, str(response.status_code), str(response.text), str(message))
        raise MicrosoftTeamsRequestException(exception_msg, code=response.status_code)",response.status_code != 202 and (response.status_code != 200 or response.text != '1'),36,response.status_code not in OK_CODES,False,8.300644568276356,N/A
"def _map_group(alert_group, compose, payload_limit):
    schema = TeamsAlertSchema()
    combined_alerts = []
    teams_alerts = []
    for alert in alert_group:
        json_alert = schema.dump(_combine_alerts_to_alert([*combined_alerts, alert]))
<mask>:
            teams_alerts.append(schema.dump(_combine_alerts_to_alert([alert])))
            teams_alerts.append(schema.dump(_combine_alerts_to_alert(combined_alerts)))
            combined_alerts.clear()
            json_alert = None
        else:
            combined_alerts.append(alert)
    if json_alert:
        teams_alerts.append(json_alert)
    return teams_alerts",len(compose(json_alert).encode('utf-8')) > payload_limit,35,compose,False,4.139937718785169e-06,N/A
"def remove_double_quotes_from_teams_alert(alert):
    """"""Remove double quotes from all the fields""""""
    for field in alert.__dict__:
<mask>:
            new_inner_map = {}
            for inner_field in alert.__getattribute__(field):
                original_value = alert.__getattribute__(field)[inner_field]
                modified_value = original_value.replace('""', '')
                new_inner_map[inner_field] = modified_value
            alert.__setattr__(field, new_inner_map)
        else:
            original_value = alert.__getattribute__(field)
            modified_value = original_value.replace('""', '')
            alert.__setattr__(field, modified_value)
    return alert",field == 'extra_annotations' or field == 'extra_labels',45,"isinstance(alert.__getattribute__(field), list)",False,3.4585921141027356,N/A
"def __call__(cls, *args, **kwargs):
<mask>:
        cls._instance = super().__call__(*args, **kwargs)
    return cls._instance",cls._instance is None,11,not cls._instance,False,54.75182535069452,N/A
"def __init__(self, template_path=None):
    log.info(template_path)
<mask>:
        template_path = TemplateComposer.DEFAULT_TEMPLATE_PATH
    if not os.path.isfile(template_path):
        raise MissingTemplatePathException('Template {} not exists'.format(template_path))
    template_dir = os.path.dirname(template_path)
    template_name = os.path.basename(template_path)
    loader = FileSystemLoader(template_dir)
    environment = Environment(loader=loader, trim_blocks=True)
    self.template = environment.get_template(template_name)",template_path is None,32,template_path is None,True,100.00000000000004,N/A
"@post_load()
def get_alerts(self, message):
    log.debug('JSON received is:\n%s', str(message))
    prom_alerts = []
    base_labels = ('alertname', 'device', 'fstype', 'instance', 'mountpoint', 'severity')
    excluded = base_labels + self.exclude_fields
    base_annotations = ('description', 'summary')
    excluded_annotations = base_annotations + self.exclude_annotations
    for alert in message['alerts']:
        status = alert['status']
        summary = alert['annotations']['summary']
        instance = alert['labels']['instance']
        name = alert['labels']['alertname']
        description = alert['annotations']['description']
        severity = alert['labels']['severity']
        runbook_url = alert['annotations'].get('runbook_url', '')
        fingerprint = alert.get('fingerprint', '')
        extra_labels = dict()
        extra_annotations = dict()
        for key in alert['labels']:
<mask>:
                extra_labels[key] = alert['labels'][key]
        for key in alert.get('annotations'):
            annotation = alert['annotations'][key]
            annotation_is_not_dict = not isinstance(annotation, dict)
            if key not in excluded_annotations and annotation_is_not_dict:
                extra_annotations[key] = annotation
        alert = PrometheusAlert(name, status, severity, summary, instance, description, fingerprint, runbook_url, extra_labels, extra_annotations)
        prom_alerts.append(alert)
    return prom_alerts",key not in excluded,114,key not in excluded,True,100.00000000000004,N/A
"def get_job_status(s):
    """"""
    extract status and progress % from pan-python string xml response
    regex parse due to pan-python output join breaking xml rules
    :param s is the input string
    :return: status text and progress %
    """"""
    status = s.split('<status>')[1].split('</status>')[0]
    progress = s.split('<progress>')[1].split('</progress>')[0]
    result = s.split('<result>')[1].split('</result>')[0]
    details = ''
<mask>:
        details = s.split('<details>')[1].split('</details>')[0]
    return (status, progress, result, details)",'<details>' in s,57,'<details>' in s,True,100.00000000000004,N/A
"def check_job_status(fw, results):
    """"""
    periodically check job status in the firewall
    :param fw is fw object being queried
    :param results is the xml-string results returned for job status
    """"""
    status = ''
    job_id = get_job_id(results)
    while status != 'FIN':
        fw.op(cmd='<show><jobs><id>{0}</id></jobs></show>'.format(job_id))
        status, progress, result, details = get_job_status(fw.xml_result())
<mask>:
            print('job {0} in progress [ {1}% complete ]'.format(job_id, progress), end='\r', flush=True)
            time.sleep(5)
    print('\njob {0} is complete as {1}'.format(job_id, result))
    print(details)
    if result == 'FAIL':
        print(details)",status != 'FIN',72,status == 'PROGRESS',False,18.99589214128981,N/A
"def commit(device):
    """"""
    commit to device after config is complete
    :param device: device object being configured
    :return:
    """"""
    cmd = '<commit></commit>'
    print('commit config')
    device.commit(cmd=cmd)
    results = device.xml_result()
<mask>:
        check_job_status(device, results)",'<job>' in results,30,results,False,0.24787521766663595,N/A
"def test_set(ip_addr, user, mypassword, dev_type, start_row, stop_row):
    """"""
    expect style scripts to login and send set commands
    :param ip_addr: device ip address
    :param user: login username
    :param mypassword: login password
    """"""
    fw = pexpect.spawn('ssh {0}@{1}'.format(user, ip_addr), encoding='utf-8')
    fw.logfile = sys.stdout
    fw.expect('Password:')
    fw.sendline(mypassword)
    fw.expect('>')
    fw.sendline('set cli pager off')
    fw.expect('>')
    fw.expect('\n')
    fw.expect('>')
    fw.sendline('configure')
    fw.expect('#')
    read_file = '../loadable_configs/sample-mgmt-dhcp/{0}/iron_skillet_{0}_full.conf'.format(dev_type)
    start = False
    with open(read_file) as config_file:
        for counter, line in enumerate(config_file, start=1):
<mask>:
                start = True
                print('start set command sequence')
            if not line.startswith('#') and start is True:
                fw.sendline(line)
                fw.expect('#')
                fw.expect('\n')
                fw.expect('#')
                fw_response = fw.before
                if not fw_response.strip().startswith('[edit]'):
                    print('error found in configuration')
                    print(line)
                    print(fw_response)
                    break
            if counter == stop_row:
                print('end set command sequence')
                exit()",counter == start_row,109,line.startswith('#') and counter == start_row,False,32.37722713145643,N/A
"def create_spreadsheet(config_type):
    """"""
    Generates the full configuration template for a given configuration type (panos or panorama).
    This will use the load order
    :param config_type: currently supported: 'panos' or 'panorama'
    :return: will print full configs to STDOUT and also overwrite the full/iron_skillet_full.xml
    """"""
    set_path = os.path.abspath(os.path.join('..', 'templates', config_type, 'set_commands'))
    sys.path.append(set_path)
    set_file = '{0}/iron_skillet_{1}_full.conf'.format(set_path, config_type)
    config_variables = '{0}/.meta-cnc.yaml'.format(set_path)
    print('creating workbook based on {0}'.format(set_file))
    workbook = xlsxwriter.Workbook('{0}/iron_skillet_{1}_full.xlsx'.format(set_path, config_type))
    worksheet_values = workbook.add_worksheet('values')
    worksheet_values.set_column(0, 0, 30)
    worksheet_values.set_column(1, 1, 30)
    worksheet_set = workbook.add_worksheet('set commands')
    bold = workbook.add_format({'bold': 1})
    try:
        with open(config_variables, 'r') as set_metadata:
            set_variables = oyaml.safe_load(set_metadata.read())
    except IOError as ioe:
        print(f'Could not open metadata file {config_variables}')
        print(ioe)
        sys.exit()
    row = 1
    variable_list = ['first row', 'second row']
    worksheet_values.write(0, 0, 'Variable Name', bold)
    worksheet_values.write(0, 1, 'Variable Value', bold)
    worksheet_values.write(0, 2, 'Description', bold)
    for variable in set_variables['variables']:
        print('working with variable: {0}'.format(variable))
        worksheet_values.write(row, 0, variable['name'])
        worksheet_values.write(row, 1, variable['default'])
        worksheet_values.write(row, 2, variable['description'])
        variable_list.append(variable['name'])
        row += 1
    row = 1
    try:
        with open(set_file, 'r') as set_commands:
            set_list = set_commands.readlines()
    except IOError as ioe:
        print(f'Could not open metadata file {set_file}')
        print(ioe)
        sys.exit()
    for line in set_list:
        env = Environment()
        var_set = sorted(meta.find_undeclared_variables(env.parse(line)))
<mask>:
            line = line.replace('""', '""""').strip()
            var_name = var_set[0]
            jinja_var = '{{ ' + var_name + ' }}'
            cell_pos = variable_list.index(var_name)
            form_line = '=SUBSTITUTE(""{0}"", ""{1}"", \'values\'!B{2})'.format(line, jinja_var, cell_pos)
            worksheet_set.write(row, 0, form_line)
        elif len(var_set) == 2:
            line = line.replace('""', '""""').strip()
            var_name_0 = var_set[0]
            jinja_var_0 = '{{ ' + var_name_0 + ' }}'
            cell_pos_0 = variable_list.index(var_name_0)
            var_name_1 = var_set[1]
            jinja_var_1 = '{{ ' + var_name_1 + ' }}'
            cell_pos_1 = variable_list.index(var_name_1)
            form_line_inner = 'SUBSTITUTE(""{0}"", ""{1}"", \'values\'!B{2})'.format(line, jinja_var_0, cell_pos_0)
            form_line_all = '=SUBSTITUTE({0}, ""{1}"", \'values\'!B{2})'.format(form_line_inner, jinja_var_1, cell_pos_1)
            worksheet_set.write(row, 0, form_line_all)
        else:
            line = line.replace('""', '""""').strip()
            worksheet_set.write(row, 0, line)
        row += 1
    workbook.close()
    print('...done')",len(var_set) == 1,279,row % 2 == 0,False,9.846052248031867,N/A
"def myconfig_newdir(myconfigdir_name, foldertime):
    """"""
    create a new main loadable_configs folder if required then new subdirectories for configs
    :param myconfigdir_name: prefix folder name from the my_variables.py file
    :param foldertime: datetime when script run; to be used as suffix of folder name
    :return: the myconfigdir full path name
    """"""
    myconfigpath = os.path.abspath(os.path.join('..', 'loadable_configs'))
<mask>:
        os.mkdir(myconfigpath, mode=493)
        print('created new loadable config directory')
    myconfigdir = '{0}/{1}-{2}'.format(myconfigpath, myconfigdir_name, foldertime)
    if os.path.isdir(myconfigdir) is False:
        os.mkdir(myconfigdir, mode=493)
        print('\ncreated new archive folder {0}-{1}'.format(myconfigdir_name, foldertime))
    if os.path.isdir('{0}/{1}'.format(myconfigdir, config_type)) is False:
        os.mkdir('{0}/{1}'.format(myconfigdir, config_type))
        print('created new subdirectories for {0}'.format(config_type))
    return myconfigdir",os.path.isdir(myconfigpath) is False,90,os.path.isdir(myconfigpath) is False,True,100.00000000000004,N/A
"def template_save(snippet_name, myconfigdir, config_type, element):
    """"""
    after rendering the template save to the myconfig directory
    each run saves with a unique prefix name + datetime
    :param snippet_name: name of the output file
    :param myconfigdir: path to the my_config directory
    :param config_type: based on initial run list; eg. panos or panorama
    :param element: xml element rendered based on input variables; used as folder name
    :param render_type: type eg. if full or snippets; aligns with folder name
    :return: no value returned (future could be success code)
    """"""
    print('..saving template for {0}'.format(snippet_name))
    filename = snippet_name
    with open('{0}/{1}/{2}'.format(myconfigdir, config_type, filename), 'w') as configfile:
        configfile.write(element)
    var_file = 'loadable_config_vars/config_variables.yaml'
<mask>:
        vfilesrc = var_file
        vfiledst = '{0}/{1}'.format(myconfigdir, var_file)
        shutil.copy(vfilesrc, vfiledst)
    return","os.path.isfile('{0}/{1}'.format(myconfigdir, var_file)) is False",114,config_type == 'full',False,0.24510303821316146,N/A
"def replace_variables(config_type, render_type, input_var):
    """"""
    get the input variables and render the output configs with jinja2
    inputs are read from the template directory and output to my_config
    :param config_type: panos or panorama to read/write to the respective directories
    :param archivetime: datetimestamp used for the output my_config folder naming
    """"""
    config_variables = 'config_variables.yaml'
    context = create_context(config_variables)
    for snippet_var in input_var:
        context[snippet_var] = input_var[snippet_var]
    template_path = os.path.abspath(os.path.join('..', 'templates', config_type))
    sys.path.append(template_path)
    myconfig_path = myconfig_newdir(input_var['output_dir'], input_var['archive_time'])
    print('\nworking with {0} config template'.format(render_type))
<mask>:
        filename = 'iron_skillet_{0}_full.xml'.format(config_type)
    if render_type == 'set_commands':
        filename = 'iron_skillet_{0}_full.conf'.format(config_type)
    element = template_render(filename, template_path, render_type, context)
    template_save(filename, myconfig_path, config_type, element)
    print('\nconfigs have been created and can be found in {0}'.format(myconfig_path))
    print('along with the metadata values used to render the configs\n')
    return",render_type == 'full',119,render_type == 'set_commands',False,51.697315395717055,N/A
"def myconfig_newdir(myconfigdir_name):
    """"""
    create a new main loadable_configs folder if required then new subdirectories for configs
    :param myconfigdir_name: prefix folder name from the my_variables.py file
    :param foldertime: datetime when script run; to be used as suffix of folder name
    :return: the myconfigdir full path name
    """"""
    myconfigpath = os.path.abspath(os.path.join('..', 'loadable_configs'))
<mask>:
        os.mkdir(myconfigpath, mode=493)
        print('created new loadable config directory')
    myconfigdir = '{0}/{1}'.format(myconfigpath, myconfigdir_name)
    if os.path.isdir(myconfigdir) is False:
        os.mkdir(myconfigdir, mode=493)
        print('\ncreated new archive folder {0}'.format(myconfigdir_name))
    if os.path.isdir('{0}/{1}'.format(myconfigdir, config_type)) is False:
        os.mkdir('{0}/{1}'.format(myconfigdir, config_type))
        print('created new subdirectories for {0}'.format(config_type))
    return myconfigdir",os.path.isdir(myconfigpath) is False,87,os.path.isdir(myconfigpath) is False,True,100.00000000000004,N/A
"def replace_variables(config_type, render_type, input_var):
    """"""
    get the input variables and render the output configs with jinja2
    inputs are read from the template directory and output to my_config
    :param config_type: panos or panorama to read/write to the respective directories
    :param archivetime: datetimestamp used for the output my_config folder naming
    """"""
    config_variables = 'config_variables.yaml'
    context = create_context(config_variables)
    for snippet_var in input_var:
        context[snippet_var] = input_var[snippet_var]
    template_path = os.path.abspath(os.path.join('..', 'templates', config_type))
    sys.path.append(template_path)
    myconfig_path = myconfig_newdir(input_var['output_dir'])
    print('\nworking with {0} config template'.format(render_type))
<mask>:
        filename = 'iron_skillet_{0}_full.xml'.format(config_type)
    if render_type == 'set_commands':
        filename = 'iron_skillet_{0}_full.conf'.format(config_type)
    element = template_render(filename, template_path, render_type, context)
    template_save(filename, myconfig_path, config_type, element)
    print('\nconfigs have been created and can be found in {0}'.format(myconfig_path))
    print('along with the metadata values used to render the configs\n')
    return",render_type == 'full',118,render_type == 'set_commands',False,51.697315395717055,N/A
"def build_xpath(parent_doc, xpath, new_element_contents):
    """"""
    attaches a new Element to the document at the specified xpath with the specified contents
    if the xpath is something like '/config/xyz/abc' then create an element 'abc' and attach it as a
    subelement of 'xyz'.
    :param parent_doc: XML document to modify
    :param xpath: where to attach the new element
    :param new_element_contents: contents of the element
    :return: None. parent_doc is modified in place
    """"""
    print('working with this xpath:')
    print(xpath)
    modified_xpath = re.sub('^/config', '.', xpath)
    print(f'Checking xpath {modified_xpath}')
    is_present = parent_doc.find(modified_xpath)
    split_path = modified_xpath.split('/')
    path_to_build = []
    orig_tail = split_path[-1]
    while True:
        tail = split_path[-1]
        print(f'tail is {tail}')
        parent_path = '/'.join(split_path[:-1])
        print(f'parent_path is {parent_path}')
        parent_element = parent_doc.find(parent_path)
        print(f'parent_element is {parent_element}')
        print(f'appending {tail} to path_to_build')
        path_to_build.append(tail)
<mask>:
            print('found a parent element')
            print(parent_element)
            break
        else:
            split_path.pop()
    while len(path_to_build) > 1:
        p = path_to_build.pop()
        print('appending {} to parent_element'.format(p))
        parent_element = ElementTree.SubElement(parent_element, p)
    leaf_node = path_to_build[0]
    if is_present is not None:
        print('tail exists so merging element')
        wrapped_snippet = f'<{leaf_node}>{new_element_contents}</{leaf_node}>'
        snippet_xml = ElementTree.fromstring(wrapped_snippet)
        for item in snippet_xml:
            print(item)
            print('merging to parent_element')
            parent_element = parent_doc.find(modified_xpath)
            parent_element.append(item)
    else:
        wrapped_snippet = f'<{leaf_node}>{new_element_contents}</{leaf_node}>'
        snippet_xml = ElementTree.fromstring(wrapped_snippet)
        print('appending to parent_element')
        parent_element.append(snippet_xml)
    return parent_doc",parent_element is not None,186,parent_element is not None,True,100.00000000000004,N/A
"def generate_full_config_template(config_type):
    """"""
    Generates the full configuration template for a given configuration type (panos or panorama).
    This will use the load order
    :param config_type: currently supported: 'panos' or 'panorama'
    :return: will print full configs to STDOUT and also overwrite the full/iron_skillet_<type>_full.xml
    """"""
    full_config_file_path = os.path.abspath(os.path.join('..', 'templates', config_type, 'baseline', 'baseline.xml'))
    output_file_path = os.path.abspath(os.path.join('..', 'templates', config_type, 'full'.format(config_type), 'iron_skillet_{0}_full.xml'.format(config_type)))
    metadata_file = os.path.abspath(os.path.join('..', 'templates', config_type, 'snippets'.format(config_type), '.meta-cnc.yaml'))
    with open(full_config_file_path, 'r') as full_config_obj:
        full_config_string = full_config_obj.read()
    full_config_element = ElementTree.fromstring(full_config_string)
    full_config = ElementTree.ElementTree(full_config_element)
    config_path = os.path.abspath(os.path.join('..', 'templates', config_type))
    sys.path.append(config_path)
    try:
        with open(metadata_file, 'r') as snippet_metadata:
            service_config = oyaml.safe_load(snippet_metadata.read())
    except IOError as ioe:
        print(f'Could not open metadata file {metadata_file}')
        print(ioe)
        sys.exit()
    for xml_snippet in service_config['snippets']:
        snippet_name = xml_snippet['file']
        xpath = xml_snippet['xpath']
        snippet_path = os.path.join(config_path, 'snippets'.format(config_type), snippet_name)
<mask>:
            print(snippet_path)
            print('this snippet does not actually exist!')
            sys.exit()
        with open(snippet_path, 'r') as snippet_obj:
            snippet_string = snippet_obj.read()
        updated_config = build_xpath(full_config, xpath, snippet_string)
        full_config = updated_config
    print('=' * 80)
    raw_xml = str(ElementTree.tostring(full_config.getroot(), encoding='unicode'))
    with open(output_file_path, 'w') as output_config_obj:
        output_config_obj.write(raw_xml)
    print('=' * 80)",not os.path.exists(snippet_path),160,os.path.exists(snippet_path),False,90.48374180359599,N/A
"def test_bestsinglemodelclassifier_default_classification():
    for data_id in [179, 4135]:
        dataset = fetch_openml(data_id=data_id, as_frame=True)
        dataset.target = dataset.target.astype('category').cat.codes
<mask>:
            crop = len(dataset.data)
        else:
            crop = 2000
        X_train, X_test, y_train, y_test = train_test_split(dataset.data[:crop], dataset.target[:crop], test_size=0.2, random_state=RANDOM_SEED)
        optimizer = automl_alex.BestSingleModelClassifier(models_names=['LinearModel', 'KNeighbors', 'RandomForest', 'LightGBM', 'ExtraTrees', 'MLP'])
        history = optimizer.opt(X_train, y_train, timeout=400, verbose=3)
        predicts = optimizer.predict(X_test)
        score = round(sklearn.metrics.roc_auc_score(y_test, predicts), 4)
        assert score is not None
        assert 0.5 < score <= 1",len(dataset.data) < 2000,64,dataset.target.shape[0] == 2,False,7.495553473355842,N/A
"def test_optimizer_default_classification():
    for data_id in [179, 4135]:
        dataset = fetch_openml(data_id=data_id, as_frame=True)
        dataset.target = dataset.target.astype('category').cat.codes
<mask>:
            crop = len(dataset.data)
        else:
            crop = 2000
        X_train, X_test, y_train, y_test = train_test_split(dataset.data[:crop], dataset.target[:crop], test_size=0.2, random_state=RANDOM_SEED)
        de = DataPrepare(normalization=True, verbose=0)
        X_train = de.fit_transform(X_train)
        X_test = de.transform(X_test)
        for model_name in all_models.keys():
            print(model_name)
            model = all_models[model_name](type_of_estimator='classifier', random_state=RANDOM_SEED)
            history = model.opt(X_train, y_train, timeout=100, verbose=3)
            predicts = model.predict(X_test)
            score = round(sklearn.metrics.roc_auc_score(y_test, predicts), 4)
            print(score)
            assert score is not None
            assert 0.49 < score <= 1",len(dataset.data) < 2000,76,dataset.target.ndim == 2,False,11.044795567078939,N/A
"def test_cross_val_score_classification():
    for data_id in [179]:
        dataset = fetch_openml(data_id=data_id, as_frame=True)
        dataset.target = dataset.target.astype('category').cat.codes
<mask>:
            crop = len(dataset.data)
        else:
            crop = 2000
        X_train, X_test, y_train, y_test = train_test_split(dataset.data[:crop], dataset.target[:crop], test_size=0.2, random_state=RANDOM_SEED)
        de = DataPrepare(normalization=True, verbose=0)
        X_train = de.fit_transform(X_train)
        X_test = de.transform(X_test)
        for model_name in all_models.keys():
            print(model_name)
            model = all_models[model_name](type_of_estimator='classifier', random_state=RANDOM_SEED)
            cv = CrossValidation(estimator=model, folds=10, score_folds=3, n_repeats=1, metric=sklearn.metrics.roc_auc_score, print_metric=False, metric_round=4, random_state=RANDOM_SEED)
            score, score_std = cv.fit_score(X_train, y_train, print_metric=True)
            print(model_name, score, score_std)
            assert score is not None
            assert 0.5 < score <= 1
            cv.fit(X_train, y_train)
            predicts = cv.predict_test(X_test)
            assert predicts is not None
            score_cv1 = round(sklearn.metrics.roc_auc_score(y_test, predicts), 4)
            assert score_cv1 is not None
            assert 0.5 < score_cv1 <= 1
            predicts = cv.predict_train(X_train)
            assert predicts is not None
            score = round(sklearn.metrics.roc_auc_score(y_train, predicts), 4)
            assert score is not None
            assert 0.5 < score <= 1
            if cv.estimator._is_possible_feature_importance():
                feature_importance = cv.get_feature_importance(X_train)
                assert isinstance(feature_importance, pd.DataFrame)
                assert not feature_importance.empty
            cv.save('test_save')
            cv_2 = CrossValidation(estimator=model)
            cv_2 = cv_2.load('test_save')
            predicts = cv_2.predict_test(X_test)
            assert predicts is not None
            score_cv2 = round(sklearn.metrics.roc_auc_score(y_test, predicts), 4)
            assert score_cv2 is not None
            assert 0.5 < score_cv2 <= 1",len(dataset.data) < 2000,171,dataset.target.ndim == 2,False,11.044795567078939,N/A
"def test_automl_default_classification():
    for data_id in [179, 4135]:
        dataset = fetch_openml(data_id=data_id, as_frame=True)
        dataset.target = dataset.target.astype('category').cat.codes
<mask>:
            crop = len(dataset.data)
        else:
            crop = 2000
        X_train, X_test, y_train, y_test = train_test_split(dataset.data[:crop], dataset.target[:crop], test_size=0.2, random_state=RANDOM_SEED)
        model = AutoMLClassifier(random_state=RANDOM_SEED)
        model.fit(X_train, y_train, timeout=600)
        predicts = model.predict(X_test)
        score = round(sklearn.metrics.roc_auc_score(y_test, predicts), 4)
        assert score is not None
        assert 0.5 < score <= 1
        model.save('AutoML_model_1', folder=TMP_FOLDER)
        model_new = AutoMLClassifier(random_state=RANDOM_SEED)
        model_new = model_new.load('AutoML_model_1', folder=TMP_FOLDER)
        predicts = model_new.predict(X_test)
        score2 = round(sklearn.metrics.roc_auc_score(y_test, predicts), 4)
        assert score2 is not None
        assert 0.5 < score2 <= 1
        assert score - score2 == 0.0",len(dataset.data) < 2000,90,dataset.target.ndim == 2,False,11.044795567078939,N/A
"def test_fit_predict_default_classification():
    for data_id in [179, 1461, 31, 1471, 151, 1067, 1046, 1489, 1494]:
        dataset = fetch_openml(data_id=data_id, as_frame=True)
        dataset.target = dataset.target.astype('category').cat.codes
<mask>:
            crop = len(dataset.data)
        else:
            crop = 2000
        X_train, X_test, y_train, y_test = train_test_split(dataset.data[:crop], dataset.target[:crop], test_size=0.2, random_state=RANDOM_SEED)
        de = DataPrepare(normalization=True, verbose=0)
        X_train = de.fit_transform(X_train)
        X_test = de.transform(X_test)
        for model_name in all_models.keys():
            print(model_name)
            model = all_models[model_name](type_of_estimator='classifier', random_state=RANDOM_SEED)
            model.fit(X_train, y_train)
            if model.is_possible_predict_proba():
                predicts = model.predict_proba(X_test)
            else:
                predicts = model.predict(X_test)
            assert predicts is not None
            score = sklearn.metrics.roc_auc_score(y_test, predicts)
            print(model_name, score)
            assert score is not None
            assert 0.49 < score <= 1
            if model._is_possible_feature_importance():
                feature_importance = model.get_feature_importance(X_train)
                assert isinstance(feature_importance, pd.DataFrame)
                assert not feature_importance.empty",len(dataset.data) < 2000,101,dataset.target.shape[0] == 2,False,7.495553473355842,N/A
"def __init__(self, model_param=None, type_of_estimator=None, gpu=False, verbose=None, random_state=42):
    self._gpu = gpu
    self._random_state = random_state
<mask>:
        logger_print_lvl(verbose)
    if type_of_estimator is not None:
        self._type_of_estimator = type_of_estimator
    self.model_param = self._init_default_model_param()
    if model_param is not None:
        self.model_param = self.model_param.update(model_param)",verbose is not None,34,verbose is not None,True,100.00000000000004,N/A
"def predict_or_predict_proba(self, X):
    """"""
        Сheck and if it is possible get predict_proba
        """"""
<mask>:
        predicts = self.predict_proba(X)
    else:
        predicts = self.predict(X)
    return predicts",self.is_possible_predict_proba() and self._type_of_estimator == 'classifier',23,self.proba_predict is not None,False,2.6508999733183747,N/A
"def get_feature_importance(self, train_x, importance_type='gain'):
    """"""
        Return:
            list feature_importance
        """"""
<mask>:
        raise Exception('Model cannot get feature_importance')
    raise NotImplementedError('Pure virtual class.')",not self._is_possible_feature_importance(),19,importance_type == 'gain',False,3.005799339448764,N/A
"def score(self, X_test, y_test, metric=None, print_metric=False, metric_round=4):
<mask>:
        raise Exception('No fit models')
    if metric is None:
        if self._type_of_estimator == 'classifier':
            metric = sklearn.metrics.roc_auc_score
        elif self._type_of_estimator == 'regression':
            metric = sklearn.metrics.mean_squared_error
    if metric.__name__ in predict_proba_metrics:
        y_pred_test = self.predict_or_predict_proba(X_test)
    else:
        y_pred_test = self.predict(X_test)
    score = round(metric(y_test, y_pred_test), metric_round)
    if print_metric:
        logger_print_lvl(3)
        logger.info(f'{metric.__name__}: {score}')
    return score",self.model is None,53,"not hasattr(self, '_type_of_estimator')",False,3.0890553181566975,N/A
"def fit_score(self, X_train, y_train, X_test, y_test, cat_features=None, metric=None, print_metric=False, metric_round=4):
    start = time.time()
    self.fit(X_train, y_train, cat_features=cat_features)
    total_time_fit = round(time.time() - start, 2)
<mask>:
        logger_print_lvl(3)
        logger.info(f'fit time: {total_time_fit} sec')
    score = self.score(X_test, y_test, metric=metric, print_metric=print_metric, metric_round=metric_round)
    return score",print_metric,37,total_time_fit > self.log_fit_sec,False,3.0890553181566975,N/A
"def __init__(self, type_of_estimator: Optional[str]=None, metric: Optional[Callable]=None, metric_round: int=4, gpu: bool=False, random_state: int=42) -> None:
    """"""
        Parameters
        ----------
        type_of_estimator : Optional[str], optional
            ['classifier', 'regression'], by default None
        metric : Callable, optional
            you can use standard metrics from sklearn.metrics or add custom metrics.
            If None, the metric is selected from the type of estimator:
            classifier: sklearn.metrics.roc_auc_score
            regression: sklearn.metrics.mean_squared_error.
        metric_round : int, optional
            round metric score., by default 4
        gpu : bool, optional
            Use GPU?, by default False
        random_state : int, optional
            Controls the generation of the random states for each repetition, by default 42
        """"""
    self._gpu = gpu
    self._random_state = random_state
<mask>:
        self._type_of_estimator = type_of_estimator
    if metric is None:
        if self._type_of_estimator == 'classifier':
            self._metric = sklearn.metrics.roc_auc_score
        elif self._type_of_estimator == 'regression':
            self._metric = sklearn.metrics.mean_squared_error
    else:
        self._metric = metric
    self._metric_round = metric_round",type_of_estimator is not None,128,type_of_estimator is not None,True,100.00000000000004,N/A
"def fit(self, X_train: pd.DataFrame, y_train: Union[list, np.array, pd.DataFrame], X_test: pd.DataFrame, y_test: Union[list, np.array, pd.DataFrame], models_names: Optional[List[str]]=None, verbose: int=3) -> pd.DataFrame:
    """"""
        Fit models from model_list and return scores

        Parameters
        ----------
        X_train : pd.DataFrame
            train data (pd.DataFrame, shape (n_samples, n_features))
        y_train : Union[list, np.array, pd.DataFrame]
            target
        X_test : pd.DataFrame
            test data (pd.DataFrame, shape (n_samples, n_features))
        y_test : Union[list, np.array, pd.DataFrame]
            test target
        models_names : Optional[List[str]], optional
            list of models from automl_alex.models.all_models, by default None
        verbose : int, optional
            print state, by default 3

        Returns
        -------
        pd.DataFrame
            results
        """"""
    logger_print_lvl(verbose)
    result = pd.DataFrame(columns=['Model_Name', 'Score', 'Time_Fit_Sec'])
    score_ls = []
    time_ls = []
<mask>:
        self.models_names = automl_alex.models.all_models.keys()
    else:
        self.models_names = models_names
    result['Model_Name'] = self.models_names
    if verbose > 0:
        disable_tqdm = False
    else:
        disable_tqdm = True
    for model_name in tqdm(self.models_names, disable=disable_tqdm):
        start_time = time.time()
        model_tmp = automl_alex.models.all_models[model_name](gpu=self._gpu, random_state=self._random_state, type_of_estimator=self._type_of_estimator)
        model_tmp.fit(X_train, y_train)
        if self._metric.__name__ in predict_proba_metrics and model_tmp.is_possible_predict_proba():
            y_pred = model_tmp.predict_proba(X_test)
        else:
            y_pred = model_tmp.predict(X_test)
        score_model = round(self._metric(y_test, y_pred), self._metric_round)
        score_ls.append(score_model)
        iter_time = round(time.time() - start_time, 2)
        time_ls.append(iter_time)
        model_tmp = None
    result['Score'] = score_ls
    result['Time_Fit_Sec'] = time_ls
    self.result = result
    return result",models_names is None,176,models_names is None,True,100.00000000000004,N/A
"def __init__(self, type_of_estimator: Optional[str]=None, metric: Optional[Callable]=None, metric_round: int=4, gpu: bool=False, random_state: int=42) -> None:
    """"""
        Parameters
        ----------
        type_of_estimator : Optional[str], optional
            ['classifier', 'regression'], by default None
        metric : Callable, optional
            you can use standard metrics from sklearn.metrics or add custom metrics.
            If None, the metric is selected from the type of estimator:
            classifier: sklearn.metrics.roc_auc_score
            regression: sklearn.metrics.mean_squared_error.
        metric_round : int, optional
            round metric score., by default 4
        gpu : bool, optional
            Use GPU?, by default False
        random_state : int, optional
            Controls the generation of the random states for each repetition, by default 42
        """"""
    self._gpu = gpu
    self._random_state = random_state
<mask>:
        self._type_of_estimator = type_of_estimator
    if metric is not None:
        self.metric = metric
    elif self._type_of_estimator == 'classifier':
        self.metric = sklearn.metrics.roc_auc_score
        self.direction = 'maximize'
    elif self._type_of_estimator == 'regression':
        self.metric = sklearn.metrics.mean_squared_error
        self.direction = 'minimize'
    self._metric_round = metric_round",type_of_estimator is not None,134,type_of_estimator is not None,True,100.00000000000004,N/A
"def predict(self, X: pd.DataFrame, verbose: int=0) -> list:
    """"""
        Predict the target for the input data

        Parameters
        ----------
        X : pd.DataFrame
            data (pd.DataFrame, shape (n_samples, n_features))
        verbose : int, optional
            print state, by default 0

        Returns
        -------
        list
            prediction

        Raises
        ------
        Exception
            If No fit models
        """"""
<mask>:
        raise Exception('No fit models')
    logger_print_lvl(verbose)
    X_source = X.copy()
    X[self._cat_cat_features] = X[self._cat_cat_features].astype('str')
    X.fillna(0, inplace=True)
    X[self._cat_cat_features] = X[self._cat_cat_features].astype('category')
    self.predict_model_1 = self.model_1.predict_or_predict_proba(X)
    models_predicts = []
    with open(os.devnull, 'w') as f, contextlib.redirect_stdout(f):
        for i in range(5):
            model = self.model_2.load(f'model_{i + 1}', folder=self._tmp_models_folder, verbose=0)
            predicts = model.predict(X_source)
            models_predicts.append(predicts)
    self.predict_model_2 = pd.DataFrame(models_predicts).mean()
    predicts = self.predict_model_1 * 0.4 + self.predict_model_2 * 0.6
    return predicts",self.model_1 is None,105,not self.fit_models,False,15.207218222740094,N/A
"def logger_print_lvl(verbose):
<mask>:
        lvl = 10
    elif verbose == 2:
        lvl = 20
    elif verbose == 1:
        lvl = 30
    else:
        lvl = 40
    logger.remove()
    logger.add(sys.stderr, colorize=True, format='<green>{time:HH:mm:ss}</green> | <level>{message}</level>', level=lvl)
    logger.add('.automl-alex_tmp/log.log', rotation='1 MB', level='DEBUG', compression='zip')",verbose > 2,36,verbose == 0,False,15.97357760615681,N/A
"def __init__(self, clean_and_encod_data: bool=False, models_names: List[str]=['LinearModel', 'LightGBM', 'XGBoost'], opt_data_prepare: bool=False, cat_encoder_names: List[str]=['HelmertEncoder', 'OneHotEncoder', 'CountEncoder', 'HashingEncoder'], clean_outliers: List[bool]=[True, False], num_generator_select_operations: bool=False, num_generator_operations: List[str]=['/', '*', '-'], normalization=False, folds: int=7, score_folds: int=2, metric: Optional[Callable]=None, metric_round: int=4, cold_start: int=15, opt_lvl: int=1, early_stoping: int=50, auto_parameters: bool=True, feature_selection: bool=False, type_of_estimator=None, gpu: bool=False, random_state: int=42, verbose: Optional[bool]=None):
<mask>:
        logger_print_lvl(verbose)
        self._verbose = verbose
    self._random_state = random_state
    self._gpu = gpu
    self._clean_and_encod_data = clean_and_encod_data
    self._opt_data_prepare = opt_data_prepare
    self._cat_encoder_names = cat_encoder_names
    self._clean_outliers = clean_outliers
    self._num_generator_select_operations = num_generator_select_operations
    self._num_generator_operations = num_generator_operations
    self._normalization = normalization
    if models_names is None:
        self.models_names = list(automl_alex.models.all_models.keys())
    else:
        self.models_names = models_names
    if type_of_estimator is not None:
        self._type_of_estimator = type_of_estimator
    self.folds = folds
    self.score_folds = score_folds
    self.metric_round = metric_round
    self._auto_parameters = auto_parameters
    self.cold_start = cold_start
    self.opt_lvl = opt_lvl
    self.early_stoping = early_stoping
    self.feature_selection = feature_selection
    if metric is None:
        logger.info('metric is None! Default metric will be used. classifier: AUC, regression: MSE')
        if self._type_of_estimator == 'classifier':
            self.metric = sklearn.metrics.roc_auc_score
        elif self._type_of_estimator == 'regression':
            self.metric = sklearn.metrics.mean_squared_error
        else:
            logger.warning('Need to set type_of_estimator!')
    else:
        self.metric = metric",verbose is not None,164,verbose is not None,True,100.00000000000004,N/A
"def __metric_direction_detected__(self, metric, y):
    zero_y = np.zeros(len(y))
    zero_score = metric(y, zero_y)
    best_score = metric(y, y)
<mask>:
        direction = 'maximize'
    else:
        direction = 'minimize'
    return direction",best_score > zero_score,25,zero_score > best_score,False,50.000000000000014,N/A
"def __calc_combined_score_opt__(self, direction, score, score_std):
    """"""
        Args:
            direction (str): 'minimize' or 'maximize'
            score (float): the input score
            score_std (float): the input score_std

        Return:
            score_opt (float): combined score
        """"""
<mask>:
        score_opt = score - score_std
    else:
        score_opt = score + score_std
    return score_opt",direction == 'maximize',42,direction == 'minimize',False,59.460355750136046,N/A
"def __auto_parameters_calc__(self, possible_iters):
    """"""
        Automatic determination of optimization parameters depending on the number of possible iterations

        Args:
            possible_iters (int): possible_iters
            verbose (int): print status

        Return:
            early_stoping (int)
            cv (int)
            score_cv_folds (int)
            opt_lvl (int)
            cold_start (int)
        """"""
    early_stoping = 25
    folds = 7
    score_folds = 3
    opt_lvl = 1
    cold_start = 15
<mask>:
        opt_lvl = 2
        cold_start = 20
        early_stoping = 30
    if possible_iters > 500:
        opt_lvl = 3
        cold_start = 25
        early_stoping = cold_start * 2
    if possible_iters > 800:
        opt_lvl = 4
        score_folds = 4
        cold_start = 40
        early_stoping = cold_start * 2
    if possible_iters > 1500:
        opt_lvl = 5
        score_folds = 5
        cold_start = 50
        early_stoping = cold_start * 2
    return (early_stoping, folds, score_folds, opt_lvl, cold_start)",possible_iters > 100,118,possible_iters > 500,False,66.87403049764218,N/A
"def _tqdm_opt_print(self, pbar, score_opt):
    """"""
        Printing information in tqdm. Use pbar.
        See the documentation for tqdm: https://github.com/tqdm/tqdm
        """"""
<mask>:
        self.best_score = self.study.best_value
        message = f'| Model: {self.model_name} | OptScore: {score_opt} | Best {self.metric.__name__}: {self.best_score} '
        pbar.set_postfix_str(message)
        pbar.update(1)",pbar is not None,37,self.verbose,False,0.0,N/A
"@logger.catch
def fit(self, data: pd.DataFrame, cols: Optional[List[str]]=None) -> None:
    """"""

        Parameters
        ----------
        data : pd.DataFrame
            dataset (pd.DataFrame shape = (n_samples, n_features))
        cols : Optional[List[str]], optional
            cols list features, by default None

        Returns
        -------
        self
        """"""
<mask>:
        data = data[cols]
    data = data._get_numeric_data()
    if self.verbose:
        for col in data.columns:
            pct_missing = np.mean(data[col].isnull())
            if pct_missing > 0.25:
                logger.warning('! Attention {} - {}% Nans!'.format(col, round(pct_missing * 100)))
    self.nan_columns = list(data.columns[data.isnull().sum() > 0])
    if not self.nan_columns:
        logger.info('No nans features')
    if self.method == 'median':
        self.fill_value = data.median()
    elif self.method == 'mean':
        self.fill_value = data.mean()
    else:
        raise ValueError('Wrong fill method')
    return self",cols is not None,96,cols,False,4.9787068367863965,N/A
"@logger.catch
def transform(self, data: pd.DataFrame, cols: Optional[List[str]]=None) -> pd.DataFrame:
    """"""Transforms the dataset.

        Parameters
        ----------
        data : pd.DataFrame
            dataset (pd.DataFrame shape = (n_samples, n_features))
        cols : Optional[List[str]], optional
            cols list features, by default None

        Returns
        -------
        pd.DataFrame
            The dataset with no missing values.
        """"""
<mask>:
        data = data[cols]
    if self.nan_columns:
        for nan_column in self.nan_columns:
            data[nan_column + '_isNAN'] = pd.isna(data[nan_column]).astype('uint8')
        data.fillna(self.fill_value, inplace=True)
    else:
        raise ValueError('No nans features')
    return data",cols is not None,67,cols,False,4.9787068367863965,N/A
"def transform(self, df: pd.DataFrame) -> pd.DataFrame:
    """"""
        Transforms the dataset.

        Parameters
        ----------
        df : pd.DataFrame
            dataset (pd.DataFrame shape = (n_samples, n_features))

        Returns
        -------
        pd.DataFrame
            dataset with new features (pd.DataFrame shape = (n_samples, n_features))

        Raises
        ------
        Exception
            if No fit cols_combinations
        """"""
<mask>:
        raise Exception('No fit cols_combinations')
    fe_df = pd.DataFrame()
    for col1 in self.columns:
        for col2 in self.columns:
            if col1 == col2:
                continue
            else:
                if '/' in self.operations:
                    fe_df['{}_/_{}'.format(col1, col2)] = df[col1] * 1.0 / df[col2]
                if '-' in self.operations:
                    fe_df['{}_-_{}'.format(col1, col2)] = df[col1] - df[col2]
    for c in self._cols_combinations:
        if '*' in self.operations:
            fe_df['{}_*_{}'.format(c[0], c[1])] = df[c[0]] * df[c[1]]
        if '+' in self.operations:
            fe_df['{}_+_{}'.format(c[0], c[1])] = df[c[0]] + df[c[1]]
    return fe_df",self._cols_combinations is None,111,not self._cols_combinations,False,70.1396726799769,N/A
"def fit(self, data: pd.DataFrame, columns: List[str]) -> None:
    """"""
        Fit CleanOutliers

        Parameters
        ----------
        data : pd.DataFrame
            dataset (pd.DataFrame shape = (n_samples, n_features))
        columns : List[str]
            list features names

        Returns
        -------
        self

        Raises
        ------
        ValueError
            Wrong method
        """"""
<mask>:
        chek_columns = columns
    else:
        chek_columns = data._get_numeric_data().columns
    self._weight = {}
    for column in chek_columns:
        if self.method == 'IQR':
            lower_bound, upper_bound = self._IQR(data, colum_name=column, threshold=self.threshold)
            self._weight[column] = [lower_bound, upper_bound]
            if self.verbose:
                total_outliers = len(data[column][(data[column] < lower_bound) | (data[column] > upper_bound)])
        elif self.method == 'z_score':
            median, mad = self._fit_z_score(data, col=column)
            self._weight[column] = [median, mad]
            if self.verbose:
                filtered_entries = self._get_z_score(median, mad, data, column, threshold=self.threshold)
                total_outliers = filtered_entries.sum()
        else:
            raise ValueError('Wrong method')
        if self.verbose:
            if total_outliers > 0:
                logger.info(f'Num of outlier detected: {total_outliers} in Feature {column}')
                logger.info(f'Proportion of outlier detected: {round(100 / (len(data) / total_outliers), 1)} %')
    return self",columns is not None,133,columns,False,4.9787068367863965,N/A
"def transform(self, data: pd.DataFrame) -> pd.DataFrame:
    """"""
        Transforms the dataset.

        Parameters
        ----------
        data : pd.DataFrame
            dataset (pd.DataFrame shape = (n_samples, n_features))

        Returns
        -------
        pd.DataFrame
            dataset with cleaned features (pd.DataFrame shape = (n_samples, n_features))
        """"""
    for weight_values in self._weight:
<mask>:
            data.loc[data[weight_values] < self._weight[weight_values][0], weight_values] = self._weight[weight_values][0]
            data.loc[data[weight_values] > self._weight[weight_values][1], weight_values] = self._weight[weight_values][1]
            feature_name = str(weight_values) + '_Is_Outliers_' + self.method
            data[feature_name] = 0
            data.loc[(data[weight_values] < self._weight[weight_values][0]) | (data[weight_values] > self._weight[weight_values][1]), feature_name] = 1
        elif self.method == 'z_score':
            filtered_entries = self._get_z_score(self._weight[weight_values][0], self._weight[weight_values][1], data, weight_values, threshold=self.threshold)
            data.loc[filtered_entries, weight_values] = data[weight_values].median()
            feature_name = str(weight_values) + '_Is_Outliers_' + self.method
            data[feature_name] = 0
            data.loc[filtered_entries, feature_name] = 1
    return data",self.method == 'IQR',102,self.method == 'x_score',False,51.697315395717055,N/A
"def __init__(self, estimator: Callable, folds: int=7, score_folds: int=5, n_repeats: int=1, metric: Optional[Callable]=None, print_metric: bool=False, metric_round: int=4, random_state: int=42):
    """"""
        Parameters
        ----------
        estimator : Callable
            model object from automl_alex.models
            The object to use to fit model.
        folds : int, optional
            Number of folds., by default 7
        score_folds : int, optional
            Number of score folds, by default 5
        n_repeats : int, optional
            Number of times cross-validator needs to be repeated, by default 1
        metric : Optional[Callable], optional
            you can use standard metrics from sklearn.metrics or add custom metrics.
            If None, the metric is selected from the type of estimator:
            classifier: sklearn.metrics.roc_auc_score
            regression: sklearn.metrics.mean_squared_error., by default None
        print_metric : bool, optional
            print metric, by default False
        metric_round : int, optional
            round metric score, by default 4
        random_state : int, optional
            Controls the generation of the random states for each repetition, by default 42
        """"""
    self.estimator = estimator
    self.folds = folds
    self.score_folds = score_folds
    self.n_repeats = n_repeats
    self.print_metric = print_metric
    self.metric_round = metric_round
<mask>:
        if estimator._type_of_estimator == 'classifier':
            self.metric = sklearn.metrics.roc_auc_score
        elif estimator._type_of_estimator == 'regression':
            self.metric = sklearn.metrics.mean_squared_error
    else:
        self.metric = metric
    if estimator._type_of_estimator == 'classifier':
        self.skf = RepeatedStratifiedKFold(n_splits=folds, n_repeats=n_repeats, random_state=random_state)
    else:
        self.skf = RepeatedKFold(n_splits=folds, n_repeats=n_repeats, random_state=random_state)",metric is None,193,metric is None,True,100.00000000000004,N/A
"def _clean_temp_folder(self):
    Path(TMP_FOLDER).mkdir(parents=True, exist_ok=True)
<mask>:
        shutil.rmtree(TMP_FOLDER + 'cross-v_tmp')
    os.mkdir(TMP_FOLDER + 'cross-v_tmp')",os.path.isdir(TMP_FOLDER + 'cross-v_tmp'),11,os.path.exists(TMP_FOLDER + 'cross-v_tmp'),False,78.25422900366432,N/A
"def fit(self, X: pd.DataFrame, y: Union[list, np.array, pd.DataFrame], cat_features: Optional[List[str]]=None):
    """"""
        Fit the model to the data.

        Parameters
        ----------
        X : pd.DataFrame
            data (pd.DataFrame, shape (n_samples, n_features))
        y : Union[list, np.array, pd.DataFrame]
            target
        cat_features : Optional[List[str]], optional
            features name list. if None -> Auto-detection categorical_features, by default None
        """"""
    self._clean_temp_folder()
<mask>:
        self.cat_features = X.columns[X.nunique() < len(X) // 100]
    else:
        self.cat_features = cat_features
    self.cv_split_idx = [(train_idx, valid_idx) for train_idx, valid_idx in self.skf.split(X, y)]
    for i, (train_idx, valid_idx) in enumerate(self.cv_split_idx):
        train_x, train_y = (X.iloc[train_idx], y.iloc[train_idx])
        self.estimator.fit(X_train=train_x, y_train=train_y)
        self.fited_models[f'model_{self.estimator.__name__}_fold_{i}'] = copy.deepcopy(self.estimator)
    self._fit_models = True
    return self",cat_features is None,93,cat_features is None,True,100.00000000000004,N/A
"def predict_test(self, X_test):
<mask>:
        raise Exception('No fit models')
    stacking_y_pred_test = np.zeros(len(X_test))
    for i in range(self.folds * self.n_repeats):
        X_test_tmp = X_test.copy()
        y_pred_test = self.fited_models[f'model_{self.estimator.__name__}_fold_{i}'].predict_or_predict_proba(X_test_tmp)
        stacking_y_pred_test += y_pred_test
    predict = stacking_y_pred_test / (self.folds * self.n_repeats)
    return predict",not self._fit_models,35,not self.fited_models,False,32.159351091190125,N/A
"def predict_train(self, X):
<mask>:
        raise Exception('No fit models')
    stacking_y_pred_train = np.zeros(len(X))
    for i, (train_idx, valid_idx) in enumerate(self.cv_split_idx):
        val_x = X.iloc[valid_idx]
        y_pred = self.fited_models[f'model_{self.estimator.__name__}_fold_{i}'].predict_or_predict_proba(val_x)
        stacking_y_pred_train[valid_idx] += y_pred
    predict = stacking_y_pred_train / self.n_repeats
    return predict",not self._fit_models,33,not self.fited_models,False,32.159351091190125,N/A
"def _init_model(self, model_param=None):
    """"""
        sets new model,
        Args:
            params: : parameters for model.
        """"""
<mask>:
        model = xgb.XGBClassifier(**model_param)
    elif self._type_of_estimator == 'regression':
        model = xgb.XGBRegressor(**model_param)
    return model",self._type_of_estimator == 'classifier',27,self._type_of_estimator == 'classifier',True,100.00000000000004,N/A
"def predict(self, X=None):
    """"""
        Args:
            X (np.array, shape (n_samples, n_features)): the input data
        Return:
            np.array, shape (n_samples, n_classes)
        """"""
<mask>:
        raise Exception('No fit models')
    if self._type_of_estimator == 'classifier':
        predicts = np.round(self.model.predict(X), 0)
    elif self._type_of_estimator == 'regression':
        predicts = self.model.predict(X)
    return predicts",self.model is None,41,not self.model,False,46.30777161991026,N/A
"def predict_proba(self, X):
    """"""
        Args:
            X (np.array, shape (n_samples, n_features)): the input data
        Return:
            np.array, shape (n_samples, n_classes)
        """"""
<mask>:
        raise Exception('No fit models')
    if not self.is_possible_predict_proba():
        raise Exception('Model cannot predict probability distribution')
    predicts = self.model.predict(X)
    return predicts",self.model is None,38,not self.model,False,46.30777161991026,N/A
"def _init_model(self, model_param=None):
    """"""
        sets new model,
        Args:
            params: : parameters for model.
        """"""
<mask>:
        model = linear_model.LogisticRegression(**model_param)
    elif self._type_of_estimator == 'regression':
        model = linear_model.LinearRegression(**model_param)
    return model",self._type_of_estimator == 'classifier',27,self._type_of_estimator == 'logistic',False,89.31539818068698,N/A
"def get_model_opt_params(self, trial, opt_lvl):
    """"""
        Return:
            dict of DistributionWrappers
        """"""
    model_param = self._init_default_model_param()
    model_param['fit_intercept'] = trial.suggest_categorical('lr_fit_intercept', [True, False])
<mask>:
        model_param['C'] = trial.suggest_uniform('lr_C', 0.1, 100.0)
        model_param['solver'] = trial.suggest_categorical('lr_solver', ['lbfgs', 'saga', 'liblinear'])
        model_param['tol'] = trial.suggest_uniform('lr_tol', 1e-06, 0.1)
        model_param['class_weight'] = trial.suggest_categorical('lr_class_weight', [None, 'balanced'])
        if model_param['solver'] == 'saga':
            model_param['penalty'] = trial.suggest_categorical('lr_penalty', ['l1', 'l2', 'elasticnet'])
            if model_param['penalty'] == 'elasticnet':
                model_param['l1_ratio'] = trial.suggest_uniform('lr_l1_ratio', 0.0, 1.0)
                model_param['max_iter'] = 5000
        if model_param['solver'] == 'liblinear':
            model_param['n_jobs'] = 1
        if model_param['solver'] == 'lbfgs':
            model_param['max_iter'] = 5000
    return model_param",self._type_of_estimator == 'classifier',78,opt_lvl == 'lbfgs',False,7.807646168419154,N/A
"def predict(self, X_test=None):
    """"""
        Args:
            X (np.array, shape (n_samples, n_features)): the input data
        Return:
            np.array, shape (n_samples, n_classes)
        """"""
<mask>:
        raise Exception('No fit models')
    return self.model.predict(X_test)",self.model is None,26,not self.model,False,46.30777161991026,N/A
"def predict_proba(self, X_test):
    """"""
        Args:
            X (np.array, shape (n_samples, n_features)): the input data
        Return:
            np.array, shape (n_samples, n_classes)
        """"""
<mask>:
        raise Exception('No fit models')
    if not self.is_possible_predict_proba():
        raise Exception('Model cannot predict probability distribution')
    return self.model.predict_proba(X_test)[:, 1]",self.model is None,36,not self.model,False,46.30777161991026,N/A
"def _init_model(self, model_param=None):
    """"""
        sets new model,
        Args:
            params: : parameters for model.
        """"""
<mask>:
        model = svm.LinearSVC(**model_param)
    elif self._type_of_estimator == 'regression':
        model = svm.LinearSVR(**model_param)
    return model",self._type_of_estimator == 'classifier',27,self._type_of_estimator == 'linear',False,89.31539818068698,N/A
"def _init_default_model_param(self):
    """"""
        Default model_param
        """"""
    model_param = {'random_seed': self._random_state, 'num_iterations': 300, 'verbose': -1, 'device_type': 'gpu' if self._gpu else 'cpu'}
<mask>:
        model_param['objective'] = 'binary'
    if self._type_of_estimator == 'regression':
        model_param['objective'] = 'regression'
    return model_param",self._type_of_estimator == 'classifier',33,self._type_of_estimator == 'binary',False,89.31539818068698,N/A
"def fit(self, X_train=None, y_train=None, cat_features=None) -> None:
    """"""
        Args:
            X (pd.DataFrame, shape (n_samples, n_features)): the input data
            y (pd.DataFrame, shape (n_samples, ) or (n_samples, n_outputs)): the target data
        Return:
            self
        """"""
    y_train = self.y_format(y_train)
    dtrain = lgb.Dataset(X_train, y_train)
    model_param = self.model_param.copy()
    num_iterations = model_param.pop('num_iterations')
<mask>:
        cat_features = 'auto'
    self.model = lgb.train(model_param, dtrain, num_boost_round=num_iterations, categorical_feature=cat_features)
    dtrain = None
    return self",cat_features is None,59,cat_features is None,True,100.00000000000004,N/A
"def predict_proba(self, X=None):
    """"""
        Args:
            X (np.array, shape (n_samples, n_features)): the input data
        Return:
            np.array, shape (n_samples, n_classes)
        """"""
<mask>:
        raise Exception('No fit models')
    if not self.is_possible_predict_proba():
        raise Exception('Model cannot predict probability distribution')
    return self.model.predict(X)",self.model is None,35,not self.model,False,46.30777161991026,N/A
"def get_feature_importance(self, X, importance_type='gain'):
    """"""
        Return:
            list feature_importance
        """"""
<mask>:
        raise Exception('Model cannot get feature_importance')
    fe_lst = self.model.feature_importance(importance_type=importance_type)
    return pd.DataFrame(fe_lst, index=X.columns, columns=['value'])",not self._is_possible_feature_importance(),22,self.model is None,False,4.773548444510098,N/A
"def _init_default_model_param(self, model_param=None):
    """"""
        Default model_param
        """"""
<mask>:
        model_param = {'verbose': 0, 'task_type': 'GPU' if self._gpu else 'CPU', 'random_seed': self._random_state}
    return model_param",model_param is None,22,model_param is None,True,100.00000000000004,N/A
"def _init_model(self, model_param=None):
    """"""
        sets new model,
        Args:
            params: : parameters for model.
        """"""
<mask>:
        model = catboost.CatBoostClassifier(**model_param)
    elif self._type_of_estimator == 'regression':
        model = catboost.CatBoostRegressor(**model_param)
    return model",self._type_of_estimator == 'classifier',27,self._type_of_estimator == 'classifier',True,100.00000000000004,N/A
"def fit(self, X_train=None, y_train=None, cat_features=None):
    """"""
        Args:
            X (pd.DataFrame, shape (n_samples, n_features)): the input data
            y (pd.DataFrame, shape (n_samples, ) or (n_samples, n_outputs)): the target data
            cat_features (list)
        Return:
            self
        """"""
    y_train = self.y_format(y_train)
<mask>:
        cat_dims = [X_train.columns.get_loc(i) for i in cat_features[:]]
        train_pool = Pool(X_train, label=y_train, cat_features=cat_dims)
    else:
        train_pool = Pool(X_train, label=y_train)
    params = self.model_param.copy()
    self.model = self._init_model(model_param=params)
    self.model.fit(train_pool, verbose=False, plot=False)
    train_pool = None
    return self",cat_features is not None,66,cat_features is not None,True,100.00000000000004,N/A
"def predict_proba(self, X=None):
    """"""
        Args:
            X (np.array, shape (n_samples, n_features)): the input data
        Return:
            np.array, shape (n_samples, n_classes)
        """"""
<mask>:
        raise Exception('No fit models')
    if not self.is_possible_predict_proba():
        raise Exception('Model cannot predict probability distribution')
    return self.model.predict_proba(X)[:, 1]",self.model is None,36,not self.model,False,46.30777161991026,N/A
"def getTeamMembers(self, teamName):
    team0 = self.mmDriver.teams.get_team_by_name(teamName)
    logger.debug('team by name %s : %s' % (teamName, team0))
    teamId = team0['id']
    team = self.mmDriver.teams.check_team_exists(teamName)
    logger.debug('team %s - exists: %s' % (teamName, team['exists']))
<mask>:
        logger.error('no team with name %s found' % teamName)
        return
    logger.debug('found team %s: %s' % (teamName, self.mmDriver.teams.get_team(teamId)))
    users = self._getAllUsersForTeam(teamId)
    logger.debug('found %s users for team ""%s""' % (len(users), teamName))
    return users",not team['exists'],60,not team,False,22.31301601484299,N/A
"def __init__(self, options):
    self._url = self._make_url(options['scheme'], options['url'], options['port'], options['basepath'])
    self._scheme = options['scheme']
    self._basepath = options['basepath']
    self._port = options['port']
    self._auth = options['auth']
<mask>:
        self.activate_verbose_logging()
    self._options = options
    self._token = ''
    self._cookies = None
    self._userid = ''
    self._username = ''
    self._proxies = None
    if options['proxy']:
        self._proxies = {'all://': options['proxy']}",options['debug'],47,self._auth,False,0.0,N/A
"def auth_header(self):
<mask>:
        return None
    if self._token == '':
        return {}
    return {'Authorization': 'Bearer {token:s}'.format(token=self._token)}",self._auth,15,self._token is None,False,30.213753973567677,N/A
"def _build_request(self, method, options=None, params=None, data=None, files=None, basepath=None):
<mask>:
        params = {}
    if data is None:
        data = {}
    if basepath:
        url = self._make_url(self._options['scheme'], self._options['url'], self._options['port'], basepath)
    else:
        url = self.url
    request_params = {'headers': self.auth_header(), 'timeout': self.request_timeout}
    if params is not None:
        request_params['params'] = params
    if method in ('post', 'put'):
        if options is not None:
            request_params['json'] = options
        if data is not None:
            request_params['data'] = data
        if files is not None:
            request_params['files'] = files
    if self._auth is not None:
        request_params['auth'] = self._auth()
    return (self._get_request_method(method, self.client), url, request_params)",params is None,87,params is None,True,100.00000000000004,N/A
"@staticmethod
def _check_response(response):
    try:
        response.raise_for_status()
    except httpx.HTTPStatusError as e:
        try:
            data = e.response.json()
            message = data.get('message', data)
        except ValueError:
            log.debug('Could not convert response to json')
            message = response.text
        log.error(message)
<mask>:
            raise InvalidOrMissingParameters(message) from None
        elif e.response.status_code == 401:
            raise NoAccessTokenProvided(message) from None
        elif e.response.status_code == 403:
            raise NotEnoughPermissions(message) from None
        elif e.response.status_code == 404:
            raise ResourceNotFound(message) from None
        elif e.response.status_code == 405:
            raise MethodNotAllowed(message) from None
        elif e.response.status_code == 413:
            raise ContentTooLarge(message) from None
        elif e.response.status_code == 501:
            raise FeatureDisabled(message) from None
        else:
            raise
    log.debug(response)",e.response.status_code == 400,85,e.response.status_code == 400,True,100.00000000000004,N/A
"@staticmethod
def _get_request_method(method, client):
    method = method.lower()
<mask>:
        return client.post
    elif method == 'put':
        return client.put
    elif method == 'delete':
        return client.delete
    else:
        return client.get",method == 'post',25,method == 'post',True,100.00000000000004,N/A
"def __init__(self, options, client_cls):
    """"""
        :param options: A dict with the values from `default_options`
        :type options: dict
        """"""
    self.options = self.default_options.copy()
<mask>:
        self.options.update(options)
    self.driver = self.options
    if self.options['debug']:
        log.setLevel(logging.DEBUG)
        log.warning('Careful!!\nSetting debug to True, will reveal your password in the log output if you do driver.login()!\nThis is NOT for production!')
    self.client = client_cls(self.options)
    self.websocket = None",options is not None,55,options,False,4.9787068367863965,N/A
"def login(self):
    """"""
        Logs the user in.

        The log in information is saved in the client
                - userid
                - username
                - cookies

        :return: The raw response from the request
        """"""
<mask>:
        self.client.token = self.options['token']
        result = self.users.get_user('me')
    else:
        response = self.users.login_user({'login_id': self.options['login_id'], 'password': self.options['password'], 'token': self.options['mfa_token']})
        if response.status_code == 200:
            self.client.token = response.headers['Token']
            self.client.cookies = response.cookies
        try:
            result = response.json()
        except ValueError:
            log.debug('Could not convert response to json, returning raw response')
            result = response
    log.debug(result)
    if 'id' in result:
        self.client.userid = result['id']
    if 'username' in result:
        self.client.username = result['username']
    return result",self.options['token'],91,self.options['token'],True,100.00000000000004,N/A
"def __init__(self, options, token):
    self.options = options
<mask>:
        log.setLevel(logging.DEBUG)
    self._token = token
    self._alive = False
    self._last_msg = 0",options['debug'],18,self.options.debug,False,10.682175159905848,N/A
"def __init__(self, config=None):
    super(Config, self).__init__()
<mask>:
        logger.debug(""Config file '%s' does not exist"", config)
        self._config = {}
    else:
        with open(config) as fp:
            self._config = yaml.load(fp, Loader=yaml.FullLoader)",not os.path.exists(config),25,not os.path.exists(config),True,100.00000000000004,N/A
"def __init__(self, output_folder):
    super(FileExporter, self).__init__()
    self._output_folder = output_folder
<mask>:
        os.makedirs(self._output_folder)
    if not os.path.isdir(self._output_folder):
        raise Exception(""'{0}' must be a directory"".format(self._output_folder))",not os.path.exists(self._output_folder),19,not os.path.isdir(self._output_folder),False,78.25422900366432,N/A
"def process_dashboard(self, project_name, dashboard_name, dashboard_data):
    super(FileExporter, self).process_dashboard(project_name, dashboard_name, dashboard_data)
    dirname = os.path.join(self._output_folder, project_name)
    try:
        os.makedirs(dirname)
    except OSError as e:
<mask>:
            raise
    dashboard_path = os.path.join(dirname, dashboard_name + '.json')
    logger.info(""Saving dashboard '%s' to '%s'"", dashboard_name, os.path.abspath(dashboard_path))
    with open(dashboard_path, 'w') as f:
        json.dump(dashboard_data, f, sort_keys=True, indent=2, separators=(',', ': '))",e.errno != errno.EEXIST,46,e.errno != errno.EEXIST,True,100.00000000000004,N/A
"def __init__(self, context=None):
    super(Context, self).__init__()
<mask>:
        self._context = None
    else:
        self._context = DictDefaultingToPlaceholder(context)",not context,13,context is None,False,27.516060407455225,N/A
"def expand_placeholders(self, to_expand):
    """"""

        :rtype : dict
        """"""
<mask>:
        return to_expand
    if isinstance(to_expand, basestring):
        result, to_expand = self._expand(to_expand)
        while result != to_expand:
            result, to_expand = self._expand(result)
        if isinstance(result, basestring):
            return string.Formatter().vformat(result, (), self._context)
        else:
            return result
    elif isinstance(to_expand, list):
        return [self.expand_placeholders(value) for value in to_expand]
    elif isinstance(to_expand, dict):
        return dict([(key, self.expand_placeholders(value)) for key, value in to_expand.items()])
    else:
        return to_expand",not self._context,59,not to_expand,False,14.794015674776452,N/A
"def _expand(self, to_expand):
<mask>:
        return (to_expand, to_expand)
    elif self._pattern.match(to_expand) and to_expand[1:-1] in self._context:
        return (self._context[to_expand[1:-1]], to_expand)
    escaped = to_expand.replace('{{', '{{{{').replace('}}', '}}}}')
    return (string.Formatter().vformat(escaped, (), self._context), to_expand)","not isinstance(to_expand, basestring)",26,self._pattern.match(to_expand),False,24.808415001701817,N/A
"def create_context(self, key, value, parent=None):
    contexts = []
<mask>:
        if key in self._keys_to_expand:
            contexts.append((context for data in value for context in self.create_context(key, data, key)))
        else:
            contexts.append(itertools.repeat({key: value}, 1))
    elif isinstance(value, dict):
        for sub_key, sub_value in value.items():
            if parent and len(value) == 1:
                contexts.append(self.create_context(parent, sub_key))
            contexts.append(self.create_context(sub_key, sub_value))
    else:
        contexts.append(itertools.repeat({key: value}, 1))
    for context in itertools.product(*contexts):
        result = {}
        multi = {}
        for context_part in context:
            if len(context_part) == 1:
                result.update(context_part)
            else:
                multi.update(context_part)
        result.update(multi)
        yield result","isinstance(value, list)",74,"isinstance(value, list)",True,100.00000000000004,N/A
"def _process_paths(paths):
    definition_files = set()
    for paths in paths:
<mask>:
            for root, dirs, filenames in os.walk(paths):
                s = set([os.path.join(root, filename) for filename in filenames])
                definition_files = definition_files.union(s)
        else:
            definition_files.add(paths)
    return definition_files",os.path.isdir(paths),31,os.path.isdir(paths),True,100.00000000000004,N/A
"def main():
    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s', level=logging.INFO)
    parser = argparse.ArgumentParser()
    parser.add_argument('-p', '--path', required=True, nargs='+', type=str, help='List of path to YAML definition files')
    parser.add_argument('--project', help='(deprecated, use path) Location of the file containing project definition.')
    parser.add_argument('-o', '--out', help='(deprecated, use config file and file exporter) Path to output folder')
    parser.add_argument('-c', '--config', default='./.grafana/grafana_dashboards.yaml', help=""Configuration file containing fine-tuned setup of builder's components."")
    parser.add_argument('--context', default='{}', help='YAML structure defining parameters for dashboard definition. Effectively overrides any parameter defined on project level.')
    parser.add_argument('--plugins', nargs='+', type=str, help='List of external component plugins to load')
    parser.add_argument('--exporter', nargs='+', type=str, default=set(), dest='exporters', help='List of dashboard exporters')
    args = parser.parse_args()
<mask>:
        for plugin in args.plugins:
            try:
                imp.load_source('grafana_dashboards.components.$loaded', plugin)
            except Exception as e:
                print('Cannot load plugin %s: %s' % (plugin, str(e)))
    if args.project:
        logging.warn(""Using deprecated option '--project'"")
        args.path.add(args.project)
    paths = _process_paths(args.path)
    config = Config(args.config)
    exporters = set(args.exporters)
    if args.out:
        logging.warn(""Using deprecated option '-o/--out'"")
        exporters.add('file')
        config.get_config('file').update(output_folder=args.out)
    dashboard_exporters = _initialize_exporters(exporters, [FileExporter, ElasticSearchExporter, GrafanaExporter], config)
    context = config.get_config('context')
    context.update(yaml.load(args.context, Loader=yaml.FullLoader))
    projects = DefinitionParser().load_projects(paths)
    project_processor = ProjectProcessor(dashboard_exporters)
    project_processor.process_projects(projects, context)",args.plugins,159,args.plugins,True,100.00000000000004,N/A
"def gen_json_from_data(self, data, context):
    template_json = super(GraphiteTarget, self).gen_json_from_data(data, context)
<mask>:
        template_json['target'] = data
    else:
        template_json['target'] = data['target']
    return template_json","isinstance(data, basestring)",19,"isinstance(data, dict)",False,53.7284965911771,N/A
"def gen_json_from_data(self, data, context):
    json_data = super(Dashboard, self).gen_json_from_data(data, context)
    nav = {'type': 'timepicker'}
    json_data.update({'title': data.get('title', self.name), 'nav': [nav]})
<mask>:
        json_data['time'] = {'from': data['time']['from'], 'to': data['time']['to']}
    if 'tags' in data:
        json_data['tags'] = data.get('tags')
    if 'time_options' in data:
        nav['time_options'] = data.get('time_options', [])
    if 'refresh_intervals' in data:
        nav['refresh_intervals'] = data.get('refresh_intervals', [])
    if 'refresh' in data:
        json_data['refresh'] = data.get('refresh')
    if 'folderId' in data:
        json_data['folderId'] = data.get('folderId')
    if get_component_type(Annotations) in data:
        json_data['annotations'] = {'list': self.registry.create_component(Annotations, data).gen_json()}
    if get_component_type(Rows) in data:
        json_data['rows'] = self.registry.create_component(Rows, data).gen_json()
    if get_component_type(Templates) in data:
        json_data['templating'] = {'list': self.registry.create_component(Templates, data).gen_json(), 'enable': True}
    return json_data",'time' in data,92,'time' in data,True,100.00000000000004,N/A
"def gen_json_from_data(self, data, context):
    link_json = super(DashboardLink, self).gen_json_from_data(data, context)
    link_json.update({'type': 'dashboard', 'name': 'Drilldown dashboard', 'title': data.get('title', None), 'dashboard': data.get('dashboard', None)})
<mask>:
        params = []
        for param in data.get('params'):
            if isinstance(param, basestring):
                params.append((param, '$' + param))
            else:
                for key, value in param.items():
                    params.append((key, value))
        link_json['params'] = '&'.join(map(lambda pair: 'var-%s=%s' % (pair[0], pair[1]), params))
    return link_json","'params' in data and isinstance(data.get('params'), list)",54,'params' in data,False,1.8315638888734187,N/A
"def gen_json_from_data(self, data, context):
    super(Query, self).gen_json_from_data(data, context)
    processed_parts = []
    queries = []
<mask>:
        return queries
    if 'name' in data:
        template_json = {'type': 'query', 'name': data['name'], 'query': data['query']}
        if 'refresh' in data:
            template_json['refresh'] = data['refresh']
        elif 'options' in data:
            template_json['refresh'] = 0
        else:
            template_json['refresh'] = 1
        if 'datasource' in data:
            template_json['datasource'] = data['datasource']
        self._copy_data(template_json, data)
        queries.append(template_json)
    else:
        refresh_only_first = data.get('refresh-only-first', False)
        for query_part in data['query'].split('.'):
            if query_part.startswith('$'):
                is_first = False if queries else True
                query = query_part[1:]
                metric = '*'
                template_json = {'type': 'query', 'refresh_on_load': not refresh_only_first or is_first, 'name': query, 'refresh': int(not refresh_only_first or is_first)}
                if 'datasource' in data:
                    template_json['datasource'] = data['datasource']
                if query in data:
                    query_config = data[query]
                    metric = query_config.get('metric', metric)
                    self._copy_data(template_json, query_config)
                template_json['query'] = '.'.join(processed_parts + [metric])
                queries.append(template_json)
            processed_parts.append(query_part)
    return queries",not data.get('query'),125,not data,False,8.208499862389884,N/A
"@staticmethod
def _copy_data(target, source):
<mask>:
        current = source['current']
        target['current'] = {'text': current, 'value': current}
    if 'options' in source:
        target['options'] = [{'text': option, 'value': option} for option in source['options']]
    for key in ['regex', 'multi', 'includeAll', 'hide', 'allFormat', 'allValue']:
        if key in source:
            target[key] = source[key]",'current' in source,44,'current' in source,True,100.00000000000004,N/A
"def gen_json_from_data(self, data, context):
    template_json = super(EnumeratedTemplateBase, self).gen_json_from_data(data, context)
    template_json.update({'type': self._template_type, 'refresh_on_load': False, 'datasource': None, 'name': data['name'], 'query': ','.join([str(options) for options in data['options']]), 'refresh': self._refresh})
<mask>:
        current = data['current']
        template_json['current'] = {'text': current, 'value': current}
    if 'options' in data:
        template_json['options'] = [{'text': option, 'value': option} for option in data['options']]
    return template_json",'current' in data,51,'current' in data,True,100.00000000000004,N/A
"def gen_json_from_data(self, data, context):
    template_json = super(IntervalTemplate, self).gen_json_from_data(data, context)
<mask>:
        template_json['auto'] = True
        template_json['auto_count'] = (data['auto'] or {}).get('count', 30)
        template_json['auto_min'] = (data['auto'] or {}).get('min', '10s')
        auto_option = {'text': 'auto', 'value': '$__auto_interval'}
        template_json.setdefault('options', []).append(auto_option)
        if data.get('current') == 'auto':
            template_json['current'] = auto_option
    return template_json",'auto' in data,42,data.get('auto'),False,9.652434877402245,N/A
"def gen_json_from_data(self, data, context):
    template_json = super(DatasourceTemplate, self).gen_json_from_data(data, context)
    template_json.update({'type': 'datasource', 'name': data.get('name', 'datasource'), 'query': data['query']})
<mask>:
        current = data['current']
        template_json['current'] = {'text': current, 'value': current}
    if 'regex' in data:
        template_json['regex'] = data['regex']
    return template_json",'current' in data,35,'current' in data,True,100.00000000000004,N/A
"def get_contexts(self, context=None):
<mask>:
        context = {}
    data = self.data.copy()
    data.update(context)
    return Context.create_context(data, self._placeholders)",context is None,14,context is None,True,100.00000000000004,N/A
"def gen_json_from_data(self, data, context):
    row_json = super(Row, self).gen_json_from_data(data, context)
    row_json.update({'title': data.get('title', ''), 'height': data.get('height', '250px'), 'showTitle': data.get('showTitle', False), 'collapse': data.get('collapse', False), 'panels': []})
<mask>:
        row_json['panels'] = self.registry.create_component(Panels, data).gen_json()
    return row_json",get_component_type(Panels) in data,30,"data.get('panelType', '') == 'Panels'",False,5.300156689756295,N/A
"def gen_json_from_data(self, data, context):
<mask>:
        data.append(data[0])
    return super(Yaxes, self).gen_json_from_data(data, context)",len(data) == 1,10,"isinstance(data[0], list)",False,10.552670315936318,N/A
"def gen_item_json(self, items, result_list):
<mask>:
        result_list.append(items)
    else:
        super(Yaxes, self).gen_item_json(items, result_list)","isinstance(items, dict) and len(items) > 1",10,"isinstance(items, list)",False,16.731227054577023,N/A
"def _class_for_type(self, component_type):
<mask>:
        component_type = self._types.get(component_type)
    if self._components.get(component_type) is None:
        raise errors.UnregisteredComponentError(""No component of type '%s' found!"" % component_type)
    return component_type","isinstance(component_type, basestring)",22,"isinstance(component_type, str)",False,70.71067811865478,N/A
"def add(self, component):
    """"""

        :type component: dict
        """"""
<mask>:
        raise errors.WrongComponentAttributeCountError('Component must have exactly 2 attributes - name and component type with data.This contains %s attributes' % len(component.keys()))
    component_name = component.get('name')
    if component_name is None:
        logger.info(""Component '%s' does not have 'name' attribute, skipping"", component.keys())
        return
    component_type = None
    for key in component.keys():
        if key == 'name':
            continue
        component_type = key
        break
    try:
        clazz = self._class_for_type(component_type)
    except errors.UnregisteredComponentError:
        logger.info(""Missing implementation class for component '%s', skipping"", component_type)
        return
    logger.debug(""Adding component '%s' with name '%s'"", component_type, component_name)
    components = self._get_component(clazz)
    if component_name in components:
        raise errors.DuplicateKeyError(""Key '%s' is already defined for component %s"" % (component_name, component_type))
    components[component_name] = self.create_component(clazz, component)",len(component) > 2,107,len(component.keys()) != 2,False,15.851165692617148,N/A
"def _get_component(self, item):
    component = self._components.get(item)
<mask>:
        raise errors.UnregisteredComponentError(""No component of type '%s' found!"" % item)
    return component",component is None,18,not component,False,30.326532985631665,N/A
"def get_component(self, component_type, name):
    component = self._get_component(component_type).get(name)
<mask>:
        raise errors.UnregisteredComponentError(""No component '%s' with name '%s' found!"" % (component_type, name))
    return component",component is None,21,not component,False,30.326532985631665,N/A
"def __init__(self, data, registry):
    """"""

        :type registry: ComponentRegistry
        """"""
    super(ComponentBase, self).__init__()
    self.data = data[get_component_type(type(self))]
<mask>:
        self.data = {}
    self.name = data.get('name')
    self.registry = registry",self.data is None,24,self.data is None,True,100.00000000000004,N/A
"def gen_json_from_data(self, data, context):
    panel_json = super(Graph, self).gen_json_from_data(data, context)
    panel_json.update({'type': 'graph', 'title': self.data.get('title', None), 'span': self.data.get('span', 12)})
    targets = self.data.get('targets', [])
<mask>:
        targets.append(self.data['target'])
    self._create_component(panel_json, Targets, {'targets': targets})
    panel_json['nullPointMode'] = self.data.get('nullPointMode', 'null')
    grid_data = self.data.get('grid', {}) or {}
    if 'grid' in self.data or 'y_formats' in self.data:
        panel_json['grid'] = {'leftMax': grid_data.get('leftMax', None), 'rightMax': grid_data.get('rightMax', None), 'leftMin': grid_data.get('leftMin', None), 'rightMin': grid_data.get('rightMin', None), 'threshold1': grid_data.get('threshold1', None), 'threshold2': grid_data.get('threshold2', None), 'threshold1Color': grid_data.get('threshold1Color', 'rgba(216, 200, 27, 0.27)'), 'threshold2Color': grid_data.get('threshold2Color', 'rgba(234, 112, 112, 0.22)')}
    if 'legend' in self.data:
        panel_json['legend'] = {'show': self.data['legend'].get('show', True), 'values': self.data['legend'].get('values', False), 'min': self.data['legend'].get('min', False), 'max': self.data['legend'].get('max', False), 'current': self.data['legend'].get('current', False), 'total': self.data['legend'].get('total', False), 'avg': self.data['legend'].get('avg', False), 'alignAsTable': self.data['legend'].get('alignAsTable', False), 'rightSide': self.data['legend'].get('rightSide', False), 'hideEmpty': self.data['legend'].get('hideEmpty', False), 'hideZero': self.data['legend'].get('hideZero', False), 'sideWidth': self.data['legend'].get('sideWidth', None)}
    if 'tooltip' in self.data:
        panel_json['tooltip'] = {'value_type': self.data['tooltip'].get('value_type', 'individual'), 'shared': self.data['tooltip'].get('shared', False), 'sort': self.data['tooltip'].get('sort', 0)}
    if 'seriesOverrides' in self.data:
        overrides = []
        for override in self.data['seriesOverrides']:
            for alias, settings in override.items():
                to_add = {'alias': alias}
                to_add.update(settings)
                overrides.append(to_add)
        panel_json['seriesOverrides'] = overrides
    self._create_component(panel_json, Links, self.data)
    if ('leftYAxisLabel' in self.data or ('grid' in self.data and ('leftMin' in grid_data or 'leftMax' in grid_data))) and 'y_formats' not in self.data:
        panel_json['y_formats'] = ['short', 'short']
    panel_json['xaxis'] = self.data.get('xaxis', {'show': True, 'format': 'time'})
    self._create_component(panel_json, Yaxes, self.data)
    return panel_json",'target' in self.data,199,self.data.get('target'),False,22.089591134157878,N/A
"def _create_component(self, panel_json, clazz, data):
<mask>:
        panel_json[get_component_type(clazz)] = self.registry.create_component(clazz, data).gen_json()",get_component_type(clazz) in data,10,get_component_type(clazz) not in panel_json,False,57.60844201603898,N/A
"def gen_json_from_data(self, data, context):
    panel_json = super(SingleStat, self).gen_json_from_data(data, context)
    panel_json.update({'type': 'singlestat', 'title': data.get('title', None), 'span': data.get('span', None), 'nullPointMode': data.get('nullPointMode', 'null'), 'valueName': data.get('valueName', 'current')})
    panel_json['targets'] = self.registry.create_component(Targets, data).gen_json() if 'targets' in data else []
<mask>:
        panel_json['sparkline'] = {'show': True, 'full': data['sparkline'].get('full', False), 'lineColor': data['sparkline'].get('lineColor', 'rgb(31, 120, 193)'), 'fillColor': data['sparkline'].get('fillColor', 'rgba(31, 118, 189, 0.18)')}
    if 'gauge' in data:
        panel_json['gauge'] = {'show': True, 'minValue': data['gauge'].get('minValue', 0), 'maxValue': data['gauge'].get('maxValue', 100), 'thresholdMarkers': data['gauge'].get('thresholdMarkers', True), 'thresholdLabels': data['gauge'].get('thresholdLabels', False)}
    if 'colors' not in data:
        panel_json['colors'] = ['rgba(50, 172, 45, 0.97)', 'rgba(237, 129, 40, 0.89)', 'rgba(245, 54, 54, 0.9)']
    if 'valueMaps' in data:
        panel_json['valueMaps'] = [{'value': value, 'op': '=', 'text': text} for value, text in data['valueMaps'].items()]
    if get_component_type(Links) in data:
        panel_json['links'] = self.registry.create_component(Links, data).gen_json()
    return panel_json",'sparkline' in data,118,'sparkline' in data,True,100.00000000000004,N/A
"def gen_json_from_data(self, data, context):
    panel_json = super(Table, self).gen_json_from_data(data, context)
    panel_json.update({'type': 'table', 'title': data.get('title', None), 'span': data.get('span', None), 'targets': [{'target': v} for v in data.get('targets', [])], 'transform': data.get('transform', None), 'columns': [{'text': v, 'value': str(v).lower()} for v in data.get('columns', [])]})
    panel_json['targets'] = self.registry.create_component(Targets, data).gen_json() if 'targets' in data else []
<mask>:
        styles = []
        for override in self.data['styles']:
            for pattern, settings in override.items():
                to_add = {'pattern': pattern}
                to_add.update(settings)
                styles.append(to_add)
        panel_json['styles'] = styles
    return panel_json",'styles' in self.data,72,'styles' in self.data,True,100.00000000000004,N/A
"def __init__(self, **kwargs):
    super(ElasticSearchExporter, self).__init__()
    self._host = os.getenv('ES_HOST', kwargs.get('host'))
    password = os.getenv('ES_PASSWORD', kwargs.get('password'))
    username = os.getenv('ES_USERNAME', kwargs.get('username'))
    use_kerberos = os.getenv('ES_USE_KERBEROS', kwargs.get('use_kerberos'))
<mask>:
        self._connection = KerberosConnection(self._host)
    else:
        self._connection = BasicAuthConnection(username, password, self._host)",use_kerberos,31,use_kerberos,True,100.00000000000004,N/A
"def __init__(self, **kwargs):
    super(GrafanaExporter, self).__init__()
    self._host = os.getenv('GRAFANA_HOST', kwargs.get('host'))
    password = os.getenv('GRAFANA_PASSWORD', kwargs.get('password'))
    username = os.getenv('GRAFANA_USERNAME', kwargs.get('username'))
    auth_token = os.getenv('GRAFANA_TOKEN', kwargs.get('token'))
    use_kerberos = os.getenv('GRAFANA_USE_KERBEROS', kwargs.get('use_kerberos'))
    client_crt = os.getenv('GRAFANA_SSL_CLIENT_CRT', kwargs.get('ssl_client_crt'))
<mask>:
        self._connection = KerberosConnection(self._host)
    elif auth_token:
        self._connection = BearerAuthConnection(auth_token, self._host)
    elif client_crt:
        client_key = os.getenv('GRAFANA_SSL_CLIENT_KEY', kwargs.get('ssl_client_key'))
        derived_key_path = os.path.splitext(client_crt)[0] + '.key'
        if client_key or (not client_key and os.path.exists(derived_key_path)):
            cert_bundle = (client_crt, client_key if client_key else derived_key_path)
        else:
            cert_bundle = client_crt
        self._connection = SSLAuthConnection(self._host, cert_bundle)
    else:
        self._connection = BasicAuthConnection(username, password, self._host)",use_kerberos,79,use_kerberos,True,100.00000000000004,N/A
"def process_dashboard(self, project_name, dashboard_name, dashboard_data):
    super(GrafanaExporter, self).process_dashboard(project_name, dashboard_name, dashboard_data)
    body = {'overwrite': True, 'dashboard': dashboard_data}
<mask>:
        body.update({'folderId': dashboard_data['folderId']})
    logger.info(""Uploading dashboard '%s' to %s"", dashboard_name, self._host)
    self._connection.make_request('/api/dashboards/db', body)",'folderId' in dashboard_data,27,'folderId' in dashboard_data,True,100.00000000000004,N/A
"def load_test_fixtures():
    for component in base.get_generators():
        component_type = grafana_dashboards.common.get_component_type(component)
        dirname = os.path.join(os.path.dirname(os.path.abspath(__file__)), component_type)
<mask>:
            continue
        for f in os.listdir(dirname):
            if not f.endswith('.yaml'):
                continue
            filename = f[:-5]
            with open(os.path.join(dirname, '%s.yaml' % filename), 'r') as fp:
                config = yaml.load(fp, Loader=yaml.FullLoader)
            with open(os.path.join(dirname, '%s.json' % filename), 'r') as fp:
                output = json.load(fp)
            yield (component, filename, config, output)",not os.path.isdir(dirname),54,not os.path.exists(dirname),False,59.694917920196445,N/A
"def test_component(component, config, expected):
    with mock.patch('grafana_dashboards.components.base.ComponentRegistry') as registry:

        def create_component(component_type, data):
            gen = mock.Mock()
<mask>:
                gen.gen_json = mock.Mock(return_value=['mocked ' + str(component_type)])
            else:
                gen.gen_json = mock.Mock(return_value='mocked ' + str(component_type))
            return gen
        registry.create_component = mock.Mock(side_effect=create_component)

        def get_component(component_type, name):
            if name == 'not-mocked':
                raise UnregisteredComponentError(""No component '%s' with name '%s' found!"" % (component_type, name))
            gen = mock.Mock()
            gen.gen_json = mock.Mock(return_value=['mocked ' + str(component_type) + ' for name ' + name])
            return gen
        registry.get_component = mock.Mock(side_effect=get_component)
        assert component(config, registry).gen_json() == expected","inspect.isclass(component_type) and issubclass(component_type, JsonListGenerator)",77,data == 'mocked',False,0.0,N/A
"def do_POST(self):
    global progresses
    content_len = int(self.headers.get('content-length', 0))
    post_body = self.rfile.read(content_len)
    hook = json.loads(post_body)
<mask>:
        called_hooks.append(hook)
    else:
        progresses += 1
    self.send_response(200)
    self.end_headers()
    return",hook['event'] != 'progress',23,hook,False,0.24787521766663595,N/A
"def process_resources(_res_iter):
    for res in _res_iter:

        def process_res(_res):
            for line in _res:
<mask>:
                    line[to_key] = line[from_key].year
                    yield line
    yield process_res(res)",from_key in line,20,from_key in line,True,100.00000000000004,N/A
"def process_resources(_res_iter):
    for res in _res_iter:

        def process_res(_res):
            for line in _res:
<mask>:
                    line[key] = line[key].title()
                    yield line
        yield process_res(res)",key in line,20,key in line,True,100.00000000000004,N/A
"def _get_procesor_env(self, filename):
<mask>:
        engine = create_engine(ENV['DPP_DB_ENGINE'])
        conn = engine.connect()
        conn.execute(text('DROP TABLE IF EXISTS test;'))
    if filename == 'dump_to_sql_update_mode__update':
        engine = create_engine(ENV['DPP_DB_ENGINE'])
        conn = engine.connect()
        conn.execute(text('\n                CREATE TABLE test (\n                  id integer not null primary key,\n                  mystring text,\n                  mynumber double precision,\n                  mydate date\n                )\n            '))
        conn.execute(text(""\n                INSERT INTO test VALUES (1, 'foo', 5.6, null);\n            ""))
    return ENV",ENV['DPP_DB_ENGINE'] != DEFAULT_TEST_DB,57,filename == 'dump_to_sql_delete_mode__delete',False,3.9297193407553004,N/A
"def process_row(row, _1, _2, resource_index, parameters, _):
<mask>:
        row[parameters['column-name']] = parameters['value']
    return row",resource_index == 0,13,parameters['column-name'],False,0.0,N/A
"@click.group(invoke_without_command=True)
@click.pass_context
def cli(ctx):
<mask>:
        click.echo('Available Pipelines:')
        for spec in pipelines():
            ps = status_mgr().get(spec.pipeline_id)
            click.echo('- {} {}{}'.format(spec.pipeline_id, '(*)' if ps.dirty() else '', '(E)' if len(spec.validation_errors) > 0 else ''))
            for error in spec.validation_errors:
                click.echo('\t{}: {}'.format(error.short_msg, error.long_msg))",ctx.invoked_subcommand is None,36,ctx.invoked_subcommand is None,True,100.00000000000004,N/A
"@cli.command()
@click.argument('pipeline_id')
@click.option('--verbose', default=False, is_flag=True)
@click.option('--use-cache/--no-use-cache', default=True, help=""Cache (or don't) intermediate results (if requested in the pipeline)"")
@click.option('--dirty', default=False, is_flag=True, help='Only run dirty pipelines')
@click.option('--force', default=False, is_flag=True)
@click.option('--concurrency', default=1)
@click.option('--slave', default=False, is_flag=True)
def run(pipeline_id, verbose, use_cache, dirty, force, concurrency, slave):
    """"""Run a pipeline by pipeline-id.
       pipeline-id supports '%' wildcard for any-suffix matching,
       'all' for running all pipelines and
       comma-delimited list of pipeline ids""""""
    exitcode = 0
    running = []
    progress = {}

    def progress_cb(report):
        pid, count, success, *_, stats = report
        print('\x1b[%sA' % (1 + len(running)))
<mask>:
            running.append(pid)
        progress[pid] = (count, success)
        for pid in running:
            count, success = progress[pid]
            if success is None:
                if count == 0:
                    print('\x1b[2K%s: \x1b[31m%s\x1b[0m' % (pid, 'WAITING FOR OUTPUT'))
                else:
                    print('\x1b[2K%s: \x1b[33mRUNNING, processed %s rows\x1b[0m' % (pid, count))
            elif success:
                print('\x1b[2K%s: \x1b[32mSUCCESS, processed %s rows\x1b[0m' % (pid, count))
            else:
                print('\x1b[2K%s: \x1b[31mFAILURE, processed %s rows\x1b[0m' % (pid, count))
    results = run_pipelines(pipeline_id, '.', use_cache, dirty, force, concurrency, verbose, progress_cb if not verbose else None, slave)
    if not slave:
        logging.info('RESULTS:')
        errd = False
        for result in results:
            stats = user_facing_stats(result.stats)
            errd = errd or result.errors or (not result.success)
            logging.info('%s: %s %s%s', 'SUCCESS' if result.success else 'FAILURE', result.pipeline_id, repr(stats) if stats is not None else '', '\nERROR log from processor %s:\n+--------\n| ' % result.errors[0] + '\n| '.join(result.errors[1:]) + '\n+--------' if result.errors else '')
    else:
        result_obj = []
        errd = False
        for result in results:
            errd = errd or result.errors or (not result.success)
            stats = user_facing_stats(result.stats)
            result_obj.append(dict(success=result.success, pipeline_id=result.pipeline_id, stats=result.stats, errors=result.errors))
            json.dump(result_obj, sys.stderr)
    if errd:
        exitcode = 1
    exit(exitcode)",pid not in progress,249,count == 0,False,0.0,N/A
"@cli.command()
def version():
    with open(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'VERSION')) as f:
        installed = f.read().strip()
    latest = requests.get('https://pypi.org/pypi/datapackage-pipelines/json').json()['info']['version']
    print(f'Installed version: {installed}')
    print(f'Latest version: {latest}\n')
<mask>:
        print('Datapackage Pipelines upgrade is available, upgrade using pip:\n')
        print('    python3 -m pip install -U datapackage-pipelines\n')
        exit(1)
    else:
        exit(0)",installed != latest,39,installed != latest,True,100.00000000000004,N/A
"def __init__(self, host=os.environ.get('DPP_REDIS_HOST'), port=6379):
    self.redis = None
<mask>:
        conn = redis.StrictRedis(host=host, port=port, db=5)
        try:
            conn.ping()
            self.redis = conn
        except redis.exceptions.ConnectionError:
            logging.warning('Failed to connect to Redis, host:%s, port:%s', host, port)
    else:
        logging.info('Skipping redis connection, host:%s, port:%s', host, port)",host is not None and len(host) > 0,37,host,False,0.004539992976248487,N/A
"@staticmethod
def encode(x):
<mask>:
        return x.encode('utf8')
    if isinstance(x, list):
        return [y.encode('utf8') for y in x]","isinstance(x, str)",15,"isinstance(x, str)",True,100.00000000000004,N/A
"@staticmethod
def decode(x):
<mask>:
        return x.decode('utf8')
    if isinstance(x, (list, set)):
        return [y.decode('utf8') for y in x]
    assert False, 'Unknown type for x: %r' % x","isinstance(x, bytes)",25,"isinstance(x, bytes)",True,100.00000000000004,N/A
"def update(self, spec):
<mask>:
        for dep in spec.dependencies:
            self.redis.sadd(self.dependents_key(dep), self.encode(spec.pipeline_id))
        self.redis.delete(self.dependencies_key(spec.pipeline_id))
        for dep in self.encode(spec.dependencies):
            self.redis.sadd(self.dependencies_key(spec.pipeline_id), dep)",self.is_init(),17,spec.pipeline_id,False,8.51528917838043,N/A
"def get_dependencies(self, pipeline_id):
<mask>:
        members = self.redis.smembers(self.dependencies_key(pipeline_id))
        if members is not None:
            return self.decode(members)
    return []",self.is_init(),16,self.redis is not None,False,15.207218222740094,N/A
"def get_dep_mgr() -> DependencyManager:
    global dep_mgr
<mask>:
        dep_mgr = DependencyManager()
    return dep_mgr",dep_mgr is None,12,dep_mgr is None,True,100.00000000000004,N/A
"def collect_dependencies(pipeline_ids):
<mask>:
        return None
    dm = get_dep_mgr()
    ret = set()
    for pipeline_id in pipeline_ids:
        ret.add(pipeline_id)
        deps = dm.get_dependencies(pipeline_id)
        if deps is not None:
            ret.update(collect_dependencies(deps))
    return ret",pipeline_ids is None,27,pipeline_ids is None,True,100.00000000000004,N/A
"def queue_pipeline(ps: PipelineStatus, spec: PipelineSpec, trigger):
    eid = gen_execution_id()
    logging.info('%s QUEUEING %s task %s', eid[:8], trigger.upper(), spec.pipeline_id)
<mask>:
        execute_pipeline_task.delay(spec.pipeline_id, spec.pipeline_details, spec.path, trigger, eid)
        return True
    return False","ps.queue_execution(eid, trigger)",27,ps.is_delayed(spec.pipeline_id),False,8.516593018819643,N/A
"@celery_app.task
def update_pipelines(action, completed_pipeline_id, completed_trigger):
    logging.info('Update Pipelines (%s)', action)
    status_all_pipeline_ids = set(status_mgr().all_pipeline_ids())
    executed_count = 0
    all_pipeline_ids = set()
    dm = get_dep_mgr()
<mask>:
        filter = collect_dependencies(dm.get_dependents(completed_pipeline_id))
        logging.info('DEPENDENTS Pipeline: %s <- %s', completed_pipeline_id, filter)
    else:
        filter = ('',)
    for spec in pipelines(filter):
        all_pipeline_ids.add(spec.pipeline_id)
        ps = status_mgr().get(spec.pipeline_id)
        ps.init(spec.pipeline_details, spec.source_details, spec.validation_errors, spec.cache_hash)
        if action == 'init':
            psle = ps.get_last_execution()
            if psle is not None and (not psle.finish_time):
                psle.invalidate()
                psle.finish_execution(False, {}, ['Cancelled'])
        elif action == 'update':
            if spec.pipeline_id not in status_all_pipeline_ids:
                dm.update(spec)
                logging.info('NEW Pipeline: %s', spec)
                ps.save()
            logging.debug('Pipeline: %s (dirty: %s, #ex=%s, ch=%s ex0-cache=%s)', spec.pipeline_id, ps.dirty(), len(ps.executions), ps.cache_hash, ps.executions[0].cache_hash if len(ps.executions) > 0 else None)
        elif action == 'complete':
            if completed_pipeline_id in spec.dependencies:
                logging.info('DEPENDENT Pipeline: %s (%d errors) (from ...%s), trigger=%s', spec.pipeline_id, len(spec.validation_errors), os.path.basename(completed_pipeline_id), completed_trigger)
                ps.save()
            else:
                continue
        elif action == 'scheduled':
            if completed_pipeline_id != spec.pipeline_id:
                continue
        last_successful = ps.state() == 'SUCCEEDED'
        if ps.runnable() and (ps.dirty() or completed_trigger or (action == 'init' and (not last_successful))):
            queued = queue_pipeline(ps, spec, 'dirty-task-%s' % action if completed_trigger is None else completed_trigger)
            if queued:
                executed_count += 1
                if executed_count == 4 and action == 'update':
                    break
    if executed_count == 0 and action == 'update':
        extra_pipelines = status_all_pipeline_ids.difference(all_pipeline_ids)
        for pipeline_id in extra_pipelines:
            logging.info('Removing Pipeline: %s', pipeline_id)
            status_mgr().deregister(pipeline_id)
            dm.remove(pipeline_id)",action == 'complete',198,completed_pipeline_id in dm.dependents(),False,0.0,N/A
"@celery_app.task
def execute_scheduled_pipeline(pipeline_id):
    for spec in pipelines([pipeline_id]):
<mask>:
            logging.info('Running scheduled pipeline %s, with schedule %s (%r)', pipeline_id, spec.schedule, spec.pipeline_details.get('schedule'))
            update_pipelines.delay('scheduled', pipeline_id, 'scheduled')",spec.pipeline_id == pipeline_id,22,spec.schedule,False,5.33657305117355,N/A
"def _ingest(debug=False):
    global cache
    global first
    params = None
    validate = False
<mask>:
        first = sys.argv[1] == '0'
        params = json.loads(sys.argv[2])
        validate = sys.argv[3] == 'True'
        cache = sys.argv[4]
    datapackage, resource_iterator, dependency_dp = process_input(sys.stdin, validate, debug)
    dependency_datapackage_urls.update(dependency_dp)
    return (params, datapackage, resource_iterator)",len(sys.argv) > 4,41,len(sys.argv) > 1,False,84.08964152537145,N/A
"def spew(dp, resources_iterator, stats=None, finalizer=None):
    files = [stdout]
    cache_filename = ''
<mask>:
        if not os.path.exists('.cache'):
            os.mkdir('.cache')
        cache_filename = os.path.join('.cache', cache)
        files.append(gzip.open(cache_filename + '.ongoing', 'wt'))
    expected_resources = len(list(filter(streaming, dp.get('resources', []))))
    row_count = 0
    try:
        for f in files:
            f.write(json.dumps(dp, sort_keys=True, ensure_ascii=True) + '\n')
            f.flush()
        num_resources = 0
        for res in resources_iterator:
            if hasattr(res, 'it') and res.it is None:
                continue
            num_resources += 1
            for f in files:
                f.write('\n')
            try:
                for rec in res:
                    try:
                        line = json.dumpl(rec, sort_keys=True, ensure_ascii=True)
                    except TypeError as e:
                        logging.error('Failed to encode row to JSON: %s\nOffending row: %r', e, rec)
                        raise
                    for f in files:
                        f.write(line + '\n')
                    row_count += 1
            except CastError as e:
                for err in e.errors:
                    logging.error('Failed to cast row: %s', err)
                raise
        if num_resources != expected_resources:
            logging.error('Expected to see %d resource(s) but spewed %d', expected_resources, num_resources)
            assert num_resources == expected_resources
        aggregated_stats = {}
        if not first:
            stats_line = sys.stdin.readline().strip()
            if len(stats_line) > 0:
                try:
                    aggregated_stats = json.loads(stats_line)
                    assert aggregated_stats is None or isinstance(aggregated_stats, dict)
                except json.JSONDecodeError:
                    logging.error('Failed to parse stats: %r', stats_line)
        if stats is not None:
            aggregated_stats.update(stats)
        stats_json = json.dumps(aggregated_stats, sort_keys=True, ensure_ascii=True)
        for f in files:
            f.write('\n' + stats_json)
    except BrokenPipeError:
        logging.error('Output pipe disappeared!')
        sys.stderr.flush()
        sys.exit(1)
    stdout.flush()
    if row_count > 0:
        logging.info('Processed %d rows', row_count)
    if finalizer is not None:
        finalizer()
    for f in files:
        f.write('\n')
        if f == stdout:
            f.flush()
        else:
            f.close()
    if len(cache) > 0:
        os.rename(cache_filename + '.ongoing', cache_filename)",len(cache) > 0,228,not os.path.exists('.cache'),False,4.456882760699063,N/A
"def generic_process_resource(rows, spec, resource_index, parameters, stats, process_row):
    for row_index, row in enumerate(rows):
        row = process_row(row, row_index, spec, resource_index, parameters, stats)
<mask>:
            yield row",row is not None,23,row,False,4.9787068367863965,N/A
"def __next__(self):
<mask>:
        raise StopIteration()
    if self.debug:
        logging.error('WAITING')
    line = self.infile.readline().strip()
    if self.debug:
        logging.error('INGESTING: %r', line)
    if line == '':
        self.stopped = True
        raise StopIteration()
    line = json.loadl(line)
    if self.validate:
        to_validate = [line.get(f) for f in self.field_names]
        try:
            self.table_schema.cast_row(to_validate)
        except CastError as e:
            logging.error('Failed to validate row: %s', e)
            for i, err in enumerate(e.errors):
                logging.error('%d) %s', i + 1, err.message)
            raise ValueError('Casting failed for row %r' % line) from e
        except TypeError as e:
            raise ValueError('Validation failed for row %r' % line) from e
    return line",self.stopped,86,self.stopped,True,100.00000000000004,N/A
"def read_json(infile, proxy=False):
    line_json = infile.readline().strip()
<mask>:
        sys.exit(-3)
    if proxy:
        print(line_json)
    try:
        return json.loads(line_json)
    except json.JSONDecodeError:
        logging.exception(""Failed to decode line %r\nPerhaps there's a rogue print statement somewhere?"", line_json)",line_json == '',28,line_json == '',True,100.00000000000004,N/A
"def process_input(infile, validate=False, debug=False):
    dependency_dp = read_json(infile, True)
    dp = read_json(infile)
    resources = dp.get('resources', [])
    original_resources = copy.deepcopy(resources)
<mask>:
        dp_to_validate = copy.deepcopy(dp)
        dp_to_validate['resources'] = [{'name': '__placeholder__', 'path': PATH_PLACEHOLDER}]
    else:
        dp_to_validate = dp
    try:
        datapackage.validate(dp_to_validate)
    except ValidationError as e:
        logging.info('FAILED TO VALIDATE %r', dp_to_validate)
        for e in e.errors:
            try:
                logging.error('Data Package validation error: %s at dp%s', e.message, '[%s]' % ']['.join((repr(index) for index in e.path)))
            except AttributeError:
                logging.error('Data Package validation error: %s', e)
        raise
    infile.readline().strip()

    def resources_iterator(_resources, _original_resources):
        ret = []
        for resource, orig_resource in zip(_resources, _original_resources):
            if not streaming(resource):
                continue
            res_iter = ResourceIterator(infile, resource, orig_resource, validate, debug)
            ret.append(res_iter)
        return iter(ret)
    return (dp, resources_iterator(resources, original_resources), dependency_dp)","len(dp.get('resources', [])) == 0",104,"isinstance(dp, dict)",False,4.3074986800341035,N/A
"def resolve_processors(spec: PipelineSpec):
    abspath = os.path.abspath(spec.path)
    for step in spec.pipeline_details['pipeline']:
<mask>:
            step['executor'] = resolve_executor(step, abspath, spec.validation_errors)",'executor' not in step,16,step.get('executor') is not None,False,6.27465531099474,N/A
"def process_schedules(spec: PipelineSpec):
<mask>:
        schedule = spec.pipeline_details.get('schedule', {})
        if 'crontab' in schedule:
            schedule = schedule['crontab'].split()
            spec.schedule = schedule",spec.schedule is None,18,spec.pipeline_details,False,21.3643503198117,N/A
"def process_environment(spec: PipelineSpec):
<mask>:
        environment = spec.pipeline_details.get('environment', {})
        spec.environment = environment",spec.environment is None,11,spec.pipeline_details,False,21.3643503198117,N/A
"def find_specs(root_dir='.') -> PipelineSpec:
    for dirpath, dirnames, filenames in dirtools.Dir(root_dir, exclude_file='.dpp_spec_ignore', excludes=['.*']).walk():
        relpath = os.path.relpath(dirpath, root_dir)
        dirpath = os.path.join(root_dir, relpath) if relpath != '.' else '.'
<mask>:
            continue
        for filename in filenames:
            for parser in SPEC_PARSERS:
                if parser.check_filename(filename):
                    fullpath = os.path.join(dirpath, filename)
                    with open(fullpath, encoding='utf8') as spec_file:
                        contents = spec_file.read()
                        try:
                            spec = yaml.load(contents, Loader=YAML_LOADER)
                            yield from parser.to_pipeline(spec, fullpath, root_dir)
                        except yaml.YAMLError as e:
                            error = SpecError('Invalid Spec File %s' % fullpath, str(e))
                            yield PipelineSpec(path=dirpath, validation_errors=[error])","dirpath.startswith(os.path.join(root_dir, '.'))",76,dirpath == '',False,0.3756625385528342,N/A
"def pipelines(prefixes=None, ignore_missing_deps=False, root_dir='.', status_manager=None):
    specs: Iterator[PipelineSpec] = find_specs(root_dir)
    hasher = HashCalculator()
<mask>:
        status_manager = status_mgr()
    if prefixes is None:
        prefixes = ('',)
    while specs is not None:
        deferred = []
        found = False
        for spec_ in specs:
            spec: PipelineSpec = spec_
            if not any((spec.pipeline_id.startswith(prefix) for prefix in prefixes)):
                continue
            if spec.pipeline_details is not None and validate_pipeline(spec.pipeline_details, spec.validation_errors):
                resolve_processors(spec)
                process_schedules(spec)
                process_environment(spec)
                try:
                    hasher.calculate_hash(spec, status_manager, ignore_missing_deps)
                    found = True
                except DependencyMissingException as e_:
                    e: DependencyMissingException = e_
                    deferred.append((e.spec, e.missing))
                    continue
            yield spec
        if found and len(deferred) > 0:
            specs = iter((x[0] for x in deferred))
        else:
            for spec, missing in deferred:
                spec.validation_errors.append(SpecError('Missing dependency', 'Failed to find a dependency: {}'.format(missing)))
                yield spec
            specs = None",status_manager is None,113,status_manager is None,True,100.00000000000004,N/A
"def processor_path():
    global _processor_path
<mask>:
        _processor_path = os.environ.get('DPP_PROCESSOR_PATH', '').split(';')
    return _processor_path",_processor_path is None,11,_processor_path is None,True,100.00000000000004,N/A
"def find_file_in_path(path, remove=0):

    def finder(parts):
        global _found_files
        filename = os.path.join(*path + parts[remove:])
<mask>:
            return filename
        if os.path.exists(filename):
            _found_files.add(filename)
            return filename
    return finder",filename in _found_files,22,_found_files,False,60.653065971263366,N/A
"def convert_dot_notation(executor):
    parts = []
    back_up = False
    while executor.startswith('..'):
        parts.append('..')
        executor = executor[1:]
        back_up = True
<mask>:
        executor = executor[1:]
    executor = executor.split('.')
    executor[-1] += '.py'
    parts.extend(executor)
    return (back_up, parts)",executor.startswith('.'),31,executor.startswith('.'),True,100.00000000000004,N/A
"def load_module(module):
    global _tried_imports
<mask>:
        return _tried_imports[module]
    module_name = 'datapackage_pipelines_' + module
    ret = None
    if find_spec(module_name):
        ret = import_module(module_name)
    _tried_imports[module] = ret
    return ret",module in _tried_imports,25,module in _tried_imports,True,100.00000000000004,N/A
"def resolve_executor(step, path, errors):
<mask>:
        filename = hashlib.md5(step['code'].encode('utf8')).hexdigest()
        code_path = os.path.join(path, '.code')
        if not os.path.exists(code_path):
            os.mkdir(code_path)
        code_path = os.path.join(code_path, filename)
        with open(code_path, 'w') as code_file:
            code_file.write(step['code'])
        return code_path
    if 'flow' in step:
        step['run'] = 'flow'
        parameters = step.setdefault('parameters', {})
        flow = step.pop('flow')
        parameters.update(__flow=flow, __path=path)
        if path not in sys.path:
            sys.path.append(path)
        m = import_module(flow)
        flow_py_file = m.__file__
        parameters['__flow_hash'] = hashlib.md5(open(flow_py_file, 'rb').read()).hexdigest()
    executor = step['run']
    back_up, parts = convert_dot_notation(executor)
    resolvers = [find_file_in_path([path])]
    if not back_up:
        if len(parts) > 1:
            module_name = parts[0]
            module = load_module(module_name)
            if module is not None:
                module = list(module.__path__)[0]
                resolvers.append(find_file_in_path([module, 'processors'], 1))
        resolvers.extend([find_file_in_path([path_]) for path_ in processor_path()])
        resolvers.append(find_file_in_path([os.path.dirname(__file__), '..', 'lib']))
    for resolver in resolvers:
        location = resolver(parts)
        if location is not None:
            return location
    message = ""Couldn't resolve {0} at {1}"".format(executor, path)
    errors.append(SpecError('Unresolved processor', message))",'code' in step,127,'code' in step,True,100.00000000000004,N/A
"@classmethod
def fix_dependency(cls, dep, dirpath, root_dir):
<mask>:
        dep = dep[2:]
    return os.path.join(cls.replace_root_dir(dirpath, root_dir), dep)",dep.startswith('./'),14,dep.startswith('.'),False,67.5291821812656,N/A
"@classmethod
def to_pipeline(cls, source_spec, fullpath, root_dir='.'):
    filename = os.path.basename(fullpath)
    dirpath = os.path.dirname(fullpath)
    module_name = filename[:-len(cls.SOURCE_FILENAME_SUFFIX)]
    pipeline_id = os.path.join(dirpath, module_name)
    generator = resolve_generator(module_name)
<mask>:
        message = 'Unknown source description kind ""{}"" in {}'.format(module_name, fullpath)
        error = SpecError('Unknown source kind', message)
        yield PipelineSpec(pipeline_id=module_name, path=dirpath, validation_errors=[error], pipeline_details={'pipeline': []})
        return
    base = cls.replace_root_dir(dirpath, root_dir)
    if generator.internal_validate(source_spec):
        try:
            spec = generator.internal_generate(source_spec, base)
            for pipeline_id, pipeline_details in spec:
                if pipeline_id[0] == ':' and pipeline_id[-1] == ':':
                    module = pipeline_id[1:-1]
                    filename = module + cls.SOURCE_FILENAME_SUFFIX
                    yield from cls.to_pipeline(pipeline_details, os.path.join(dirpath, filename))
                else:
                    yield PipelineSpec(path=pipeline_details.get('__path', dirpath), pipeline_id=pipeline_id, pipeline_details=pipeline_details, source_details=source_spec)
        except Exception as e:
            message = '""{}"" in {}'.format(e, fullpath)
            error = SpecError('Error converting source', message)
            yield PipelineSpec(pipeline_id=pipeline_id, path=dirpath, validation_errors=[error], pipeline_details={'pipeline': []})
    else:
        message = 'Invalid source description for ""{}"" in {}'.format(module_name, fullpath)
        error = SpecError('Invalid Source', message)
        yield PipelineSpec(pipeline_id=pipeline_id, path=dirpath, validation_errors=[error], pipeline_details={'pipeline': []})",generator is None,134,not generator,False,30.326532985631665,N/A
"@staticmethod
def replace_root_dir(path, root_dir):
<mask>:
        root_dir = root_dir[:-1]
    if path.startswith(root_dir):
        path = '.' + path[len(root_dir):]
    return path",root_dir.endswith('/'),17,root_dir.endswith('.'),False,70.71067811865478,N/A
"def calculate_hash(self, spec: PipelineSpec, status_mgr, ignore_missing_deps=False):
    cache_hash = None
<mask>:
        message = 'Duplicate key {0} in {1}'.format(spec.pipeline_id, spec.path)
        spec.validation_errors.append(SpecError('Duplicate Pipeline Id', message))
    else:
        if ignore_missing_deps:
            cache_hash = ''
        else:
            cache_hash = resolve_dependencies(spec, self.all_pipeline_ids, status_mgr)
        self.all_pipeline_ids[spec.pipeline_id] = spec
        if len(spec.validation_errors) > 0:
            return cache_hash
        for step in spec.pipeline_details['pipeline']:
            m = hashlib.md5()
            m.update(cache_hash.encode('ascii'))
            with open(step['executor'], 'rb') as f:
                m.update(f.read())
            m.update(json.dumps(step, ensure_ascii=True, sort_keys=True).encode('ascii'))
            cache_hash = m.hexdigest()
            step['_cache_hash'] = cache_hash
    spec.cache_hash = cache_hash",spec.pipeline_id in self.all_pipeline_ids,69,spec.pipeline_id in self.all_pipeline_ids,True,100.00000000000004,N/A
"def resolve_dependencies(spec: PipelineSpec, all_pipeline_ids, status_mgr):
    cache_hash = ''
    dependencies = spec.pipeline_details.get('dependencies', ())
    for dependency in dependencies:
<mask>:
            pipeline_id = dependency['pipeline']
            if pipeline_id not in all_pipeline_ids:
                raise DependencyMissingException(spec, pipeline_id)
    for dependency in dependencies:
        if 'pipeline' in dependency:
            pipeline_id = dependency['pipeline']
            ps = status_mgr.get(pipeline_id)
            if not ps.runnable():
                spec.validation_errors.append(SpecError('Invalid dependency', 'Cannot run until dependency passes validation: {}'.format(pipeline_id)))
            elif ps.dirty():
                spec.validation_errors.append(SpecError('Dirty dependency', 'Cannot run until dependency is executed: {}'.format(pipeline_id)))
            elif ps.get_last_execution() is not None and (not ps.get_last_execution().success):
                spec.validation_errors.append(SpecError('Dependency unsuccessful', 'Cannot run until dependency ""{}"" is successfully executed'.format(pipeline_id)))
            for dep_err in ps.validation_errors:
                spec.validation_errors.append(SpecError('From {}'.format(pipeline_id), dep_err))
            pipeline_hash = all_pipeline_ids.get(pipeline_id).cache_hash
            assert pipeline_hash is not None
            cache_hash += pipeline_hash
            spec.dependencies.append(pipeline_id)
        elif 'datapackage' in dependency:
            dp_id = dependency['datapackage']
            try:
                dp = datapackage.DataPackage(dp_id)
                if 'hash' in dp.descriptor:
                    cache_hash += dp.descriptor['hash']
                else:
                    spec.validation_errors.append(SpecError('Missing dependency', ""Couldn't get data from datapackage %s"" % dp_id))
            except DataPackageException:
                spec.validation_errors.append(SpecError('Missing dependency', ""Couldn't open datapackage %s"" % dp_id))
        else:
            spec.validation_errors.append(SpecError('Missing dependency', 'Unknown dependency provided (%r)' % dependency))
    return cache_hash",'pipeline' in dependency,152,'pipeline' in dependency,True,100.00000000000004,N/A
"def make_hierarchies(statuses):

    def group(lvl):
        pipelines = list(filter(lambda x: len(x['id']) == 1, lvl))
        children_ = list(filter(lambda x: len(x['id']) > 1, lvl))
        groups_ = {}
        for child in children_:
            child_key = child['id'].pop(0)
            groups_.setdefault(child_key, []).append(child)
        children_ = dict(((k, group(v)) for k, v in groups_.items()))
        for p in pipelines:
            p['id'] = p['id'][0]
        return {'pipelines': pipelines, 'children': children_}

    def flatten(children_):
        ret = dict()
        for k, v in children_.items():
            v['children'] = flatten(v['children'])
            child_keys = list(v['children'].keys())
<mask>:
                child_key = child_keys[0]
                ret['/'.join([k, child_key])] = v['children'][child_key]
            else:
                ret[k] = v
        return ret
    statuses = [{'id': st['id'].split('/'), 'title': st.get('title'), 'stats': st.get('stats'), 'slug': st.get('slug')} for st in statuses]
    groups = group(statuses)
    children = groups.get('children', {})
    groups['children'] = flatten(children)
    return groups",len(child_keys) == 1 and len(v['pipelines']) == 0,109,len(child_keys) == 1,False,29.457482831010747,N/A
"def basic_auth_required(view_func):
    """"""
    A decorator that can be used to protect specific views with HTTP basic
    access authentication. Conditional on having BASIC_AUTH_USERNAME and
    BASIC_AUTH_PASSWORD set as env vars.
    """"""

    @wraps(view_func)
    def wrapper(*args, **kwargs):
<mask>:
            if basic_auth.authenticate():
                return view_func(*args, **kwargs)
            else:
                return basic_auth.challenge()
        else:
            return view_func(*args, **kwargs)
    return wrapper","app.config.get('BASIC_AUTH_ACTIVE', False)",48,"getattr(basic_auth, 'authenticate', False)",False,11.896441524336442,N/A
"@blueprint.route('')
@blueprint.route('<path:pipeline_path>')
@basic_auth_required
def main(pipeline_path=None):
    pipeline_ids = sorted(status.all_pipeline_ids())
<mask>:
        if not pipeline_path.startswith('./'):
            pipeline_path = './' + pipeline_path
        pipeline_ids = [p for p in pipeline_ids if p.startswith(pipeline_path)]
    statuses = []
    for pipeline_id in pipeline_ids:
        pipeline_status = status.get(pipeline_id)
        ex = pipeline_status.get_last_execution()
        success_ex = pipeline_status.get_last_successful_execution()
        pipeline_obj = {'id': pipeline_id.lstrip('./'), 'title': pipeline_status.pipeline_details.get('title'), 'stats': user_facing_stats(ex.stats) if ex else None, 'slug': slugify.slugify(pipeline_id), 'trigger': ex.trigger if ex else None, 'error_log': pipeline_status.errors(), 'state': pipeline_status.state(), 'pipeline': pipeline_status.pipeline_details, 'message': pipeline_status.state().capitalize(), 'dirty': pipeline_status.dirty(), 'runnable': pipeline_status.runnable(), 'class': {'INIT': 'primary', 'QUEUED': 'primary', 'INVALID': 'danger', 'RUNNING': 'warning', 'SUCCEEDED': 'success', 'FAILED': 'danger'}[pipeline_status.state()], 'ended': datestr(ex.finish_time) if ex else None, 'started': datestr(ex.start_time) if ex else None, 'last_success': datestr(success_ex.finish_time) if success_ex else None}
        statuses.append(pipeline_obj)

    def state_and_not_dirty(state, p):
        return p.get('state') == state and (not p.get('dirty'))

    def state_or_dirty(state, p):
        return p.get('state') == state or p.get('dirty')
    categories = [['ALL', 'All Pipelines', lambda _, __: True], ['INVALID', ""Can't start"", lambda _, p: not p['runnable']], ['QUEUED', 'Waiting to run', lambda state, p: p['state'] == state], ['RUNNING', 'Running', state_and_not_dirty], ['FAILED', 'Failed Execution', state_and_not_dirty], ['SUCCEEDED', 'Successful Execution', state_and_not_dirty]]
    for item in categories:
        item.append([p for p in deepcopy(statuses) if item[2](item[0], p)])
        item.append(len(item[-1]))
        item.append(make_hierarchies(item[-2]))
    return render_template('dashboard.html', categories=categories, yamlize=yamlize, markdown=markdown)",pipeline_path is not None,182,pipeline_path,False,36.78794411714425,N/A
"@blueprint.route('api/raw/status')
@basic_auth_required
def pipeline_raw_api_status():
    pipelines = sorted(status.all_statuses(), key=lambda x: x.get('id'))
    for pipeline in pipelines:
        for attr in ['pipeline', 'reason', 'error_log']:
<mask>:
                del pipeline[attr]
    return jsonify(pipelines)",attr in pipeline,25,pipeline[attr] == 'error_log',False,5.669791110976001,N/A
"def _send(hook, payload):
    try:
        response = requests.post(hook, json=payload)
<mask>:
            logging.warning('Server returned %s, hook %s with payload %r ', response.status_code, hook, payload)
    except RequestException as e:
        logging.warning('Failed to call hook %s with payload %r (%s)', hook, payload, e)",response.status_code != 200,37,response.status_code != 200,True,100.00000000000004,N/A
"def send(self, hook, payload, blocking=False):
<mask>:
        _send(hook, payload)
    else:
        tpe.submit(_send, hook, payload)",blocking,12,blocking,True,100.00000000000004,N/A
"def __init__(self, backend, pipeline_id, pipeline_details, cache_hash, trigger, execution_id, log='', queue_time=None, start_time=None, finish_time=None, success=None, stats=None, error_log=None, save=True):
    self.backend = backend
    self.pipeline_id = pipeline_id
    self.pipeline_details = pipeline_details
    self.cache_hash = cache_hash
    self.trigger = trigger
    self.execution_id = execution_id
    self.log = log
    self.queue_time = queue_time
    self.start_time = start_time
    self.finish_time = finish_time
    self.success = success
    self.stats = stats or {}
    self.error_log = error_log or []
<mask>:
        self.__save()",save,61,save,True,100.00000000000004,N/A
"def queue_execution(self, trigger):
<mask>:
        return False
    self.queue_time = time.time()
    self.trigger = trigger
    self.__save()
    return True",self.queue_time is not None,15,self.trigger != trigger,False,11.631736348831648,N/A
"def start_execution(self):
<mask>:
        return False
    self.start_time = time.time()
    self.__save()
    return True",self.queue_time is None or self.start_time is not None,11,self.start_time - self.start_time > self.MAX_START_TIME,False,24.29335519279862,N/A
"def finish_execution(self, success, stats, error_log):
<mask>:
        return False
    self.finish_time = time.time()
    self.success = success
    self.stats = stats
    self.error_log = error_log
    self.__save()
    return True",self.queue_time is None or self.start_time is None,23,self.finished,False,1.0079484521270912,N/A
"def update_execution(self, log):
<mask>:
        return False
    self.log = '\n'.join(log)
    self.__save()
    return True",self.queue_time is None or self.start_time is None or self.finish_time is not None,12,not log,False,0.0008350850395122828,N/A
"def __init__(self, host=None, port=6379):
    self.redis = None
<mask>:
        conn = redis.StrictRedis(host=host, port=port, db=5)
        try:
            conn.ping()
            self.redis = conn
        except redis.exceptions.ConnectionError:
            logging.warning('Failed to connect to Redis, host:%s, port:%s', host, port)",host is not None and len(host) > 0,29,host is not None,False,17.37739434504452,N/A
"def get_status(self, pipeline_id):
<mask>:
        status = self.redis.get(pipeline_id)
        if status is not None:
            status = json.loads(status.decode('ascii'))
            return status",self.is_init(),17,self.redis is not None,False,15.207218222740094,N/A
"def __load(self):
    data = self.backend.get_status('PipelineStatus:' + self.pipeline_id)
<mask>:
        data = {}
    self.pipeline_details = data.get('pipeline_details', {})
    self.source_spec = data.get('source_spec', {})
    self.validation_errors = data.get('validation_errors', [])
    self.cache_hash = data.get('cache_hash', '')
    self._execution_ids = data.get('executions', [])
    self._executions = None
    self._last_execution = None",data is None,37,data is None,True,100.00000000000004,N/A
"@property
def executions(self):
<mask>:
        self._executions = [PipelineExecution.from_execution_id(self.backend, ex) for ex in self._execution_ids]
    return self._executions",self._executions is None,14,not self._executions,False,54.75182535069452,N/A
"@property
def last_execution(self):
<mask>:
        if len(self._executions) > 0:
            return self._executions[0]
    elif len(self._execution_ids) > 0:
        if self._last_execution is None:
            self._last_execution = PipelineExecution.from_execution_id(self.backend, self._execution_ids[0])
        return self._last_execution",self._executions is not None,24,self.backend is None,False,17.030578356760866,N/A
"def errors(self):
<mask>:
        return ['%s :%s' % tuple(err) for err in self.validation_errors]
    else:
        ex = self.last_execution
        if ex is not None:
            return ex.error_log
    return []",not self.runnable(),25,self.validation_errors,False,17.491650626361256,N/A
"def all_pipeline_ids(self):
    dec_ids = []
    enc_ids = sorted(os.listdir(self.base_dir))
    for enc_id in enc_ids:
        dec_id = codecs.decode(enc_id.encode('utf8'), 'base64').decode('utf8')
<mask>:
            dec_id = dec_id.replace('PipelineStatus:', '')
            dec_ids.append(dec_id)
    return dec_ids",dec_id.startswith('PipelineStatus:'),24,'PipelineStatus:' in dec_id,False,26.782849591300856,N/A
"def __getitem__(self, key):
    conn = sqlite3.connect(self.filename)
    cursor = conn.cursor()
    result = cursor.execute('SELECT _value from d where _key=?', (key,)).fetchone()
    conn.close()
<mask>:
        return json.loads(result[0])
    return None",result is not None,24,result,False,4.9787068367863965,N/A
"def register_pipeline_id(self, pipeline_id):
    all_pipelines = self.db[self.ALL_PIPELINES_KEY]
<mask>:
        all_pipelines = []
    if pipeline_id not in all_pipelines:
        all_pipelines.append(pipeline_id)
    self.db[self.ALL_PIPELINES_KEY] = all_pipelines",all_pipelines is None,19,not all_pipelines,False,46.30777161991026,N/A
"def deregister_pipeline_id(self, pipeline_id):
    all_pipelines = self.db[self.ALL_PIPELINES_KEY]
<mask>:
        all_pipelines = []
    if pipeline_id in all_pipelines:
        all_pipelines = filter(lambda x: x != pipeline_id, all_pipelines)
    self.db[self.ALL_PIPELINES_KEY] = all_pipelines",all_pipelines is None,25,not all_pipelines,False,46.30777161991026,N/A
"@property
def backend(self):
<mask>:
        redis = RedisBackend(self._host, self._port)
        self._backend = redis if redis.is_init() else FilesystemBackend(self._root_dir)
    return self._backend",self._backend is None,17,self._backend is None,True,100.00000000000004,N/A
"def get_errors(self, _id):
    ex = self.get(_id).get_last_execution()
<mask>:
        return ex.error_log
    return []",ex is not None,11,ex,False,4.9787068367863965,N/A
"def status_mgr(root_dir='.') -> StatusManager:
    global _status
    global _root_dir
<mask>:
        return _status
    _root_dir = root_dir
    _status = StatusManager(host=os.environ.get('DPP_REDIS_HOST'), root_dir=root_dir)
    return _status",_status is not None and _root_dir == root_dir,20,_status is not None,False,13.533528323661276,N/A
"def load_lazy_json(resources):

    def func(package):
        matcher = ResourceMatcher(resources, package.pkg)
        yield package.pkg
        for rows in package:
<mask>:
                yield (row.inner if isinstance(row, LazyJsonLine) else row for row in rows)
            else:
                yield rows
    return func",matcher.match(rows.res.name),31,matcher.matches(rows),False,12.462989337200145,N/A
"def __iter__(self):
<mask>:
        for x in self.ds_stats:
            yield from x.items()
    if self.ctx_stats is not None:
        yield from self.ctx_stats.items()",self.ds_stats is not None,18,self.ds_stats is not None,True,100.00000000000004,N/A
"def _filehash(filepath, blocksize=4096):
    """""" Return the hash object for the file `filepath', processing the file
    by chunk of `blocksize'.

    :type filepath: str
    :param filepath: Path to file

    :type blocksize: int
    :param blocksize: Size of the chunk when processing the file

    """"""
    sha = hashlib.sha256()
    with open(filepath, 'rb') as fp:
        while 1:
            data = fp.read(blocksize)
<mask>:
                sha.update(data)
            else:
                break
    return sha",data,60,data,True,100.00000000000004,N/A
"def compress_to(self, archive_path=None):
    """""" Compress the directory with gzip using tarlib.

        :type archive_path: str
        :param archive_path: Path to the archive, if None, a tempfile is created

        """"""
<mask>:
        archive = tempfile.NamedTemporaryFile(delete=False)
        tar_args = ()
        tar_kwargs = {'fileobj': archive}
        _return = archive.name
    else:
        tar_args = archive_path
        tar_kwargs = {}
        _return = archive_path
    tar_kwargs.update({'mode': 'w:gz'})
    with closing(tarfile.open(*tar_args, **tar_kwargs)) as tar:
        tar.add(self.path, arcname=self.file)
    return _return",archive_path is None,62,archive_path is None,True,100.00000000000004,N/A
"def __init__(self, directory='.', exclude_file='.exclude', excludes=['.git/', '.hg/', '.svn/']):
<mask>:
        raise TypeError('Directory must be a directory.')
    self.directory = os.path.basename(directory)
    self.path = os.path.abspath(directory)
    self.parent = os.path.dirname(self.path)
    self.exclude_file = os.path.join(self.path, exclude_file)
    self.patterns = excludes
    if os.path.isfile(self.exclude_file):
        self.patterns.extend(load_patterns(self.exclude_file))
    self.globster = Globster(self.patterns)",not os.path.isdir(directory),36,not os.path.isdir(directory),True,100.00000000000004,N/A
"def iterfiles(self, pattern=None, abspath=False):
    """""" Generator for all the files not excluded recursively.

        Return relative path.

        :type pattern: str
        :param pattern: Unix style (glob like/gitignore like) pattern

        """"""
<mask>:
        globster = Globster([pattern])
    for root, dirs, files in self.walk():
        for f in files:
            if pattern is None or (pattern is not None and globster.match(f)):
                if abspath:
                    yield os.path.join(root, f)
                else:
                    yield self.relpath(os.path.join(root, f))",pattern is not None,62,pattern is not None,True,100.00000000000004,N/A
"def get(self, pattern, sort_key=lambda k: k, sort_reverse=False, abspath=False):
    res = self.files(pattern, sort_key=sort_key, sort_reverse=sort_reverse, abspath=abspath)
<mask>:
        return res[0]",res,17,len(res) == 1,False,6.567274736060395,N/A
"def _run_processor(self, processor, parameters, data_in, env):
    """"""Run the passed `processor` and return the output""""""
    process = subprocess.run([sys.executable, processor, '1', parameters, 'False', ''], input=data_in, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)
    print('\nProcessor output:\n')
    for line in process.stderr.decode('utf8').split('\n'):
        print(f'OUT> {line}')
<mask>:
        raise Exception(f'processor execution failed with {process.returncode}')
    return process.stdout.decode('utf8')",process.returncode != 0,43,process.returncode,False,36.78794411714425,N/A
"@staticmethod
def test_fixture(processor_output, dp_out, data_out):
    """"""Receives processor output and performs standard tests. Can be
        overridden in subclasses.""""""
    actual_dp, *actual_data = processor_output.split('\n\n', 1)
<mask>:
        print('unexpected value for output datapackage:\n{!r}\n{!r}'.format(actual_dp, dp_out), file=sys.stderr)
    actual_dp = json.loads(actual_dp)
    dp_out = json.loads(dp_out)
    for ares, eres in zip(actual_dp.get('resources', []), dp_out.get('resources', [])):
        assert ares.get('schema', {}).get('fields') == eres.get('schema', {}).get('fields')
        assert ares.get('schema', {}) == eres.get('schema', {})
        assert ares == eres, 'error comparing actual:\n%r\nto expected:\n%r\n...' % (ares, eres)
    assert actual_dp == dp_out
    if len(actual_data) > 0:
        actual_data = actual_data[0]
        actual_data = actual_data.split('\n')
        expected_data = data_out.split('\n')
        assert len(actual_data) == len(expected_data), 'unexpected number of output lines: {}, actual_data = {}'.format(len(actual_data), actual_data)
        for line_num, (actual, expected) in enumerate(zip(actual_data, expected_data)):
            line_msg = 'output line {}'.format(line_num)
            if len(actual) == 0:
                assert len(expected) == 0, '{}: did not get any data (but expected some)'.format(line_msg)
            else:
                rj_actual = rejsonize(actual)
                if rj_actual != rejsonize(expected):
                    print('{}: unexpected data: {} (expected {})'.format(line_msg, rj_actual, expected), file=sys.stderr)
                assert json.loads(actual) == json.loads(expected), 'a: %r, e: %r' % (json.loads(actual), json.loads(expected))",actual_dp.strip() != dp_out.strip(),155,actual_dp != dp_out,False,25.436713811085983,N/A
"def reline(data):
    data = data.strip().split('\n')
    out = ''
    buf = ''
    for line in data:
<mask>:
            out += '\n'
        buf += line
        try:
            buf = json.loads(buf)
        except Exception:
            continue
        out += json.dumps(buf, sort_keys=True, ensure_ascii=True) + '\n'
        buf = ''
    return out",not line.strip(),41,out,False,0.0,N/A
"def open(self, source, encoding=None, force_parse=False):
    self.close()
    self.__chars = self.__loader.load(source, encoding)
    self.__encoding = getattr(self.__chars, 'encoding', encoding)
<mask>:
        self.__encoding.lower()
    self.reset()",self.__encoding,18,force_parse,False,14.127216461522432,N/A
"def __iter_extended_rows(self):
    for number, line in enumerate(self.__chars, start=1):
<mask>:
            line = line[:-1]
        yield (number, None, [line])",line.endswith('\n'),16,line.endswith('\n'),True,100.00000000000004,N/A
"def get_path(descriptor):
    path = descriptor.get('path')
<mask>:
        return path
    if isinstance(path, list):
        if len(path) > 0:
            return path.pop(0)
        else:
            return None
    assert path is None, '%r' % path
    return None","isinstance(path, str)",29,"isinstance(path, str)",True,100.00000000000004,N/A
"def user_facing_stats(stats):
<mask>:
        return dict(((k, v) for k, v in stats.items() if k != STATS_DPP_KEY))
    return None","stats is not None and isinstance(stats, dict)",17,stats,False,0.004539992976248487,N/A
"@classmethod
def object_hook(cls, obj):
<mask>:
        try:
            return decimal.Decimal(obj['type{decimal}'])
        except decimal.InvalidOperation:
            pass
    if 'type{time}' in obj:
        try:
            return datetime.datetime.strptime(obj['type{time}'], TIME_FORMAT).time()
        except ValueError:
            pass
    if 'type{datetime}' in obj:
        try:
            return datetime.datetime.strptime(obj['type{datetime}'], DATETIME_P_FORMAT)
        except ValueError:
            pass
    if 'type{date}' in obj:
        try:
            return datetime.datetime.strptime(obj['type{date}'], DATE_P_FORMAT).date()
        except ValueError:
            pass
    if 'type{duration}' in obj:
        try:
            return isodate.parse_duration(obj['type{duration}'])
        except ValueError:
            pass
    if 'type{set}' in obj:
        try:
            return set(obj['type{set}'])
        except ValueError:
            pass
    return obj",'type{decimal}' in obj,66,'type{decimal}' in obj,True,100.00000000000004,N/A
"def default(self, obj):
<mask>:
        return {'type{decimal}': str(obj)}
    elif isinstance(obj, datetime.time):
        return {'type{time}': obj.strftime(TIME_FORMAT)}
    elif isinstance(obj, datetime.datetime):
        return {'type{datetime}': obj.strftime(DATETIME_F_FORMAT)}
    elif isinstance(obj, datetime.date):
        return {'type{date}': obj.strftime(DATE_F_FORMAT)}
    elif isinstance(obj, (isodate.Duration, datetime.timedelta)):
        return {'type{duration}': isodate.duration_isoformat(obj)}
    elif isinstance(obj, set):
        return {'type{set}': list(obj)}
    elif isinstance(obj, LazyDict):
        return obj.inner
    return super().default(obj)","isinstance(obj, decimal.Decimal)",45,"isinstance(obj, decimal.Decimal)",True,100.00000000000004,N/A
"def _dumpl(*args, **kwargs):
    obj = args[0]
<mask>:
        if not obj.dirty:
            return obj.line
        else:
            kwargs['cls'] = CommonJSONEncoder
            return _json.dumps(obj.inner, **kwargs)
    kwargs['cls'] = CommonJSONEncoder
    return _json.dumps(*args, **kwargs)","isinstance(obj, LazyJsonLine)",25,"isinstance(obj, Line)",False,53.7284965911771,N/A
"def remote_execute_pipeline(spec, root_dir, use_cache, verbose, progress_report_queue):
    args = ['dpp', 'run', '--slave', '--use-cache' if use_cache else '--no-use-cache', spec.pipeline_id]
    popen = subprocess.Popen(args, encoding='utf8', stderr=subprocess.PIPE, stdout=subprocess.DEVNULL, env=os.environ.copy(), cwd=root_dir)
    progress = 0
    lines = []
    for line in popen.stderr:
<mask>:
            continue
        if line.startswith(MAGIC):
            if progress_report_queue is not None:
                progress = int(line[len(MAGIC):].strip())
                progress_report_queue.put(ProgressReport(spec.pipeline_id, progress, None, None, None))
            continue
        while len(lines) > 0:
            log = lines.pop(0)
            if verbose:
                sys.stderr.write('[%s:%s] >>> %s' % (spec.pipeline_id, threading.current_thread().name, log))
        lines.append(line)
    if len(lines) > 0:
        results = lines.pop(0)
    else:
        if progress_report_queue is not None:
            progress_report_queue.put(ProgressReport(spec.pipeline_id, progress, False, ['Empty'], None))
        return (spec.pipeline_id, False, {}, ['Empty'])
    try:
        results = json.loads(results)
        if progress_report_queue is not None:
            progress_report_queue.put(ProgressReport(spec.pipeline_id, progress, results[0]['success'], results[0]['errors'], results[0]['stats']))
    except json.decoder.JSONDecodeError:
        if verbose:
            sys.stderr.write('[%s:%s] >>> %s' % (spec.pipeline_id, threading.current_thread().name, results))
        if progress_report_queue is not None:
            progress_report_queue.put(ProgressReport(spec.pipeline_id, progress, False, ['Crashed', results], None))
        return (spec.pipeline_id, False, {}, [results])
    results = results[0]
    return (spec.pipeline_id, results['success'], results['stats'], results['errors'])",len(line) == 0,142,line.startswith('#') or line.startswith('#'),False,3.0098043843528286,N/A
"def progress_report_handler(callback, queue):
    while True:
        report = queue.get()
<mask>:
            callback(report)
        else:
            return",report is not None,12,report,False,4.9787068367863965,N/A
"def match_pipeline_id(arg, pipeline_id):
<mask>:
        return True
    elif ',' in arg:
        return any((match_pipeline_id(a, pipeline_id) for a in arg.split(',')))
    elif arg.endswith('%'):
        return pipeline_id.startswith(arg[:-1])
    else:
        return pipeline_id == arg",arg == 'all',26,arg == '*',False,30.213753973567677,N/A
"def specs_to_execute(argument, root_dir, status_manager, ignore_missing_deps, dirty, results):
    pending = set()
    executed = set()
    completed = set()
    for spec in pipelines(ignore_missing_deps=ignore_missing_deps, root_dir=root_dir, status_manager=status_manager):
<mask>:
            if dirty:
                ps = status_manager.get(spec.pipeline_id)
                if not ps.dirty():
                    continue
            pending.add(spec.pipeline_id)
    while len(pending) > 0:
        to_yield = None
        for spec in pipelines(ignore_missing_deps=ignore_missing_deps, root_dir=root_dir, status_manager=status_manager):
            pipeline_id = spec.pipeline_id
            if pipeline_id not in pending:
                continue
            unresolved = set(spec.dependencies) - completed
            if len(unresolved) == 0:
                to_yield = spec
                break
            unresolved = unresolved - executed - pending
            if len(unresolved) > 0:
                to_yield = spec
                break
        if to_yield is not None:
            executed.add(to_yield.pipeline_id)
            pending.remove(to_yield.pipeline_id)
        completed_pipeline_id = (yield to_yield)
        if completed_pipeline_id is not None:
            completed.add(completed_pipeline_id)
    yield None","match_pipeline_id(argument, spec.pipeline_id)",102,spec.pipeline_id not in pending,False,24.420082659439853,N/A
"def run_pipelines(pipeline_id_pattern, root_dir, use_cache=True, dirty=False, force=False, concurrency=1, verbose_logs=True, progress_cb=None, slave=False):
    """"""Run a pipeline by pipeline-id.
       pipeline-id supports the '%' wildcard for any-suffix matching.
       Use 'all' or '%' for running all pipelines""""""
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency, thread_name_prefix='T') as executor:
        try:
            results = []
            pending_futures = set()
            done_futures = set()
            finished_futures = []
            progress_thread = None
            progress_queue = None
            status_manager = status_mgr(root_dir)
<mask>:
                progress_queue = Queue()
                progress_thread = threading.Thread(target=progress_report_handler, args=(progress_cb, progress_queue))
                progress_thread.start()
            all_specs = specs_to_execute(pipeline_id_pattern, root_dir, status_manager, force, dirty, results)
            while True:
                done = None
                if len(done_futures) > 0:
                    done = done_futures.pop()
                    finished_futures.append(done)
                    done = done.result()[0]
                try:
                    spec = all_specs.send(done)
                except StopIteration:
                    spec = None
                if spec is None:
                    if len(done_futures) == 0:
                        if len(pending_futures) > 0:
                            done_futures, pending_futures = concurrent.futures.wait(pending_futures, return_when=concurrent.futures.FIRST_COMPLETED)
                            continue
                        else:
                            break
                    else:
                        continue
                if len(spec.validation_errors) > 0:
                    results.append(ExecutionResult(spec.pipeline_id, False, {}, ['init'] + list(map(str, spec.validation_errors))))
                    continue
                if slave:
                    for key, value in spec.environment.items():
                        os.environ[key] = str(value)
                    ps = status_manager.get(spec.pipeline_id)
                    ps.init(spec.pipeline_details, spec.source_details, spec.validation_errors, spec.cache_hash)
                    eid = gen_execution_id()
                    if ps.queue_execution(eid, 'manual'):
                        success, stats, errors = execute_pipeline(spec, eid, use_cache=use_cache)
                        results.append(ExecutionResult(spec.pipeline_id, success, stats, errors))
                    else:
                        results.append(ExecutionResult(spec.pipeline_id, False, None, ['Already Running']))
                else:
                    f = executor.submit(remote_execute_pipeline, spec, root_dir, use_cache, verbose_logs, progress_queue)
                    pending_futures.add(f)
            for f in finished_futures:
                ret = f.result()
                results.append(ExecutionResult(*ret))
        except KeyboardInterrupt:
            pass
        finally:
            if slave:
                finalize()
            if progress_thread is not None:
                progress_queue.put(None)
                progress_thread.join()
    return results",progress_cb is not None,209,progress_cb,False,36.78794411714425,N/A
"def create_process(args, cwd, wfd, rfd):
    pass_fds = {rfd, wfd}
<mask>:
        pass_fds.remove(None)
    rfd = asyncio.subprocess.PIPE if rfd is None else rfd
    wfd = asyncio.subprocess.DEVNULL if wfd is None else wfd
    ret = asyncio.create_subprocess_exec(*args, stdin=rfd, stdout=wfd, stderr=asyncio.subprocess.PIPE, pass_fds=pass_fds, cwd=cwd)
    return ret",None in pass_fds,39,rfd is not None and wfd is not None,False,4.767707020457095,N/A
"def find_caches(pipeline_steps, pipeline_cwd):
<mask>:
        return pipeline_steps
    for i, step in reversed(list(enumerate(pipeline_steps))):
        cache_filename = os.path.join(pipeline_cwd, '.cache', step['_cache_hash'])
        if os.path.exists(cache_filename):
            try:
                canary = gzip.open(cache_filename, 'rt')
                canary.seek(1)
                canary.close()
            except Exception:
                continue
            logging.info('Found cache for step %d: %s', i, step['run'])
            pipeline_steps = pipeline_steps[i + 1:]
            step = {'run': 'cache_loader', 'parameters': {'load-from': os.path.join('.cache', step['_cache_hash'])}}
            step['executor'] = resolve_executor(step, '.', [])
            pipeline_steps.insert(0, step)
            break
    return pipeline_steps",not any((step.get('cache') for step in pipeline_steps)),59,not pipeline_cwd,False,1.067638729600582,N/A
"def wait_for_finish(_error_collectors, _error_queue, _error_aggregator):

    async def _func(failed_index=None):
        *errors, count = await asyncio.gather(*_error_collectors)
<mask>:
            errors = errors[failed_index]
        else:
            errors = None
        await _error_queue.put(None)
        await _error_aggregator
        return (count, errors)
    return _func",failed_index is not None,29,failed_index is not None,True,100.00000000000004,N/A
"def execute_pipeline(spec, execution_id, trigger='manual', use_cache=True):
    debug = trigger == 'manual' or os.environ.get('DPP_DEBUG')
    logging.info('%s RUNNING %s', execution_id[:8], spec.pipeline_id)
    loop = asyncio.get_event_loop()
<mask>:
        logging.info('%s Collecting dependencies', execution_id[:8])
    dependencies = {}
    for dep in spec.pipeline_details.get('dependencies', []):
        if 'pipeline' in dep:
            dep_pipeline_id = dep['pipeline']
            pipeline_execution = status_mgr().get(dep_pipeline_id).get_last_successful_execution()
            if pipeline_execution is not None:
                result_dp = pipeline_execution.stats.get(STATS_DPP_KEY, {}).get(STATS_OUT_DP_URL_KEY)
                if result_dp is not None:
                    dependencies[dep_pipeline_id] = result_dp
    if debug:
        logging.info('%s Running async task', execution_id[:8])
    pipeline_task = asyncio.ensure_future(async_execute_pipeline(spec.pipeline_id, spec.pipeline_details.get('pipeline', []), spec.path, trigger, execution_id, use_cache, dependencies, debug))
    try:
        if debug:
            logging.info('%s Waiting for completion', execution_id[:8])
        return loop.run_until_complete(pipeline_task)
    except KeyboardInterrupt:
        logging.info('Caught keyboard interrupt. Cancelling tasks...')
        pipeline_task.cancel()
        loop.run_forever()
        logging.info('Caught keyboard interrupt. DONE!')
        raise KeyboardInterrupt()",debug,103,debug,True,100.00000000000004,N/A
"def find_version(*file_paths):
    """"""
    Build a path from *file_paths* and search for a ``__version__``
    string inside.
    """"""
    version_file = read(*file_paths)
    version_match = re.search('^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]', version_file, re.M)
<mask>:
        return version_match.group(1)
    raise RuntimeError('Unable to find version string.')",version_match,35,version_match,True,100.00000000000004,N/A
"def __init__(self, input_geometry, interpolation_distance=0.5, **attributes):
    self._input_geometry = input_geometry
    self._interpolation_distance = abs(interpolation_distance)
<mask>:
        raise exceptions.InvalidInputTypeError
    self._min_x, self._min_y = self._get_reduced_coordinates()
    self.assign_attributes_to_instance(attributes)
    self.geometry = MultiLineString(lines=self._construct_centerline())",not self.input_geometry_is_valid(),22,"not isinstance(input_geometry, Polygon)",False,15.12263738306194,N/A
"def input_geometry_is_valid(self):
    """"""Input geometry is of a :py:class:`shapely.geometry.Polygon`
        or a :py:class:`shapely.geometry.MultiPolygon`.

        :return: geometry is valid
        :rtype: bool
        """"""
<mask>:
        return True
    else:
        return False","isinstance(self._input_geometry, Polygon) or isinstance(self._input_geometry, MultiPolygon)",24,self.geometry is not None,False,1.0566500056142598,N/A
"def _construct_centerline(self):
    vertices, ridges = self._get_voronoi_vertices_and_ridges()
    linestrings = []
    for ridge in ridges:
<mask>:
            starting_point = self._create_point_with_restored_coordinates(x=vertices[ridge[0]][0], y=vertices[ridge[0]][1])
            ending_point = self._create_point_with_restored_coordinates(x=vertices[ridge[1]][0], y=vertices[ridge[1]][1])
            linestring = LineString((starting_point, ending_point))
            linestrings.append(linestring)
    str_tree = STRtree(linestrings)
    linestrings_indexes = str_tree.query(self._input_geometry, predicate='contains')
    contained_linestrings = [linestrings[i] for i in linestrings_indexes]
    if len(contained_linestrings) < 2:
        raise exceptions.TooFewRidgesError
    return unary_union(contained_linestrings)",self._ridge_is_finite(ridge),49,ridge[0] != None,False,3.708659055657029,N/A
"def _get_densified_borders(self):
    polygons = self._extract_polygons_from_input_geometry()
    points = []
    for polygon in polygons:
        points += self._get_interpolated_boundary(polygon.exterior)
<mask>:
            for interior in polygon.interiors:
                points += self._get_interpolated_boundary(interior)
    return array(points)",self._polygon_has_interior_rings(polygon),25,polygon.interiors,False,1.2367482744213496,N/A
"def _extract_polygons_from_input_geometry(self):
<mask>:
        return (polygon for polygon in self._input_geometry.geoms)
    else:
        return (self._input_geometry,)","isinstance(self._input_geometry, MultiPolygon)",12,"isinstance(self._input_geometry, geometry.Geometry)",False,66.52049901111006,N/A
"def __init__(self, *args, **kwargs):
<mask>:
        args = (self.default_message,)
    super(CenterlineError, self).__init__(*args, **kwargs)",not (args or kwargs),11,not args,False,9.569649651041097,N/A
"def get_ogr_driver(filepath):
    """"""Get the OGR driver based on the file's extension.

    :param filepath: file's path
    :type filepath: str
    :raises UnsupportedVectorType: unsupported extension
    :return: OGR driver
    :rtype: osgeo.ogr.Driver
    """"""
    filename, file_extension = os.path.splitext(filepath)
    extension = file_extension[1:]
    ogr_driver_count = ogr.GetDriverCount()
    for idx in range(ogr_driver_count):
        driver = ogr.GetDriver(idx)
        driver_extension = driver.GetMetadataItem(str('DMD_EXTENSION')) or ''
        driver_extensions = driver.GetMetadataItem(str('DMD_EXTENSIONS')) or ''
<mask>:
            return driver
    raise UnsupportedVectorType",extension == driver_extension or extension in driver_extensions,60,driver_extension == extension and driver_extensions == driver_extensions,False,31.314224813827344,N/A
"def test_using_custom_data_type_func(self):

    class FooSchema(m.Schema):
        value = f.String()

    class BarSchema(m.Schema):
        value = f.Integer()
        type = f.String()

    class CustomDataTypeSchema(OneOfSchema):
        type_schemas = {'foo': FooSchema, 'bar': BarSchema}

        def get_data_type(self, data):
<mask>:
                return 'foo'
            return 'bar'
    schema = CustomDataTypeSchema()
    source_data = [{'value': 'hello'}, {'type': 'blahblahblah', 'value': 111}]
    loaded_data = schema.load(source_data, many=True)
    assert source_data == loaded_data",data.get('type') is None,50,data.value == 111,False,11.631736348831648,N/A
"def get_data_type(self, data):
    """"""Returns name of the schema during load() calls, given the data being
        loaded. Defaults to looking up `type_field` in the data.""""""
    data_type = data.get(self.type_field)
<mask>:
        data.pop(self.type_field)
    return data_type",self.type_field in data and self.type_field_remove,31,data_type is None,False,1.9026155630072006,N/A
"def dump(self, obj, *, many=None, **kwargs):
    errors = {}
    result_data = []
    result_errors = {}
    many = self.many if many is None else bool(many)
<mask>:
        result = result_data = self._dump(obj, **kwargs)
    else:
        for idx, o in enumerate(obj):
            try:
                result = self._dump(o, **kwargs)
                result_data.append(result)
            except ValidationError as error:
                result_errors[idx] = error.normalized_messages()
                result_data.append(error.valid_data)
    result = result_data
    errors = result_errors
    if not errors:
        return result
    else:
        exc = ValidationError(errors, data=obj, valid_data=result)
        raise exc",not many,70,many,False,36.78794411714425,N/A
"def _dump(self, obj, *, update_fields=True, **kwargs):
    obj_type = self.get_obj_type(obj)
<mask>:
        return (None, {'_schema': f'Unknown object class: {obj.__class__.__name__}'})
    type_schema = self.type_schemas.get(obj_type)
    if not type_schema:
        return (None, {'_schema': f'Unsupported object type: {obj_type}'})
    schema = type_schema if isinstance(type_schema, Schema) else type_schema()
    schema.context.update(getattr(self, 'context', {}))
    result = schema.dump(obj, many=False, **kwargs)
    if result is not None:
        result[self.type_field] = obj_type
    return result",obj_type is None,56,not obj_type,False,46.30777161991026,N/A
"def load(self, data, *, many=None, partial=None, unknown=None, **kwargs):
    errors = {}
    result_data = []
    result_errors = {}
    many = self.many if many is None else bool(many)
<mask>:
        partial = self.partial
    if not many:
        try:
            result = result_data = self._load(data, partial=partial, unknown=unknown, **kwargs)
        except ValidationError as error:
            result_errors = error.normalized_messages()
            result_data.append(error.valid_data)
    else:
        for idx, item in enumerate(data):
            try:
                result = self._load(item, partial=partial, **kwargs)
                result_data.append(result)
            except ValidationError as error:
                result_errors[idx] = error.normalized_messages()
                result_data.append(error.valid_data)
    result = result_data
    errors = result_errors
    if not errors:
        return result
    else:
        exc = ValidationError(errors, data=data, valid_data=result)
        raise exc",partial is None,90,partial is None,True,100.00000000000004,N/A
"def _load(self, data, *, partial=None, unknown=None, **kwargs):
<mask>:
        raise ValidationError({'_schema': f'Invalid data type: {data}'})
    data = dict(data)
    unknown = unknown or self.unknown
    data_type = self.get_data_type(data)
    if data_type is None:
        raise ValidationError({self.type_field: ['Missing data for required field.']})
    try:
        type_schema = self.type_schemas.get(data_type)
    except TypeError as error:
        raise ValidationError({self.type_field: [f'Invalid value: {data_type}']}) from error
    if not type_schema:
        raise ValidationError({self.type_field: [f'Unsupported value: {data_type}']})
    schema = type_schema if isinstance(type_schema, Schema) else type_schema()
    schema.context.update(getattr(self, 'context', {}))
    return schema.load(data, many=False, partial=partial, unknown=unknown, **kwargs)","not isinstance(data, dict)",76,"isinstance(data, Schema) and (not isinstance(data, Schema))",False,23.961829057131983,N/A
"def add_checksum(self, chunk: bytes):
<mask>:
        self._request_headers['upload-checksum'] = ' '.join((self._checksum_algorithm_name, base64.b64encode(self._checksum_algorithm(chunk).digest()).decode('ascii')))",self._upload_checksum,10,self._request_headers.get('upload-checksum') is None,False,12.35622127262679,N/A
"def perform(self):
    """"""
        Perform actual request.
        """"""
    try:
        chunk = self.file.read(self._content_length)
        stream_eof = len(chunk) < self._content_length
        self.add_checksum(chunk)
        headers = self._request_headers
<mask>:
            headers['upload-length'] = str(self._offset + len(chunk))
        resp = requests.patch(self._url, data=chunk, headers=headers, verify=self.verify_tls_cert, stream=True, cert=self.client_cert)
        self.status_code = resp.status_code
        self.response_content = resp.content
        self.response_headers = {k.lower(): v for k, v in resp.headers.items()}
        self.stream_eof = stream_eof
    except requests.exceptions.RequestException as error:
        raise TusUploadFailed(error)",stream_eof and self._upload_length_deferred,58,self._offset + len(chunk) > self._offset,False,11.359354890271161,N/A
"def _verify_upload(request: TusRequest):
<mask>:
        return True
    else:
        raise TusUploadFailed('', request.status_code, request.response_content)",200 <= request.status_code < 300,11,request.response_content == 'OK',False,10.229197414177778,N/A
"def upload(self, stop_at: Optional[int]=None):
    """"""
        Perform file upload.

        Performs continous upload of chunks of the file. The size uploaded at each cycle is
        the value of the attribute 'chunk_size'.

        :Args:
            - stop_at (Optional[int]):
                Determines at what offset value the upload should stop. If not specified this
                defaults to the file size.
        """"""
    self.stop_at = stop_at or self.file_size
<mask>:
        self.set_url(self.create_url())
        self.offset = 0
    while self.stop_at is None or self.offset < self.stop_at:
        self.upload_chunk()",not self.url,71,self.url is None,False,39.76353643835252,N/A
"def upload_chunk(self):
    """"""
        Upload chunk of file.
        """"""
    self._retried = 0
<mask>:
        self.set_url(self.create_url())
        self.offset = 0
    self._do_request()
    self.offset = int(self.request.response_headers.get('upload-offset'))
    if self.upload_length_deferred and self.request.stream_eof:
        self.stop_at = self.offset",not self.url,27,not self.request.url,False,35.930411196308434,N/A
"@catch_requests_error
def create_url(self):
    """"""
        Return upload url.

        Makes request to tus server to create a new upload url for the required file upload.
        """"""
    resp = requests.post(self.client.url, headers=self.get_url_creation_headers(), verify=self.verify_tls_cert, cert=self.client_cert)
    url = resp.headers.get('location')
<mask>:
        msg = 'Attempt to retrieve create file url with status {}'.format(resp.status_code)
        raise TusCommunicationError(msg, resp.status_code, resp.content)
    return urljoin(self.client.url, url)",url is None,52,resp.status_code != 200,False,0.0,N/A
"def _retry_or_cry(self, error):
<mask>:
        time.sleep(self.retry_delay)
        self._retried += 1
        try:
            self.offset = self.get_offset()
        except TusCommunicationError as err:
            self._retry_or_cry(err)
        else:
            self._do_request()
    else:
        raise error",self.retries > self._retried,22,error is None,False,0.0,N/A
"def __init__(self, file_path: Optional[str]=None, file_stream: Optional[IO]=None, url: Optional[str]=None, client: Optional['TusClient']=None, chunk_size: int=MAXSIZE, metadata: Optional[Dict]=None, metadata_encoding: Optional[str]='utf-8', retries: int=0, retry_delay: int=30, verify_tls_cert: bool=True, store_url=False, url_storage: Optional[Storage]=None, fingerprinter: Optional[interface.Fingerprint]=None, upload_checksum=False, upload_length_deferred=False):
<mask>:
        raise ValueError(""Either 'file_path' or 'file_stream' cannot be None."")
    if url is None and client is None:
        raise ValueError(""Either 'url' or 'client' cannot be None."")
    if store_url and url_storage is None:
        raise ValueError('Please specify a storage instance to enable resumablility.')
    self.verify_tls_cert = verify_tls_cert
    self.file_path = file_path
    self.file_stream = file_stream
    self.file_size = self.get_file_size() if not upload_length_deferred else None
    self.stop_at = self.file_size
    self.client = client
    self.metadata = metadata or {}
    self.metadata_encoding = metadata_encoding
    self.store_url = store_url
    self.url_storage = url_storage
    self.fingerprinter = fingerprinter or fingerprint.Fingerprint()
    self.offset = 0
    self.url = None
    self.__init_url_and_offset(url)
    self.chunk_size = chunk_size
    self.retries = retries
    self.request = None
    self._retried = 0
    self.retry_delay = retry_delay
    self.upload_checksum = upload_checksum
    self.upload_length_deferred = upload_length_deferred
    self.__checksum_algorithm_name, self.__checksum_algorithm = self.CHECKSUM_ALGORITHM_PAIR",file_path is None and file_stream is None,143,file_path is None and file_stream is None,True,100.00000000000004,N/A
"def get_url_creation_headers(self):
    """"""Return headers required to create upload url""""""
    headers = self.get_headers()
<mask>:
        headers['upload-defer-length'] = '1'
    else:
        headers['upload-length'] = str(self.file_size)
    headers['upload-metadata'] = ','.join(self.encode_metadata())
    return headers",self.upload_length_deferred,25,self.file_size is None,False,14.535768424205482,N/A
"@catch_requests_error
def get_offset(self):
    """"""
        Return offset from tus server.

        This is different from the instance attribute 'offset' because this makes an
        http request to the tus server to retrieve the offset.
        """"""
    resp = requests.head(self.url, headers=self.get_headers(), verify=self.verify_tls_cert, cert=self.client_cert)
    offset = resp.headers.get('upload-offset')
<mask>:
        msg = 'Attempt to retrieve offset fails with status {}'.format(resp.status_code)
        raise TusCommunicationError(msg, resp.status_code, resp.content)
    return int(offset)",offset is None,58,not offset,False,30.326532985631665,N/A
"def encode_metadata(self):
    """"""
        Return list of encoded metadata as defined by the Tus protocol.
        """"""
    encoded_list = []
    for key, value in self.metadata.items():
        key_str = str(key)
<mask>:
            msg = 'Upload-metadata key ""{}"" cannot be empty nor contain spaces or commas.'
            raise ValueError(msg.format(key_str))
        value_bytes = value.encode(self.metadata_encoding)
        encoded_list.append('{} {}'.format(key_str, b64encode(value_bytes).decode('ascii')))
    return encoded_list","re.search('^$|[\\s,]+', key_str)",50,not key_str.strip() or key_str.strip() == '',False,8.06629036597784,N/A
"def __init_url_and_offset(self, url: Optional[str]=None):
    """"""
        Return the tus upload url.

        If resumability is enabled, this would try to get the url from storage if available,
        otherwise it would request a new upload url from the tus server.
        """"""
<mask>:
        self.set_url(url)
    if self.store_url and self.url_storage:
        key = self._get_fingerprint()
        self.set_url(self.url_storage.get_item(key))
    if self.url:
        self.offset = self.get_offset()",url,53,url,True,100.00000000000004,N/A
"def set_item(self, key: str, url: str):
    """"""
        Store the url value under the unique key.

        :Args:
            - key[str]: The unique id to which the item (in this case, url) would be stored.
            - value[str]: The actual url value to be stored.
        """"""
<mask>:
        self._db.update({'url': url}, self._urls.key == key)
    else:
        self._db.insert({'key': key, 'url': url})",self._db.search(self._urls.key == key),53,key in self._urls,False,8.590098392241092,N/A
"def usage_version(cmd):
<mask>:
        usage()
    if cmd in {'version', '--version', '-v'}:
        version()","cmd in {'help', '--help', '-h'}",11,"cmd in {'usage', '--help', '-v'}",False,31.02016197007,N/A
"def call_daemon(shell, req, interactive):
    try:
        sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
    except OSError as err:
        shell.con.display(shell.con.render_text(err, 'red'))
        sys.exit(1)
    try:
        sock.connect(socket_path)
    except OSError as err:
        shell.con.display(shell.con.render_text(err, 'red'))
        shell.con.display(shell.con.render_text(""Currently auto_use_daemon is true, hence please make sure targetclid daemon is running ...\n(or)\nIncase if you wish to turn auto_use_daemon to false then run '#targetcli --disable-daemon'"", 'red'))
        sys.exit(1)
    get_pwd = False
<mask>:
        if not req:
            req = 'pwd'
            get_pwd = True
        elif 'cd ' in req:
            req += '%pwd'
            get_pwd = True
    else:
        req = 'cd /%' + req
    try:
        sock.sendall(req.encode())
    except OSError as err:
        shell.con.display(shell.con.render_text(err, 'red'))
        sys.exit(1)
    var = sock.recv(4)
    sending = struct.unpack('i', var)
    amount_expected = sending[0]
    amount_received = 0
    output = ''
    path = ''
    while amount_received < amount_expected:
        data = sock.recv(1024)
        data = data.decode()
        amount_received += len(data)
        output += data
    if get_pwd:
        output_split = output.splitlines()
        lines = len(output_split)
        for i in range(lines):
            if i == lines - 1:
                path = str(output_split[i])
            else:
                print(str(output_split[i]), end='\n')
    else:
        print(output, end='')
    sock.send(b'-END@OF@DATA-')
    sock.close()
    return path",interactive,157,interactive,True,100.00000000000004,N/A
"def switch_to_daemon(shell, interactive):
    readline.set_completer(completer)
    readline.set_completer_delims('')
<mask>:
        readline.parse_and_bind('bind ^I rl_complete')
    else:
        readline.parse_and_bind('tab: complete')
    if len(sys.argv) > 1:
        command = ' '.join(sys.argv[1:])
        call_daemon(shell, command, False)
        sys.exit(0)
    if interactive:
        shell.con.display(f""targetcli shell version {targetcli_version}\nEntering targetcli interactive mode for daemonized approach.\nType 'exit' to quit.\n"")
    else:
        shell.con.display(f""targetcli shell version {targetcli_version}\nEntering targetcli batch mode for daemonized approach.\nEnter multiple commands separated by newline and type 'exit' to run them all in one go.\n"")
    prompt_path = '/'
    if interactive:
        prompt_path = call_daemon(shell, None, interactive)
    inputs = []
    real_exit = False
    while True:
        command = input(f'{prompt_path}> ')
        if command.lower() == 'exit':
            real_exit = True
        elif not command:
            continue
        if not interactive:
            inputs.append(command)
            if real_exit:
                command = '%'.join(inputs)
                call_daemon(shell, command, interactive)
                break
        else:
            if real_exit:
                break
            path = call_daemon(shell, command, interactive)
            if path:
                if path[0] == '/':
                    prompt_path = path
                else:
                    print(path)
    sys.exit(0)",'libedit' in readline.__doc__,132,sys.platform == 'win32',False,4.923026124015933,N/A
"def main():
    """"""
    Start the targetcli shell.
    """"""
    shell = TargetCLI(getenv('TARGETCLI_HOME', '~/.targetcli'))
    is_root = False
<mask>:
        is_root = True
    try:
        lkfd = open(lock_file, 'w+')
    except OSError as e:
        shell.con.display(shell.con.render_text(f'opening lockfile failed: {e!s}', 'red'))
        sys.exit(1)
    try_op_lock(shell, lkfd)
    use_daemon = False
    if shell.prefs['auto_use_daemon']:
        use_daemon = True
    disable_daemon = False
    if len(sys.argv) > 1:
        usage_version(sys.argv[1])
        if sys.argv[1] in {'disable-daemon', '--disable-daemon'}:
            disable_daemon = True
    interactive_mode = True
    if shell.prefs['daemon_use_batch_mode']:
        interactive_mode = False
    if use_daemon and (not disable_daemon):
        switch_to_daemon(shell, interactive_mode)
    try:
        root_node = UIRoot(shell, as_root=is_root)
        root_node.refresh()
    except Exception as error:
        shell.con.display(shell.con.render_text(str(error), 'red'))
        if not is_root:
            shell.con.display(shell.con.render_text('Retry as root.', 'red'))
        sys.exit(-1)
    if len(sys.argv) > 1:
        try:
            if disable_daemon:
                shell.run_cmdline('set global auto_use_daemon=false')
            else:
                shell.run_cmdline(' '.join(sys.argv[1:]))
        except Exception as e:
            print(str(e), file=sys.stderr)
            sys.exit(1)
        sys.exit(0)
    shell.con.display(f""targetcli shell version {targetcli_version}\nCopyright 2011-2013 by Datera, Inc and others.\nFor help on commands, type 'help'.\n"")
    if not is_root:
        shell.con.display('You are not root, disabling privileged commands.\n')
    while not shell._exit:
        try:
            shell.run_interactive()
        except (RTSLibError, ExecutionError) as msg:
            shell.log.error(str(msg))
    if shell.prefs['auto_save_on_exit'] and is_root:
        shell.log.info('Global pref auto_save_on_exit=true')
        root_node.ui_command_saveconfig()
    release_op_lock(shell, lkfd)",getuid() == 0,162,os.path.isdir(lock_file),False,4.990049701936832,N/A
"def __init__(self, fabric_module, parent):
    super().__init__(fabric_module.name, fabric_module, parent, late_params=True)
    self.refresh()
<mask>:
        for param in discovery_params:
            if param in int_params:
                self.define_config_group_param('discovery_auth', param, 'number')
            else:
                self.define_config_group_param('discovery_auth', param, 'string')
    self.refresh()",self.rtsnode.has_feature('discovery_auth'),26,discovery_params,False,1.3699439807202476,N/A
"def list_config_groups(self):
    groups = super().list_config_groups()
<mask>:
        groups.append('parameter')
    if len(self.rtsnode.list_attributes()):
        groups.append('attribute')
    return groups",len(self.rtsnode.list_parameters()),12,len(self.rtsnode.list_parameters()),True,100.00000000000004,N/A
"def list_group_params(self, group, writable=None):
<mask>:
        return super().list_group_params(group, writable)
    params_func = getattr(self.rtsnode, f'list_{group}s')
    params = params_func()
    params_ro = params_func(writable=False)
    ret_list = []
    for param in params:
        p_writable = param not in params_ro
        if writable is not None and p_writable != writable:
            continue
        ret_list.append(param)
    ret_list.sort()
    return ret_list","group not in {'parameter', 'attribute'}",45,self.rtsnode is None,False,0.0,N/A
"def get_group_param(self, group, param):
<mask>:
        return super().get_group_param(group, param)
    if param not in self.list_group_params(group):
        raise ValueError(f'Not such parameter {param} in configuration group {group}')
    description = f'The {param} {group}.'
    writable = param in self.list_group_params(group, writable=True)
    return {'name': param, 'group': group, 'type': 'string', 'description': description, 'writable': writable}","group not in {'parameter', 'attribute'}",44,param == 'name',False,0.0,N/A
"def ui_getgroup_discovery_auth(self, auth_attr):
    """"""
        This is the backend method for getting discovery_auth attributes.
        @param auth_attr: The auth attribute to get the value of.
        @type auth_attr: str
        @return: The auth attribute's value
        @rtype: str
        """"""
<mask>:
        return self.rtsnode.discovery_enable_auth
    return getattr(self.rtsnode, 'discovery_' + auth_attr)",auth_attr == 'enable',42,auth_attr == 'enable_discovery_auth',False,39.281465090051306,N/A
"def assert_root(self):
    """"""
        For commands requiring root privileges, disable command if not the root
        node's as_root attribute is False.
        """"""
    root_node = self.get_root()
<mask>:
        raise ExecutionError('This privileged command is disabled: you are not root.')","hasattr(root_node, 'as_root') and (not self.get_root().as_root)",34,root_node is None,False,0.7282945743438879,N/A
"def new_node(self, new_node):
    """"""
        Used to honor global 'auto_cd_after_create'.
        Either returns None if the global is False, or the new_node if the
        global is True. In both cases, set the @last bookmark to last_node.
        """"""
    self.shell.prefs['bookmarks']['last'] = new_node.path
    self.shell.prefs.save()
<mask>:
        self.shell.log.info(f'Entering new node {new_node.path}')
        return self.ui_command_cd(new_node.path)
    return None",self.shell.prefs['auto_cd_after_create'],48,self.shell.prefs['bookmarks']['last'] == new_node.path,False,28.43329181530769,N/A
"def ui_type_yesno(self, value=None, enum=False, reverse=False):
    """"""
        UI parameter type helper for ""Yes"" and ""No"" boolean values.
        ""Yes"" and ""No"" are used for boolean iSCSI session parameters.
        """"""
<mask>:
        if value is not None:
            return value
        return 'n/a'
    type_enum = ('Yes', 'No')
    syntax = '|'.join(type_enum)
    if value is None:
        if enum:
            return type_enum
        return syntax
    if value in type_enum:
        return value
    raise ValueError(f""Syntax error, '{value}' is not {syntax}."")",reverse,67,reverse,True,100.00000000000004,N/A
"def __init__(self, name, rtslib_object, parent, late_params=False):
    """"""
        Call from the class that inherits this, with the rtslib object that
        should be checked upon.
        """"""
    UINode.__init__(self, name, parent)
    self.rtsnode = rtslib_object
<mask>:
        return
    parameters = self.rtsnode.list_parameters()
    parameters_ro = self.rtsnode.list_parameters(writable=False)
    for parameter in parameters:
        writable = parameter not in parameters_ro
        param_type, desc = getattr(self.__class__, 'ui_desc_parameters', {}).get(parameter, ('string', ''))
        self.define_config_group_param('parameter', parameter, param_type, desc, writable)
    attributes = self.rtsnode.list_attributes()
    attributes_ro = self.rtsnode.list_attributes(writable=False)
    for attribute in attributes:
        writable = attribute not in attributes_ro
        param_type, desc = getattr(self.__class__, 'ui_desc_attributes', {}).get(attribute, ('string', ''))
        self.define_config_group_param('attribute', attribute, param_type, desc, writable)",late_params,90,late_params,True,100.00000000000004,N/A
"def ui_command_info(self):
    info = self.rtsnode.dump()
    for item in ('attributes', 'parameters'):
<mask>:
            del info[item]
    for name, value in sorted(info.items()):
        if not isinstance(value, (dict, list)):
            self.shell.log.info(f'{name}: {value}')",item in info,25,item in info,True,100.00000000000004,N/A
"def human_to_bytes(hsize, kilo=1024):
    """"""
    This function converts human-readable amounts of bytes to bytes.
    It understands the following units :
        - B or no unit present for Bytes
        - k, K, kB, KB for kB (kilobytes)
        - m, M, mB, MB for MB (megabytes)
        - g, G, gB, GB for GB (gigabytes)
        - t, T, tB, TB for TB (terabytes)

    Note: The definition of kilo defaults to 1kB = 1024Bytes.
    Strictly speaking, those should not be called ""kB"" but ""kiB"".
    You can override that with the optional kilo parameter.

    @param hsize: The human-readable version of the Bytes amount to convert
    @type hsize: string or int
    @param kilo: Optional base for the kilo prefix
    @type kilo: int
    @return: An int representing the human-readable string converted to bytes
    """"""
    size = hsize.replace('i', '').lower()
<mask>:
        raise RTSLibError(f'Cannot interpret size, wrong format: {hsize}')
    size = size.rstrip('ib')
    units = ['k', 'm', 'g', 't']
    try:
        power = units.index(size[-1]) + 1
    except ValueError:
        power = 0
        size = int(size)
    else:
        size = int(size[:-1])
    return size * int(kilo) ** power","not re.match('^[0-9]+[k|m|g|t]?[b]?$', size)",171,size.count('ib') != size.count('ib'),False,1.110273706430991,N/A
"def bytes_to_human(size):
    kilo = 1024.0
<mask>:
        return '%d bytes' % size
    size /= kilo
    for x in ('KiB', 'MiB', 'GiB', 'TiB', 'PiB'):
        if size < kilo:
            return f'{size:3.1f}{x}'
        size /= kilo
    return None",size < kilo,33,size < kilo,True,100.00000000000004,N/A
"def complete_path(path, stat_fn):
    filtered = []
    for entry in glob.glob(path + '*'):
        st = os.stat(entry)
<mask>:
            filtered.append(entry + '/')
        elif stat_fn(st.st_mode):
            filtered.append(entry)
    return sorted(filtered, key=lambda s: '~' + s if s.endswith('/') else s)",stat.S_ISDIR(st.st_mode),33,stat_fn(st.st_mode),False,55.70438815301074,N/A
"def ui_setgroup_alua(self, alua_attr, value):
    self.assert_root()
<mask>:
        return
    setattr(self.rtsnode, alua_attr, value)",value is None,10,not self.rtsnode,False,0.0,N/A
"def ui_complete_delete(self, parameters, text, current_param):
    """"""
        Parameter auto-completion method for user command delete.
        @param parameters: Parameters on the command line.
        @type parameters: dict
        @param text: Current text of parameter being typed by the user.
        @type text: str
        @param current_param: Name of parameter to complete.
        @type current_param: str
        @return: Possible completions
        @rtype: list of str
        """"""
<mask>:
        so = self.parent.rtsnode
        tpgs = [tpg.name for tpg in so.alua_tpgs]
        completions = [tpg for tpg in tpgs if tpg.startswith(text)]
    else:
        completions = []
    if len(completions) == 1:
        return [completions[0] + ' ']
    return completions",current_param == 'name',90,self.parent,False,0.0,N/A
"def refresh(self):
    """"""
        Refreshes the tree of target fabric modules.
        """"""
    self._children = set()
<mask>:
        self.rtsroot.invalidate_caches()
    UIBackstores(self)
    for fm in self.rtsroot.fabric_modules:
        if fm.wwns is None or any(fm.wwns):
            UIFabricModule(fm, self)",'invalidate_caches' in dir(RTSRoot),29,self.rtsroot is not None,False,0.0,N/A
"def _compare_files(self, backupfile, savefile):
    """"""
        Compare backfile and saveconfig file
        """"""
    backupfilepath = Path(backupfile)
<mask>:
        try:
            with gzip.open(backupfilepath, 'rb') as fbkp:
                fdata_bkp = fbkp.read()
        except OSError as e:
            self.shell.log.warning(f'Could not gzip open backupfile {backupfile}: {e.strerror}')
    else:
        try:
            fdata_bkp = backupfilepath.read_bytes()
        except OSError as e:
            self.shell.log.warning(f'Could not open backupfile {backupfile}: {e.strerror}')
    try:
        fdata = Path(savefile).read_bytes()
    except OSError as e:
        self.shell.log.warning(f'Could not open saveconfig file {savefile}: {e.strerror}')
    return fdata_bkp == fdata",PurePosixPath(backupfile).suffix == '.gz',69,backupfilepath.is_file(),False,4.880869806051147,N/A
"def _create_dir(self, dirname):
    """"""
        create directory with permissions 0o600 set
        if directory already exists, set right perms
        """"""
    mode = stat.S_IRUSR | stat.S_IWUSR
    dir_path = Path(dirname)
<mask>:
        umask = 511 ^ mode
        umask_original = os.umask(umask)
        try:
            dir_path.mkdir(mode=mode)
        except OSError as exe:
            raise ExecutionError(f'Cannot create directory [{dirname}] {exe.strerror}.')
        finally:
            os.umask(umask_original)
    elif dirname == default_target_dir and os.stat(dirname).st_mode & 511 != mode:
        os.chmod(dirname, mode)",not dir_path.exists(),61,not dir_path.exists(),True,100.00000000000004,N/A
"def _save_backups(self, savefile):
    """"""
        Take backup of config-file if needed.
        """"""
<mask>:
        return
    backup_dir = os.path.dirname(savefile) + '/backup/'
    backup_name = 'saveconfig-' + datetime.now().strftime('%Y%m%d-%H:%M:%S') + '-json.gz'
    backupfile = backup_dir + backup_name
    backup_error = None
    self._create_dir(backup_dir)
    if not Path(savefile).exists():
        return
    backed_files_list = sorted(glob(os.path.dirname(savefile) + '/backup/saveconfig-*json*'))
    if not backed_files_list or not self._compare_files(backed_files_list[-1], savefile):
        mode = stat.S_IRUSR | stat.S_IWUSR
        umask = 511 ^ mode
        umask_original = os.umask(umask)
        try:
            with open(savefile, 'rb') as f_in, gzip.open(backupfile, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
                f_out.flush()
        except OSError as ioe:
            backup_error = ioe.strerror or 'Unknown error'
        finally:
            os.umask(umask_original)
        if backup_error is None:
            max_backup_files = int(self.shell.prefs['max_backup_files'])
            try:
                prefs = Path(universal_prefs_file).read_text()
                backups = [line for line in prefs.splitlines() if re.match('^max_backup_files\\s*=', line)]
                if max_backup_files < int(backups[0].split('=')[1].strip()):
                    max_backup_files = int(backups[0].split('=')[1].strip())
            except:
                self.shell.log.debug(f""No universal prefs file '{universal_prefs_file}'."")
            files_to_unlink = list(reversed(backed_files_list))[max_backup_files - 1:]
            for f in files_to_unlink:
                with ignored(IOError):
                    Path(f).unlink()
            self.shell.log.info('Last %d configs saved in %s.' % (max_backup_files, backup_dir))
        else:
            self.shell.log.warning(f'Could not create backup file {backupfile}: {backup_error}.')",savefile != default_save_file,151,not Path(savefile).exists(),False,4.767707020457095,N/A
"def ui_command_saveconfig(self, savefile=default_save_file):
    """"""
        Saves the current configuration to a file so that it can be restored
        on next boot.
        """"""
    self.assert_root()
<mask>:
        savefile = default_save_file
    savefile = os.path.expanduser(savefile)
    save_dir = os.path.dirname(savefile)
    self._create_dir(save_dir)
    self._save_backups(savefile)
    self.rtsroot.save_to_file(savefile)
    self.shell.log.info(f'Configuration saved to {savefile}')",not savefile,39,savefile is None,False,27.516060407455225,N/A
"def __init__(self):
    """"""
        initializer
        """"""
    self.socket_path = '/var/run/targetclid.sock'
    self.pid_file = '/var/run/targetclid.pid'
    self.NoSignal = True
    self.sock = None
    self.shell = ConfigShell(getenv('TARGETCLI_HOME', '~/.targetcli'))
    self.con = self.shell.con
    self.display = self.shell.con.display
    self.render = self.shell.con.render_text
    signal.signal(signal.SIGINT, self.signal_handler)
    signal.signal(signal.SIGTERM, self.signal_handler)
    signal.signal(signal.SIGHUP, self.signal_handler)
    try:
        self.pfd = open(self.pid_file, 'w+')
    except OSError as e:
        self.display(self.render(f'opening pidfile failed: {e!s}', 'red'))
        sys.exit(1)
    self.try_pidfile_lock()
    is_root = False
<mask>:
        is_root = True
    try:
        root_node = UIRoot(self.shell, as_root=is_root)
        root_node.refresh()
    except Exception as error:
        self.display(self.render(str(error), 'red'))
        if not is_root:
            self.display(self.render('Retry as root.', 'red'))
        self.pfd.close()
        sys.exit(1)
    self.con_stdout_ = self.con._stdout
    self.con_stderr_ = self.con._stderr",getuid() == 0,86,os.path.isdir(self.pid_file),False,4.02724819242185,N/A
"def signal_handler(self):
    """"""
        signal handler
        """"""
    self.NoSignal = False
<mask>:
        self.sock.close()",self.sock,11,self.sock is not None,False,30.213753973567677,N/A
"def client_thread(self, connection):
    """"""
        Handle commands from client
        """"""
    self.shell.prefs.load()
    still_listen = True
    while still_listen:
        data = connection.recv(65535)
<mask>:
            connection.close()
            still_listen = False
        else:
            self.con._stdout = self.con._stderr = f = tempfile.NamedTemporaryFile(mode='w', delete=False)
            try:
                list_data = data.decode().split('%')
                for cmd in list_data:
                    self.shell.run_cmdline(cmd)
            except Exception as e:
                print(str(e), file=f)
            self.con._stdout = self.con_stdout_
            self.con._stderr = self.con_stderr_
            f.close()
            with open(f.name) as f:
                output = f.read()
                var = struct.pack('i', len(output))
                connection.sendall(var)
                if len(output):
                    connection.sendall(output.encode())
            Path(f.name).unlink()",b'-END@OF@DATA-' in data,70,not data,False,4.104249931194939,N/A
"def __init__(self, en_resx: Path, translation_resx: Path, code: str, output_po: Path):
<mask>:
        raise Exception('EN Resx {} not found'.format(en_resx.absolute()))
    if not translation_resx.is_file():
        raise Exception('Translation {} {} Resx bound not found'.format(code, translation_resx.absolute()))
    self.en_resx = self.resx2dict(en_resx)
    self.translation_resx = self.resx2dict(translation_resx)
    self.code = code
    self.output_po = output_po
    self.generate()",not en_resx.is_file(),42,not en_resx.is_file(),True,100.00000000000004,N/A
"def generate(self):
    po = polib.POFile()
    now = datetime.datetime.now(datetime.timezone.utc)
    po.metadata = {'Project-Id-Version': '1.0', 'Report-Msgid-Bugs-To': 'adam.schubert@sg1-game.net', 'POT-Creation-Date': now.strftime('%Y-%m-%d %H:%M%z'), 'PO-Revision-Date': now.strftime('%Y-%m-%d %H:%M%z'), 'Last-Translator': 'Adam Schubert <adam.schubert@sg1-game.net>', 'Language-Team': '', 'MIME-Version': '1.0', 'Content-Type': 'text/plain; charset=utf-8', 'Content-Transfer-Encoding': '8bit', 'Language': self.code}
    for message_en_id, message_en in self.en_resx.items():
<mask>:
            entry = polib.POEntry(msgid=message_en, msgstr=self.translation_resx[message_en_id], comment=message_en_id)
            po.append(entry)
        else:
            print('WARNING: {} not found in {} resx'.format(message_en_id, self.code))
    po.save(str(self.output_po.absolute()))",message_en_id in self.translation_resx,57,message_en_id in self.translation_resx,True,100.00000000000004,N/A
"def load_locale(self, locale_code, locale_location=None):
<mask>:
        filename = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'locale', '{}.mo'.format(locale_code))
    else:
        filename = os.path.join(locale_location, '{}.mo'.format(locale_code))
    with open(filename, 'rb') as f:
        trans = gettext.GNUTranslations(f)
    logger.debug('{} Loaded'.format(filename))
    return trans",locale_location is None,27,locale_location is None,True,100.00000000000004,N/A
"def parse(self):
    """"""Parses the cron expression string
        Returns:
            A 7 part string array, one part for each component of the cron expression (seconds, minutes, etc.)
        Raises:
            MissingFieldException: if _expression is empty or None
            FormatException: if _expression has wrong format
        """"""
    parsed = ['', '', '', '', '', '', '']
<mask>:
        raise MissingFieldException('ExpressionDescriptor.expression')
    else:
        expression_parts_temp = self._expression.split()
        expression_parts_temp_length = len(expression_parts_temp)
        if expression_parts_temp_length < 5:
            raise FormatException('Error: Expression only has {0} parts.  At least 5 part are required.'.format(expression_parts_temp_length))
        elif expression_parts_temp_length == 5:
            for i, expression_part_temp in enumerate(expression_parts_temp):
                parsed[i + 1] = expression_part_temp
        elif expression_parts_temp_length == 6:
            year_regex = re.compile('\\d{4}$')
            is_year_with_no_seconds_part = bool(year_regex.search(expression_parts_temp[5])) or '?' in [expression_parts_temp[4], expression_parts_temp[2]]
            for i, expression_part_temp in enumerate(expression_parts_temp):
                if is_year_with_no_seconds_part:
                    parsed[i + 1] = expression_part_temp
                else:
                    parsed[i] = expression_part_temp
        elif expression_parts_temp_length == 7:
            parsed = expression_parts_temp
        else:
            raise FormatException('Error: Expression has too many parts ({0}).  Expression must not have more than 7 parts.'.format(expression_parts_temp_length))
    self.normalize_expression(parsed)
    return parsed",self._expression is None or len(self._expression) == 0,148,self._expression is None,False,15.987974607969395,N/A
"def __init__(self, expression, options=None, **kwargs):
    """"""Initializes a new instance of the ExpressionDescriptor

        Args:
            expression: The cron expression string
            options: Options to control the output description
        Raises:
            WrongArgumentException: if kwarg is unknown

        """"""
<mask>:
        options = Options()
    self._expression = expression
    self._options = options
    self._expression_parts = []
    for kwarg in kwargs:
        if hasattr(self._options, kwarg):
            setattr(self._options, kwarg, kwargs[kwarg])
        else:
            raise WrongArgumentException('Unknown {} configuration argument'.format(kwarg))
    self.get_text = GetText(options.locale_code, options.locale_location)
    parser = ExpressionParser(self._expression, self._options)
    self._expression_parts = parser.parse()",options is None,72,options is None,True,100.00000000000004,N/A
"def get_time_of_day_description(self):
    """"""Generates a description for only the TIMEOFDAY portion of the expression

        Returns:
            The TIMEOFDAY description

        """"""
    seconds_expression = self._expression_parts[0]
    minute_expression = self._expression_parts[1]
    hour_expression = self._expression_parts[2]
    description = StringBuilder()
<mask>:
        description.append(self._('At '))
        description.append(self.format_time(hour_expression, minute_expression, seconds_expression))
    elif seconds_expression == '' and '-' in minute_expression and (',' not in minute_expression) and (any((exp in hour_expression for exp in self._special_characters)) is False):
        minute_parts = minute_expression.split('-')
        description.append(self._('Every minute between {0} and {1}').format(self.format_time(hour_expression, minute_parts[0]), self.format_time(hour_expression, minute_parts[1])))
    elif seconds_expression == '' and ',' in hour_expression and ('-' not in hour_expression) and (any((exp in minute_expression for exp in self._special_characters)) is False):
        hour_parts = hour_expression.split(',')
        description.append(self._('At'))
        for i, hour_part in enumerate(hour_parts):
            description.append(' ')
            description.append(self.format_time(hour_part, minute_expression))
            if i < len(hour_parts) - 2:
                description.append(',')
            if i == len(hour_parts) - 2:
                description.append(self._(' and'))
    else:
        seconds_description = self.get_seconds_description()
        minutes_description = self.get_minutes_description()
        hours_description = self.get_hours_description()
        description.append(seconds_description)
        if description and minutes_description:
            description.append(', ')
        description.append(minutes_description)
        if description and hours_description:
            description.append(', ')
        description.append(hours_description)
    return str(description)",any((exp in minute_expression for exp in self._special_characters)) is False and any((exp in hour_expression for exp in self._special_characters)) is False and (any((exp in seconds_expression for exp in self._special_characters)) is False),149,seconds_expression != '' and '-' in hour_expression and (any((exp in hour_expression for exp in self._special_characters)),False,24.877426660320904,N/A
"def get_seconds_description(self):
    """"""Generates a description for only the SECONDS portion of the expression

        Returns:
            The SECONDS description

        """"""

    def get_description_format(s):
<mask>:
            return ''
        try:
            if int(s) < 20:
                return self._('at {0} seconds past the minute')
            else:
                return self._('at {0} seconds past the minute [grThen20]') or self._('at {0} seconds past the minute')
        except ValueError:
            return self._('at {0} seconds past the minute')
    return self.get_segment_description(self._expression_parts[0], self._('every second'), lambda s: s, lambda s: self._('every {0} seconds').format(s), lambda s: self._('seconds {0} through {1} past the minute'), get_description_format, lambda s: self._(', second {0} through second {1}') or self._(', {0} through {1}'))",s == '0',95,s is None,False,19.716118825581447,N/A
"def get_minutes_description(self):
    """"""Generates a description for only the MINUTE portion of the expression

        Returns:
            The MINUTE description

        """"""
    seconds_expression = self._expression_parts[0]

    def get_description_format(s):
<mask>:
            return ''
        try:
            if int(s) < 20:
                return self._('at {0} minutes past the hour')
            else:
                return self._('at {0} minutes past the hour [grThen20]') or self._('at {0} minutes past the hour')
        except ValueError:
            return self._('at {0} minutes past the hour')
    return self.get_segment_description(self._expression_parts[1], self._('every minute'), lambda s: s, lambda s: self._('every {0} minutes').format(s), lambda s: self._('minutes {0} through {1} past the hour'), get_description_format, lambda s: self._(', minute {0} through minute {1}') or self._(', {0} through {1}'))",s == '0' and seconds_expression == '',98,s == '',False,14.6125886111981,N/A
"def get_day_of_week_description(self):
    """"""Generates a description for only the DAYOFWEEK portion of the expression

        Returns:
            The DAYOFWEEK description

        """"""
<mask>:
        return ''

    def get_day_name(s):
        exp = s
        if '#' in s:
            exp, _ = s.split('#', 2)
        elif 'L' in s:
            exp = exp.replace('L', '')
        return ExpressionDescriptor.number_to_day(int(exp))

    def get_format(s):
        if '#' in s:
            day_of_week_of_month = s[s.find('#') + 1:]
            try:
                day_of_week_of_month_number = int(day_of_week_of_month)
                choices = {1: self._('first'), 2: self._('second'), 3: self._('third'), 4: self._('fourth'), 5: self._('fifth')}
                day_of_week_of_month_description = choices.get(day_of_week_of_month_number, '')
            except ValueError:
                day_of_week_of_month_description = ''
            formatted = '{}{}{}'.format(self._(', on the '), day_of_week_of_month_description, self._(' {0} of the month'))
        elif 'L' in s:
            formatted = self._(', on the last {0} of the month')
        else:
            formatted = self._(', only on {0}')
        return formatted
    return self.get_segment_description(self._expression_parts[5], self._(', every day'), lambda s: get_day_name(s), lambda s: self._(', every {0} days of the week').format(s), lambda s: self._(', {0} through {1}'), lambda s: get_format(s), lambda s: self._(', {0} through {1}'))",self._expression_parts[5] == '*',148,not self._expression_parts,False,29.765372490051615,N/A
"def append(self, string):
    """"""Appends non empty string

        Args:
            string: String to append
        Returns:
            None
        """"""
<mask>:
        self.string.append(string)",string,17,string,True,100.00000000000004,N/A
"def __init__(self, cronfile):
    """"""Initialize CrontabReader

        Args:
            cronfile: Path to cronfile
        Returns:
            None
        """"""
    options = Options()
    options.day_of_week_start_index_zero = False
    options.use_24hour_time_format = True
    with open(cronfile) as f:
        for line in f.readlines():
            parsed_line = self.parse_cron_line(line)
<mask>:
                print('{} -> {}'.format(parsed_line, ExpressionDescriptor(parsed_line, options)))",parsed_line,39,parsed_line,True,100.00000000000004,N/A
"def parse_cron_line(self, line):
    """"""Parses crontab line and returns only starting time string

        Args:
            line: crontab line
        Returns:
            Time part of cron line
        """"""
    stripped = line.strip()
<mask>:
        rexres = self.rex.search(stripped)
        if rexres:
            return ' '.join(rexres.group(1).split())
    return None",stripped and stripped.startswith('#') is False,37,stripped,False,0.0016701700790245667,N/A
"def create_message_segments(subject, recipients, text_to_post):
    """""" Creates a chatter post with simple text, it can also @mention, post to groups, users or records.
        To facilitate its usage, it build the message based on the arguments provided as required, see link below
                    https://developer.salesforce.com/docs/atlas.en-us.chatterapi.meta/chatterapi/quickreference_post_comment_or_feed_item_with_mention.htm?search_text=mention
            """"""
<mask>:
        text_to_post = ''
    if subject is None or (recipients is None and len(text_to_post) < 1):
        raise ValueError('You must provide a subject and you must provide a text or @mention someone.')
    messages = list()
    messages.append({'type': 'text', 'text': ' ' + text_to_post + ' '})
    message_segments = {'messageSegments': ''}
    if recipients:
        [messages.append({'type': 'Mention', 'id': i}) for i in recipients if len(i) > 0]
    message_segments['messageSegments'] = messages
    body = {'body': message_segments, 'feedElementType': 'FeedItem', 'subjectId': subject}
    return body",text_to_post is None,117,text_to_post is None,True,100.00000000000004,N/A
"def add_response(res_key):
<mask>:
        res = mock_responses[res_key]
    else:
        with open(os.path.join(tests_dir, 'fixtures/%s.json' % res_key)) as f:
            res = mock_responses[res_key] = json.loads(f.read())
    body = None if res['body'] is None else json.dumps(res['body'])
    responses.add(res['method'], res['url'], body=body, status=res['status_code'], content_type=res['content_type'])",res_key in mock_responses,33,res_key in mock_responses,True,100.00000000000004,N/A
"def __init__(self, session_id, instance_url, api_version, **kwargs):
    """""" Constructor. Calls `super`, then encodes the `service` including the `query_string` provided

          :param: session_id: Session ID used to make request
          :type: session_id: string
          :param: instance_url: Instance URL used to make the request (eg. `'eu11.salesforce.com'`)
          :type: instance_url: string
          :param: api_version: API version
          :type: api_version: string
          :param: **kwargs: kwargs
          :type: **kwargs: dict
        """"""
    super(GetJob, self).__init__(session_id, instance_url, **kwargs)
    successes = kwargs.get('successes', False)
    failures = kwargs.get('failures', False)
    unprocessed = kwargs.get('unprocessed', False)
    job_id = kwargs.get('job_id')
    job_info = kwargs.get('job_info', successes is False and failures is False and (unprocessed is False))
    self.http_method = 'GET'
<mask>:
        self.service = GET_SUCCESSES_URI % (api_version, job_id)
    elif failures:
        self.service = GET_FAILURES_URI % (api_version, job_id)
    elif unprocessed:
        self.service = GET_UNPROCESSED_URI % (api_version, job_id)
    elif job_info and job_id is not None:
        self.service = GET_INFO_URI % (api_version, job_id)
    else:
        self.service = GET_ALL_URI % api_version",successes,135,successes,True,100.00000000000004,N/A
"def element_to_dict(element):
    result = {}
    for child in element:
<mask>:
            result[child.tag] = element_to_dict(child)
        else:
            result[child.tag] = child.text
    return result",len(child) > 0,19,"isinstance(child, Element)",False,17.965205598154213,N/A
"def kwarg_adder(func):
    """"""
    Decorator to add the kwargs from the client to the kwargs at the function level. If the same
    parameters are used in both, the function level kwarg will supersede the one at the client level.

    :param func: client function to add client kwargs to
    :return: the function with updated kwargs
    """"""

    def decorated(self, *args, **function_kwarg):
<mask>:
            function_kwarg = collections.ChainMap(function_kwarg, self.client_kwargs)
        return func(self, *args, **function_kwarg)
    return decorated","hasattr(self, 'client_kwargs')",69,"isinstance(function_kwarg, dict)",False,7.809849842300637,N/A
"def get_request_url(self):
    """""" Returns the request URL. (default: `'https://<instance_url><service>'`)

          :return: request_url
          :rtype: string
        """"""
<mask>:
        self.request_url = 'https://%s%s' % (self.instance_url, self.service)
    return self.request_url",self.request_url is None,23,self.instance_url and self.service,False,13.485111859503691,N/A
"def get_headers(self):
    """""" Returns headers dict for the request.

          :return: headers
          :rtype: dict
        """"""
<mask>:
        self.headers = {'Content-Type': 'application/json', 'Accept-Encoding': 'application/json', 'Authorization': 'OAuth %s' % self.session_id}
    return self.headers",self.headers is None,28,not self.headers,False,46.30777161991026,N/A
"def request(self):
    """""" Makes request to Salesforce and returns serialised response. Catches any exceptions and appends them to
        `self.exceptions`.

          :return: response: Salesforce response, if available
          :rtype: list|dict|None
        """"""
    headers, logger, request_object, response, service = self.get_request_vars()
    logging.getLogger('sfdc_py').info('%s %s' % (self.http_method, service))
<mask>:
        request_fn = post_request
    elif self.http_method == 'PUT':
        request_fn = put_request
    elif self.http_method == 'PATCH':
        request_fn = patch_request
    elif self.http_method == 'DELETE':
        request_fn = delete_request
    else:
        request_fn = get_request
    try:
        request_object = request_fn(self)
        self.status = request_object.status_code
        if request_object.content.decode('utf-8') == 'null':
            raise SFDCRequestException('Request body is null')
        else:
            response = request_object.json()
    except Exception as e:
        self.exceptions.append(e)
        logger.error('%s %s %s' % (self.http_method, service, self.status))
        logger.error(e.message)
        return
    finally:
        return response",self.http_method == 'POST',106,self.http_method == 'POST',True,100.00000000000004,N/A
"def __init__(self, session_id, instance_url, action, http_method, request_params, request_body, **kwargs):
<mask>:
        p = urlencode(request_params)
        action = '%s?%s' % (action, p)
    super().__init__(session_id, instance_url, http_method=http_method, request_body=request_body, **kwargs)
    self.service = '/services/apexrest/%s' % action",request_params is not None,29,request_params,False,36.78794411714425,N/A
"@commons.kwarg_adder
def login(self, **kwargs):
    """""" Performs a login request.

          :param: **kwargs: kwargs
          :type: **kwargs: dict
          :return: Login response
          :rtype: (dict, Login)
        """"""
    login_response = Login(self.username, self.password, self.client_id, self.client_secret, **kwargs)
    req = login_response.request()
<mask>:
        self.session_id = login_response.get_session_id()
        self.set_instance_url(req.get('instance_url', str()))
        self.set_api_version()
    return (req, login_response)",req is not None,42,req.get('session_id'),False,5.522397783539471,N/A
"@commons.kwarg_adder
def login_via_soap(self, **kwargs):
    """""" Performs a clientless login request using the Soap API.

          :param: **kwargs: kwargs
          :type: **kwargs: dict
          :return: Login response
          :rtype: (dict, commons.SoapLoginRequest)
        """"""
    login_response = commons.SoapLoginRequest(self.username, self.password, **kwargs)
    req = login_response.request()
<mask>:
        self.session_id = login_response.session_id
        self.instance_url = login_response.instance_url
        self.set_api_version()
    return (req, login_response)",login_response.status == 200,46,login_response.is_success(),False,29.84745896009822,N/A
"@commons.kwarg_adder
def login_via_device_flow(self, **kwargs):
    """"""
        Performs OAuth device flow end-to-end and returns final authentication response if successful. Raises `SFDCRequestException` otherwise.

        :param: **kwargs: kwargs
        :type: **kwargs: dict
        :return: Authentication response
        :rtype: (dict, device_flow.AuthNRequest)
        """"""
    on_authorize = kwargs.get('on_authorize', lambda *args, **kwargs: None)
    on_authenticate = kwargs.get('on_authenticate', lambda *args, **kwargs: None)
    device_code_authorization = device_flow.AuthZRequest(self.client_id, **kwargs)
    authz_response = device_code_authorization.request()
<mask>:
        raise device_code_authorization.exceptions[0]
    else:
        on_authorize(authz_response, device_code_authorization)
        device_code = device_code_authorization.device_code
        interval = authz_response.get('interval', 5)
        _client = self

        def next(is_initial):
            if is_initial is False:
                time.sleep(interval)
            device_code_authentication = device_flow.AuthNRequest(self.client_id, device_code, **kwargs)
            authn_response = device_code_authentication.request()
            if device_code_authentication.status == requests.codes.ok:
                _client.session_id = authn_response.get('access_token')
                _client.set_instance_url(authn_response.get('instance_url', str()))
                on_authenticate(authn_response, device_code_authentication)
                return (authn_response, device_code_authentication)
            else:
                on_authenticate(authn_response, device_code_authentication)
                return next(False)
        next(True)
        self.set_api_version()
        return self",len(device_code_authorization.exceptions) > 0,108,device_code_authorization.status == requests.codes.ok,False,34.98761149110957,N/A
"@commons.kwarg_adder
def set_api_version(self, **kwargs):
    """"""
        Sets the api version to be used by the client. If not provided, it will get the latest version
        available

        :return: set version kwarg on client if not defined
        """"""
<mask>:
        service = 'https://' + self.instance_url + VERSIONS_SERVICE
        headers = {'Content-Type': 'application/json'}
        r = requests.get(service, headers=headers, proxies=self.proxies)
        if r.status_code == 200:
            self.client_kwargs.update({'version': max((i['version'] for i in r.json()))})
        else:
            self.client_kwargs.update({'version': DEFAULT_API_VERSION})",'version' not in self.client_kwargs,65,self.instance_url,False,12.975849993980741,N/A
"def request(self):
    """""" Gets the result of `super` for this method, then assigns the `access_token` to `session_id`.
        Returns request response.

          :return: Response dict
          :rtype: dict
        """"""
    response = super(AuthNRequest, self).request()
<mask>:
        if 'access_token' in response:
            self.session_id = response['access_token']
        return response",response is not None,40,response,False,4.9787068367863965,N/A
"def get_payload(self):
    """""" Returns the payload dict to be used in the request.

        :return: OAuth2 request body required to obtain code.
        :rtype: dict
        """"""
    payload = {'response_type': 'device_code', 'client_id': self.client_id}
<mask>:
        payload['scope'] = self.scope
    return payload",self.scope is not None,36,self.scope,False,36.78794411714425,N/A
"def request(self):
    """""" Gets the result of `super` for this method, then assigns the `device_code`.
        Returns request response.

        :return: Response dict
        :rtype: dict
        """"""
    response = super(AuthZRequest, self).request()
<mask>:
        if 'device_code' in response:
            self.device_code = response['device_code']
        return response",response is not None,38,response,False,4.9787068367863965,N/A
"def __call__(self, fn):

    @functools.wraps(fn)
    def wrapped(this, *args):
        cache = caches[api_settings.OIDC_CACHE_NAME]
        key = api_settings.OIDC_CACHE_PREFIX + '.'.join([fn.__name__] + list(map(str, args)))
        cached_value = cache.get(key, version=self.cache_version)
<mask>:
            cached_value = fn(this, *args)
            cache.set(key, cached_value, timeout=self.ttl, version=self.cache_version)
        return cached_value
    return wrapped",not cached_value,35,cached_value is None,False,39.76353643835252,N/A
"def get(self, url, *args, **kwargs):
    wanted_response = self.responses.get(url)
<mask>:
        status_code, content = (404, '')
    else:
        status_code, content = wanted_response
    response = Response()
    response._content = content.encode('utf-8')
    response.status_code = status_code
    return response",not wanted_response,30,wanted_response is None,False,39.76353643835252,N/A
"def validate_exp(self, now, leeway):
    super(DRFIDToken, self).validate_exp(now, leeway)
<mask>:
        msg = _('Invalid Authorization header. JWT has expired.')
        raise AuthenticationFailed(msg)",now > self['exp'],18,now - self.token_expiry < self.EXPIRE_TIME,False,3.673526562988939,N/A
"def validate_iat(self, now, leeway):
    super(DRFIDToken, self).validate_iat(now, leeway)
<mask>:
        msg = _('Invalid Authorization header. JWT too old.')
        raise AuthenticationFailed(msg)",self['iat'] < leeway,18,now < self.jwt_now,False,7.809849842300637,N/A
"def authenticate(self, request):
    bearer_token = self.get_bearer_token(request)
<mask>:
        return None
    try:
        userinfo = self.get_userinfo(bearer_token)
    except HTTPError:
        msg = _('Invalid Authorization header. Unable to verify bearer token')
        raise AuthenticationFailed(msg)
    user = api_settings.OIDC_RESOLVE_USER_FUNCTION(request, userinfo)
    return (user, userinfo)",bearer_token is None,34,not bearer_token,False,46.30777161991026,N/A
"def get_bearer_token(self, request):
    auth = get_authorization_header(request).split()
    auth_header_prefix = api_settings.BEARER_AUTH_HEADER_PREFIX.lower()
<mask>:
        return None
    if len(auth) == 1:
        msg = _('Invalid Authorization header. No credentials provided')
        raise AuthenticationFailed(msg)
    elif len(auth) > 2:
        msg = _('Invalid Authorization header. Credentials string should not contain spaces.')
        raise AuthenticationFailed(msg)
    return auth[1]",not auth or smart_str(auth[0].lower()) != auth_header_prefix,45,auth_header_prefix not in auth,False,8.623783528009913,N/A
"@cache(ttl=api_settings.OIDC_BEARER_TOKEN_EXPIRATION_TIME)
def get_userinfo(self, token):
    userinfo_endpoint = self.oidc_config.get('userinfo_endpoint', api_settings.USERINFO_ENDPOINT)
<mask>:
        raise AuthenticationFailed(_('Invalid userinfo_endpoint URL. Did not find a URL from OpenID connect discovery metadata nor settings.OIDC_AUTH.USERINFO_ENDPOINT.'))
    response = requests.get(userinfo_endpoint, headers={'Authorization': 'Bearer {0}'.format(token.decode('ascii'))})
    response.raise_for_status()
    return response.json()",not userinfo_endpoint,34,userinfo_endpoint is None,False,39.76353643835252,N/A
"def __init__(self, *args, css_class=None, **kwargs):
<mask>:
        self.field_classes = 'bg-green-500 hover:bg-green-700 text-white font-bold py-2 px-4 rounded'
    else:
        self.field_classes = css_class
    super().__init__(*args, **kwargs)",css_class is None,21,css_class is None,True,100.00000000000004,N/A
"def __init__(self, *args, css_class=None, **kwargs):
<mask>:
        self.field_classes = 'bg-red-500 hover:bg-red-700 text-white font-bold py-2 px-4 rounded'
    else:
        self.field_classes = css_class
    super().__init__(*args, **kwargs)",css_class is None,21,css_class is None,True,100.00000000000004,N/A
"def __init__(self, *args, css_class=None, **kwargs):
<mask>:
        self.field_classes = 'bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded'
    else:
        self.field_classes = css_class
    super().__init__(*args, **kwargs)",css_class is None,21,css_class is None,True,100.00000000000004,N/A
"def __init__(self, css_styles):
    default_items = ['text', 'number', 'email', 'url', 'password', 'hidden', 'multiplehidden', 'file', 'clearablefile', 'textarea', 'date', 'datetime', 'time', 'checkbox', 'select', 'nullbooleanselect', 'selectmultiple', 'radioselect', 'checkboxselectmultiple', 'multi', 'splitdatetime', 'splithiddendatetime', 'selectdate', 'error_border']
    base = css_styles.get('base', '')
    for item in default_items:
        setattr(self, item, base)
    for key, value in css_styles.items():
<mask>:
            current_class = set(getattr(self, key).split())
            current_class.update(set(value.split()))
            new_classes = ' '.join(current_class)
            setattr(self, key, new_classes)",key != 'base',58,"hasattr(self, key)",False,8.116697886877475,N/A
"@register.filter(name='crispy')
def as_crispy_form(form, template_pack=TEMPLATE_PACK, label_class='block text-gray-700 text-sm font-bold mb-2', field_class='mb-3'):
    """"""
    The original and still very useful way to generate a div elegant form/formset::

        {% load tailwind_filters %}

        <form class=""uniForm"" method=""post"">
            {% csrf_token %}
            {{ myform|crispy }}
        </form>

    or, if you want to explicitly set the template pack::

        {{ myform|crispy:""bootstrap"" }}

    In ``bootstrap3`` or ``bootstrap4`` for horizontal forms you can do::

        {{ myform|label_class:""col-lg-2"",field_class:""col-lg-8"" }}
    """"""
    c = Context({'field_class': field_class, 'field_template': '%s/field.html' % template_pack, 'form_show_errors': True, 'form_show_labels': True, 'label_class': label_class}).flatten()
<mask>:
        template = uni_formset_template(template_pack)
        c['formset'] = form
    else:
        template = uni_form_template(template_pack)
        c['form'] = form
    return template.render(c)","isinstance(form, BaseFormSet)",95,"isinstance(form, FormSet)",False,53.7284965911771,N/A
"@register.filter(name='as_crispy_errors')
def as_crispy_errors(form, template_pack=TEMPLATE_PACK):
    """"""
    Renders only form errors the same way as django-crispy-forms::

        {% load crispy_forms_tags %}
        {{ form|as_crispy_errors }}

    or::

        {{ form|as_crispy_errors:""bootstrap"" }}
    """"""
<mask>:
        template = get_template('%s/errors_formset.html' % template_pack)
        c = Context({'formset': form}).flatten()
    else:
        template = get_template('%s/errors.html' % template_pack)
        c = Context({'form': form}).flatten()
    return template.render(c)","isinstance(form, BaseFormSet)",48,"isinstance(form, FormSet)",False,53.7284965911771,N/A
"@register.filter(name='as_crispy_field')
def as_crispy_field(field, template_pack=TEMPLATE_PACK, label_class='', field_class=''):
    """"""
    Renders a form field like a django-crispy-forms field::

        {% load crispy_forms_tags %}
        {{ form.field|as_crispy_field }}

    or::

        {{ form.field|as_crispy_field:""tailwind"" }}
    """"""
<mask>:
        raise CrispyError('|as_crispy_field got passed an invalid or inexistent field')
    attributes = {'field': field, 'form_show_errors': True, 'form_show_labels': True, 'label_class': label_class, 'field_class': field_class}
    helper = getattr(field.form, 'helper', None)
    template_path = None
    if helper is not None:
        attributes.update(helper.get_attributes(template_pack))
        template_path = helper.field_template
    if not template_path:
        template_path = '%s/field.html' % template_pack
    template = get_template(template_path)
    c = Context(attributes).flatten()
    return template.render(c)","not isinstance(field, boundfield.BoundField) and settings.DEBUG",82,"field.name not in ('form', 'as_crispy_field')",False,4.444587794585869,N/A
"@register.filter
def build_attrs(field):
    """"""
    Build HTML attributes for a form field, also checking for a
    ``widget.allow_multiple_selected`` attribute  and adding ``multiple`` to the
    attributes if it is set to ``True``.
    """"""
    attrs = field.field.widget.attrs
    attrs.setdefault('id', field.auto_id)
    field_built_widget_attrs = field.build_widget_attrs(attrs)
    attrs.update(field_built_widget_attrs)
    built_widget_attrs = field.field.widget.build_attrs(attrs)
    attrs.update(built_widget_attrs)
<mask>:
        attrs['multiple'] = attrs.get('multiple', field.field.widget.allow_multiple_selected)
    return mark_safe(flatatt(attrs))","hasattr(field.field.widget, 'allow_multiple_selected')",50,field.field.widget.allow_multiple_selected,False,35.72379274367096,N/A
"def render(self, context):
<mask>:
        context.render_context[self] = (template.Variable(self.field), self.attrs, template.Variable(self.html5_required))
    field, attrs, html5_required = context.render_context[self]
    field = field.resolve(context)
    try:
        html5_required = html5_required.resolve(context)
    except template.VariableDoesNotExist:
        html5_required = False
    template_pack = context.get('template_pack', TEMPLATE_PACK)
    widgets = getattr(field.field.widget, 'widgets', [getattr(field.field.widget, 'widget', field.field.widget)])
    if isinstance(attrs, dict):
        attrs = [attrs] * len(widgets)
    converters = {}
    converters.update(getattr(settings, 'CRISPY_CLASS_CONVERTERS', {}))
    for widget, attr in zip(widgets, attrs):
        class_name = widget.__class__.__name__.lower()
        class_name = converters.get(class_name, class_name)
        css_class = widget.attrs.get('class', '')
        if css_class:
            if css_class.find(class_name) == -1:
                css_class += ' %s' % class_name
        else:
            css_class = class_name
        if template_pack == 'tailwind' and '""class""' not in attr.keys():
            css_container = context.get('css_container', self.default_container)
            if css_container:
                css = ' ' + css_container.get_input_class(field)
                css_class += css
            if field.errors:
                error_border_class = css_container.error_border
                css_class = re.sub('border-\\S+', error_border_class, css_class)
        widget.attrs['class'] = css_class
        if html5_required and field.field.required and ('required' not in widget.attrs):
            if field.field.widget.__class__.__name__ != 'RadioSelect':
                widget.attrs['required'] = 'required'
        for attribute_name, attribute in attr.items():
            attribute_name = template.Variable(attribute_name).resolve(context)
            if attribute_name in widget.attrs:
                widget.attrs[attribute_name] += ' ' + template.Variable(attribute).resolve(context)
            else:
                widget.attrs[attribute_name] = template.Variable(attribute).resolve(context)
    return str(field)",self not in context.render_context,161,self not in context.render_context,True,100.00000000000004,N/A
"@register.simple_tag()
def crispy_addon(field, append='', prepend='', form_show_labels=True):
    """"""
    Renders a form field using bootstrap's prepended or appended text::

        {% crispy_addon form.my_field prepend=""$"" append="".00"" %}

    You can also just prepend or append like so

        {% crispy_addon form.my_field prepend=""$"" %}
        {% crispy_addon form.my_field append="".00"" %}
    """"""
<mask>:
        context = Context({'field': field, 'form_show_errors': True, 'form_show_labels': form_show_labels})
        template = loader.get_template('%s/layout/prepended_appended_text.html' % get_template_pack())
        context['crispy_prepended_text'] = prepend
        context['crispy_appended_text'] = append
        if not prepend and (not append):
            raise TypeError('Expected a prepend and/or append argument')
        context = context.flatten()
    return template.render(context)",field,81,prepend and append,False,0.0,N/A
"def test_failing_CharField(self):
    form = CharFieldForm(data={'name': ''})
    form.helper = FormHelper()
<mask>:
        expected = 'helper/charfield_failing_lt50.html'
    else:
        expected = 'helper/charfield_failing.html'
    assert parse_form(form) == parse_expected(expected)","django.VERSION < (5, 0)",21,sys.version_info[0] < 3,False,5.522397783539471,N/A
"def test_col_row(self):
    form = SampleForm()
    form.helper = FormHelper()
    form.helper.form_tag = False
    form.helper.layout = Layout(Row(Column(Field('first_name'), Field('last_name'), css_class='px-2'), Column('email', css_class='px-2')))
<mask>:
        expected = 'helper/col_row_lt50.html'
    else:
        expected = 'helper/col_row.html'
    assert parse_form(form) == parse_expected(expected)","django.VERSION < (5, 0)",30,sys.version_info[0] < 3,False,5.522397783539471,N/A
"def test_formset(self):
    SampleFormSet = formset_factory(SampleForm, extra=2)
    formset = SampleFormSet()
    formset.helper = FormHelper()
    formset.helper.form_tag = False
    formset.helper.layout = Layout('email')
<mask>:
        expected = 'helper/formset_lt50.html'
    else:
        expected = 'helper/formset.html'
    assert parse_form(formset) == parse_expected(expected)","django.VERSION < (5, 0)",30,sys.version_info[0] == 2,False,4.456882760699063,N/A
"def test_formset_with_errors(self):
    SampleFormSet = formset_factory(ShortCharFieldForm, extra=1, max_num=2, validate_max=True)
    data = {'name-0-name': 'test', 'name-INITIAL_FORMS': '0', 'name-MIN_NUM_FORMS': '0', 'name-MAX_NUM_FORMS': '0', 'name-TOTAL_FORMS': '3'}
    formset = SampleFormSet(data=data, prefix='name')
    formset.helper = FormHelper()
    formset.helper.formset_error_title = 'Non Form Errors'
    formset.helper.form_tag = False
    formset.helper.layout = Layout('email')
<mask>:
        expected = 'helper/formset_errors_lt50.html'
    else:
        expected = 'helper/formset_errors.html'
    assert parse_form(formset) == parse_expected(expected)","django.VERSION < (5, 0)",50,parse_lt50(formset.data) == LT50,False,4.9323515694897075,N/A
"def test_formset_with_form_tag(self):
    SampleFormSet = formset_factory(SampleForm, extra=2)
    formset = SampleFormSet()
    formset.helper = FormHelper()
    formset.helper.form_tag = True
    formset.helper.layout = Layout('email')
<mask>:
        expected = 'helper/formset_form_tag_lt50.html'
    else:
        expected = 'helper/formset_form_tag.html'
    assert parse_form(formset) == parse_expected(expected)","django.VERSION < (5, 0)",30,sys.version_info[0] == 2,False,4.456882760699063,N/A
"def test_crispy_filter(self):
    template = Template('\n            {% load tailwind_filters %}\n            {{ form|crispy }}\n            ')
    form = SampleForm()
    c = Context({'form': form})
    html = template.render(c)
<mask>:
        expected = 'filter/crispy_filter_lt50.html'
    else:
        expected = 'filter/crispy_filter.html'
    assert parse_html(html) == parse_expected(expected)","django.VERSION < (5, 0)",35,c.lt50,False,3.7238938287986976,N/A
"def test_prepended_with_help(self):
    form = SampleForm()
    form.helper = FormHelper()
    form.helper.form_tag = False
    form.helper.form_show_labels = False
    form.helper.layout = Layout(PrependedText('email', '@'))
<mask>:
        expected = 'prepended/prepended_help_lt50.html'
    else:
        expected = 'prepended/prepended_help.html'
    assert parse_form(form) == parse_expected(expected)","django.VERSION < (5, 0)",30,sys.version_info[0] < 3,False,5.522397783539471,N/A
"def test_prepended_with_errors(self):
    form = CharFieldForm(data={'name': ''})
    form.helper = FormHelper()
    form.helper.form_tag = False
    form.helper.form_show_labels = False
    form.helper.layout = Layout(PrependedText('name', '@'))
<mask>:
        expected = 'prepended/prepended_errors_lt50.html'
    else:
        expected = 'prepended/prepended_errors.html'
    assert parse_form(form) == parse_expected(expected)","django.VERSION < (5, 0)",31,sys.version_info[0] < 3,False,5.522397783539471,N/A
"def test_appended_with_errors(self):
    form = CharFieldForm(data={'name': ''})
    form.helper = FormHelper()
    form.helper.form_tag = False
    form.helper.form_show_labels = False
    form.helper.layout = Layout(AppendedText('name', '@'))
<mask>:
        expected = 'prepended/appended_errors_lt50.html'
    else:
        expected = 'prepended/appended_errors.html'
    assert parse_form(form) == parse_expected(expected)","django.VERSION < (5, 0)",31,sys.version_info[0] < 3,False,5.522397783539471,N/A
"def test_prepended_and_appended_with_errors(self):
    form = CharFieldForm(data={'name': ''})
    form.helper = FormHelper()
    form.helper.form_tag = False
    form.helper.form_show_labels = False
    form.helper.layout = Layout(PrependedAppendedText('name', '@', '.com'))
<mask>:
        expected = 'prepended/prepended_appended_errors_lt50.html'
    else:
        expected = 'prepended/prepended_appended_errors.html'
    assert parse_form(form) == parse_expected(expected)","django.VERSION < (5, 0)",32,sys.version_info[0] < 3,False,5.522397783539471,N/A
"def clean(self):
    super().clean()
    password1 = self.cleaned_data.get('password1', None)
    password2 = self.cleaned_data.get('password2', None)
<mask>:
        raise forms.ValidationError('Passwords dont match')
    return self.cleaned_data",not password1 and (not password2) or password1 != password2,18,password1 and password2 and (password1 != password2),False,31.32487945288165,N/A
"def test_table_inline_formset(self):
    SampleFormSet = formset_factory(SampleForm, extra=2)
    formset = SampleFormSet()
    formset.helper = FormHelper()
    formset.helper.form_tag = False
    formset.helper.add_input(Submit('submit', 'submit'))
    formset.helper.template = 'tailwind/table_inline_formset.html'
<mask>:
        expected = 'table_inline_formset/table_inline_formset_lt50.html'
    else:
        expected = 'table_inline_formset/table_inline_formset.html'
    assert parse_form(formset) == parse_expected(expected)","django.VERSION < (5, 0)",32,sys.version_info[0] == 2,False,4.456882760699063,N/A
"def test_failing_table_inline_formset(self):
    SampleFormSet = formset_factory(ShortCharFieldForm, extra=1, max_num=2, validate_max=True)
    data = {'name-0-name': 'test', 'name-INITIAL_FORMS': '0', 'name-MIN_NUM_FORMS': '0', 'name-MAX_NUM_FORMS': '0', 'name-TOTAL_FORMS': '3'}
    formset = SampleFormSet(data=data, prefix='name')
    formset.helper = FormHelper()
    formset.helper.add_input(Submit('submit', 'submit'))
    formset.helper.template = 'tailwind/table_inline_formset.html'
<mask>:
        expected = 'table_inline_formset/table_inline_formset_failing_lt50.html'
    else:
        expected = 'table_inline_formset/table_inline_formset_failing.html'
    assert parse_form(formset) == parse_expected(expected)","django.VERSION < (5, 0)",44,parse_lt50(formset.helper.template) == 'tailwind/table_inline_formset_failing_lt50',False,2.1671320168371846,N/A
"def contains_partial(haystack, needle):
    """"""Search for a html element with at least the corresponding elements
    (other elements may be present in the matched element from the haystack)
    """"""
<mask>:
        haystack = parse_html(haystack)
    if not isinstance(needle, Element):
        needle = parse_html(needle)
    if needle.name == haystack.name and set(needle.attributes).issubset(haystack.attributes):
        return True
    return any((contains_partial(child, needle) for child in haystack.children if isinstance(child, Element)))","not isinstance(haystack, Element)",56,"not isinstance(haystack, Element)",True,100.00000000000004,N/A
"def get_delimiter(df: pd.DataFrame) -> str:
    for delim in _DELIMITER_OPTIONS:
<mask>:
            return delim
    raise BCPandasValueError(error_msg.format(typ='delimiter', opts=_DELIMITER_OPTIONS))","not df.map(lambda x: delim in x if isinstance(x, str) else False).any().any()",15,df.columns[delim].empty,False,0.9514635725147856,N/A
"def get_quotechar(df: pd.DataFrame) -> str:
    for qc in _QUOTECHAR_OPTIONS:
<mask>:
            return qc
    raise BCPandasValueError(error_msg.format(typ='quote', opts=_QUOTECHAR_OPTIONS))","not df.map(lambda x: qc in x if isinstance(x, str) else False).any().any()",15,qc in df.columns,False,0.2486515526037858,N/A
"def bcp(sql_item: str, direction: str, flat_file: Path, creds, print_output: bool, sql_type: str='table', schema: str='dbo', format_file_path: Optional[Path]=None, batch_size: Optional[int]=None, use_tablock: bool=False, col_delimiter: Optional[str]=None, row_terminator: Optional[str]=None, bcp_path: Optional[Union[str, Path]]=None, identity_insert: bool=False):
    """"""
    See https://docs.microsoft.com/en-us/sql/tools/bcp-utility
    """"""
    combos = {TABLE: [IN, OUT], QUERY: [QUERYOUT], VIEW: [IN, OUT]}
    direc = direction.lower()
<mask>:
        raise BCPandasValueError(f""Param 'direction' must be one of {DIRECTIONS}, you passed {direc}"")
    if direc not in combos[sql_type]:
        raise BCPandasValueError(f'Wrong combo of direction and SQL object, you passed {sql_type} and {direc} .')
    if creds.with_krb_auth:
        auth = ['-T']
    elif creds.entra_id_token:
        auth = ['-G', '-P', quote_this(creds.entra_id_token)]
    else:
        auth = ['-U', quote_this(creds.username), '-P', quote_this(creds.password)]
    if creds.odbc_kwargs:
        kwargs = {k.lower(): v for k, v in creds.odbc_kwargs.items()}
        false_values = ('n', 'no', 'f', 'false', 'off', '0')
        if sys.platform != 'win32' and 'encrypt' in kwargs:
            auth += [f""-Y{('m' if kwargs['encrypt'] not in false_values else 'o')}""]
    if sql_type == QUERY:
        sql_item_string = quote_this(''.join(sql_item.splitlines()))
    else:
        sql_item_string = f'{schema}.{sql_item}'
    if creds.port is not None and creds.port != 1433:
        server = f'{creds.server},{creds.port}'
    else:
        server = creds.server
    bcp_command = ['bcp' if bcp_path is None else quote_this(str(bcp_path)), sql_item_string, direc, str(flat_file), '-S', server, '-d', creds.database, '-q'] + auth
    if batch_size:
        bcp_command += ['-b', str(batch_size)]
    if use_tablock:
        bcp_command += ['-h', 'TABLOCK']
    if identity_insert:
        bcp_command += ['-E']
    if direc == IN and format_file_path is not None:
        bcp_command += ['-f', str(format_file_path)]
    elif direc in (OUT, QUERYOUT):
        bcp_command += ['-c', quote_this(f""-t{(read_data_settings['delimiter'] if col_delimiter is None else col_delimiter)}""), quote_this(f""-r{(read_data_settings['newline'] if row_terminator is None else row_terminator)}"")]
    bcp_command_log = ', '.join(bcp_command)
    bcp_command_log_msg = sub('-P,\\s.*,', '-P, [REDACTED],', bcp_command_log)
    logger.info(f'Executing BCP command now... \nBCP command is: {bcp_command_log_msg}')
    ret_code, output = run_cmd(bcp_command, print_output=print_output)
    if ret_code != 0:
        raise BCPandasException(f'Bcp command failed with exit code {ret_code}', details=[line for line in output if line.startswith('Error =')])",direc not in DIRECTIONS,274,direc not inDIRECTIONS,False,39.43223765116288,N/A
"def get_temp_file(directory: Optional[Path]=None) -> Path:
    """"""
    Returns full path to a temporary file without creating it.
    """"""
<mask>:
        tmp_dir = Path(tempfile.gettempdir())
    else:
        tmp_dir = directory
    return tmp_dir / ''.join(random.choices(string.ascii_letters + string.digits, k=21))",directory is None,32,directory is None,True,100.00000000000004,N/A
"def quote_this(this: str, skip: bool=False) -> str:
    """"""
    OS-safe way to quote a string.

    Returns the string with quotes around it.
    On Windows ~~it's double quotes~~ we skip quoting,
    on Linux it's single quotes.
    """"""
<mask>:
        if IS_WIN32:
            return this
        else:
            return shlex.quote(this)
    else:
        return this","isinstance(this, str)",46,skip,False,0.0,N/A
"def run_cmd(cmd: List[str], *, print_output: bool) -> Tuple[int, List[str]]:
    """"""
    Runs the given command.

    Prints STDOUT in real time,  prints STDERR when command is complete,
    and logs both STDOUT and STDERR.

    Paramters
    ---------
    cmd : list of str
        The command to run, to be submitted to `subprocess.Popen()`
    print_output: bool
        Whether to print output to STDOUT in real time.
        Regardless, the output will be logged.

    Returns
    -------
    The exit code of the command and all of its output.
    """"""
<mask>:
        with_shell = False
    else:
        with_shell = True
        cmd = ' '.join(cmd).replace('\\', '\\\\')
    proc = Popen(cmd, stdout=PIPE, stderr=STDOUT, encoding='utf-8', errors='utf-8', shell=with_shell)
    stdout = []
    while True:
        outs = proc.stdout.readline()
        if outs:
            if print_output:
                print(outs, end='')
            logger.info(outs)
            stdout.append(outs)
        if proc.poll() is not None and outs == '':
            break
    return (proc.returncode, stdout)",IS_WIN32,128,sys.platform == 'win32',False,0.0,N/A
"def __init__(self, server: str, database: str, username: Optional[str]=None, password: Optional[str]=None, driver_version: Optional[int]=None, port: int=1433, odbc_kwargs: Optional[Dict[str, Union[str, int]]]=None, entra_id_token: Optional[str]=None):
    self.server = server
    self.database = database
    self.port = port
    self.odbc_kwargs = odbc_kwargs
<mask>:
        all_drivers: List[str] = pyodbc.drivers()
        driver_candidates: List[str] = [d.split('Driver ')[-1].split(' ')[0] for d in all_drivers if 'SQL Server' in d]
        new_driver_version: int = max((int(v) for v in driver_candidates if v.isnumeric()))
        self.driver = f'{{ODBC Driver {new_driver_version} for SQL Server}}'
        self.driver_version = new_driver_version
    else:
        self.driver = f'{{ODBC Driver {driver_version} for SQL Server}}'
        self.driver_version = driver_version
    if port:
        port_str = f',{self.port}'
    else:
        port_str = ''
    db_url = f'Driver={self.driver};Server=tcp:{self.server}{port_str};Database={self.database};'
    if username and password:
        self.username = username
        self.password = password
        self.with_krb_auth = False
        db_url += f'UID={username};PWD={password};'
    else:
        self.username = ''
        self.password = ''
        self.with_krb_auth = True
        db_url += 'Trusted_Connection=yes;'
    self.entra_id_token = entra_id_token
    self_msg = sub(""password=\\'.*\\'"", 'password=[REDACTED]', str(self))
    logger.info(f'Created creds:\t{self_msg}')
    if odbc_kwargs:
        db_url += ';'.join((f'{k}={v}' for k, v in odbc_kwargs.items()))
    conn_string = f'mssql+pyodbc:///?odbc_connect={quote_plus(db_url)}'
    self.engine = sa.engine.create_engine(conn_string)
    engine_msg = sub('PWD%3D.*%3B', 'PWD%3D[REDACTED]%3B', str(self.engine))
    logger.info(f'Created engine for sqlalchemy:\t{engine_msg}')",driver_version is None,161,driver_version is None,True,100.00000000000004,N/A
"@classmethod
def from_engine(cls, engine: sa.engine.base.Engine) -> 'SqlCreds':
    """"""
        Alternate constructor, from a `sqlalchemy.engine.base.Engine` that uses `pyodbc` as the DBAPI
        (which is the SQLAlchemy default for MS SQL) and using an exact PyODBC connection string (not DSN or hostname).
        See https://docs.sqlalchemy.org/en/13/dialects/mssql.html#connecting-to-pyodbc for more.

        Parameters
        ----------
        engine : `sqlalchemy.engine.base.Engine`
            The SQLAlchemy engine object, configured as described above

        Returns
        -------
        `bcpandas.SqlCreds`
        """"""
    try:
<mask>:
            conn_url = engine.url.query['odbc_connect'].split(';')
            conn_dict = {x.split('=')[0].lower(): x.split('=')[1] for x in conn_url if '=' in x}
            if ',' in conn_dict['server']:
                conn_dict['port'] = int(conn_dict['server'].split(',')[1])
            sql_creds = cls(server=conn_dict['server'].replace('tcp:', '').split(',')[0], database=conn_dict['database'], username=conn_dict.get('uid', None), password=conn_dict.get('pwd', None), port=conn_dict.get('port', None))
        elif 'driver' in engine.url.query:
            sql_creds = cls(server=engine.url.host, database=engine.url.database, username=engine.url.username, password=engine.url.password, port=engine.url.port)
        sql_creds.engine = engine
        return sql_creds
    except (KeyError, AttributeError) as ex:
        raise BCPandasValueError(f""The supplied 'engine' object could not be parsed correctly, try creating a SqlCreds object manually.\nOriginal Error: \n {ex}"")",'odbc_connect' in engine.url.query,134,'odbc_connect' in engine.url.query,True,100.00000000000004,N/A
"def __repr__(self):
    clsName = self.__class__.__qualname__
    kwargs = ', '.join((f'{k}={v!r}' for k, v in self.__dict__.items() if k != 'password'))
<mask>:
        kwargs += ', password=[REDACTED]'
    return f'{clsName}({kwargs})'","hasattr(self, 'password')",25,self.is_redacted,False,8.745825313180626,N/A
"def _handle_cols_for_append(df: pd.DataFrame, table_name: str, creds: SqlCreds, sql_item_exists: bool, schema: str, if_exists: str):
    cols_dict = None
<mask>:
        cols_dict = dict(pd.read_sql_query(dedent(""\n                SELECT COLUMN_NAME, ORDINAL_POSITION\n                FROM INFORMATION_SCHEMA.COLUMNS\n                WHERE TABLE_SCHEMA = '{_schema}'\n                AND TABLE_NAME = '{_tbl}'\n            "".format(_schema=schema, _tbl=table_name)), creds.engine).values)
        if sql_item_exists:
            extra_cols = [str(x) for x in df.columns if str(x) not in cols_dict.keys()]
            if extra_cols:
                raise BCPandasValueError(f""Column(s) detected in the dataframe that are not in the database, cannot have new columns if `if_exists=='append'`, the extra column(s): {extra_cols}"")
    return cols_dict",if_exists == 'append',76,schema != 'append',False,19.37692912686648,N/A
"def _prepare_table(df: pd.DataFrame, table_name: str, creds: SqlCreds, sql_item_exists: bool, sql_type: str, schema: str, if_exists: str, dtype: Optional[dict]) -> None:
    """"""
    Prepares the destination SQL table, handling the `if_exists` param.
    """"""
<mask>:
        if sql_item_exists:
            raise BCPandasValueError(f'The {sql_type} called {schema}.{table_name} already exists, `if_exists` param was set to `fail`.')
        else:
            _create_table(schema=schema, table_name=table_name, creds=creds, df=df, if_exists=if_exists, dtype=dtype)
    elif if_exists == 'replace':
        _create_table(schema=schema, table_name=table_name, creds=creds, df=df, if_exists=if_exists, dtype=dtype)
    elif if_exists == 'append':
        if not sql_item_exists:
            _create_table(schema=schema, table_name=table_name, creds=creds, df=df, if_exists=if_exists, dtype=dtype)",if_exists == 'fail',76,if_exists == 'find',False,75.98356856515926,N/A
"@pytest.fixture(scope='session')
def docker_db(pytestconfig):
    docker_image = pytestconfig.getoption('--mssql-docker-image', default=None)
<mask>:
        docker_db_obj.mssql_image = docker_image
    docker_db_obj.start()
    time.sleep(_docker_startup)
    print('successfully started DB in docker...')
    yield
    print('Stopping container')
    docker_db_obj.stop()
    print('Deleting container')
    docker_db_obj.remove()
    print('all done!')",docker_image is not None,27,docker_image,False,36.78794411714425,N/A
"def _get_bcp_path() -> Optional[str]:
    """"""
    On Windows, is typically in C:/Program Files/Microsoft SQL Server/Client SDK/ODBC/170/Tools/Binn
    On Linux it depends where you install it, but usually in /opt/mssql-tools/bin
    """"""
    try:
<mask>:
            first_part = Path(expandvars('%ProgramFiles%')) / 'Microsoft SQL Server' / 'Client SDK' / 'ODBC'
            version = max((x.parts[-1] for x in first_part.iterdir()))
            bcp_path = first_part / version / 'Tools' / 'Binn'
            return str(bcp_path / 'bcp.exe')
        else:
            bcp_path = Path('/opt/mssql-tools/bin')
            if bcp_path.resolve().exists():
                return str(bcp_path / 'bcp')
            else:
                return None
    except FileNotFoundError:
        return None",sys.platform == 'win32',79,platform.system() == 'Windows',False,13.134549472120788,N/A
"def prep_df_for_comparison(df: pd.DataFrame, index: bool) -> pd.DataFrame:
    """"""
    Prepares a test dataframe for comparison with its corresponding data that is read from SQL.
    Becuase SQL does some implicit conversions, we need to make the df match that.

    Also uses the index=True/False param to set the df here to the expected value.
    """"""
<mask>:
        df = df.reset_index()
    try:
        df.columns = df.columns.astype(str)
    except TypeError:
        pass
    if 'object' in df.dtypes.values:
        df = df.replace({'': None})
    return df",index,73,index,True,100.00000000000004,N/A
"def start(self):
<mask>:
        raise ValueError(""Must accept Microsft's End User License Agreement"")
    env = {'ACCEPT_EULA': 'Y', 'SA_PASSWORD': self.sa_sql_password}
    if self.mssql_image.startswith('mcr.microsoft.com/mssql/server'):
        env['MSSQL_PID'] = self.mssql_pid
    self.container = self.client.containers.run(image=self.mssql_image, name=self.container_name, detach=True, environment=env, ports={self.port_container: self.port_host})",not self.accept_eula,30,self.mssql_image is None,False,14.535768424205482,N/A
"def _quote_engine_url(conn_str: str) -> str:
    prefix = 'mssql+pyodbc:///?odbc_connect='
    sa_version = _get_sqlalchemy_version()
<mask>:
        return prefix + quote_plus(conn_str)
    else:
        return prefix + conn_str",sa_version >= Version('1.3.18'),21,sa_version == '1.0',False,19.692104496063735,N/A
"def _parse_cmd(cmd: List[str]) -> Union[List[str], str]:
<mask>:
        return cmd
    else:
        return ' '.join(cmd)",_IS_WIN32,13,"isinstance(cmd, list)",False,0.0,N/A
"def gather_env_info():
    config = {'os_hardware': {}, 'python': {}, 'python_libs': {}, 'mssql': {}, 'mssql_tools': {}, 'docker': {}}
    config['os_hardware'] = {i: getattr(platform.uname(), i) for i in ['system', 'release', 'version', 'machine', 'processor']}
    config['os_hardware']['python_compiler'] = platform.python_compiler()
    config['python'] = {k: v for k, v in sys.implementation.__dict__.items()}
    with capture_stdout() as mystdout:
        pd.show_versions(as_json=True)
        pdv = mystdout.getvalue().replace(""'"", '""').replace('None', 'null')
        pdv_j = json.loads(pdv)
    config['python_libs']['pandas_versions'] = pdv_j
    config['mssql'] = {'docker-image': mssql_image, 'MSSQL_PID': 'Express'}
    cmd_bcp = ['bcp', '-v']
    res = run(_parse_cmd(cmd_bcp), capture_output=True, shell=with_shell)
<mask>:
        config['mssql_tools'] = {'bcp-version': res.stdout.decode().strip().split('\r\n')}
    cmd_docker = ['docker', 'version', '--format', ""'{{json .}}'""]
    res = run(_parse_cmd(cmd_docker), capture_output=True, shell=with_shell)
    if res.returncode == 0:
        docker_out = res.stdout.decode().strip()[1:-1]
        config['docker'] = {'docker-version-output': json.loads(docker_out)}
    return config",res.returncode == 0,102,res.returncode == 0,True,100.00000000000004,N/A
"@cli.command()
@click.option('-f', '--func', type=click.Choice(['tosql', 'readsql'], case_sensitive=False), required=True, help='The Bcpandas function to benchmark')
@click.option('--num-cols', type=int, default=6, show_default=True, help='Number of columns in the DataFrames')
@click.option('--min-rows', type=click.IntRange(0), default=50000, show_default=True, help='Min rows in the DataFrames')
@click.option('--max-rows', type=int, default=1000000, show_default=True, help='Max rows in the DataFrames')
@click.option('--num-examples', type=int, default=10, show_default=True, help='How many Dataframes to run')
def main(func, num_cols, min_rows, max_rows, num_examples):
    """"""
    Will generate `num-examples` of DataFrames using numpy.linspace, going from `min-rows` rows to
    `max-rows` rows.
    """"""
    bmark_name = f'Benchmark run: func={func}, num_cols={num_cols}, min_rows={min_rows}, max_rows={max_rows}, num_examples={num_examples}'
    print(f'Starting {bmark_name}')
    timer = Timer(name=bmark_name)
    timer.start()
    docker_db = DockerDB('bcpandas-benchmarks', 'MyBigSQLPasswordAlso!!!')
    try:
        creds = setup(docker_db)
        results = []
        for n in np.linspace(min_rows, max_rows, num=num_examples):
            num_rows = int(n)
            df = pd.DataFrame(data=np.ndarray(shape=(num_rows, num_cols), dtype=int), columns=[f'col-{x}' for x in range(num_cols)])
<mask>:
                _results = run_benchmark_readsql(df=df, creds=creds)
            elif func == 'tosql':
                _results = run_benchmark_tosql(df=df, creds=creds)
            results.append({'num_rows': num_rows, **_results})
    finally:
        teardown(docker_db)
    save_and_plot(func=func, results=results, num_cols=num_cols)",func == 'readsql',137,func == 'readsql',True,100.00000000000004,N/A
"def next_logname(logname):
    files = glob.glob(f'{logname}.*')
<mask>:
        return logname
    highest_index = max([1] + [int(f.split('.')[-1]) for f in files])
    return f'{logname}.{highest_index}'",not os.path.exists(logname),19,not files,False,1.509869171115925,N/A
"def show_logs(follow, number_lines):
<mask>:
        logging.info('Node is not running')
        sys.exit(1)
    with open(NODE_PID_FILE) as pid_file:
        pid_info = pid_file.readline()
    logging.info(pid_info)
    _, _, network = pid_info.strip().split('|')
    if network == 'localnet':
        logging.info('You are running local net. Logs are in: ~/.nearup/logs/localnet/')
        sys.exit(0)
    command = ['tail', '-n', str(number_lines), '-F' if follow else '', os.path.expanduser(f'~/.nearup/logs/{network}.log')]
    try:
        subprocess.run(command, start_new_session=True, check=True)
    except KeyboardInterrupt:
        sys.exit(0)
    except subprocess.CalledProcessError:
        logging.error('Unable to read logs. Please try again.')
        sys.exit(1)",not os.path.exists(NODE_PID_FILE),64,not os.path.exists(NODE_PID_FILE),True,100.00000000000004,N/A
"def binary_download_url(net, uname, branch, commit, binary):
<mask>:
        return f'nearcore/{uname}/{branch}/{commit}/nightly/{binary}'
    elif net == 'shardnet':
        return f'nearcore/{uname}/{branch}/{commit}/shardnet/{binary}'
    else:
        return f'nearcore/{uname}/{branch}/{commit}/{binary}'",net == 'betanet',18,net == 'nightly',False,59.460355750136046,N/A
"def new_release_ready(net, uname):
    """"""Sanity check that a new release is ready for download.""""""
<mask>:
        commit = latest_deployed_release_commit('testnet')
        branch = latest_deployed_release_branch('testnet')
    else:
        commit = latest_deployed_release_commit(net)
        branch = latest_deployed_release_branch(net)
    if not commit:
        return False
    path = binary_download_url(net, uname, branch, commit, 'neard')
    return exists_on_s3(S3_BUCKETS['default'], path)","net in ['localnet', 'guildnet']",42,net is None,False,7.253154775624655,N/A
"def download_binaries(net, uname):
    commit = latest_deployed_release_commit(net)
    branch = latest_deployed_release_branch(net)
<mask>:
        logging.info(f'Downloading latest deployed version for {net}')
        binary = 'neard'
        download_url = binary_download_url(net, uname, branch, commit, binary)
        download_url = f'nearcore/{uname}/{branch}/{commit}/{binary}'
        if net == 'betanet':
            download_url = f'nearcore/{uname}/{branch}/{commit}/nightly/{binary}'
        elif net == 'shardnet':
            download_url = f'nearcore/{uname}/{branch}/{commit}/shardnet/{binary}'
        download_path = os.path.expanduser(f'~/.nearup/near/{net}/{binary}')
        logging.info(f'Downloading {binary} to {download_path} from {download_url}...')
        download_from_s3(S3_BUCKETS['default'], download_url, download_path)
        logging.info(f'Downloaded {binary} to {download_path}...')
        logging.info(f'Making the {binary} executable...')
        status = os.stat(download_path)
        os.chmod(download_path, status.st_mode | stat.S_IEXEC)
        with open(os.path.expanduser(f'~/.nearup/near/{net}/version'), 'w') as version_file:
            version_file.write(commit)",commit,76,commit,True,100.00000000000004,N/A
"def latest_deployed_release_commit_has_changed(net, commit):
    latest_commit = latest_deployed_release_commit(net)
    logging.info(f'Current release commit is: {commit}')
    logging.info(f'Latest release commit is {latest_commit}')
<mask>:
        return False
    return commit != latest_commit",not commit,23,commit == latest_commit,False,8.116697886877475,N/A
"def print_validator_info(home_dir):
    key_data = read_validator_key(home_dir)
<mask>:
        logging.warning('Validator key not generated by neard as expected')
        return
    print()
    print(wraptext(f""\n        The validator’s public key is {key_data['public_key']}\n\n        The node will now run and will sync with the network, but this will not\n        automatically initiate staking.  In order to do that, you need to send\n        a transaction to the network indicating your intent to stake.  Please\n        see\n        https://docs.near.org/docs/develop/node/validator/staking-and-delegation.\n\n        If you have near-cli (https://github.com/near/near-cli) installed, you\n        can do this with the following command:\n    ""))
    print()
    print(f""    near stake {key_data['account_id']} {key_data['public_key']} <amount>"")
    print()",key_data is None,86,key_data is None,True,100.00000000000004,N/A
"def init_near(home_dir, binary_path, chain_id, account_id, interactive=False):
    logging.info('Initializing the node configuration using near binary...')
    msg = wraptext(""\n        Would you like to initialize the NEAR node as a validator? If so, this\n        will generate an extra key pair you’ll be able to use to stake NEAR as\n        a validator, which is distinct from the keys you'd use in normal\n        transactions.\n    "")
<mask>:
        msg = '\n            Enter an account ID. This should be an existing account you’d like\n            to use to stake NEAR as a validator. If you don’t already have an\n            account, please see\n            https://docs.near.org/docs/develop/basics/create-account\n        '
        account_id = prompt_flag(msg, account_id, default=None, interactive=interactive, type=str)
    else:
        account_id = None
    cmd = [f'{binary_path}/neard', f'--home={home_dir}', 'init', f'--chain-id={chain_id}']
    if account_id:
        cmd.append(f'--account-id={account_id}')
    if chain_id in ['betanet', 'testnet', 'shardnet']:
        cmd.append('--download-genesis')
        cmd.append('--download-config')
        genesis_md5sum = latest_genesis_md5sum(chain_id)
    while True:
        if interactive:
            print(f'''Running ""{' '.join(cmd)}""''')
        subprocess.check_call(cmd)
        if chain_id in ['betanet', 'shardnet', 'testnet']:
            new_genesis_md5sum = latest_genesis_md5sum(chain_id)
            if new_genesis_md5sum != genesis_md5sum:
                genesis_md5sum = new_genesis_md5sum
                logger.info(f'genesis md5sum changed while neard init was running. reinitializing...')
                shutil.rmtree(home_dir)
                continue
            else:
                write_genesis_md5sum(home_dir, genesis_md5sum)
        break
    if interactive:
        print_validator_info(home_dir)","prompt_bool_flag(msg, bool(account_id), interactive=interactive)",167,account_id,False,0.48279499938314435,N/A
"def genesis_changed(chain_id, home_dir):
    genesis_md5sum, records_md5sum = latest_genesis_md5sum(chain_id)
    local_genesis_md5sum, local_records_md5sum = read_genesis_md5sum(home_dir)
<mask>:
        logging.info(f'genesis md5sum has changed. ours: {local_genesis_md5sum} remote: {genesis_md5sum}')
        return True
    if records_md5sum is None:
        return False
    ret = records_md5sum != local_records_md5sum
    if ret:
        logging.info(f'records md5sum has changed. ours: {local_records_md5sum} remote: {records_md5sum}')
    return ret",genesis_md5sum != local_genesis_md5sum,45,genesis_md5sum is not None,False,15.512258520268645,N/A
"def genesis_files_equivalent(old_genesis_path, new_genesis_path, new_records_path):
    with open(old_genesis_path, 'r') as old_genesis, open(new_genesis_path, 'r') as new_genesis, open(new_records_path, 'r') as new_records:
        old_genesis = json.load(old_genesis)
        new_genesis = json.load(new_genesis)
        seen_keys = set()
        for key in old_genesis:
<mask>:
                continue
            seen_keys.add(key)
            try:
                new_value = new_genesis[key]
            except KeyError:
                return False
            old_value = old_genesis[key]
            if old_value != new_value:
                return False
        for key in new_genesis:
            if key == 'records':
                continue
            if key not in seen_keys:
                return False
        h = hashlib.sha256()
        for record in old_genesis['records']:
            h.update(json.dumps(record).encode('utf-8'))
        old_records_hash = h.digest()
        del old_genesis
        new_records = json.load(new_records)
        h = hashlib.sha256()
        for record in new_records:
            h.update(json.dumps(record).encode('utf-8'))
        new_records_hash = h.digest()
        return new_records_hash == old_records_hash",key == 'records',97,key == 'records',True,100.00000000000004,N/A
"def check_and_update_genesis(chain_id, home_dir, binary_path):
<mask>:
        logging.info(f'Update genesis config and remove stale data for {chain_id}')
        with tempfile.TemporaryDirectory() as tmp_dir:
            init_near(tmp_dir, binary_path, chain_id, None, interactive=False)
            with open(os.path.join(tmp_dir, 'config.json'), 'r') as config_fd:
                config = json.load(config_fd)
                config_has_records = config['genesis_records_file'] is not None
                records_downloaded = os.path.exists(os.path.join(tmp_dir, 'records.json'))
                if config_has_records != records_downloaded:
                    if config_has_records:
                        logging.warning(f'newly downloaded config shows records needed, but neard init did not download records.json')
                    else:
                        logging.warning(f'newly downloaded config shows records not needed, but neard init downloaded records.json')
            with open(os.path.join(home_dir, 'config.json'), 'r+') as config_fd:
                config = json.load(config_fd)
                config_had_records = config['genesis_records_file'] is not None
                if config_has_records != config_had_records:
                    config['genesis_records_file'] = 'records.json' if config_has_records else None
                    config_fd.seek(0)
                    json.dump(config, config_fd, indent=2)
                    config_fd.truncate()
            if os.path.exists(os.path.join(home_dir, 'data')):
                keep_data = not config_had_records and config_has_records and records_downloaded and genesis_files_equivalent(os.path.join(home_dir, 'genesis.json'), os.path.join(tmp_dir, 'genesis.json'), os.path.join(tmp_dir, 'records.json'))
                if not keep_data:
                    shutil.rmtree(os.path.join(home_dir, 'data'))
            shutil.move(os.path.join(tmp_dir, 'genesis.json'), os.path.join(home_dir, 'genesis.json'))
            if records_downloaded:
                shutil.move(os.path.join(tmp_dir, 'records.json'), os.path.join(home_dir, 'records.json'))
            shutil.move(os.path.join(tmp_dir, '.nearup/genesis_md5sum'), os.path.join(home_dir, '.nearup/genesis_md5sum'))
            try:
                shutil.move(os.path.join(tmp_dir, '.nearup/records_md5sum'), os.path.join(home_dir, '.nearup/records_md5sum'))
            except FileNotFoundError:
                pass
        return True
    return False","genesis_changed(chain_id, home_dir)",154,"os.path.exists(os.path.join(home_dir, 'data'))",False,8.961672320242714,N/A
"def run(binary_path, home, num_nodes, num_shards, override, fix_accounts, archival_nodes, tracked_shards, verbose=True, interactive=False):
    home = pathlib.Path(home)
<mask>:
        if util.prompt_bool_flag('Would you like to remove data from the previous localnet run?', override, interactive=interactive):
            logging.info('Removing old data.')
            shutil.rmtree(home)
    elif interactive:
        print(util.wraptext('\n            Starting localnet NEAR nodes.  This is a test network entirely local\n            to this machine.  Validators and non-validating nodes will be\n            started, and will communicate with each other on localhost,\n            producing blocks on top of a genesis block generated locally.\n        '))
        print()
    if not home.exists():
        num_nodes = util.prompt_flag('How many validator nodes would you like to initialize this localnet with?', num_nodes, default=4, interactive=interactive, type=int)
        num_shards = util.prompt_flag('How many shards would you like to initialize this localnet with?\nSee https://near.org/papers/nightshade/#sharding-basics', num_shards, default=1, interactive=interactive, type=int)
        fixed_shards = False
        if num_shards > 1:
            fixed_shards = util.prompt_bool_flag('Would you like to setup fixed accounts for first (N-1) shards (shard0, shard1, ...)?', fix_accounts, interactive=interactive)
        archival_nodes = util.prompt_bool_flag('Should these nodes be archival nodes (keep full history)?', archival_nodes, interactive=interactive)
        tracked_shards = util.prompt_flag(""What shards should be tracked? Comma separated list of shards to track, the word 'all' to track all shards or the word 'none' to track no shards."", tracked_shards, interactive=interactive, default='all')
        run_binary(binary_path, home, 'localnet', shards=num_shards, validators=num_nodes, fixed_shards=fixed_shards, archival_nodes=archival_nodes, tracked_shards=tracked_shards, print_command=interactive).wait()
    num_nodes = 0
    while True:
        path = home / f'node{num_nodes}' / 'config.json'
        if not path.exists():
            break
        data = json.loads(path.read_text())
        data['rpc']['addr'] = f'0.0.0.0:{3030 + num_nodes}'
        data['network']['addr'] = f'0.0.0.0:{24567 + num_nodes}'
        path.write_text(json.dumps(data, indent=2))
        num_nodes += 1
    data = json.loads((home / 'node0' / 'node_key.json').read_text())
    public_key = data['public_key']
    shutil.rmtree(LOCALNET_LOGS_FOLDER, ignore_errors=True)
    os.mkdir(LOCALNET_LOGS_FOLDER)
    with open(NODE_PID_FILE, 'w') as pid_fd:
        for i in range(0, num_nodes):
            proc = run_binary(binary_path, os.path.join(home, f'node{i}'), 'run', verbose=verbose, boot_nodes=f'{public_key}@127.0.0.1:24567' if i > 0 else None, output=os.path.join(LOCALNET_LOGS_FOLDER, f'node{i}'), print_command=interactive)
            proc_name = proc_name_from_pid(proc.pid)
            pid_fd.write(f'{proc.pid}|{proc_name}|localnet\n')
    logging.info('Localnet was spawned successfully...')
    logging.info(f'Localnet logs written in: {LOCALNET_LOGS_FOLDER}')
    logging.info('Check localnet status at http://127.0.0.1:3030/status')",home.exists(),287,home.exists(),True,100.00000000000004,N/A
"def entry(binary_path, home, num_nodes, num_shards, override, fix_accounts, archival_nodes, tracked_shards, verbose, interactive):
<mask>:
        binary_path = os.path.join(binary_path, 'neard')
    else:
        uname = os.uname()[0]
        binary_path = os.path.join(LOCALNET_FOLDER, 'neard')
        if not os.path.exists(LOCALNET_FOLDER):
            os.makedirs(LOCALNET_FOLDER)
        util.download_binaries('localnet', uname)
    if is_neard_running():
        sys.exit(1)
    run(binary_path, home, num_nodes, num_shards, override, fix_accounts, archival_nodes, tracked_shards, verbose, interactive)",binary_path,43,binary_path,True,100.00000000000004,N/A
"def check_watcher_file():
<mask>:
        return False
    with open(WATCHER_PID_FILE) as pid_file:
        try:
            pid = int(pid_file.readline().strip())
        except Exception:
            logging.error(f'Nearup watcher PID file {WATCHER_PID_FILE} has unexpected content.')
            return True
        logging.warning(f'Old Nearup watcher PID file {WATCHER_PID_FILE} found.')
        try:
            os.kill(pid, 0)
        except OSError:
            return False
        else:
            logging.error('Nearup watcher is running.')
            return True",not os.path.exists(WATCHER_PID_FILE),46,not os.path.isfile(WATCHER_PID_FILE),False,76.11606003349888,N/A
"def is_watcher_running():
<mask>:
        logging.error('Run nearup stop or kill the process manually!')
        logging.warning(f'If this is a mistake, remove {WATCHER_PID_FILE}')
        return True
    return False",check_watcher_file(),22,os.path.exists(WATCHER_PID_FILE),False,4.789232204309912,N/A
"def run_watcher(net, path=os.path.join(site.USER_BASE, 'bin/watcher'), home=''):
    logging.info('Starting the nearup watcher...')
<mask>:
        sys.exit(1)
    if not os.path.exists(path):
        logging.error('Delete current nearup and install the new with `pip3 install --user nearup`')
        logging.error('To run nearup locally use: `pip3 install --user .` from root directory')
        sys.exit(1)
    proc = Popen(['python3', path, 'run', net, home])
    with open(WATCHER_PID_FILE, 'w') as watcher_pid_file:
        watcher_pid_file.write(str(proc.pid))",is_watcher_running(),52,net != '0',False,0.0,N/A
"def stop_watcher(timeout=DEFAULT_WAIT_TIMEOUT):
    try:
<mask>:
            with open(WATCHER_PID_FILE) as pid_file:
                pid = int(pid_file.read())
                process = psutil.Process(pid)
                logging.info(f'Stopping near watcher {process.name()} with pid {pid}...')
                try:
                    process.terminate()
                    process.wait(timeout=timeout)
                except psutil.TimeoutExpired:
                    logging.warning('Watcher taking a long time to terminate...')
                    logging.warning('Killing watcher with pid {pid}')
                    process.kill()
                os.remove(WATCHER_PID_FILE)
        else:
            logging.info('Nearup watcher is not running...')
    except Exception as ex:
        logging.error(f'There was an error while stopping watcher: {ex}')",os.path.exists(WATCHER_PID_FILE),58,os.path.exists(WATCHER_PID_FILE),True,100.00000000000004,N/A
"def cleanup():
    logging.info('running cleanup')
<mask>:
        shutil.rmtree(BETANET_NEAR_DIR)
    if os.path.exists(LOCALNET_NEAR_DIR):
        shutil.rmtree(LOCALNET_NEAR_DIR)
    if os.path.exists(NEARUP_DIR):
        shutil.rmtree(NEARUP_DIR)
    if os.path.exists(LOGS_FOLDER):
        shutil.rmtree(LOGS_FOLDER)",os.path.exists(BETANET_NEAR_DIR),15,os.path.exists(BETANET_NEAR_DIR),True,100.00000000000004,N/A
"def setup_module(module):
<mask>:
        shutil.rmtree(HOME)
    if os.path.exists(NEARUP_PATH):
        shutil.rmtree(NEARUP_PATH)
    os.makedirs(BINARY_PATH)
    os.makedirs(HOME)
    uname = os.uname()[0]
    download_binaries('betanet', uname)",os.path.exists(HOME),14,os.path.exists(HOME),True,100.00000000000004,N/A
"def get_adjacency_matrix(labels, n_cluster):
    temp_matrix = np.zeros((n_cluster, n_cluster), dtype=np.float64)
    adjacency_matrix = np.zeros((n_cluster, n_cluster), dtype=np.float64)
    cntMat = np.zeros(n_cluster)
    steps = len(labels)
    for i in range(n_cluster):
        for k in range(steps - 1):
            idx = labels[k]
<mask>:
                idx2 = labels[k + 1]
                if idx == idx2:
                    continue
                else:
                    cntMat[idx2] = cntMat[idx2] + 1
        temp_matrix[i] = cntMat
        cntMat = np.zeros(n_cluster)
    for k in range(steps - 1):
        idx = labels[k]
        idx2 = labels[k + 1]
        if idx == idx2:
            continue
        adjacency_matrix[idx, idx2] = 1
        adjacency_matrix[idx2, idx] = 1
    transition_matrix = get_transition_matrix(temp_matrix)
    return (adjacency_matrix, transition_matrix, temp_matrix)",idx == i,89,idx == i,True,100.00000000000004,N/A
"def get_transition_matrix(adjacency_matrix, threshold=0.0):
    row_sum = adjacency_matrix.sum(axis=1)
    transition_matrix = adjacency_matrix / row_sum[:, np.newaxis]
    transition_matrix[transition_matrix <= threshold] = 0
<mask>:
        transition_matrix = np.nan_to_num(transition_matrix)
    return transition_matrix",np.any(np.isnan(transition_matrix)),23,np.isnan(transition_matrix),False,53.52614285189905,N/A
"def create_community_bag(files, labels, transition_matrices, cut_tree, n_cluster):
    trees = []
    communities_all = []
    for i, file in enumerate(files):
        _, usage = np.unique(labels[i], return_counts=True)
        T = graph_to_tree(usage, transition_matrices[i], n_cluster, merge_sel=1)
        trees.append(T)
<mask>:
            community_bag = traverse_tree_cutline(T, cutline=cut_tree)
            communities_all.append(community_bag)
            draw_tree(T)
        else:
            draw_tree(T)
            plt.pause(0.5)
            flag_1 = 'no'
            while flag_1 == 'no':
                cutline = int(input('Where do you want to cut the Tree? 0/1/2/3/...'))
                community_bag = traverse_tree_cutline(T, cutline=cutline)
                print(community_bag)
                flag_2 = input('\nAre all motifs in the list? (yes/no/restart)')
                if flag_2 == 'no':
                    while flag_2 == 'no':
                        add = input('Extend list or add in the end? (ext/end)')
                        if add == 'ext':
                            motif_idx = int(input('Which motif number? '))
                            list_idx = int(input('At which position in the list? (pythonic indexing starts at 0) '))
                            community_bag[list_idx].append(motif_idx)
                        if add == 'end':
                            motif_idx = int(input('Which motif number? '))
                            community_bag.append([motif_idx])
                        print(community_bag)
                        flag_2 = input('\nAre all motifs in the list? (yes/no/restart)')
                if flag_2 == 'restart':
                    continue
                if flag_2 == 'yes':
                    communities_all.append(community_bag)
                    flag_1 = 'yes'
    return (communities_all, trees)",cut_tree != None,151,cut_tree,False,36.78794411714425,N/A
"def umap_embedding(cfg, file, model_name, n_cluster, parameterization):
    reducer = umap.UMAP(n_components=2, min_dist=cfg['min_dist'], n_neighbors=cfg['n_neighbors'], random_state=cfg['random_state'])
    print('UMAP calculation for file %s' % file)
    folder = os.path.join(cfg['project_path'], 'results', file, model_name, parameterization + '-' + str(n_cluster), '')
    latent_vector = np.load(os.path.join(folder, 'latent_vector_' + file + '.npy'))
    num_points = cfg['num_points']
<mask>:
        num_points = latent_vector.shape[0]
    print('Embedding %d data points..' % num_points)
    embed = reducer.fit_transform(latent_vector[:num_points, :])
    return embed",num_points > latent_vector.shape[0],58,num_points is None,False,9.805567361279172,N/A
"def umap_vis(cfg, file, embed, community_labels_all):
    num_points = cfg['num_points']
<mask>:
        num_points = community_labels_all.shape[0]
    print('Embedding %d data points..' % num_points)
    num = np.unique(community_labels_all)
    fig = plt.figure(1)
    plt.scatter(embed[:, 0], embed[:, 1], c=community_labels_all[:num_points], cmap='Spectral', s=2, alpha=1)
    plt.colorbar(boundaries=np.arange(np.max(num) + 2) - 0.5).set_ticks(np.arange(np.max(num) + 1))
    plt.gca().set_aspect('equal', 'datalim')
    plt.grid(False)",num_points > community_labels_all.shape[0],42,num_points == 0,False,8.558153335723478,N/A
"def get_cluster_vid(cfg, path_to_file, file, n_cluster, videoType, flag):
<mask>:
        print('Motif videos getting created for ' + file + ' ...')
        labels = np.load(os.path.join(path_to_file, str(n_cluster) + '_km_label_' + file + '.npy'))
    if flag == 'community':
        print('Community videos getting created for ' + file + ' ...')
        labels = np.load(os.path.join(path_to_file, 'community', 'community_label_' + file + '.npy'))
    capture = cv.VideoCapture(os.path.join(cfg['project_path'], 'videos', file + videoType))
    if capture.isOpened():
        width = capture.get(cv.CAP_PROP_FRAME_WIDTH)
        height = capture.get(cv.CAP_PROP_FRAME_HEIGHT)
        fps = 25
    cluster_start = cfg['time_window'] / 2
    for cluster in range(n_cluster):
        print('Cluster: %d' % cluster)
        cluster_lbl = np.where(labels == cluster)
        cluster_lbl = cluster_lbl[0]
        if flag == 'motif':
            output = os.path.join(path_to_file, 'cluster_videos', file + '-motif_%d.avi' % cluster)
        if flag == 'community':
            output = os.path.join(path_to_file, 'community_videos', file + '-community_%d.avi' % cluster)
        video = cv.VideoWriter(output, cv.VideoWriter_fourcc('M', 'J', 'P', 'G'), fps, (int(width), int(height)))
        if len(cluster_lbl) < cfg['length_of_motif_video']:
            vid_length = len(cluster_lbl)
        else:
            vid_length = cfg['length_of_motif_video']
        for num in tqdm.tqdm(range(vid_length)):
            idx = cluster_lbl[num]
            capture.set(1, idx + cluster_start)
            ret, frame = capture.read()
            video.write(frame)
        video.release()
    capture.release()",flag == 'motif',157,flag == 'motif',True,100.00000000000004,N/A
"def motif_videos(config, videoType='.mp4'):
    config_file = Path(config).resolve()
    cfg = read_config(config_file)
    model_name = cfg['model_name']
    n_cluster = cfg['n_cluster']
    param = cfg['parameterization']
    flag = 'motif'
    files = []
<mask>:
        all_flag = input('Do you want to write motif videos for your entire dataset? \nIf you only want to use a specific dataset type filename: \nyes/no/filename ')
    else:
        all_flag = 'yes'
    if all_flag == 'yes' or all_flag == 'Yes':
        for file in cfg['video_sets']:
            files.append(file)
    elif all_flag == 'no' or all_flag == 'No':
        for file in cfg['video_sets']:
            use_file = input('Do you want to quantify ' + file + '? yes/no: ')
            if use_file == 'yes':
                files.append(file)
            if use_file == 'no':
                continue
    else:
        files.append(all_flag)
    print('Cluster size is: %d ' % n_cluster)
    for file in files:
        path_to_file = os.path.join(cfg['project_path'], 'results', file, model_name, param + '-' + str(n_cluster), '')
        if not os.path.exists(os.path.join(path_to_file, 'cluster_videos')):
            os.mkdir(os.path.join(path_to_file, 'cluster_videos'))
        get_cluster_vid(cfg, path_to_file, file, n_cluster, videoType, flag)
    print('All videos have been created!')",cfg['all_data'] == 'No',146,videoType == 'mp4',False,9.153013214364877,N/A
"def community_videos(config, videoType='.mp4'):
    config_file = Path(config).resolve()
    cfg = read_config(config_file)
    model_name = cfg['model_name']
    n_cluster = cfg['n_cluster']
    param = cfg['parameterization']
    flag = 'community'
    files = []
<mask>:
        all_flag = input('Do you want to write motif videos for your entire dataset? \nIf you only want to use a specific dataset type filename: \nyes/no/filename ')
    else:
        all_flag = 'yes'
    if all_flag == 'yes' or all_flag == 'Yes':
        for file in cfg['video_sets']:
            files.append(file)
    elif all_flag == 'no' or all_flag == 'No':
        for file in cfg['video_sets']:
            use_file = input('Do you want to quantify ' + file + '? yes/no: ')
            if use_file == 'yes':
                files.append(file)
            if use_file == 'no':
                continue
    else:
        files.append(all_flag)
    print('Cluster size is: %d ' % n_cluster)
    for file in files:
        path_to_file = os.path.join(cfg['project_path'], 'results', file, model_name, param + '-' + str(n_cluster), '')
        if not os.path.exists(os.path.join(path_to_file, 'community_videos')):
            os.mkdir(os.path.join(path_to_file, 'community_videos'))
        get_cluster_vid(cfg, path_to_file, file, n_cluster, videoType, flag)
    print('All videos have been created!')",cfg['all_data'] == 'No',146,cfg['dataset_type'] == 'filename',False,23.356898886410015,N/A
"def load_model(cfg, model_name, fixed):
    use_gpu = torch.cuda.is_available()
<mask>:
        pass
    else:
        torch.device('cpu')
    ZDIMS = cfg['zdims']
    FUTURE_DECODER = cfg['prediction_decoder']
    TEMPORAL_WINDOW = cfg['time_window'] * 2
    FUTURE_STEPS = cfg['prediction_steps']
    NUM_FEATURES = cfg['num_features']
    if fixed == False:
        NUM_FEATURES = NUM_FEATURES - 2
    hidden_size_layer_1 = cfg['hidden_size_layer_1']
    hidden_size_layer_2 = cfg['hidden_size_layer_2']
    hidden_size_rec = cfg['hidden_size_rec']
    hidden_size_pred = cfg['hidden_size_pred']
    dropout_encoder = cfg['dropout_encoder']
    dropout_rec = cfg['dropout_rec']
    dropout_pred = cfg['dropout_pred']
    softplus = cfg['softplus']
    if use_gpu:
        model = RNN_VAE(TEMPORAL_WINDOW, ZDIMS, NUM_FEATURES, FUTURE_DECODER, FUTURE_STEPS, hidden_size_layer_1, hidden_size_layer_2, hidden_size_rec, hidden_size_pred, dropout_encoder, dropout_rec, dropout_pred, softplus).cuda()
    else:
        model = RNN_VAE(TEMPORAL_WINDOW, ZDIMS, NUM_FEATURES, FUTURE_DECODER, FUTURE_STEPS, hidden_size_layer_1, hidden_size_layer_2, hidden_size_rec, hidden_size_pred, dropout_encoder, dropout_rec, dropout_pred, softplus).to()
    model.load_state_dict(torch.load(os.path.join(cfg['project_path'], 'model', 'best_model', model_name + '_' + cfg['Project'] + '.pkl')))
    model.eval()
    return model",use_gpu,107,use_gpu,True,100.00000000000004,N/A
"def embedd_latent_vectors(cfg, files, model, fixed):
    project_path = cfg['project_path']
    temp_win = cfg['time_window']
    num_features = cfg['num_features']
<mask>:
        num_features = num_features - 2
    use_gpu = torch.cuda.is_available()
    if use_gpu:
        pass
    else:
        torch.device('cpu')
    latent_vector_files = []
    for file in files:
        print('Embedding of latent vector for file %s' % file)
        data = np.load(os.path.join(project_path, 'data', file, file + '-PE-seq-clean.npy'))
        latent_vector_list = []
        with torch.no_grad():
            for i in tqdm.tqdm(range(data.shape[1] - temp_win)):
                data_sample_np = data[:, i:temp_win + i].T
                data_sample_np = np.reshape(data_sample_np, (1, temp_win, num_features))
                if use_gpu:
                    h_n = model.encoder(torch.from_numpy(data_sample_np).type('torch.FloatTensor').cuda())
                else:
                    h_n = model.encoder(torch.from_numpy(data_sample_np).type('torch.FloatTensor').to())
                mu, _, _ = model.lmbda(h_n)
                latent_vector_list.append(mu.cpu().data.numpy())
        latent_vector = np.concatenate(latent_vector_list, axis=0)
        latent_vector_files.append(latent_vector)
    return latent_vector_files",fixed == False,97,fixed,False,4.9787068367863965,N/A
"def get_motif_usage(label):
    motif_usage = np.unique(label, return_counts=True)
    cons = consecutive(motif_usage[0])
<mask>:
        usage_list = list(motif_usage[1])
        for i in range(len(cons) - 1):
            a = cons[i + 1][0]
            b = cons[i][-1]
            d = a - b - 1
            for j in range(1, d + 1):
                index = cons[i][-1] + j
                usage_list.insert(index, 0)
        usage = np.array(usage_list)
        motif_usage = usage
    else:
        motif_usage = motif_usage[1]
    return motif_usage",len(cons) != 1,60,len(cons) > 1,False,45.48019047027906,N/A
"def same_parameterization(cfg, files, latent_vector_files, states, parameterization):
    random_state = cfg['random_state_kmeans']
    n_init = cfg['n_init_kmeans']
    labels = []
    cluster_centers = []
    motif_usages = []
    latent_vector_cat = np.concatenate(latent_vector_files, axis=0)
<mask>:
        print('Using kmeans as parameterization!')
        kmeans = KMeans(init='k-means++', n_clusters=states, random_state=42, n_init=20).fit(latent_vector_cat)
        clust_center = kmeans.cluster_centers_
        label = kmeans.predict(latent_vector_cat)
    elif parameterization == 'hmm':
        if cfg['hmm_trained'] == False:
            print('Using a HMM as parameterization!')
            hmm_model = hmm.GaussianHMM(n_components=states, covariance_type='full', n_iter=100)
            hmm_model.fit(latent_vector_cat)
            label = hmm_model.predict(latent_vector_cat)
            save_data = os.path.join(cfg['project_path'], 'results', '')
            with open(save_data + 'hmm_trained.pkl', 'wb') as file:
                pickle.dump(hmm_model, file)
        else:
            print('Using a pretrained HMM as parameterization!')
            save_data = os.path.join(cfg['project_path'], 'results', '')
            with open(save_data + 'hmm_trained.pkl', 'rb') as file:
                hmm_model = pickle.load(file)
            label = hmm_model.predict(latent_vector_cat)
    idx = 0
    for i, file in enumerate(files):
        file_len = latent_vector_files[i].shape[0]
        labels.append(label[idx:idx + file_len])
        if parameterization == 'kmeans':
            cluster_centers.append(clust_center)
        motif_usage = get_motif_usage(label[idx:idx + file_len])
        motif_usages.append(motif_usage)
        idx += file_len
    return (labels, cluster_centers, motif_usages)",parameterization == 'kmeans',135,parameterization == 'kmeans',True,100.00000000000004,N/A
"def generative_model(config, mode='sampling'):
    config_file = Path(config).resolve()
    cfg = read_config(config_file)
    model_name = cfg['model_name']
    n_cluster = cfg['n_cluster']
    files = []
<mask>:
        all_flag = input('Do you want to write motif videos for your entire dataset? \nIf you only want to use a specific dataset type filename: \nyes/no/filename ')
    else:
        all_flag = 'yes'
    if all_flag == 'yes' or all_flag == 'Yes':
        for file in cfg['video_sets']:
            files.append(file)
    elif all_flag == 'no' or all_flag == 'No':
        for file in cfg['video_sets']:
            use_file = input('Do you want to quantify ' + file + '? yes/no: ')
            if use_file == 'yes':
                files.append(file)
            if use_file == 'no':
                continue
    else:
        files.append(all_flag)
    model = load_model(cfg, model_name)
    for file in files:
        path_to_file = os.path.join(cfg['project_path'], 'results', file, model_name, 'kmeans-' + str(n_cluster), '')
        if mode == 'sampling':
            latent_vector = np.load(os.path.join(path_to_file, 'latent_vector_' + file + '.npy'))
            random_generative_samples(cfg, model, latent_vector)
        if mode == 'reconstruction':
            latent_vector = np.load(os.path.join(path_to_file, 'latent_vector_' + file + '.npy'))
            random_reconstruction_samples(cfg, model, latent_vector)
        if mode == 'centers':
            cluster_center = np.load(os.path.join(path_to_file, 'cluster_center_' + file + '.npy'))
            visualize_cluster_center(cfg, model, cluster_center)
        if mode == 'motifs':
            latent_vector = np.load(os.path.join(path_to_file, 'latent_vector_' + file + '.npy'))
            labels = np.load(os.path.join(path_to_file, '', str(n_cluster) + '_km_label_' + file + '.npy'))
            random_generative_samples_motif(cfg, model, latent_vector, labels, n_cluster)",cfg['all_data'] == 'No',191,'video_sets' in cfg,False,5.70796903405875,N/A
"def behavior_segmentation(config, model_name=None, cluster_method='kmeans', n_cluster=[30]):
    config_file = Path(config).resolve()
    cfg = read_config(config_file)
    for folders in cfg['video_sets']:
<mask>:
            os.mkdir(os.path.join(cfg['project_path'], 'results', folders, '', model_name))
    files = []
    if cfg['all_data'] == 'No':
        all_flag = input('Do you want to qunatify your entire dataset? \nIf you only want to use a specific dataset type filename: \nyes/no/filename ')
    else:
        all_flag = 'yes'
    if all_flag == 'yes' or all_flag == 'Yes':
        for file in cfg['video_sets']:
            files.append(file)
    elif all_flag == 'no' or all_flag == 'No':
        for file in cfg['video_sets']:
            use_file = input('Do you want to quantify ' + file + '? yes/no: ')
            if use_file == 'yes':
                files.append(file)
            if use_file == 'no':
                continue
    else:
        files.append(all_flag)
    use_gpu = torch.cuda.is_available()
    if use_gpu:
        print('Using CUDA')
        print('GPU active:', torch.cuda.is_available())
        print('GPU used:', torch.cuda.get_device_name(0))
    else:
        print('CUDA is not working! Attempting to use the CPU...')
        torch.device('cpu')
    z, z_logger = temporal_quant(cfg, model_name, files, use_gpu)
    cluster_latent_space(cfg, files, z, z_logger, cluster_method, n_cluster, model_name)","not os.path.exists(os.path.join(cfg['project_path'], 'results', folders, '', model_name))",144,"not os.path.exists(os.path.join(cfg['project_path'], 'results', folders, '', '', model_name))",False,93.64250605898111,N/A
"def temporal_quant(cfg, model_name, files, use_gpu):
    SEED = 19
    ZDIMS = cfg['zdims']
    FUTURE_DECODER = cfg['prediction_decoder']
    TEMPORAL_WINDOW = cfg['time_window'] * 2
    FUTURE_STEPS = cfg['prediction_steps']
    NUM_FEATURES = cfg['num_features']
    PROJECT_PATH = cfg['project_path']
    hidden_size_layer_1 = cfg['hidden_size_layer_1']
    hidden_size_layer_2 = cfg['hidden_size_layer_2']
    hidden_size_rec = cfg['hidden_size_rec']
    hidden_size_pred = cfg['hidden_size_pred']
    dropout_encoder = cfg['dropout_encoder']
    dropout_rec = cfg['dropout_rec']
    dropout_pred = cfg['dropout_pred']
    temp_win = int(TEMPORAL_WINDOW / 2)
<mask>:
        torch.cuda.manual_seed(SEED)
        model = RNN_VAE(TEMPORAL_WINDOW, ZDIMS, NUM_FEATURES, FUTURE_DECODER, FUTURE_STEPS, hidden_size_layer_1, hidden_size_layer_2, hidden_size_rec, hidden_size_pred, dropout_encoder, dropout_rec, dropout_pred).cuda()
    else:
        torch.cuda.manual_seed(SEED)
        model = RNN_VAE(TEMPORAL_WINDOW, ZDIMS, NUM_FEATURES, FUTURE_DECODER, FUTURE_STEPS, hidden_size_layer_1, hidden_size_layer_2, hidden_size_rec, hidden_size_pred, dropout_encoder, dropout_rec, dropout_pred).to()
    if cfg['snapshot'] == 'yes':
        if use_gpu:
            model.load_state_dict(torch.load(os.path.join(cfg['project_path'], 'model', 'best_model', 'snapshots', model_name + '_' + cfg['Project'] + '_epoch_' + cfg['snapshot_epoch'] + '.pkl')))
        else:
            model.load_state_dict(torch.load(os.path.join(cfg['project_path'], 'model', 'best_model', 'snapshots', model_name + '_' + cfg['Project'] + '_epoch_' + cfg['snapshot_epoch'] + '.pkl'), map_location=torch.device('cpu')))
    elif use_gpu:
        model.load_state_dict(torch.load(os.path.join(cfg['project_path'], 'model', 'best_model', model_name + '_' + cfg['Project'] + '.pkl')))
    else:
        model.load_state_dict(torch.load(os.path.join(cfg['project_path'], 'model', 'best_model', model_name + '_' + cfg['Project'] + '.pkl'), map_location=torch.device('cpu')))
    model.eval()
    z_list = []
    z_logger = []
    logger = 0
    for file in files:
        print('Computing latent space for %s ' % file)
        z_logger.append(logger)
        data = cfg['load_data']
        X = load_data(PROJECT_PATH, file, data)
        if X.shape[0] > X.shape[1]:
            X = X.T
        num_frames = len(X[0, :]) - temp_win
        window_start = int(temp_win / 2)
        idx = int(temp_win / 2)
        x_decoded = []
        with torch.no_grad():
            for i in range(num_frames):
                if idx >= num_frames:
                    break
                data = X[:, idx - window_start:idx + window_start]
                data = np.reshape(data, (1, temp_win, NUM_FEATURES))
                if use_gpu:
                    dataTorch = torch.from_numpy(data).type(torch.FloatTensor).cuda()
                else:
                    dataTorch = torch.from_numpy(data).type(torch.FloatTensor).to()
                h_n = model.encoder(dataTorch)
                latent, _, _ = model.lmbda(h_n)
                z = latent.cpu().data.numpy()
                x_decoded.append(z)
                idx += 1
        z_temp = np.concatenate(x_decoded, axis=0)
        logger_temp = len(z_temp)
        logger += logger_temp
        z_list.append(z_temp)
    z_array = np.concatenate(z_list)
    z_logger.append(len(z_array))
    return (z_array, z_logger)",use_gpu,272,cfg['cuda'],False,0.0,N/A
"def cluster_latent_space(cfg, files, z_data, z_logger, cluster_method, n_cluster, model_name):
    for cluster in n_cluster:
<mask>:
            print('Behavior segmentation via k-Means for %d cluster.' % cluster)
            data_labels = kmeans_clustering(z_data, n_clusters=cluster)
            data_labels = np.int64(scipy.signal.medfilt(data_labels, cfg['median_filter']))
        if cluster_method == 'GMM':
            print('Behavior segmentation via GMM.')
            data_labels = gmm_clustering(z_data, n_components=cluster)
            data_labels = np.int64(scipy.signal.medfilt(data_labels, cfg['median_filter']))
        for idx, file in enumerate(files):
            print('Segmentation for file %s...' % file)
            if not os.path.exists(os.path.join(cfg['project_path'], 'results', file, '', model_name, '', cluster_method + '-' + str(cluster))):
                os.mkdir(os.path.join(cfg['project_path'], 'results', file, '', model_name, '', cluster_method + '-' + str(cluster)))
            save_data = os.path.join(cfg['project_path'], 'results', file, '', model_name, '')
            z_latent = z_data[z_logger[idx]:z_logger[idx + 1], :]
            labels = data_labels[z_logger[idx]:z_logger[idx + 1]]
            if cluster_method == 'kmeans':
                np.save(save_data + cluster_method + '-' + str(cluster) + '/' + str(cluster) + '_km_label_' + file, labels)
                np.save(save_data + cluster_method + '-' + str(cluster) + '/' + 'latent_vector_' + file, z_latent)
            if cluster_method == 'GMM':
                np.save(save_data + cluster_method + '-' + str(cluster) + '/' + str(cluster) + '_gmm_label_' + file, labels)
                np.save(save_data + cluster_method + '-' + str(cluster) + '/' + 'latent_vector_' + file, z_latent)
            if cluster_method == 'all':
                np.save(save_data + cluster_method + '-' + str(cluster) + '/' + str(cluster) + '_km_label_' + file, labels)
                np.save(save_data + cluster_method + '-' + str(cluster) + '/' + str(cluster) + '_gmm_label_' + file, labels)
                np.save(save_data + cluster_method + '-' + str(cluster) + '/' + 'latent_vector_' + file, z_latent)",cluster_method == 'kmeans',218,cluster_method == 'kmeans',True,100.00000000000004,N/A
"def visualization(config, label=None):
    config_file = Path(config).resolve()
    cfg = read_config(config_file)
    model_name = cfg['model_name']
    n_cluster = cfg['n_cluster']
    param = cfg['parameterization']
    files = []
<mask>:
        all_flag = input('Do you want to write motif videos for your entire dataset? \nIf you only want to use a specific dataset type filename: \nyes/no/filename ')
    else:
        all_flag = 'yes'
    if all_flag == 'yes' or all_flag == 'Yes':
        for file in cfg['video_sets']:
            files.append(file)
    elif all_flag == 'no' or all_flag == 'No':
        for file in cfg['video_sets']:
            use_file = input('Do you want to quantify ' + file + '? yes/no: ')
            if use_file == 'yes':
                files.append(file)
            if use_file == 'no':
                continue
    else:
        files.append(all_flag)
    for idx, file in enumerate(files):
        path_to_file = os.path.join(cfg['project_path'], 'results', file, '', model_name, '', param + '-' + str(n_cluster))
        try:
            embed = np.load(os.path.join(path_to_file, '', 'community', '', 'umap_embedding_' + file + '.npy'))
            num_points = cfg['num_points']
            if num_points > embed.shape[0]:
                num_points = embed.shape[0]
        except:
            if not os.path.exists(os.path.join(path_to_file, 'community')):
                os.mkdir(os.path.join(path_to_file, 'community'))
            print('Compute embedding for file %s' % file)
            reducer = umap.UMAP(n_components=2, min_dist=cfg['min_dist'], n_neighbors=cfg['n_neighbors'], random_state=cfg['random_state'])
            latent_vector = np.load(os.path.join(path_to_file, '', 'latent_vector_' + file + '.npy'))
            num_points = cfg['num_points']
            if num_points > latent_vector.shape[0]:
                num_points = latent_vector.shape[0]
            print('Embedding %d data points..' % num_points)
            embed = reducer.fit_transform(latent_vector[:num_points, :])
            np.save(os.path.join(path_to_file, 'community', 'umap_embedding_' + file + '.npy'), embed)
        print('Visualizing %d data points.. ' % num_points)
        if label == None:
            umap_vis(file, embed, num_points)
        if label == 'motif':
            motif_label = np.load(os.path.join(path_to_file, '', str(n_cluster) + '_km_label_' + file + '.npy'))
            umap_label_vis(file, embed, motif_label, n_cluster, num_points)
        if label == 'community':
            community_label = np.load(os.path.join(path_to_file, '', 'community', '', 'community_label_' + file + '.npy'))
            umap_vis_comm(file, embed, community_label, num_points)",cfg['all_data'] == 'No',253,cfg['output_format'] == 'yes',False,23.356898886410015,N/A
"def create_video(path_to_file, file, embed, clabel, frames, start, length, max_lag, num_points):
    cmap = matplotlib.cm.gray
    cmap_reversed = matplotlib.cm.get_cmap('gray_r')
    fig = plt.figure()
    spec = GridSpec(ncols=2, nrows=1, width_ratios=[6, 3])
    ax1 = fig.add_subplot(spec[0])
    ax2 = fig.add_subplot(spec[1])
    ax2.axis('off')
    ax2.grid(False)
    lag = 0
    for i in tqdm.tqdm(range(length)):
<mask>:
            lag = i - max_lag
        ax1.cla()
        ax1.axis('off')
        ax1.grid(False)
        ax1.scatter(embed[:num_points, 0], embed[:num_points, 1], c=clabel[:num_points], cmap='Spectral', s=1, alpha=0.4)
        ax1.set_aspect('equal', 'datalim')
        ax1.plot(embed[start + lag:start + i, 0], embed[start + lag:start + i, 1], '.b-', alpha=0.6, linewidth=2, markersize=4)
        ax1.plot(embed[start + i, 0], embed[start + i, 1], 'gx', markersize=4)
        frame = frames[i]
        ax2.imshow(frame, cmap=cmap_reversed)
        fig.savefig(os.path.join(path_to_file, 'gif_frames', file + 'gif_%d.png') % i)",i > max_lag,97,i > max_lag,True,100.00000000000004,N/A
"def gif(config, pose_ref_index, subtract_background=True, start=None, length=500, max_lag=30, label='community', file_format='.mp4', crop_size=(300, 300)):
    config_file = Path(config).resolve()
    cfg = read_config(config_file)
    model_name = cfg['model_name']
    n_cluster = cfg['n_cluster']
    param = cfg['parameterization']
    files = []
<mask>:
        all_flag = input('Do you want to write motif videos for your entire dataset? \nIf you only want to use a specific dataset type filename: \nyes/no/filename ')
    else:
        all_flag = 'yes'
    if all_flag == 'yes' or all_flag == 'Yes':
        for file in cfg['video_sets']:
            files.append(file)
    elif all_flag == 'no' or all_flag == 'No':
        for file in cfg['video_sets']:
            use_file = input('Do you want to quantify ' + file + '? yes/no: ')
            if use_file == 'yes':
                files.append(file)
            if use_file == 'no':
                continue
    else:
        files.append(all_flag)
    for file in files:
        path_to_file = os.path.join(cfg['project_path'], 'results', file, model_name, param + '-' + str(n_cluster), '')
        if not os.path.exists(os.path.join(path_to_file, 'gif_frames')):
            os.mkdir(os.path.join(path_to_file, 'gif_frames'))
        embed = np.load(os.path.join(path_to_file, 'community', 'umap_embedding_' + file + '.npy'))
        try:
            embed = np.load(os.path.join(path_to_file, '', 'community', '', 'umap_embedding_' + file + '.npy'))
            num_points = cfg['num_points']
            if num_points > embed.shape[0]:
                num_points = embed.shape[0]
        except:
            print('Compute embedding for file %s' % file)
            reducer = umap.UMAP(n_components=2, min_dist=cfg['min_dist'], n_neighbors=cfg['n_neighbors'], random_state=cfg['random_state'])
            latent_vector = np.load(os.path.join(path_to_file, '', 'latent_vector_' + file + '.npy'))
            num_points = cfg['num_points']
            if num_points > latent_vector.shape[0]:
                num_points = latent_vector.shape[0]
            print('Embedding %d data points..' % num_points)
            embed = reducer.fit_transform(latent_vector[:num_points, :])
            np.save(os.path.join(path_to_file, 'community', 'umap_embedding_' + file + '.npy'), embed)
        if label == 'motif':
            umap_label = np.load(os.path.join(path_to_file, str(n_cluster) + '_km_label_' + file + '.npy'))
        elif label == 'community':
            umap_label = np.load(os.path.join(path_to_file, 'community', 'community_label_' + file + '.npy'))
        elif label == None:
            umap_label = None
        if start == None:
            start = np.random.choice(embed[:num_points].shape[0] - length)
        else:
            start = start
        frames = get_animal_frames(cfg, file, pose_ref_index, start, length, subtract_background, file_format, crop_size)
        create_video(path_to_file, file, embed, umap_label, frames, start, length, max_lag, num_points)",cfg['all_data'] == 'No',281,cfg['enable_video'],False,11.708995388048026,N/A
"def hierarchy_pos(G, root=None, width=0.5, vert_gap=0.2, vert_loc=0, xcenter=0.5):
    """"""
    From Joel's answer at https://stackoverflow.com/a/29597209/2966723.  
    """"""
<mask>:
        raise TypeError('cannot use hierarchy_pos on a graph that is not a tree')
    if root is None:
        if isinstance(G, nx.DiGraph):
            root = next(iter(nx.topological_sort(G)))
        else:
            root = random.choice(list(G.nodes))

    def _hierarchy_pos(G, root, width=1.0, vert_gap=0.2, vert_loc=0, xcenter=0.5, pos=None, parent=None):
        if pos is None:
            pos = {root: (xcenter, vert_loc)}
        else:
            pos[root] = (xcenter, vert_loc)
        children = list(G.neighbors(root))
        if not isinstance(G, nx.DiGraph) and parent is not None:
            children.remove(parent)
        if len(children) != 0:
            dx = width / len(children)
            nextx = xcenter - width / 2 - dx / 2
            for child in children:
                nextx += dx
                pos = _hierarchy_pos(G, child, width=dx, vert_gap=vert_gap, vert_loc=vert_loc - vert_gap, xcenter=nextx, pos=pos, parent=root)
        return pos
    return _hierarchy_pos(G, root, width, vert_gap, vert_loc, xcenter)",not nx.is_tree(G),125,G is None,False,4.691812222477093,N/A
"def merge_func(transition_matrix, n_cluster, motif_norm, merge_sel):
<mask>:
        cost = np.max(transition_matrix)
        merge_nodes = np.where(cost == transition_matrix)
    if merge_sel == 1:
        cost_temp = 100
        for i in range(n_cluster):
            for j in range(n_cluster):
                try:
                    cost = (motif_norm[i] + motif_norm[j]) / np.abs(transition_matrix[i, j] + transition_matrix[j, i])
                except ZeroDivisionError:
                    print('Error: Transition probabilities between motif ' + str(i) + ' and motif ' + str(j) + ' are zero.')
                if cost <= cost_temp:
                    cost_temp = cost
                    merge_nodes = (np.array([i]), np.array([j]))
    return merge_nodes",merge_sel == 0,75,merge_sel == 0,True,100.00000000000004,N/A
"def traverse_tree(T, root_node=None):
<mask>:
        node = ['Root']
    else:
        node = [root_node]
    traverse_list = []
    traverse_preorder = '{'

    def _traverse_tree(T, node, traverse_preorder):
        traverse_preorder += str(node[0])
        traverse_list.append(node[0])
        children = list(T.neighbors(node[0]))
        if len(children) == 3:
            for child in children:
                if child in traverse_list:
                    children.remove(child)
        if len(children) > 1:
            traverse_preorder += '{'
            traverse_preorder_temp = _traverse_tree(T, [children[0]], '')
            traverse_preorder += traverse_preorder_temp
            traverse_preorder += '}{'
            traverse_preorder_temp = _traverse_tree(T, [children[1]], '')
            traverse_preorder += traverse_preorder_temp
            traverse_preorder += '}'
        return traverse_preorder
    traverse_preorder = _traverse_tree(T, node, traverse_preorder)
    traverse_preorder += '}'
    return traverse_preorder",root_node == None,82,root_node is None,False,34.98330125272253,N/A
"def _traverse_tree(T, node, traverse_preorder, traverse_list):
    traverse_preorder += str(node[0])
    traverse_list.append(node[0])
    children = list(T.neighbors(node[0]))
<mask>:
        for child in children:
            if child in traverse_list:
                children.remove(child)
    if len(children) > 1:
        traverse_preorder += '{'
        traverse_preorder_temp = _traverse_tree(T, [children[0]], '', traverse_list)
        traverse_preorder += traverse_preorder_temp
        traverse_preorder += '}{'
        traverse_preorder_temp = _traverse_tree(T, [children[1]], '', traverse_list)
        traverse_preorder += traverse_preorder_temp
        traverse_preorder += '}'
    return traverse_preorder",len(children) == 3,55,len(children) > 1,False,43.01250851313264,N/A
"def crop_and_flip(rect, src, points, ref_index):
    center, size, theta = rect
    center, size = (tuple(map(int, center)), tuple(map(int, size)))
    M = cv.getRotationMatrix2D(center, theta, 1)
    x_diff = center[0] - size[0] // 2
    y_diff = center[1] - size[1] // 2
    dlc_points_shifted = []
    for i in points:
        point = cv.transform(np.array([[[i[0], i[1]]]]), M)[0][0]
        point[0] -= x_diff
        point[1] -= y_diff
        dlc_points_shifted.append(point)
    dst = cv.warpAffine(src.astype('float32'), M, src.shape[:2])
    out = cv.getRectSubPix(dst, size, center)
<mask>:
        rect = ((size[0] // 2, size[0] // 2), size, 180)
        center, size, theta = rect
        center, size = (tuple(map(int, center)), tuple(map(int, size)))
        M = cv.getRotationMatrix2D(center, theta, 1)
        x_diff = center[0] - size[0] // 2
        y_diff = center[1] - size[1] // 2
        points = dlc_points_shifted
        dlc_points_shifted = []
        for i in points:
            point = cv.transform(np.array([[[i[0], i[1]]]]), M)[0][0]
            point[0] -= x_diff
            point[1] -= y_diff
            dlc_points_shifted.append(point)
        dst = cv.warpAffine(out.astype('float32'), M, out.shape[:2])
        out = cv.getRectSubPix(dst, size, center)
    return (out, dlc_points_shifted)",dlc_points_shifted[ref_index[1]][0] >= dlc_points_shifted[ref_index[0]][0],142,ref_index == -1,False,0.30530350516336524,N/A
"def background(path_to_file, filename, file_format='.mp4', num_frames=1000):
    """"""
    Compute background image from fixed camera
    """"""
    capture = cv.VideoCapture(os.path.join(path_to_file, 'videos', filename + file_format))
<mask>:
        raise Exception('Unable to open video file: {0}'.format(os.path.join(path_to_file, 'videos', filename + file_format)))
    frame_count = int(capture.get(cv.CAP_PROP_FRAME_COUNT))
    ret, frame = capture.read()
    height, width, _ = frame.shape
    frames = np.zeros((height, width, num_frames))
    for i in tqdm.tqdm(range(num_frames), disable=not True, desc='Compute background image for video %s' % filename):
        rand = np.random.choice(frame_count, replace=False)
        capture.set(1, rand)
        ret, frame = capture.read()
        gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
        frames[..., i] = gray
    print('Finishing up!')
    medFrame = np.median(frames, 2)
    background = scipy.ndimage.median_filter(medFrame, (5, 5))
    np.save(os.path.join(path_to_file, 'videos', filename + '-background.npy'), background)
    capture.release()
    return background",not capture.isOpened(),101,not capture.is_valid(),False,25.848657697858535,N/A
"def get_rotation_matrix(adjacent, opposite, crop_size=(300, 300)):
    tan_alpha = np.abs(opposite) / np.abs(adjacent)
    alpha = np.arctan(tan_alpha)
    alpha = np.rad2deg(alpha)
<mask>:
        alpha = 180 - alpha
    if adjacent < 0 and opposite < 0:
        alpha = -(180 - alpha)
    if adjacent > 0 and opposite < 0:
        alpha = -alpha
    rot_mat = cv.getRotationMatrix2D((crop_size[0] // 2, crop_size[1] // 2), alpha, 1)
    return rot_mat",adjacent < 0 and opposite > 0,58,adjacent > 0 and opposite < 0,False,35.930411196308434,N/A
"def get_animal_frames(cfg, filename, pose_ref_index, start, length, subtract_background, file_format='.mp4', crop_size=(300, 300)):
    path_to_file = cfg['project_path']
    time_window = cfg['time_window']
    lag = int(time_window / 2)
    data = pd.read_csv(os.path.join(path_to_file, 'videos', 'pose_estimation', filename + '.csv'), skiprows=2)
    data_mat = pd.DataFrame.to_numpy(data)
    data_mat = data_mat[:, 1:]
    pose_list = []
    for i in range(int(data_mat.shape[1] / 3)):
        pose_list.append(data_mat[:, i * 3:(i + 1) * 3])
    pose_ref_index = pose_ref_index
    pose_flip_ref = pose_ref_index
<mask>:
        try:
            print('Loading background image ...')
            bg = np.load(os.path.join(path_to_file, 'videos', filename + '-background.npy'))
        except:
            print(""Can't find background image... Calculate background image..."")
            bg = background(path_to_file, filename, file_format)
    images = []
    points = []
    for i in pose_list:
        for j in i:
            if j[2] <= 0.8:
                j[0], j[1] = (np.nan, np.nan)
    for i in pose_list:
        i = interpol(i)
    capture = cv.VideoCapture(os.path.join(path_to_file, 'videos', filename + file_format))
    if not capture.isOpened():
        raise Exception('Unable to open video file: {0}'.format(os.path.join(path_to_file, 'videos', filename + +file_format)))
    for idx in tqdm.tqdm(range(length), disable=not True, desc='Align frames'):
        try:
            capture.set(1, idx + start + lag)
            ret, frame = capture.read()
            frame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
            if subtract_background == True:
                frame = frame - bg
                frame[frame <= 0] = 0
        except:
            print(""Couldn't find a frame in capture.read(). #Frame: %d"" % idx + start + lag)
            continue
        pose_list_bordered = []
        for i in pose_list:
            pose_list_bordered.append((int(i[idx + start + lag][0] + crop_size[0]), int(i[idx + start + lag][1] + crop_size[1])))
        img = cv.copyMakeBorder(frame, crop_size[1], crop_size[1], crop_size[0], crop_size[0], cv.BORDER_CONSTANT, 0)
        punkte = []
        for i in pose_ref_index:
            coord = []
            coord.append(pose_list_bordered[i][0])
            coord.append(pose_list_bordered[i][1])
            punkte.append(coord)
        punkte = [punkte]
        punkte = np.asarray(punkte)
        rect = cv.minAreaRect(punkte)
        lst = list(rect)
        lst[1] = crop_size
        rect = tuple(lst)
        center, size, theta = rect
        out, shifted_points = crop_and_flip(rect, img, pose_list_bordered, pose_flip_ref)
        images.append(out)
        points.append(shifted_points)
    capture.release()
    return images",subtract_background == True,268,subtract_background,False,36.78794411714425,N/A
"def read_config(configname):
    """"""
    Reads structured config file defining a project.
    """"""
    ruamelFile = ruamel.yaml.YAML()
    path = Path(configname)
<mask>:
        try:
            with open(path, 'r') as f:
                cfg = ruamelFile.load(f)
                curr_dir = os.path.dirname(configname)
                if cfg['project_path'] != curr_dir:
                    cfg['project_path'] = curr_dir
                    write_config(configname, cfg)
        except Exception as err:
            if len(err.args) > 2:
                if err.args[2] == ""could not determine a constructor for the tag '!!python/tuple'"":
                    with open(path, 'r') as ymlfile:
                        cfg = yaml.load(ymlfile, Loader=yaml.SafeLoader)
                        write_config(configname, cfg)
                else:
                    raise
    else:
        raise FileNotFoundError('Config file is not found. Please make sure that the file exists and/or that you passed the path of the config file correctly!')
    return cfg",os.path.exists(path),99,path.exists(),False,38.80684294761701,N/A
"def update_config(config):
    config_file = Path(config).resolve()
    cfg = read_config(config_file)
    project = cfg['Project']
    project_path = cfg['project_path']
    video_names = []
    for file in cfg['video_sets']:
        video_names.append(file)
    flag = input('ATTENTION! You are about to overwrite your current config.yaml. If you did changes, back up your current version and compare to the updated version. Do you want to continue? (yes/no)')
<mask>:
        cfg_file, ruamelFile = create_config_template()
        cfg_file['Project'] = str(project)
        cfg_file['project_path'] = str(project_path) + '/'
        cfg_file['test_fraction'] = 0.1
        cfg_file['video_sets'] = video_names
        cfg_file['all_data'] = 'yes'
        cfg_file['load_data'] = '-PE-seq-clean'
        cfg_file['anneal_function'] = 'linear'
        cfg_file['batch_size'] = 256
        cfg_file['max_epochs'] = 500
        cfg_file['transition_function'] = 'GRU'
        cfg_file['beta'] = 1
        cfg_file['zdims'] = 30
        cfg_file['learning_rate'] = 0.0005
        cfg_file['time_window'] = 30
        cfg_file['prediction_decoder'] = 1
        cfg_file['prediction_steps'] = 15
        cfg_file['model_convergence'] = 50
        cfg_file['model_snapshot'] = 50
        cfg_file['num_features'] = 12
        cfg_file['savgol_filter'] = True
        cfg_file['savgol_length'] = 5
        cfg_file['savgol_order'] = 2
        cfg_file['hidden_size_layer_1'] = 256
        cfg_file['hidden_size_layer_2'] = 256
        cfg_file['dropout_encoder'] = 0
        cfg_file['hidden_size_rec'] = 256
        cfg_file['dropout_rec'] = 0
        cfg_file['hidden_size_pred'] = 256
        cfg_file['dropout_pred'] = 0
        cfg_file['kl_start'] = 2
        cfg_file['annealtime'] = 4
        cfg_file['mse_reconstruction_reduction'] = 'sum'
        cfg_file['mse_prediction_reduction'] = 'sum'
        cfg_file['kmeans_loss'] = cfg_file['zdims']
        cfg_file['kmeans_lambda'] = 0.1
        cfg_file['scheduler'] = 1
        cfg_file['length_of_motif_video'] = 1000
        cfg_file['noise'] = False
        cfg_file['scheduler_step_size'] = 100
        cfg_file['legacy'] = False
        cfg_file['individual_parameterization'] = False
        cfg_file['random_state_kmeans'] = 42
        cfg_file['n_init_kmeans'] = 15
        cfg_file['model_name'] = 'VAME'
        cfg_file['n_cluster'] = 15
        cfg_file['pretrained_weights'] = False
        cfg_file['pretrained_model'] = 'None'
        cfg_file['min_dist'] = 0.1
        cfg_file['n_neighbors'] = 200
        cfg_file['random_state'] = 42
        cfg_file['num_points'] = 30000
        cfg_file['scheduler_gamma'] = 0.2
        cfg_file['scheduler_threshold'] = 0.1
        cfg_file['softplus'] = False
        cfg_file['pose_confidence'] = 0.99
        cfg_file['iqr_factor'] = 4
        cfg_file['robust'] = True
        cfg_file['beta_norm'] = False
        cfg_file['n_layers'] = 1
        cfg_file['axis'] = 'None'
        cfg_file['egocentric_data'] = True
        cfg_file['parameterization'] = 'kmeans'
        projconfigfile = os.path.join(str(project_path), 'config.yaml')
        write_config(projconfigfile, cfg_file)
        print('Your config.yaml has been updated.')
    else:
        print('No changes have been applied.')",flag == 'yes',264,flag == 'yes',True,100.00000000000004,N/A
"def csv_to_numpy(config):
    """"""
    This is a function to convert your pose-estimation.csv file to a numpy array.

    Note that this code is only useful for data which is a priori egocentric, i.e. head-fixed
    or otherwise restrained animals.

    example use:
    vame.csv_to_npy('pathto/your/config/yaml', 'path/toYourFolderwithCSV/')
    """"""
    config_file = Path(config).resolve()
    cfg = read_config(config_file)
    path_to_file = cfg['project_path']
    filename = cfg['video_sets']
    confidence = cfg['pose_confidence']
<mask>:
        raise ValueError('The config.yaml indicates that the data is not egocentric. Please check the parameter egocentric_data')
    for file in filename:
        print(file)
        data = pd.read_csv(os.path.join(path_to_file, 'videos', 'pose_estimation', file + '.csv'), skiprows=3, header=None)
        data_mat = pd.DataFrame.to_numpy(data)
        data_mat = data_mat[:, 1:]
        pose_list = []
        for i in range(int(data_mat.shape[1] / 3)):
            pose_list.append(data_mat[:, i * 3:(i + 1) * 3])
        for i in pose_list:
            for j in i:
                if j[2] <= confidence:
                    j[0], j[1] = (np.nan, np.nan)
        for i in pose_list:
            i = interpol(i)
        positions = np.concatenate(pose_list, axis=1)
        final_positions = np.zeros((data_mat.shape[0], int(data_mat.shape[1] / 3) * 2))
        jdx = 0
        idx = 0
        for i in range(int(data_mat.shape[1] / 3)):
            final_positions[:, idx:idx + 2] = positions[:, jdx:jdx + 2]
            jdx += 3
            idx += 2
        np.save(os.path.join(path_to_file, 'data', file, file + '-PE-seq.npy'), final_positions.T)
        print('conversion from DeepLabCut csv to numpy complete...')
    print('Your data is now in right format and you can call vame.create_trainset()')",cfg['egocentric_data'] == False,200,not os.path.exists(path_to_file),False,3.0890553181566975,N/A
"def background(path_to_file, filename, video_format='.mp4', num_frames=1000):
    """"""
    Compute background image from fixed camera 
    """"""
    import scipy.ndimage
    capture = cv.VideoCapture(os.path.join(path_to_file, 'videos', filename + video_format))
<mask>:
        raise Exception('Unable to open video file: {0}'.format(os.path.join(path_to_file, 'videos', filename + video_format)))
    frame_count = int(capture.get(cv.CAP_PROP_FRAME_COUNT))
    ret, frame = capture.read()
    height, width, _ = frame.shape
    frames = np.zeros((height, width, num_frames))
    for i in tqdm.tqdm(range(num_frames), disable=not True, desc='Compute background image for video %s' % filename):
        rand = np.random.choice(frame_count, replace=False)
        capture.set(1, rand)
        ret, frame = capture.read()
        gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
        frames[..., i] = gray
    print('Finishing up!')
    medFrame = np.median(frames, 2)
    background = scipy.ndimage.median_filter(medFrame, (5, 5))
    capture.release()
    return background",not capture.isOpened(),97,not os.path.isfile(capture.file),False,9.425159511373677,N/A
"def align_mouse(path_to_file, filename, video_format, crop_size, pose_list, pose_ref_index, confidence, pose_flip_ref, bg, frame_count, use_video=True):
    images = []
    points = []
    for i in pose_list:
        for j in i:
<mask>:
                j[0], j[1] = (np.nan, np.nan)
    for i in pose_list:
        i = interpol(i)
    if use_video:
        capture = cv.VideoCapture(os.path.join(path_to_file, 'videos', filename + video_format))
        if not capture.isOpened():
            raise Exception('Unable to open video file: {0}'.format(os.path.join(path_to_file, 'videos', filename + video_format)))
    for idx in tqdm.tqdm(range(frame_count), disable=not True, desc='Align frames'):
        if use_video:
            try:
                ret, frame = capture.read()
                frame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
                frame = frame - bg
                frame[frame <= 0] = 0
            except:
                print(""Couldn't find a frame in capture.read(). #Frame: %d"" % idx)
                continue
        else:
            frame = np.zeros((1, 1))
        pose_list_bordered = []
        for i in pose_list:
            pose_list_bordered.append((int(i[idx][0] + crop_size[0]), int(i[idx][1] + crop_size[1])))
        img = cv.copyMakeBorder(frame, crop_size[1], crop_size[1], crop_size[0], crop_size[0], cv.BORDER_CONSTANT, 0)
        punkte = []
        for i in pose_ref_index:
            coord = []
            coord.append(pose_list_bordered[i][0])
            coord.append(pose_list_bordered[i][1])
            punkte.append(coord)
        punkte = [punkte]
        punkte = np.asarray(punkte)
        rect = cv.minAreaRect(punkte)
        lst = list(rect)
        lst[1] = crop_size
        rect = tuple(lst)
        center, size, theta = rect
        out, shifted_points = crop_and_flip(rect, img, pose_list_bordered, pose_flip_ref)
        if use_video:
            images.append(out)
        points.append(shifted_points)
    if use_video:
        capture.release()
    time_series = np.zeros((len(pose_list) * 2, frame_count))
    for i in range(frame_count):
        idx = 0
        for j in range(len(pose_list)):
            time_series[idx:idx + 2, i] = points[i][j]
            idx += 2
    return (images, points, time_series)",j[2] <= confidence,210,np.isnan(j[0]) and j[1] is None,False,5.412989186545263,N/A
"def play_aligned_video(a, n, frame_count):
    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255), (0, 0, 0), (255, 255, 255)]
    for i in range(frame_count):
        ret, frame = (True, a[i])
<mask>:
            frame = cv.cvtColor(frame.astype('uint8') * 255, cv.COLOR_GRAY2BGR)
            im_color = cv.applyColorMap(frame, cv.COLORMAP_JET)
            for c, j in enumerate(n[i]):
                cv.circle(im_color, (j[0], j[1]), 5, colors[c], -1)
            cv.imshow('Frame', im_color)
            if cv.waitKey(25) & 255 == ord('q'):
                break
        else:
            break
    cv.destroyAllWindows()",ret == True,73,ret,False,4.9787068367863965,N/A
"def alignment(path_to_file, filename, pose_ref_index, video_format, crop_size, confidence, use_video=False, check_video=False):
    dataFile = glob.glob(os.path.join(path_to_file, 'videos', 'pose_estimation', filename + '*'))
<mask>:
        raise AssertionError('Multiple data files match video {}'.format(filename))
    else:
        dataFile = dataFile[0]
    if dataFile.endswith('.csv'):
        data = pd.read_csv(dataFile, skiprows=2, index_col=0)
    elif dataFile.endswith('.h5'):
        data = pd.read_hdf(dataFile)
    data_mat = pd.DataFrame.to_numpy(data)
    pose_list = []
    for i in range(int(data_mat.shape[1] / 3)):
        pose_list.append(data_mat[:, i * 3:(i + 1) * 3])
    pose_ref_index = pose_ref_index
    pose_flip_ref = pose_ref_index
    if use_video:
        bg = background(path_to_file, filename, video_format)
        capture = cv.VideoCapture(os.path.join(path_to_file, 'videos', filename + video_format))
        if not capture.isOpened():
            raise Exception('Unable to open video file: {0}'.format(os.path.join(path_to_file, 'videos', filename + video_format)))
        frame_count = int(capture.get(cv.CAP_PROP_FRAME_COUNT))
        capture.release()
    else:
        bg = 0
        frame_count = len(data)
    frames, n, time_series = align_mouse(path_to_file, filename, video_format, crop_size, pose_list, pose_ref_index, confidence, pose_flip_ref, bg, frame_count, use_video)
    if check_video:
        play_aligned_video(frames, n, frame_count)
    return (time_series, frames)",len(dataFile) > 1,129,len(dataFile) > 1,True,100.00000000000004,N/A
"def __init__(self, ZDIMS, hidden_size_layer_1, hidden_size_layer_2, softplus):
    super(Lambda, self).__init__()
    self.hid_dim = hidden_size_layer_1 * 4
    self.latent_length = ZDIMS
    self.softplus = softplus
    self.hidden_to_mean = nn.Linear(self.hid_dim, self.latent_length)
    self.hidden_to_logvar = nn.Linear(self.hid_dim, self.latent_length)
<mask>:
        print('Using a softplus activation to ensures that the variance is parameterized as non-negative and activated by a smooth function')
        self.softplus_fn = nn.Softplus()",self.softplus == True,50,softplus.ndim < 0,False,10.400597689005304,N/A
"def forward(self, hidden):
    self.mean = self.hidden_to_mean(hidden)
<mask>:
        self.logvar = self.softplus_fn(self.hidden_to_logvar(hidden))
    else:
        self.logvar = self.hidden_to_logvar(hidden)
    if self.training:
        std = torch.exp(0.5 * self.logvar)
        eps = torch.randn_like(std)
        return (eps.mul(std).add_(self.mean), self.mean, self.logvar)
    else:
        return (self.mean, self.mean, self.logvar)",self.softplus == True,33,self.softplus_fn is not None,False,20.556680845025987,N/A
"def __init__(self, TEMPORAL_WINDOW, ZDIMS, NUM_FEATURES, FUTURE_DECODER, FUTURE_STEPS, hidden_size_layer_1, hidden_size_layer_2, hidden_size_rec, hidden_size_pred, dropout_encoder, dropout_rec, dropout_pred, softplus):
    super(RNN_VAE, self).__init__()
    self.FUTURE_DECODER = FUTURE_DECODER
    self.seq_len = int(TEMPORAL_WINDOW / 2)
    self.encoder = Encoder(NUM_FEATURES, hidden_size_layer_1, hidden_size_layer_2, dropout_encoder)
    self.lmbda = Lambda(ZDIMS, hidden_size_layer_1, hidden_size_layer_2, softplus)
    self.decoder = Decoder(self.seq_len, ZDIMS, NUM_FEATURES, hidden_size_rec, dropout_rec)
<mask>:
        self.decoder_future = Decoder_Future(self.seq_len, ZDIMS, NUM_FEATURES, FUTURE_STEPS, hidden_size_pred, dropout_pred)",FUTURE_DECODER,53,self.FUTURE_STEPS,False,21.3643503198117,N/A
"def forward(self, seq):
    """""" Encode input sequence """"""
    h_n = self.encoder(seq)
    ' Compute the latent state via reparametrization trick '
    z, mu, logvar = self.lmbda(h_n)
    ins = z.unsqueeze(2).repeat(1, 1, self.seq_len)
    ins = ins.permute(0, 2, 1)
    ' Predict the future of the sequence from the latent state'
    prediction = self.decoder(ins, z)
<mask>:
        future = self.decoder_future(ins, z)
        return (prediction, future, z, mu, logvar)
    else:
        return (prediction, z, mu, logvar)",self.FUTURE_DECODER,67,self.future_decoder,False,23.643540225079384,N/A
"def forward(self, cell_output):
    self.latent_mean = self.hidden_to_mean(cell_output)
    self.latent_logvar = self.softplus(self.hidden_to_logvar(cell_output))
<mask>:
        std = self.latent_logvar.mul(0.5).exp_()
        eps = Variable(std.data.new(std.size()).normal_())
        return (eps.mul(std).add_(self.latent_mean), self.latent_mean, self.latent_logvar)
    else:
        return (self.latent_mean, self.latent_mean, self.latent_logvar)",self.training,25,self.use_std,False,21.3643503198117,N/A
"def plot_check_parameter(cfg, iqr_val, num_frames, X_true, X_med, anchor_1, anchor_2):
    plot_X_orig = np.concatenate(X_true, axis=0).T
    plot_X_med = X_med.copy()
    iqr_cutoff = cfg['iqr_factor'] * iqr_val
    plt.figure()
    plt.plot(plot_X_orig.T)
    plt.axhline(y=iqr_cutoff, color='r', linestyle='--', label='IQR cutoff')
    plt.axhline(y=-iqr_cutoff, color='r', linestyle='--')
    plt.title('Full Signal z-scored')
    plt.legend()
<mask>:
        rnd = np.random.choice(num_frames)
        plt.figure()
        plt.plot(plot_X_med[:, rnd:rnd + 1000].T)
        plt.axhline(y=iqr_cutoff, color='r', linestyle='--', label='IQR cutoff')
        plt.axhline(y=-iqr_cutoff, color='r', linestyle='--')
        plt.title('Filtered signal z-scored')
        plt.legend()
        plt.figure()
        plt.plot(plot_X_orig[:, rnd:rnd + 1000].T)
        plt.axhline(y=iqr_cutoff, color='r', linestyle='--', label='IQR cutoff')
        plt.axhline(y=-iqr_cutoff, color='r', linestyle='--')
        plt.title('Original signal z-scored')
        plt.legend()
        plt.figure()
        plt.plot(plot_X_orig[:, rnd:rnd + 1000].T, 'g', alpha=0.5)
        plt.plot(plot_X_med[:, rnd:rnd + 1000].T, '--m', alpha=0.6)
        plt.axhline(y=iqr_cutoff, color='r', linestyle='--', label='IQR cutoff')
        plt.axhline(y=-iqr_cutoff, color='r', linestyle='--')
        plt.title('Overlayed z-scored')
        plt.legend()
    else:
        plt.figure()
        plt.plot(plot_X_med.T)
        plt.axhline(y=iqr_cutoff, color='r', linestyle='--', label='IQR cutoff')
        plt.axhline(y=-iqr_cutoff, color='r', linestyle='--')
        plt.title('Filtered signal z-scored')
        plt.legend()
        plt.figure()
        plt.plot(plot_X_orig.T)
        plt.axhline(y=iqr_cutoff, color='r', linestyle='--', label='IQR cutoff')
        plt.axhline(y=-iqr_cutoff, color='r', linestyle='--')
        plt.title('Original signal z-scored')
        plt.legend()
    print('Please run the function with check_parameter=False if you are happy with the results')",num_frames > 1000,138,cfg['iqr_factor'] == 'random',False,4.767707020457095,N/A
"def traindata_aligned(cfg, files, testfraction, num_features, savgol_filter, check_parameter):
    X_train = []
    pos = []
    pos_temp = 0
    pos.append(0)
<mask>:
        X_true = []
        files = [files[0]]
    for file in files:
        print('z-scoring of file %s' % file)
        path_to_file = os.path.join(cfg['project_path'], 'data', file, file + '-PE-seq.npy')
        data = np.load(path_to_file)
        X_mean = np.mean(data, axis=None)
        X_std = np.std(data, axis=None)
        X_z = (data.T - X_mean) / X_std
        if check_parameter == True:
            X_z_copy = X_z.copy()
            X_true.append(X_z_copy)
        if cfg['robust'] == True:
            iqr_val = iqr(X_z)
            print('IQR value: %.2f, IQR cutoff: %.2f' % (iqr_val, cfg['iqr_factor'] * iqr_val))
            for i in range(X_z.shape[0]):
                for marker in range(X_z.shape[1]):
                    if X_z[i, marker] > cfg['iqr_factor'] * iqr_val:
                        X_z[i, marker] = np.nan
                    elif X_z[i, marker] < -cfg['iqr_factor'] * iqr_val:
                        X_z[i, marker] = np.nan
            X_z = interpol(X_z)
        X_len = len(data.T)
        pos_temp += X_len
        pos.append(pos_temp)
        X_train.append(X_z)
    X = np.concatenate(X_train, axis=0)
    detect_anchors = np.std(X.T, axis=1)
    sort_anchors = np.sort(detect_anchors)
    if sort_anchors[0] == sort_anchors[1]:
        anchors = np.where(detect_anchors == sort_anchors[0])[0]
        anchor_1_temp = anchors[0]
        anchor_2_temp = anchors[1]
    else:
        anchor_1_temp = int(np.where(detect_anchors == sort_anchors[0])[0])
        anchor_2_temp = int(np.where(detect_anchors == sort_anchors[1])[0])
    if anchor_1_temp > anchor_2_temp:
        anchor_1 = anchor_1_temp
        anchor_2 = anchor_2_temp
    else:
        anchor_1 = anchor_2_temp
        anchor_2 = anchor_1_temp
    X = np.delete(X, anchor_1, 1)
    X = np.delete(X, anchor_2, 1)
    X = X.T
    if savgol_filter:
        X_med = scipy.signal.savgol_filter(X, cfg['savgol_length'], cfg['savgol_order'])
    else:
        X_med = X
    num_frames = len(X_med.T)
    test = int(num_frames * testfraction)
    z_test = X_med[:, :test]
    z_train = X_med[:, test:]
    if check_parameter == True:
        plot_check_parameter(cfg, iqr_val, num_frames, X_true, X_med, anchor_1, anchor_2)
    else:
        np.save(os.path.join(cfg['project_path'], 'data', 'train', 'train_seq.npy'), z_train)
        np.save(os.path.join(cfg['project_path'], 'data', 'train', 'test_seq.npy'), z_test)
        for i, file in enumerate(files):
            np.save(os.path.join(cfg['project_path'], 'data', file, file + '-PE-seq-clean.npy'), X_med[:, pos[i]:pos[i + 1]])
        print('Lenght of train data: %d' % len(z_train.T))
        print('Lenght of test data: %d' % len(z_test.T))",check_parameter == True,272,check_parameter == True,True,100.00000000000004,N/A
"def traindata_fixed(cfg, files, testfraction, num_features, savgol_filter, check_parameter):
    X_train = []
    pos = []
    pos_temp = 0
    pos.append(0)
<mask>:
        X_true = []
        rnd_file = np.random.choice(len(files))
        files = [files[0]]
    for file in files:
        print('z-scoring of file %s' % file)
        path_to_file = os.path.join(cfg['project_path'], 'data', file, file + '-PE-seq.npy')
        data = np.load(path_to_file)
        X_mean = np.mean(data, axis=None)
        X_std = np.std(data, axis=None)
        X_z = (data.T - X_mean) / X_std
        if check_parameter == True:
            X_z_copy = X_z.copy()
            X_true.append(X_z_copy)
        if cfg['robust'] == True:
            iqr_val = iqr(X_z)
            print('IQR value: %.2f, IQR cutoff: %.2f' % (iqr_val, cfg['iqr_factor'] * iqr_val))
            for i in range(X_z.shape[0]):
                for marker in range(X_z.shape[1]):
                    if X_z[i, marker] > cfg['iqr_factor'] * iqr_val:
                        X_z[i, marker] = np.nan
                    elif X_z[i, marker] < -cfg['iqr_factor'] * iqr_val:
                        X_z[i, marker] = np.nan
                X_z[i, :] = interpol(X_z[i, :])
        X_len = len(data.T)
        pos_temp += X_len
        pos.append(pos_temp)
        X_train.append(X_z)
    X = np.concatenate(X_train, axis=0).T
    if savgol_filter:
        X_med = scipy.signal.savgol_filter(X, cfg['savgol_length'], cfg['savgol_order'])
    else:
        X_med = X
    num_frames = len(X_med.T)
    test = int(num_frames * testfraction)
    z_test = X_med[:, :test]
    z_train = X_med[:, test:]
    if check_parameter == True:
        plot_check_parameter(cfg, iqr_val, num_frames, X_true, X_med)
    else:
        np.save(os.path.join(cfg['project_path'], 'data', 'train', 'train_seq.npy'), z_train)
        np.save(os.path.join(cfg['project_path'], 'data', 'train', 'test_seq.npy'), z_test)
        for i, file in enumerate(files):
            np.save(os.path.join(cfg['project_path'], 'data', file, file + '-PE-seq-clean.npy'), X_med[:, pos[i]:pos[i + 1]])
        print('Lenght of train data: %d' % len(z_train.T))
        print('Lenght of test data: %d' % len(z_test.T))",check_parameter == True,212,check_parameter == True,True,100.00000000000004,N/A
"def create_trainset(config, check_parameter=False):
    config_file = Path(config).resolve()
    cfg = read_config(config_file)
    legacy = cfg['legacy']
    fixed = cfg['egocentric_data']
<mask>:
        os.mkdir(os.path.join(cfg['project_path'], 'data', 'train', ''))
    files = []
    if cfg['all_data'] == 'No':
        for file in cfg['video_sets']:
            use_file = input('Do you want to train on ' + file + '? yes/no: ')
            if use_file == 'yes':
                files.append(file)
            if use_file == 'no':
                continue
    else:
        for file in cfg['video_sets']:
            files.append(file)
    print('Creating training dataset...')
    if cfg['robust'] == True:
        print('Using robust setting to eliminate outliers! IQR factor: %d' % cfg['iqr_factor'])
    if fixed == False:
        print('Creating trainset from the vame.egocentrical_alignment() output ')
        traindata_aligned(cfg, files, cfg['test_fraction'], cfg['num_features'], cfg['savgol_filter'], check_parameter)
    else:
        print('Creating trainset from the vame.csv_to_numpy() output ')
        traindata_fixed(cfg, files, cfg['test_fraction'], cfg['num_features'], cfg['savgol_filter'], check_parameter)
    if check_parameter == False:
        print('A training and test set has been created. Next step: vame.train_model()')","not os.path.exists(os.path.join(cfg['project_path'], 'data', 'train', ''))",126,"not os.path.exists(os.path.join(cfg['project_path'], 'data', 'train'))",False,87.17867889044781,N/A
"def plot_reconstruction(filepath, test_loader, seq_len_half, model, model_name, FUTURE_DECODER, FUTURE_STEPS, suffix=None):
    dataiter = iter(test_loader)
    x = next(dataiter)
    x = x.permute(0, 2, 1)
<mask>:
        data = x[:, :seq_len_half, :].type('torch.FloatTensor').cuda()
        data_fut = x[:, seq_len_half:seq_len_half + FUTURE_STEPS, :].type('torch.FloatTensor').cuda()
    else:
        data = x[:, :seq_len_half, :].type('torch.FloatTensor').to()
        data_fut = x[:, seq_len_half:seq_len_half + FUTURE_STEPS, :].type('torch.FloatTensor').to()
    if FUTURE_DECODER:
        x_tilde, future, latent, mu, logvar = model(data)
        fut_orig = data_fut.cpu()
        fut_orig = fut_orig.data.numpy()
        fut = future.cpu()
        fut = fut.detach().numpy()
    else:
        x_tilde, latent, mu, logvar = model(data)
    data_orig = data.cpu()
    data_orig = data_orig.data.numpy()
    data_tilde = x_tilde.cpu()
    data_tilde = data_tilde.detach().numpy()
    if FUTURE_DECODER:
        fig, axs = plt.subplots(2, 5)
        fig.suptitle('Reconstruction [top] and future prediction [bottom] of input sequence')
        for i in range(5):
            axs[0, i].plot(data_orig[i, ...], color='k', label='Sequence Data')
            axs[0, i].plot(data_tilde[i, ...], color='r', linestyle='dashed', label='Sequence Reconstruction')
            axs[1, i].plot(fut_orig[i, ...], color='k')
            axs[1, i].plot(fut[i, ...], color='r', linestyle='dashed')
        axs[0, 0].set(xlabel='time steps', ylabel='reconstruction')
        axs[1, 0].set(xlabel='time steps', ylabel='predction')
        fig.savefig(os.path.join(filepath, 'evaluate', 'Future_Reconstruction.png'))
    else:
        fig, ax1 = plt.subplots(1, 5)
        for i in range(5):
            fig.suptitle('Reconstruction of input sequence')
            ax1[i].plot(data_orig[i, ...], color='k', label='Sequence Data')
            ax1[i].plot(data_tilde[i, ...], color='r', linestyle='dashed', label='Sequence Reconstruction')
        fig.set_tight_layout(True)
        if not suffix:
            fig.savefig(os.path.join(filepath, 'evaluate', 'Reconstruction_' + model_name + '.png'), bbox_inches='tight')
        elif suffix:
            fig.savefig(os.path.join(filepath, 'evaluate', 'Reconstruction_' + model_name + '_' + suffix + '.png'), bbox_inches='tight')",use_gpu,190,sys.platform == 'darwin',False,0.0,N/A
"def eval_temporal(cfg, use_gpu, model_name, fixed, snapshot=None, suffix=None):
    SEED = 19
    ZDIMS = cfg['zdims']
    FUTURE_DECODER = cfg['prediction_decoder']
    TEMPORAL_WINDOW = cfg['time_window'] * 2
    FUTURE_STEPS = cfg['prediction_steps']
    NUM_FEATURES = cfg['num_features']
<mask>:
        NUM_FEATURES = NUM_FEATURES - 2
    TEST_BATCH_SIZE = 64
    PROJECT_PATH = cfg['project_path']
    hidden_size_layer_1 = cfg['hidden_size_layer_1']
    hidden_size_layer_2 = cfg['hidden_size_layer_2']
    hidden_size_rec = cfg['hidden_size_rec']
    hidden_size_pred = cfg['hidden_size_pred']
    dropout_encoder = cfg['dropout_encoder']
    dropout_rec = cfg['dropout_rec']
    dropout_pred = cfg['dropout_pred']
    softplus = cfg['softplus']
    filepath = os.path.join(cfg['project_path'], 'model')
    seq_len_half = int(TEMPORAL_WINDOW / 2)
    if use_gpu:
        torch.cuda.manual_seed(SEED)
        model = RNN_VAE(TEMPORAL_WINDOW, ZDIMS, NUM_FEATURES, FUTURE_DECODER, FUTURE_STEPS, hidden_size_layer_1, hidden_size_layer_2, hidden_size_rec, hidden_size_pred, dropout_encoder, dropout_rec, dropout_pred, softplus).cuda()
        model.load_state_dict(torch.load(os.path.join(cfg['project_path'], 'model', 'best_model', model_name + '_' + cfg['Project'] + '.pkl')))
    else:
        model = RNN_VAE(TEMPORAL_WINDOW, ZDIMS, NUM_FEATURES, FUTURE_DECODER, FUTURE_STEPS, hidden_size_layer_1, hidden_size_layer_2, hidden_size_rec, hidden_size_pred, dropout_encoder, dropout_rec, dropout_pred, softplus).to()
        if not snapshot:
            model.load_state_dict(torch.load(os.path.join(cfg['project_path'], 'model', 'best_model', model_name + '_' + cfg['Project'] + '.pkl'), map_location=torch.device('cpu')))
        elif snapshot:
            model.load_state_dict(torch.load(snapshot), map_location=torch.device('cpu'))
    model.eval()
    testset = SEQUENCE_DATASET(os.path.join(cfg['project_path'], 'data', 'train', ''), data='test_seq.npy', train=False, temporal_window=TEMPORAL_WINDOW)
    test_loader = Data.DataLoader(testset, batch_size=TEST_BATCH_SIZE, shuffle=True, drop_last=True)
    if not snapshot:
        plot_reconstruction(filepath, test_loader, seq_len_half, model, model_name, FUTURE_DECODER, FUTURE_STEPS)
    elif snapshot:
        plot_reconstruction(filepath, test_loader, seq_len_half, model, model_name, FUTURE_DECODER, FUTURE_STEPS, suffix=suffix)
    if use_gpu:
        plot_loss(cfg, filepath, model_name)
    else:
        plot_loss(cfg, filepath, model_name)",fixed == False,179,NUM_FEATURES > 2,False,0.0,N/A
"def evaluate_model(config, use_snapshots=False):
    """"""
    Evaluation of testset.
        
    Parameters
    ----------
    config : str
        Path to config file.
    model_name : str
        name of model (same as in config.yaml)
    use_snapshots : bool
        Whether to plot for all snapshots or only the best model.
    """"""
    config_file = Path(config).resolve()
    cfg = read_config(config_file)
    model_name = cfg['model_name']
    fixed = cfg['egocentric_data']
<mask>:
        os.mkdir(os.path.join(cfg['project_path'], 'model', 'evaluate'))
    use_gpu = torch.cuda.is_available()
    if use_gpu:
        print('Using CUDA')
        print('GPU active:', torch.cuda.is_available())
        print('GPU used:', torch.cuda.get_device_name(0))
    else:
        torch.device('cpu')
        print('CUDA is not working, or a GPU is not found; using CPU!')
    print('\n\nEvaluation of %s model. \n' % model_name)
    if not use_snapshots:
        eval_temporal(cfg, use_gpu, model_name, fixed)
    elif use_snapshots:
        snapshots = os.listdir(os.path.join(cfg['project_path'], 'model', 'best_model', 'snapshots'))
        for snap in snapshots:
            fullpath = os.path.join(cfg['project_path'], 'model', 'best_model', 'snapshots', snap)
            epoch = snap.split('_')[-1]
            eval_temporal(cfg, use_gpu, model_name, fixed, snapshot=fullpath, suffix='snapshot' + str(epoch))
    print(""You can find the results of the evaluation in '/Your-VAME-Project-Apr30-2020/model/evaluate/' \nOPTIONS:\n- vame.pose_segmentation() to identify behavioral motifs.\n- re-run the model for further fine tuning. Check again with vame.evaluate_model()"")","not os.path.exists(os.path.join(cfg['project_path'], 'model', 'evaluate'))",155,"not os.path.exists(os.path.join(cfg['project_path'], 'model', 'evaluate'))",True,100.00000000000004,N/A
"def __init__(self, path_to_file, data, train, temporal_window):
    self.temporal_window = temporal_window
    self.X = np.load(path_to_file + data)
<mask>:
        self.X = self.X.T
    self.data_points = len(self.X[0, :])
    if train and (not os.path.exists(os.path.join(path_to_file, 'seq_mean.npy'))):
        print('Compute mean and std for temporal dataset.')
        self.mean = np.mean(self.X)
        self.std = np.std(self.X)
        np.save(path_to_file + 'seq_mean.npy', self.mean)
        np.save(path_to_file + 'seq_std.npy', self.std)
    else:
        self.mean = np.load(path_to_file + 'seq_mean.npy')
        self.std = np.load(path_to_file + 'seq_std.npy')
    if train:
        print('Initialize train data. Datapoints %d' % self.data_points)
    else:
        print('Initialize test data. Datapoints %d' % self.data_points)",self.X.shape[0] > self.X.shape[1],77,"isinstance(self.X, np.ndarray)",False,8.334915981947734,N/A
"def kl_annealing(epoch, kl_start, annealtime, function):
    """"""
        Annealing of Kullback-Leibler loss to let the model learn first
        the reconstruction of the data before the KL loss term gets introduced.
    """"""
<mask>:
        if function == 'linear':
            new_weight = min(1, (epoch - kl_start) / annealtime)
        elif function == 'sigmoid':
            new_weight = float(1 / (1 + np.exp(-0.9 * (epoch - annealtime))))
        else:
            raise NotImplementedError('currently only ""linear"" and ""sigmoid"" are implemented')
        return new_weight
    else:
        new_weight = 0
        return new_weight",epoch > kl_start,74,np.isnan(epoch - kl_start),False,16.784459625186194,N/A
"def gaussian(ins, is_training, seq_len, std_n=0.8):
<mask>:
        emp_std = ins.std(1) * std_n
        emp_std = emp_std.unsqueeze(2).repeat(1, 1, seq_len)
        emp_std = emp_std.permute(0, 2, 1)
        noise = Variable(ins.data.new(ins.size()).normal_(0, 1))
        return ins + noise * emp_std
    return ins",is_training,33,is_training,True,100.00000000000004,N/A
"def train(train_loader, epoch, model, optimizer, anneal_function, BETA, kl_start, annealtime, seq_len, future_decoder, future_steps, scheduler, mse_red, mse_pred, kloss, klmbda, bsize, noise):
    model.train()
    train_loss = 0.0
    mse_loss = 0.0
    kullback_loss = 0.0
    kmeans_losses = 0.0
    fut_loss = 0.0
    loss = 0.0
    seq_len_half = int(seq_len / 2)
    for idx, data_item in enumerate(train_loader):
        data_item = Variable(data_item)
        data_item = data_item.permute(0, 2, 1)
<mask>:
            data = data_item[:, :seq_len_half, :].type('torch.FloatTensor').cuda()
            fut = data_item[:, seq_len_half:seq_len_half + future_steps, :].type('torch.FloatTensor').cuda()
        else:
            data = data_item[:, :seq_len_half, :].type('torch.FloatTensor').to()
            fut = data_item[:, seq_len_half:seq_len_half + future_steps, :].type('torch.FloatTensor').to()
        if noise == True:
            data_gaussian = gaussian(data, True, seq_len_half)
        else:
            data_gaussian = data
        if future_decoder:
            data_tilde, future, latent, mu, logvar = model(data_gaussian)
            rec_loss = reconstruction_loss(data, data_tilde, mse_red)
            fut_rec_loss = future_reconstruction_loss(fut, future, mse_pred)
            kmeans_loss = cluster_loss(latent.T, kloss, klmbda, bsize)
            kl_loss = kullback_leibler_loss(mu, logvar)
            kl_weight = kl_annealing(epoch, kl_start, annealtime, anneal_function)
            loss = rec_loss + fut_rec_loss + BETA * kl_weight * kl_loss + kl_weight * kmeans_loss
            fut_loss += fut_rec_loss.item()
        else:
            data_tilde, latent, mu, logvar = model(data_gaussian)
            rec_loss = reconstruction_loss(data, data_tilde, mse_red)
            kl_loss = kullback_leibler_loss(mu, logvar)
            kmeans_loss = cluster_loss(latent.T, kloss, klmbda, bsize)
            kl_weight = kl_annealing(epoch, kl_start, annealtime, anneal_function)
            loss = rec_loss + BETA * kl_weight * kl_loss + kl_weight * kmeans_loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
        mse_loss += rec_loss.item()
        kullback_loss += kl_loss.item()
        kmeans_losses += kmeans_loss.item()
    scheduler.step(loss)
    if future_decoder:
        print('Train loss: {:.3f}, MSE-Loss: {:.3f}, MSE-Future-Loss {:.3f}, KL-Loss: {:.3f}, Kmeans-Loss: {:.3f}, weight: {:.2f}'.format(train_loss / idx, mse_loss / idx, fut_loss / idx, BETA * kl_weight * kullback_loss / idx, kl_weight * kmeans_losses / idx, kl_weight))
    else:
        print('Train loss: {:.3f}, MSE-Loss: {:.3f}, KL-Loss: {:.3f}, Kmeans-Loss: {:.3f}, weight: {:.2f}'.format(train_loss / idx, mse_loss / idx, BETA * kl_weight * kullback_loss / idx, kl_weight * kmeans_losses / idx, kl_weight))
    return (kl_weight, train_loss / idx, kl_weight * kmeans_losses / idx, kullback_loss / idx, mse_loss / idx, fut_loss / idx)",use_gpu,290,future_decoder,False,27.516060407455225,N/A
"def test(test_loader, epoch, model, optimizer, BETA, kl_weight, seq_len, mse_red, kloss, klmbda, future_decoder, bsize):
    model.eval()
    test_loss = 0.0
    mse_loss = 0.0
    kullback_loss = 0.0
    kmeans_losses = 0.0
    loss = 0.0
    seq_len_half = int(seq_len / 2)
    with torch.no_grad():
        for idx, data_item in enumerate(test_loader):
            data_item = Variable(data_item)
            data_item = data_item.permute(0, 2, 1)
<mask>:
                data = data_item[:, :seq_len_half, :].type('torch.FloatTensor').cuda()
            else:
                data = data_item[:, :seq_len_half, :].type('torch.FloatTensor').to()
            if future_decoder:
                recon_images, _, latent, mu, logvar = model(data)
                rec_loss = reconstruction_loss(data, recon_images, mse_red)
                kl_loss = kullback_leibler_loss(mu, logvar)
                kmeans_loss = cluster_loss(latent.T, kloss, klmbda, bsize)
                loss = rec_loss + BETA * kl_weight * kl_loss + kl_weight * kmeans_loss
            else:
                recon_images, latent, mu, logvar = model(data)
                rec_loss = reconstruction_loss(data, recon_images, mse_red)
                kl_loss = kullback_leibler_loss(mu, logvar)
                kmeans_loss = cluster_loss(latent.T, kloss, klmbda, bsize)
                loss = rec_loss + BETA * kl_weight * kl_loss + kl_weight * kmeans_loss
            test_loss += loss.item()
            mse_loss += rec_loss.item()
            kullback_loss += kl_loss.item()
            kmeans_losses += kmeans_loss
    print('Test loss: {:.3f}, MSE-Loss: {:.3f}, KL-Loss: {:.3f}, Kmeans-Loss: {:.3f}'.format(test_loss / idx, mse_loss / idx, BETA * kl_weight * kullback_loss / idx, kl_weight * kmeans_losses / idx))
    return (mse_loss / idx, test_loss / idx, kl_weight * kmeans_losses)",use_gpu,181,future_decoder,False,27.516060407455225,N/A
"def generate_links(path):
<mask>:
        return [dict(source=path[n], target=path[n + 1], type='path') for n in range(len(path) - 1)]
    return []",path and len(path) >= 2,17,len(path) > 1,False,46.08636396914616,N/A
"def enrich_report(self, report):
    for key, host in report.hosts.items():
<mask>:
            cves = self.software_enrich(host.software)
            if cves:
                host.cves.append(cves)
        if host.os:
            cves = self.host_enrich(host.os)
            if cves:
                host.cves.append(cves)
        report.hosts[key] = host
    return report",host.software,28,host.software,True,100.00000000000004,N/A
"def software_enrich(self, software):
    exploits = []
    for soft in software:
<mask>:
            try:
                cves = cve.keyword_cve(soft.subtype)
            except Exception as e:
                self.log.error(f'exception when enriching: {repr(e)}')
                continue
            ids = [cve.id for cve in cves]
            if ids:
                exploits.append(ids)
    return exploits",soft.subtype,36,soft.subtype,True,100.00000000000004,N/A
"def host_enrich(self, os):
    exploits = []
    try:
        cves = cve.keyword_cve(os.os_type)
    except Exception as e:
        self.log.error(f'exception when enriching: {repr(e)}')
        return []
    ids = [cve.id for cve in cves if isinstance(cve, CVE)]
<mask>:
        exploits.append(ids)
    return exploits",ids,34,ids,True,100.00000000000004,N/A
"def parse_json_report(self, siesta_report, name):
    report = VulnerabilityReport(name=name)
    hosts = siesta_report['facts']['components']
    all_ports = siesta_report['facts']['ports']
    all_vulnerabilities = siesta_report['facts']['vulnerabilities']
    for h in hosts:
        host = Host(h['target'], hostname=h['host_name'])
        ports = [p for p in all_ports if p['target'] == host.ip]
        for p in ports:
            port = Port(p['port_number'], protocol=p['protocol'], service=p['service'], state=p['port_state'])
            vulnerabilities = [v for v in all_vulnerabilities if v['target'] == host.ip and v['port_number'] == port.number]
            for v in vulnerabilities:
<mask>:
                    port.cves.append(v['check_id'])
                    host.cves.append(v['check_id'])
            host.ports[port.number] = port
        report.hosts[host.ip] = host
    return report",v['severity'] != '0 - info',75,v['check_id'] not in port.cves,False,8.29519350710986,N/A
"def generate_network_map(self, report):
    network_map = nx.Graph()
    for host in report.hosts.values():
        network_map.add_node(host.hostname)
        for h2 in report.hosts.values():
<mask>:
                network_map.add_edge(host.hostname, h2.hostname)
    report.network_map = network_map",h2 != host,21,h2.hostname != host.hostname,False,22.089591134157878,N/A
"def parse_xml_report(self, root, name=None):
    cve_pattern = '(CVE-\\d{4}-\\d{4,})'
    report = VulnerabilityReport(name=name)
    for host in root.findall('host'):
        host_exists = False
        cves = []
        report_host = Host(host.find('address').get('addr'))
<mask>:
            if host.find('hostnames').find('hostname') is not None:
                report_host.hostname = host.find('hostnames').find('hostname').get('name')
        for port in host.find('ports').findall('port'):
            report_port = Port(port.get('portid'))
            report_port.protocol = port.get('protocol', '')
            port_state = port.find('state')
            if port_state is not None:
                state = port_state.get('state')
                if state == 'filtered':
                    continue
                report_port.state = state
            host_exists = True
            port_service = port.find('service')
            if port_service is not None:
                report_port.service = port_service.get('name')
                report_port.product = port_service.get('product')
                report_port.version = port_service.get('version')
            for script in port.findall('script'):
                if script.get('output') is not None:
                    script_output = script.get('output')
                    port_cves = list(set(re.findall(cve_pattern, script_output)))
                    report_port.cves = port_cves
                    cves.extend(port_cves)
            report_host.ports[report_port.number] = report_port
        report_host.cves = cves
        if host_exists:
            report.hosts[report_host.ip] = report_host
    return report",host.find('hostnames') is not None,115,host.find('hostnames').find('hostname') is not None,False,51.78745926965375,N/A
"def generate_network_map(self, report):
<mask>:
        return
    network_map = nx.Graph()
    for host_object in report.hosts.values():
        network_map.add_node(host_object.ip)
        for host2 in report.hosts.values():
            if host2 != host_object:
                network_map.add_edge(host_object.ip, host2.ip)
    report.network_map = network_map",report.network_map_nodes and report.network_map_edges,26,report.network_map,False,13.533528323661276,N/A
"def generate_network_map(self, report):
<mask>:
        return
    network_map = nx.Graph()
    for host1 in report.hosts.values():
        network_map.add_node(host1.ip)
        for host2 in report.hosts.values():
            if host2 != host1:
                network_map.add_edge(host1.ip, host2.ip)
    report.network_map = network_map",report.network_map_nodes and report.network_map_edges,26,report.network_map,False,13.533528323661276,N/A
"def store(self, ram):
    existing = self.retrieve(ram['vulnerabilityreports'], self.unique)
<mask>:
        ram['vulnerabilityreports'].append(self)
        return self.retrieve(ram['vulnerabilityreports'], self.unique)
    existing.update('name', self.name)
    existing.update('hosts', self.hosts)
    existing.update('network_map', self.network_map)
    return existing",not existing,20,existing is None,False,27.516060407455225,N/A
"def keyword_cve(keyword, exact_match=False):
    """"""Map CVEs to arbitrary keywords using an API call to the NIST CVE database.

    Args:
        keyword (str or list): CVE search keyword(s).
        exact_match (bool): Software/service version (default: False).

    Returns (list): of CVE objects
    """"""
    sess = _get_sess()
<mask>:
        keyword_url = f'{CVE_KEYWORD_URL}{keyword}'
    elif isinstance(keyword, list):
        keyword_search = '+'.join(keyword)
        keyword_url = f'{CVE_KEYWORD_URL}{keyword_search}'
    else:
        raise ValueError('Must supply string or list.')
    if exact_match:
        keyword_url = f'{keyword_url}?isExactMatch=true'
    cve_json = sess.get(keyword_url, verify=False, timeout=8).json()
    cve = []
    for cve_ in cve_json['result']['CVE_Items']:
        r = sess.get(url=f""{CVE_SEARCH_URL}/cve/{cve_['cve']['CVE_data_meta']['ID']}"", verify=False, timeout=8).json()
        cve.append(_create_pyd_cve(r))
    return cve","isinstance(keyword, str)",85,"isinstance(keyword, str)",True,100.00000000000004,N/A
"def match_cve(service, service_version=None, os=None, os_version=None, greedy=False, exact_match=False):
    """"""Find CVEs from CPE match strings using an API call to the NIST CVE database.

    Args:
        service (str): Software/service name.
        service_version (str): Software/service version.
        os (str): Operating system name.
        os_version (str): Operating system version.
        greedy (bool): Boolean to determine whether OS and version data is
          omited when obtaining CVEs (default: False)
        exact_match (bool): Software/service version (default: False).

    Returns (list): of CVE objects
    """"""
    sess = _get_sess()
    match_url = f""{CPE_MATCH_URL}cpe:2.3:a:*:{service}:{service_version or '*'}""
<mask>:
        match_url = f'{match_url}?isExactMatch=true'
    cve_json = sess.get(match_url, verify=False, timeout=8).json()
    cve = []
    for cve_ in cve_json['result']['CVE_Items']:
        r = sess.get(url=f""{CVE_SEARCH_URL}/cve/{cve_['cve']['CVE_data_meta']['ID']}"", verify=False, timeout=8).json()
        match = _create_pyd_cve(r)
        if greedy or not os:
            cve.append(match)
        else:
            try:
                match_e_os = match.e_os
                for k, v in match_e_os.items():
                    if os in k and ('-' in v or os_version in v):
                        cve.append(match)
            except (KeyError, ValueError):
                continue
                cve.append(match)
    return cve",exact_match,138,exact_match,True,100.00000000000004,N/A
"def get_cve_id(full_cve):
    """"""Get CVE id from full CVE dictionary returned by API call.

    Args:
        full_cve (dict): Full CVE data as a JSON dictionary from API call.

    Returns (list): list of unique CVE id's found in the full CVE dictionary.
    """"""
    cves = []
    cve = None
    for json in full_cve[0]['data']:
<mask>:
            cve = _get_cve_id(json)
        if cve:
            cves.append(cve)
    return cves",json != '',59,json['type'] == 'CVE',False,7.809849842300637,N/A
"def _get_service_query(service_name, service_version='', os_name='', os_version='', greedy=False):
    """"""Create a query using the system configuration, as described by software and OS on host.

    Args:
        service_name (str); Software/service name.
        service_version (str): Software/service version.
        os_name (str): Operating system name.
        os_version (str): Operating system version.
        greedy (bool): Boolean to determine whether OS and version data
          is omited when obtaining CVEs (default: False)

    Returns (dict): Complete search query for API call.
    """"""
<mask>:
        return _get_search_filter(service_name=service_name, service_version='', os_name='', os_version='')
    return _get_search_filter(service_name=service_name, service_version=service_version, os_name=os_name, os_version=os_version)",greedy,77,greedy,True,100.00000000000004,N/A
"def _get_vulnerable_configs(full_cve):
    """"""Return the vulnerable configurations as a list from CVE JSON.

    Args:
        full_cve (dict): Full CVE data as a JSON dictionary from API call.

    Returns (list): list of vulnerable configuration details
    """"""
<mask>:
        return full_cve['vulnerable_configuration']
    else:
        return []",'vulnerable_configuration' in full_cve,39,'vulnerable_configuration' in full_cve,True,100.00000000000004,N/A
"def bootstrap(tmpdir=None):
    import pip
    from pip.commands.install import InstallCommand
    from pip.req import InstallRequirement

    class CertInstallCommand(InstallCommand):

        def parse_args(self, args):
<mask>:
                self.parser.defaults['cert'] = cert_path
            return super(CertInstallCommand, self).parse_args(args)
    pip.commands_dict['install'] = CertInstallCommand
    implicit_pip = True
    implicit_setuptools = True
    implicit_wheel = True
    if '--no-setuptools' in sys.argv or os.environ.get('PIP_NO_SETUPTOOLS'):
        args = [x for x in sys.argv[1:] if x != '--no-setuptools']
        implicit_setuptools = False
    else:
        args = sys.argv[1:]
    if '--no-wheel' in args or os.environ.get('PIP_NO_WHEEL'):
        args = [x for x in args if x != '--no-wheel']
        implicit_wheel = False
    if implicit_setuptools:
        try:
            import setuptools
            implicit_setuptools = False
        except ImportError:
            pass
    if implicit_wheel:
        try:
            import wheel
            implicit_wheel = False
        except ImportError:
            pass
    for arg in args:
        try:
            req = InstallRequirement.from_line(arg)
        except:
            continue
        if implicit_pip and req.name == 'pip':
            implicit_pip = False
        elif implicit_setuptools and req.name == 'setuptools':
            implicit_setuptools = False
        elif implicit_wheel and req.name == 'wheel':
            implicit_wheel = False
    if implicit_pip:
        args += ['pip']
    if implicit_setuptools:
        args += ['setuptools']
    if implicit_wheel:
        args += ['wheel']
    delete_tmpdir = False
    try:
        if tmpdir is None:
            tmpdir = tempfile.mkdtemp()
            delete_tmpdir = True
        cert_path = os.path.join(tmpdir, 'cacert.pem')
        with open(cert_path, 'wb') as cert:
            cert.write(pkgutil.get_data('pip._vendor.requests', 'cacert.pem'))
        sys.exit(pip.main(['install', '--upgrade'] + args))
    finally:
        if delete_tmpdir and tmpdir:
            shutil.rmtree(tmpdir, ignore_errors=True)",not self.parser.get_default_values().cert,190,self.parser.get_default_values()['cert'] is None,False,65.25452579142082,N/A
"def get_logger(name=None, log_path=settings.LOG_PATH):
    """"""
    get logger by name and store it
    :param name:
    :return:
    """"""
    global loggers
<mask>:
        name = __name__
    if loggers.get(name):
        return loggers.get(name)
    log_dir = dirname(log_path)
    if not exists(log_dir):
        makedirs(log_dir)
    logger = logging.getLogger(name)
    logger.setLevel(settings.LOG_LEVEL)
    if settings.LOG_ENABLED and settings.LOG_TO_CONSOLE:
        stream_handler = logging.StreamHandler(sys.stdout)
        stream_handler.setLevel(level=settings.LOG_LEVEL)
        formatter = logging.Formatter(settings.LOG_FORMAT)
        stream_handler.setFormatter(formatter)
        logger.addHandler(stream_handler)
    if settings.LOG_ENABLED and settings.LOG_TO_FILE:
        file_handler = logging.FileHandler(log_path, encoding='utf-8')
        file_handler.setLevel(level=settings.LOG_LEVEL)
        formatter = logging.Formatter(settings.LOG_FORMAT)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    loggers[name] = logger
    return logger",not name,68,name is None,False,27.516060407455225,N/A
"def get_random_cookies(self):
    try:
        response = requests.get(self.cookies_url)
<mask>:
            cookies = json.loads(response.text)
            return cookies
    except requests.ConnectionError:
        return False",response.status_code == 200,16,response.status_code == 200,True,100.00000000000004,N/A
"def process_request(self, request, spider):
    self.logger.debug('Getting Cookies')
    cookies = self.get_random_cookies()
<mask>:
        request.cookies = cookies
        self.logger.debug('Using Cookies {cookies}'.format(cookies=json.dumps(cookies)))",cookies,16,cookies,True,100.00000000000004,N/A
"def get_random_proxy(self):
    try:
        response = requests.get(self.proxy_url)
<mask>:
            proxy = response.text
            return proxy
    except requests.ConnectionError:
        return False",response.status_code == 200,16,response.status_code == 200,True,100.00000000000004,N/A
"def process_request(self, request, spider):
<mask>:
        proxy = self.get_random_proxy()
        if proxy:
            uri = 'https://{proxy}'.format(proxy=proxy)
            self.logger.debug('Using Proxy {proxy}'.format(proxy=proxy))
            request.meta['proxy'] = uri",request.meta.get('retry_times') >= self.proxy_fail_times,19,request.method == 'OPTIONS',False,2.058073185415509,N/A
"def render(self, url, retries=1, script=None, wait=0.3, scrolldown=False, sleep=0, timeout=8.0, keep_page=False):
    """"""
        render page with pyppeteer
        :param url: page url
        :param retries: max retry times
        :param script: js script to evaluate
        :param wait: number of seconds to wait before loading the page, preventing timeouts
        :param scrolldown: how many times to page down
        :param sleep: how many long to sleep after initial render
        :param timeout: the longest wait time, otherwise raise timeout error
        :param keep_page: keep page not to be closed, browser object needed
        :param browser: pyppetter browser object
        :param with_result: return with js evaluation result
        :return: content, [result]
        """"""
    browser = self.loop.run_until_complete(pyppeteer.launch(headless=True, handleSIGTERM=False, handleSIGINT=False))

    async def async_render(url, script, scrolldown, sleep, wait, timeout, keep_page):
        try:
            page = await browser.newPage()
            await asyncio.sleep(wait)
            response = await page.goto(url, options={'timeout': int(timeout * 1000)})
<mask>:
                return (None, None, response.status)
            result = None
            if script:
                result = await page.evaluate(script)
            if scrolldown:
                for _ in range(scrolldown):
                    await page._keyboard.down('PageDown')
                    await asyncio.sleep(sleep)
            else:
                await asyncio.sleep(sleep)
            if scrolldown:
                await page._keyboard.up('PageDown')
            content = await page.content()
            return (content, result, response.status)
        except TimeoutError:
            return (None, None, 500)
        finally:
            if not keep_page:
                await page.close()
    content, result, status = [None] * 3
    for i in range(retries):
        if not content:
            content, result, status = self.loop.run_until_complete(async_render(url=url, script=script, sleep=sleep, wait=wait, scrolldown=scrolldown, timeout=timeout, keep_page=keep_page))
        else:
            break
    self.loop.run_until_complete(browser.close())
    return (content, result, status)",response.status != 200,210,response.status != 200,True,100.00000000000004,N/A
"def process_request(self, request, spider):
    """"""
        :param request: request object
        :param spider: spider object
        :return: HtmlResponse
        """"""
<mask>:
        try:
            html, result, status = self.render(request.url, **self.args)
            return HtmlResponse(url=request.url, body=html, request=request, encoding='utf-8', status=status)
        except websockets.exceptions.ConnectionClosed:
            pass",request.meta.get('render'),33,spider.is_connected(),False,7.492442692259767,N/A
"def add_usage(self, usage, actions, groups, prefix=None):
<mask>:
        prefix = 'Usage: '
    return super(CapitalisedHelpFormatter, self).add_usage(usage, actions, groups, prefix)",prefix is None,17,prefix is None,True,100.00000000000004,N/A
"def format_help(self):
<mask>:
        self.formatter._indent()
    join = self.formatter._join_parts
    item_help = join([func(*args) for func, args in self.items])
    if self.parent is not None:
        self.formatter._dedent()
    if not item_help:
        return ''
    if self.heading is not argparse.SUPPRESS and self.heading is not None:
        current_indent = self.formatter._current_indent
        if self.heading == optional_title:
            heading = '%*s%s:\n' % (current_indent, '', self.heading)
        else:
            heading = '%*s%s:' % (current_indent, '', self.heading)
    else:
        heading = ''
    return join(['\n', heading, item_help])",self.parent is not None,66,self.parent is not None,True,100.00000000000004,N/A
"def cmd():
    """"""
    run from cmd
    :return:
    """"""
    args = parser.parse_args()
    command = args.command
<mask>:
        from gerapy.cmd.init import init
        init(args.folder)
    elif command == 'generate':
        from gerapy.cmd.generate import generate
        generate(args.project)
    elif command == 'parse':
        from gerapy.cmd.parse import parse
        parse(args)
    elif command == 'initadmin':
        from gerapy.cmd.initadmin import initadmin
        initadmin()
    else:
        from gerapy.server.manage import manage
        manage()",command == 'init',53,command == 'init',True,100.00000000000004,N/A
"def initadmin():
    """"""
    create super user
    :return:
    """"""
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gerapy.server.server.settings')
    django.setup()
    from django.contrib.auth.models import User
    admins = User.objects.filter(is_superuser=True)
<mask>:
        print('Admin user already exists, you can use them to login:')
        for admin in admins:
            print('- %s(%s)' % (admin.username, admin.email))
    else:
        print('No Admin user exists, create temp admin user')
        for user in settings.ADMINS:
            username = user
            email = '%s@gerapy.com' % username
            password = username
            admin, created = User.objects.update_or_create(email=email, username=username)
            admin.set_password(password)
            admin.is_active = True
            admin.is_superuser = True
            admin.is_staff = True
            admin.save()
            print('%s admin account: %s(%s), initial password: %s, just use it temporarily and change the password for safety' % ('Created' if created else 'Reset', username, email, password))",admins,104,admins.count(),False,10.682175159905848,N/A
"def parse(args):
    """"""
    parse result by request args
    :param args:
    :return:
    """"""
    project_path = join(args.dir, args.project)
<mask>:
        results = get_start_requests(project_path, args.spider)
    else:
        results = get_follow_requests_and_items(project_path, args.spider, args)
    print(json.dumps(results, ensure_ascii=False))",args.start,29,args.start,True,100.00000000000004,N/A
"def __init__(self, link_extractor, method='GET', data=None, params=None, headers=None, callback=None, cb_kwargs=None, follow=None, priority=0, dont_filter=False, meta=None, proxy=None, render=False, dont_redirect=None, dont_retry=None, handle_httpstatus_list=None, handle_httpstatus_all=None, dont_cache=None, dont_obey_robotstxt=None, download_timeout=None, max_retry_times=None, process_links=None, process_request=lambda x: x, process_body=None):
    self.link_extractor = link_extractor
    self.callback = callback
    self.method = method
    self.data = str2body(data)
    self.params = str2dict(params)
    self.headers = str2dict(headers)
    self.priority = priority
    self.dont_filter = dont_filter
    self.meta = str2dict(meta)
    self.cb_kwargs = str2dict(cb_kwargs)
    self.proxy = proxy
    self.render = render
    self.dont_redirect = dont_redirect
    self.dont_retry = dont_retry
    self.handle_httpstatus_list = str2list(handle_httpstatus_list, lambda x: int(x))
    self.handle_httpstatus_all = handle_httpstatus_all
    self.dont_cache = dont_cache
    self.dont_obey_robotstxt = dont_obey_robotstxt
    self.download_timeout = download_timeout
    self.max_retry_times = max_retry_times
    self.process_links = process_links
    self.process_request = process_request
    self.process_body = process_body
<mask>:
        self.follow = False if callback else True
    else:
        self.follow = follow",follow is None,112,follow is None,True,100.00000000000004,N/A
"def _generate_request(self, index, rule, link, response):
    """"""
        generate request by rule
        :param index: rule index
        :param rule: rule object
        :param link: link object
        :return: new request object
        """"""
    url = furl(link.url).add(rule.params).url if rule.params else link.url
    body = None
<mask>:
        if callable(rule.process_body):
            body = rule.process_body(response)
        if rule.data:
            body = rule.data
    r = Request(url=url, method=rule.method, body=body, headers=rule.headers, priority=rule.priority, dont_filter=rule.dont_filter, callback=self._response_downloaded)
    r.meta.update(**rule.meta)
    meta_items = ['dont_redirect', 'dont_retry', 'handle_httpstatus_list', 'handle_httpstatus_all', 'dont_cache', 'dont_obey_robotstxt', 'download_timeout', 'max_retry_times', 'proxy', 'render']
    meta_args = {meta_item: getattr(rule, meta_item) for meta_item in meta_items if not getattr(rule, meta_item) is None}
    r.meta.update(**meta_args)
    return r",rule.method.upper() == 'POST',89,rule.process_body,False,7.859505256643253,N/A
"def _requests_to_follow(self, response):
    """"""
        requests to follow
        :param response:
        :return:
        """"""
    seen = set()
    for index, rule in enumerate(self._rules):
        links = [lnk for lnk in rule.link_extractor.extract_links(response) if lnk not in seen]
<mask>:
            links = rule.process_links(links)
        for link in links:
            seen.add(link)
            r = self._generate_request(index, rule, link, response)
            yield rule.process_request(r)",links and rule.process_links,48,links,False,0.24787521766663595,N/A
"def get_value(self, dict, *keys):
<mask>:
        return dict.get(keys[0])
    else:
        result = []
        for key in keys:
            result.append(dict.get(key))
    return result",len(keys) == 1,18,len(keys) == 1,True,100.00000000000004,N/A
"def get_slice(self, list, start, end=None):
<mask>:
        return list
    else:
        return list[start:end]",start == '*',11,end is None,False,0.0,N/A
"def extract_links(self, response):
    result = json.loads(response.text)
    for pattern in self.patterns:
        extractors = pattern.get('extractors')
        format = pattern.get('format')
        data = result
        for extractor in extractors:
            type = extractor.get('type')
<mask>:
                if type == 'value':
                    data = self.get_value(*[data] + extractor.get('args'))
            elif isinstance(data, list):
                if type == 'value':
                    data = [self.get_value(*[item] + extractor.get('args')) for item in data]
                elif type == 'slice':
                    data = self.get_slice(*[data] + extractor.get('args'))
        if not isinstance(data, list):
            data = [data]
        all_links = [Link(response.urljoin(format.format(*[item]))) if not isinstance(item, list) else Link(response.urljoin(format.format(*item))) for item in data]
        return unique_list(all_links)","isinstance(data, dict)",83,"isinstance(data, dict)",True,100.00000000000004,N/A
"def _requests_to_follow(self, response):
    seen = set()
    for n, rule in enumerate(self._rules):
        links = [lnk for lnk in rule.link_extractor.extract_links(response) if lnk not in seen]
<mask>:
            links = rule.process_links(links)
        for link in links:
            seen.add(link)
            r = self._build_request(n, link)
            yield rule.process_request(r)",links and rule.process_links,38,links,False,0.24787521766663595,N/A
"def retry_on_eintr(function, *args, **kw):
    """"""Run a function and retry it while getting EINTR errors""""""
    while True:
        try:
            return function(*args, **kw)
        except IOError as e:
<mask>:
                raise",e.errno != errno.EINTR,26,e.errno != errno.EINTR,True,100.00000000000004,N/A
"def build_egg(project):
    """"""
    build project to egg file
    :param project: 
    :return: 
    """"""
    work_path = os.getcwd()
    try:
        path = os.path.abspath(join(os.getcwd(), PROJECTS_FOLDER))
        project_path = join(path, project)
        os.chdir(project_path)
        settings = config(project_path, 'settings', 'default')
        setup_file_path = join(project_path, 'setup.py')
        create_default_setup_py(setup_file_path, settings=settings, project=project)
        d = tempfile.mkdtemp(prefix='gerapy-')
        o = open(os.path.join(d, 'stdout'), 'wb')
        e = open(os.path.join(d, 'stderr'), 'wb')
        retry_on_eintr(check_call, [sys.executable, 'setup.py', 'clean', '-a', 'bdist_uberegg', '-d', d], stdout=o, stderr=e)
        o.close()
        e.close()
        egg = glob.glob(os.path.join(d, '*.egg'))[0]
<mask>:
            os.remove(join(project_path, find_egg(project_path)))
        shutil.move(egg, project_path)
        return join(project_path, find_egg(project_path))
    except Exception as e:
        logger.error('error occurred %s', e.args)
    finally:
        os.chdir(work_path)",find_egg(project_path),84,"os.path.exists(join(project_path, find_egg(egg)))",False,23.972125922151484,N/A
"def find_egg(path):
    """"""
    find egg from path
    :param path:
    :return:
    """"""
    items = os.listdir(path)
    for name in items:
<mask>:
            return name",name.endswith('.egg'),21,name.startswith('egg__'),False,11.99014838091355,N/A
"def create_default_setup_py(path, **kwargs):
    """"""
    create setup.py file to path
    :param path:
    :param kwargs:
    :return:
    """"""
<mask>:
        logger.debug('setup.py file already exists at %s', path)
    else:
        with open(path, 'w', encoding='utf-8') as f:
            file = _SETUP_PY_TEMPLATE % kwargs
            f.write(file)
            f.close()
            logger.debug('successfully created setup.py file at %s', path)",os.path.exists(path),44,os.path.exists(path),True,100.00000000000004,N/A
"def __init__(self, data, encoder=JSONEncoder, safe=False, json_dumps_params=None, **kwargs):
<mask>:
        raise TypeError('In order to allow non-dict objects to be serialized set the safe parameter to False.')
    if json_dumps_params is None:
        json_dumps_params = {}
    kwargs.setdefault('content_type', 'application/json')
    data = json.dumps(data, cls=encoder, **json_dumps_params)
    super(JsonResponse, self).__init__(content=data, **kwargs)","safe and (not isinstance(data, dict))",41,"safe and (not isinstance(data, dict))",True,100.00000000000004,N/A
"def default(self, o):
<mask>:
        return timezone.localtime(o).strftime(DATE_TIME_FORMAT)
    elif isinstance(o, datetime.date):
        return o.isoformat()
    elif isinstance(o, datetime.time):
        if is_aware(o):
            raise ValueError(""JSON can't represent timezone-aware times."")
        r = o.isoformat()
        if o.microsecond:
            r = r[:12]
        return r
    elif isinstance(o, datetime.timedelta):
        return duration_iso_string(o)
    elif isinstance(o, decimal.Decimal):
        return str(o)
    elif isinstance(o, uuid.UUID):
        return str(o)
    elif isinstance(o, Promise):
        return six.text_type(o)
    elif isinstance(o, QuerySet):
        return list(o.values())
    elif isinstance(o, Client):
        return model_to_dict(o)
    else:
        return super(JSONEncoder, self).default(o)","isinstance(o, datetime.datetime)",66,"isinstance(o, datetime.datetime)",True,100.00000000000004,N/A
"def get_callback(self, request):
    """"""
        get callback from obj or rules
        :param request:
        :return:
        """"""
<mask>:
        rules = self.spidercls.rules
        for rule in rules:
            if rule.link_extractor.matches(request.url):
                return rule.callback
    return self.default_callback","getattr(self.spidercls, 'rules', None)",28,self.spidercls,False,9.697196786440509,N/A
"def run_callback(self, response, cb):
    """"""
        run callback and get items and requests
        :param response:
        :param cb:
        :return:
        """"""
    items, requests = ([], [])
    for x in iterate_spider_output(cb(response)):
<mask>:
            items.append(x)
        elif isinstance(x, Request):
            requests.append(x)
    return (items, requests)","isinstance(x, (BaseItem, dict))",36,"isinstance(x, Item)",False,27.585129929794586,N/A
"def prepare_request(self, spider, request, args):
    """"""
        get request
        :param spider:
        :param request:
        :param args:
        :return:
        """"""

    def callback(response):
        """"""
            this callback wraps truly request's callback to get follows
            :param response:
            :return:
            """"""
        cb = self.args.callback or self.default_callback
<mask>:
            cb_method = getattr(spider, cb, None)
            if callable(cb_method):
                cb = cb_method
        items, requests = self.run_callback(response, cb)
        for request in requests:
            request.callback = self.get_callback(request)
            request.meta['callback'] = request.callback
        self.items += list(map(lambda item: process_item(item), items))
        self.requests += list(map(lambda request: process_request(request), requests))
        self.response = process_response(response)
    if args.meta:
        request.meta.update(args.meta)
    request.method = args.method if args.method else request.method
    if request.method.lower() != 'get':
        if isinstance(args.body, dict):
            request = request.replace(body=json.dumps(args.body))
        else:
            request = request.replace(body=args.body)
    request.headers = args.headers if args.headers else request.headers
    request.cookies = args.cookies if args.cookies else request.cookies
    request.dont_filter = args.filter if hasattr(args, 'filter') else request.dont_filter
    request.priority = int(args.priority) if hasattr(args, 'priority') else request.priority
    request.callback = callback
    return request",not callable(cb),137,cb,False,1.8315638888734187,N/A
"def run(self):
    """"""
        run main
        :return:
        """"""
    request = Request(self.args.url, callback=None)

    def start_requests(spider):
        return [self.prepare_request(spider, request, self.args)]
    self.spidercls.start_requests = start_requests
    self.crawler_process.crawl(self.spidercls)
<mask>:
        return {'ok': False}
    self.pcrawler = list(self.crawler_process.crawlers)[0]
    d = self.crawler_process.join()
    d.addBoth(lambda _: reactor.stop())
    reactor.run()
    return {'items': self.items, 'requests': self.requests, 'response': self.response, 'ok': True}",not len(self.crawler_process.crawlers) > 0,44,not self.crawler_process.crawlers,False,46.53786298485943,N/A
"def get_start_requests(project_path, spider_name):
    """"""
    get start requests
    :param project_path: project path
    :param spider_name: spider name
    :return:
    """"""
    work_cwd = os.getcwd()
    try:
        os.chdir(project_path)
        settings = get_project_settings()
        runner = CrawlerRunner(settings=settings)
        spider_cls = runner.spider_loader.load(spider_name)
        runner.crawl(spider_cls)
        crawler = list(runner.crawlers)[0]
        spider = crawler.spider
        requests = list(spider.start_requests())
<mask>:
            requests = list(spider.start())
        requests = list(map(lambda r: process_request(r), requests))
        return {'finished': True, 'requests': requests}
    finally:
        os.chdir(work_cwd)","not requests and hasattr(spider, 'start')",58,not requests,False,3.0197383422318516,N/A
"def __call__(self, request):
    """"""
        Change request body to str type
        :param request:
        :return:
        """"""
<mask>:
        data = getattr(request, '_body', request.body)
        request._body = data.decode('utf-8')
    response = self.get_response(request)
    return response","isinstance(request.body, bytes) and request.path not in self.white_list",28,"isinstance(request, requests.Message)",False,6.180954968603771,N/A
"def get_scrapyd(client):
<mask>:
        return ScrapydAPI(scrapyd_url(client.ip, client.port))
    return ScrapydAPI(scrapyd_url(client.ip, client.port), auth=(client.username, client.password))",not client.auth,11,client.username == 'root',False,16.233395773754953,N/A
"def ignored(ignores, path, file):
    """"""
    judge if the file is ignored
    :param ignores: ignored list
    :param path: file path
    :param file: file name
    :return: bool
    """"""
    file_name = join(path, file)
    for ignore in ignores:
<mask>:
            return True
        if fnmatch.fnmatch(file_name, ignore):
            return True
        if file == ignore:
            return True
    return False",'/' in ignore and ignore.rstrip('/') in file_name,50,"not fnmatch.fnmatch(file_name, ignore)",False,9.124813543640702,N/A
"def is_valid_name(project_name):
    """"""
    judge name is valid
    :param project_name:
    :return:
    """"""
    logger = get_logger(__name__)
<mask>:
        logger.error('project name %s must begin with a letter and contain only letters, numbers and underscores', project_name)
        return False
    return True","not re.search('^[_a-zA-Z]\\w*$', project_name)",35,"not re.match('^[a-zA-Z0-9_]+$', project_name)",False,44.86354899165034,N/A
"def copy_tree(src, dst):
    """"""
    copy tree
    :param src:
    :param dst:
    :return:
    """"""
    ignore = ignore_patterns(*IGNORES)
    names = os.listdir(src)
    ignored_names = ignore(src, names)
<mask>:
        os.makedirs(dst)
    for name in names:
        if name in ignored_names:
            continue
        src_name = os.path.join(src, name)
        dst_name = os.path.join(dst, name)
        if os.path.isdir(src_name):
            copy_tree(src_name, dst_name)
        else:
            copy2(src_name, dst_name)
    copystat(src, dst)",not os.path.exists(dst),50,not os.path.isdir(dst),False,59.694917920196445,N/A
"def get_tree(path, ignores=IGNORES):
    """"""
    get tree structure
    :param path: Folder path
    :param ignores: Ignore files
    :return: Json
    """"""
    result = []
    for file in os.listdir(path):
<mask>:
            if not ignored(ignores, path, file):
                children = get_tree(join(path, file), ignores)
                if children:
                    result.append({'label': file, 'children': children, 'path': path})
        elif not ignored(ignores, path, file):
            result.append({'label': file, 'path': path})
    return result","os.path.isdir(join(path, file))",55,"isinstance(file, str)",False,3.57451796074295,N/A
"def sync_jobs(self, force=False):
    """"""
        sync jobs
        :return:
        """"""
    logger.debug('syncing jobs from tasks configured...')
    tasks = self.realtime_tasks()
    logger.debug('get realtime tasks %s', tasks)
    for task in tasks:
        self._add_or_modify_new_jobs(task, force)
        self._remove_deprecated_jobs(task, force)
        task.modified = 0
        task.save()
    logger.debug('successfully synced task with jobs')
<mask>:
        logger.info('successfully synced task with jobs with force')",force,46,force,True,100.00000000000004,N/A
"def _remove_deprecated_jobs(self, task, force=False):
    """"""
        remove jobs
        :return:
        """"""
<mask>:
        return
    existed_jobs = self.existed_jobs()
    existed_job_ids = list(map(lambda obj: obj.id, existed_jobs))
    realtime_job_ids = list(self.realtime_jobs())
    logger.debug('existed job ids %s, task job ids %s', existed_job_ids, realtime_job_ids)
    deprecated_job_ids = [job_id for job_id in existed_job_ids if not job_id in realtime_job_ids]
    if deprecated_job_ids:
        logger.info('deleting deprecated jobs')
        for job_id in deprecated_job_ids:
            self.scheduler.remove_job(job_id)
        logger.info('deleted deprecated jobs %s', deprecated_job_ids)",not task.modified and (not force),60,"not force and self.task_id(task) in [None, None]",False,6.256118460580956,N/A
"def _add_or_modify_new_jobs(self, task, force=False):
    """"""
        add new jobs or modify existed jobs
        :return:
        """"""
<mask>:
        return
    task_job_ids = []
    clients = list(clients_of_task(task))
    for client in clients:
        job_id = get_job_id(client, task)
        task_job_ids.append(job_id)
        configuration = json.loads(task.configuration)
        trigger = task.trigger
        configuration = {arg: configuration.get(arg) for arg in args_map.get(trigger) if configuration.get(arg)}
        logger.debug('adding or modifying job %s, trigger %s, configuration %s', job_id, trigger, configuration)
        self.scheduler.add_job(execute, task.trigger, args=[client, task.project, task.spider], id=job_id, replace_existing=True, **configuration)",not task.modified and (not force),67,force and task.is_completed,False,11.737849637633069,N/A
"@log_exception()
@api_view(['GET'])
@permission_classes([IsAuthenticated])
def index_status(request):
    """"""
    index statistics
    :param request: request object
    :return: json
    """"""
<mask>:
        clients = Client.objects.all()
        data = {'success': 0, 'error': 0, 'project': 0}
        for client in clients:
            try:
                requests.get(scrapyd_url(client.ip, client.port), timeout=1)
                data['success'] += 1
            except ConnectionError:
                data['error'] += 1
        path = os.path.abspath(join(os.getcwd(), PROJECTS_FOLDER))
        files = os.listdir(path)
        for file in files:
            if os.path.isdir(join(path, file)) and (not file in IGNORES):
                data['project'] += 1
        return JsonResponse(data)",request.method == 'GET',67,request.method == 'GET',True,100.00000000000004,N/A
"@log_exception()
@api_view(['GET'])
@permission_classes([IsAuthenticated])
def client_info(request, client_id):
    """"""
    get client info
    :param request: request object
    :param id: client id
    :return: json
    """"""
<mask>:
        return JsonResponse(model_to_dict(Client.objects.get(id=client_id)))",request.method == 'GET',24,request.user.is_authenticated(),False,9.535414040914192,N/A
"@log_exception()
@api_view(['GET'])
@permission_classes([IsAuthenticated])
def client_status(request, client_id):
    """"""
    get client status
    :param request: request object
    :param client_id: client id
    :return: json
    """"""
<mask>:
        client = Client.objects.get(id=client_id)
        requests.get(scrapyd_url(client.ip, client.port), timeout=3)
        return JsonResponse({'result': '1'})",request.method == 'GET',31,request.method == 'GET',True,100.00000000000004,N/A
"@log_exception()
@api_view(['POST'])
@permission_classes([IsAuthenticated])
def client_update(request, client_id):
    """"""
    update client info
    :param request: request object
    :param client_id: client id
    :return: json
    """"""
<mask>:
        client = Client.objects.filter(id=client_id)
        data = json.loads(request.body)
        client.update(**data)
        return JsonResponse(model_to_dict(Client.objects.get(id=client_id)))",request.method == 'POST',31,request.method == 'POST',True,100.00000000000004,N/A
"@log_exception()
@api_view(['POST'])
@permission_classes([IsAuthenticated])
def client_create(request):
    """"""
    create a client
    :param request: request object
    :return: json
    """"""
<mask>:
        data = json.loads(request.body)
        client = Client.objects.create(**data)
        return JsonResponse(model_to_dict(client))",request.method == 'POST',25,request.method == 'POST',True,100.00000000000004,N/A
"def _process_item(self, item, spider):
    allowed_spiders = item.mongodb_spiders
    allowed_collections = item.mongodb_collections
<mask>:
        for allowed_collection in allowed_collections:
            self.database[allowed_collection].insert(dict(item))
    return item",allowed_spiders and spider.name in allowed_spiders,18,spider in allowed_spiders and (not item.is_collection_allowed),False,26.518122980477767,N/A
"def _process_item(self, item, spider):
    allowed_spiders = item.mongodb_spiders
    allowed_tables = item.mongodb_tables
<mask>:
        for allowed_table in allowed_tables:
            data = dict(item)
            keys = ', '.join(data.keys())
            values = ', '.join(['%s'] * len(data))
            sql = 'insert into %s (%s) values (%s)' % (allowed_table, keys, values)
            self.cursor.execute(sql, tuple(data.values()))
            self.db.commit()
    return item",allowed_spiders and spider.name in allowed_spiders,45,allowed_spiders,False,6.948345122280157,N/A
"def test_clean_reader():
    """"""Test clean method of Reader class.""""""
    reader = Reader()
    with pytest.raises(ValueError):
        reader.clean(path=None, drop_duplicate=False)
    with pytest.raises(ValueError):
        reader.clean(path='data_for_tests/train.csv')
    reader = Reader(sep=',')
    df = reader.clean(path='data_for_tests/train.csv')
    assert np.shape(df) == (891, 12)
    with pytest.raises(ValueError):
        reader.clean(path='data_for_tests/train.wrong_extension')
    df_drop = reader.clean(path='data_for_tests/train.csv', drop_duplicate=True)
    assert np.shape(df_drop) == (891, 12)
    assert np.all(df['Name'] == df_drop['Name'])
    reader = Reader()
    df_excel = reader.clean(path='data_for_tests/train.xls')
    assert np.shape(df_excel) == (891, 12)
    assert np.all(df['Name'] == df_excel['Name'])
<mask>:
        pass
    else:
        if sys.version_info[0] >= 3:
            df_hdf = reader.clean(path='data_for_tests/train.h5')
            assert np.shape(df_hdf) == (891, 12)
            assert np.all(df['Name'] == df_hdf['Name'])
        df_json = reader.clean(path='data_for_tests/train.json')
        assert np.shape(df_json) == (891, 12)",sys.platform == 'win32' and sys.version_info[0] <= 3 and (sys.version_info[1] <= 5),87,sys.version_info[0] < 3,False,10.012195590250379,N/A
"def test_train_test_split_reader():
    """"""Test train_test_split method of Reader class.""""""
    reader = Reader(sep=',')
    with pytest.raises(ValueError):
        reader.train_test_split(Lpath=None, target_name='target')
    with pytest.raises(ValueError):
        reader.train_test_split(Lpath=['data_for_tests/train.csv'], target_name=None)
    with pytest.raises(ValueError):
        reader = Reader(to_path=None)
        reader.train_test_split(Lpath=['data_for_tests/train.csv'], target_name='Survived')
    reader = Reader(sep=',')
    dict = reader.train_test_split(Lpath=['data_for_tests/train.csv'], target_name='Survived')
    assert len(dict) == 3
    assert 'train' in list(dict.keys())
    assert 'test' in list(dict.keys())
    assert 'target' in list(dict.keys())
    assert np.all(dict['train'].columns == dict['train'].columns)
<mask>:
        reader = Reader(to_hdf5=True)
        dict = reader.train_test_split(Lpath=['data_for_tests/train.h5'], target_name='Survived')
        assert len(dict) == 3
        assert 'train' in list(dict.keys())
        assert 'test' in list(dict.keys())
        assert 'target' in list(dict.keys())
        assert np.all(dict['train'].columns == dict['train'].columns)",sys.version_info[0] >= 3 and sys.platform != 'win32',81,"sys.version_info < (3, 0)",False,20.196532338767586,N/A
"def set_params(self, **params):
    self.__fitOK = False
    for k, v in params.items():
<mask>:
            warnings.warn('Invalid parameter a for predictor Predictor. Parameter IGNORED. Check the list of available parameters with `predictor.get_params().keys()`')
        else:
            setattr(self, k, v)",k not in self.get_params(),32,k not in self.get_params(),True,100.00000000000004,N/A
"def __save_feature_importances(self, importance, fig_name='feature_importance.png'):
    """"""Saves feature importances plot

        Parameters
        ----------
        importance : dict
            Dictionary with features (key) and importances (values)

        fig_name : str, default = ""feature_importance.png""
            figure name

        Returns
        -------
        NoneType
            None
        """"""
<mask>:
        importance_sum = np.sum(list(importance.values()))
        tuples = [(k, np.round(importance[k] * 100.0 / importance_sum, 2)) for k in importance]
        tuples = sorted(tuples, key=lambda x: x[1])
        labels, values = zip(*tuples)
        plt.figure(figsize=(20, int(len(importance) * 0.3) + 1))
        ylocs = np.arange(len(values))
        plt.barh(ylocs, values, align='center')
        for x, y in zip(values, ylocs):
            plt.text(x + 1, y, x, va='center')
        plt.yticks(ylocs, labels)
        plt.title('Feature importance (%)')
        plt.grid(True)
        plt.savefig(fig_name)
        plt.close()
        leak = sorted(dict(tuples).items(), key=operator.itemgetter(1))[-1]
        if (leak[-1] > 70) & (len(importance) > 1):
            warnings.warn('WARNING : ' + str(leak[0]) + ' is probably a leak ! Please check and delete it...')
    else:
        pass",len(importance) > 0,123,len(importance) > 0,True,100.00000000000004,N/A
"def __plot_feature_importances(self, importance, top=10):
    """"""Plots top 10 feature importances

        Parameters
        ----------
        importance : dict
            Dictionary with features (key) and importances (values)

        top : int
            Number of top features to display.

        Returns
        -------
        NoneType
            None
        """"""
<mask>:
        importance_sum = np.sum(list(importance.values()))
        tuples = [(k, np.round(importance[k] * 100.0 / importance_sum, 2)) for k in importance]
        tuples = sorted(tuples, key=lambda x: x[1])[-top:]
        labels, values = zip(*tuples)
        plt.figure(figsize=(20, top * 0.3 + 1))
        ylocs = np.arange(len(values))
        plt.barh(ylocs, values, align='center')
        for x, y in zip(values, ylocs):
            plt.text(x + 1, y, x, va='center')
        plt.yticks(ylocs, labels)
        plt.title('Top ' + str(top) + ' feature importance (%)')
        plt.grid(True)
        plt.show()
        plt.close()
    else:
        pass",len(importance) > 0,102,len(importance) > 0,True,100.00000000000004,N/A
"def set_params(self, **params):
    """"""Set parameters for a NA_encoder object.

        Set numerical strategy and categorical strategy.

        Parameters
        ----------
        numerical_strategy : str or float or int. default = ""mean""
            The strategy to encode NA for numerical features.

        categorical_strategy : str, default = '<NULL>'
            The strategy to encode NA for categorical features.

        """"""
    self.__fitOK = False
    for k, v in params.items():
<mask>:
            warnings.warn('Invalid parameter(s) for encoder NA_encoder. Parameter(s) IGNORED. Check the list of available parameters with `encoder.get_params().keys()`')
        else:
            setattr(self, k, v)",k not in self.get_params(),78,k.startswith('_'),False,6.431267393706692,N/A
"def fit(self, df_train, y_train=None):
    """"""Fits NA Encoder.

        Parameters
        ----------
        df_train : pandas dataframe of shape = (n_train, n_features)
            The train dataset with numerical and categorical features.

        y_train : pandas series of shape = (n_train, ), default = None
            The target for classification or regression tasks.

        Returns
        -------
        object
            self

        """"""
    self.__Lcat = df_train.dtypes[df_train.dtypes == 'object'].index
    self.__Lnum = df_train.dtypes[df_train.dtypes != 'object'].index
<mask>:
        self.__imp = SimpleImputer(strategy=self.numerical_strategy)
        if len(self.__Lnum) != 0:
            self.__imp.fit(df_train[self.__Lnum])
        else:
            pass
    elif (type(self.numerical_strategy) == int) | (type(self.numerical_strategy) == float):
        pass
    else:
        raise ValueError('Numerical strategy for NA encoding is not valid')
    if type(self.categorical_strategy) == str:
        if self.categorical_strategy == 'most_frequent':
            na_count = df_train[self.__Lcat].isnull().sum()
            for col in na_count[na_count > 0].index:
                try:
                    self.__mode[col] = df_train[col].mode()[0]
                except:
                    self.__mode[col] = '<NULL>'
        else:
            pass
    else:
        raise ValueError('Categorical strategy for NA encoding is not valid')
    self.__fitOK = True
    return self","self.numerical_strategy in ['mean', 'median', 'most_frequent']",132,type(self.numerical_strategy) == str,False,24.38870552576843,N/A
"def transform(self, df):
    """"""Transform the dataset.

        Parameters
        ----------
        df : pandas.Dataframe of shape = (n, n_features)
            The dataset with numerical and categorical features.

        Returns
        -------
        pandas.Dataframe of shape = (n, n_features)
            The dataset with no missing values.

        """"""
<mask>:
        if len(self.__Lnum) == 0:
            if self.categorical_strategy != 'most_frequent':
                return df[self.__Lcat].fillna(self.categorical_strategy)
            else:
                return df[self.__Lcat].fillna(self.__mode)
        elif self.numerical_strategy in ['mean', 'median', 'most_frequent']:
            if len(self.__Lcat) != 0:
                if self.categorical_strategy != 'most_frequent':
                    return pd.concat((pd.DataFrame(self.__imp.transform(df[self.__Lnum]), columns=self.__Lnum, index=df.index), df[self.__Lcat].fillna(self.categorical_strategy)), axis=1)[df.columns]
                else:
                    return pd.concat((pd.DataFrame(self.__imp.transform(df[self.__Lnum]), columns=self.__Lnum, index=df.index), df[self.__Lcat].fillna(self.__mode)), axis=1)[df.columns]
            else:
                return pd.DataFrame(self.__imp.transform(df[self.__Lnum]), columns=self.__Lnum, index=df.index)
        elif (type(self.numerical_strategy) == int) | (type(self.numerical_strategy) == float):
            if len(self.__Lcat) != 0:
                if self.categorical_strategy != 'most_frequent':
                    return pd.concat((df[self.__Lnum].fillna(self.numerical_strategy), df[self.__Lcat].fillna(self.categorical_strategy)), axis=1)[df.columns]
                else:
                    return pd.concat((df[self.__Lnum].fillna(self.numerical_strategy), df[self.__Lcat].fillna(self.__mode)), axis=1)[df.columns]
            else:
                return df[self.__Lnum].fillna(self.numerical_strategy)
    else:
        raise ValueError('Call fit or fit_transform function before')",self.__fitOK,120,self.numerical_strategy == 'average',False,12.22307556087252,N/A
"def set_params(self, **params):
    """"""Set param method for Categorical encoder.

        Set strategy parameters and verbose parameters

        Parameters
        ----------
        strategy : str, default = ""label_encoding""
            The strategy to encode categorical features.
            Available strategies = {""label_encoding"", ""dummification"",
            ""random_projection"", entity_embedding""}
        verbose : bool, default = False
            Verbose mode. Useful for entity embedding strategy.

        """"""
    self.__fitOK = False
    for k, v in params.items():
<mask>:
            warnings.warn('Invalid parameter(s) for encoder Categorical_encoder. Parameter(s) IGNORED. Check the list of available parameters with `encoder.get_params().keys()`')
        else:
            setattr(self, k, v)",k not in self.get_params(),78,k not in strategies,False,13.267398701010466,N/A
"def set_params(self, **params):
    self.__fitOK = False
    for k, v in params.items():
<mask>:
            warnings.warn('Invalid parameter a for feature selectorClf_feature_selector. Parameter IGNORED. Checkthe list of available parameters with`feature_selector.get_params().keys()`')
        else:
            setattr(self, k, v)",k not in self.get_params(),30,k == 'feature_selector',False,4.955725306405571,N/A
"def fit(self, df_train, y_train):
    """"""Fits Clf_feature_selector

        Parameters
        ----------
        df_train : pandas dataframe of shape = (n_train, n_features)
            The train dataset with numerical features and no NA

        y_train : pandas series of shape = (n_train, )
            The target for classification task. Must be encoded.

        Returns
        -------
        object
            self
        """"""
<mask>:
        raise ValueError('df_train must be a DataFrame')
    if type(y_train) != pd.core.series.Series:
        raise ValueError('y_train must be a Series')
    if self.strategy == 'variance':
        coef = df_train.std()
        abstract_threshold = np.percentile(coef, 100.0 * self.threshold)
        self.__to_discard = coef[coef < abstract_threshold].index
        self.__fitOK = True
    elif self.strategy == 'l1':
        model = LogisticRegression(C=0.01, penalty='l1', solver='saga', n_jobs=-1, random_state=0)
        model.fit(df_train, y_train)
        coef = np.mean(np.abs(model.coef_), axis=0)
        abstract_threshold = np.percentile(coef, 100.0 * self.threshold)
        self.__to_discard = df_train.columns[coef < abstract_threshold]
        self.__fitOK = True
    elif self.strategy == 'rf_feature_importance':
        model = RandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=0)
        model.fit(df_train, y_train)
        coef = model.feature_importances_
        abstract_threshold = np.percentile(coef, 100.0 * self.threshold)
        self.__to_discard = df_train.columns[coef < abstract_threshold]
        self.__fitOK = True
    else:
        raise ValueError(""Strategy invalid. Please choose between 'variance', 'l1' or 'rf_feature_importance'"")
    return self",type(df_train) != pd.SparseDataFrame and type(df_train) != pd.DataFrame,158,type(df_train) != pd.core.dataframe.DataFrame,False,39.480497923992814,N/A
"def transform(self, df):
    """"""Transforms the dataset

        Parameters
        ----------
        df : pandas dataframe of shape = (n, n_features)
            The dataset with numerical features and no NA

        Returns
        -------
        pandas dataframe of shape = (n_train, n_features*(1-threshold))
            The train dataset with relevant features
        """"""
<mask>:
        if type(df) != pd.SparseDataFrame and type(df) != pd.DataFrame:
            raise ValueError('df must be a DataFrame')
        return df.drop(self.__to_discard, axis=1)
    else:
        raise ValueError('call fit or fit_transform function before')",self.__fitOK,67,self.__to_discard is not None,False,26.269098944241588,N/A
"def __init__(self, **params):
    """"""Init Classifier object.

        User can define strategy parameters.

        Parameters
        ----------
        strategy : str, default = ""LightGBM""
            The choice of the classifier.
            Available strategies = {""LightGBM"", ""RandomForest"", ""ExtraTrees"",
            ""Tree"", ""Bagging"", ""AdaBoost"" or ""Linear""}.

        """"""
<mask>:
        self.__strategy = params['strategy']
    else:
        self.__strategy = 'LightGBM'
    self.__classif_params = {}
    self.__classifier = None
    self.__set_classifier(self.__strategy)
    self.__col = None
    self.set_params(**params)
    self.__fitOK = False",'strategy' in params,58,'sevice' in params,False,55.03212081491043,N/A
"def set_params(self, **params):
    """"""Set strategy parameters of Classifier object.""""""
    self.__fitOK = False
<mask>:
        self.__set_classifier(params['strategy'])
        for k, v in self.__classif_params.items():
            if k not in self.get_params().keys():
                warnings.warn('Invalid parameter for classifier ' + str(self.__strategy) + '. Parameter IGNORED. Check the list of available parameters with `classifier.get_params().keys()`')
            else:
                setattr(self.__classifier, k, v)
    for k, v in params.items():
        if k == 'strategy':
            pass
        elif k not in self.__classifier.get_params().keys():
            warnings.warn('Invalid parameter for classifier ' + str(self.__strategy) + '. Parameter IGNORED. Check the list of available parameters with `classifier.get_params().keys()`')
        else:
            setattr(self.__classifier, k, v)
            self.__classif_params[k] = v",'strategy' in params.keys(),88,'sevice' in params,False,14.506309551249304,N/A
"def __set_classifier(self, strategy):
    """"""Set the classifier using scikitlearn Classifier.""""""
    self.__strategy = strategy
<mask>:
        self.__classifier = RandomForestClassifier(n_estimators=400, max_depth=10, max_features='sqrt', bootstrap=True, n_jobs=-1, random_state=0)
    elif strategy == 'LightGBM':
        self.__classifier = LGBMClassifier(n_estimators=500, learning_rate=0.05, colsample_bytree=0.8, subsample=0.9, nthread=-1, seed=0)
    elif strategy == 'ExtraTrees':
        self.__classifier = ExtraTreesClassifier(n_estimators=400, max_depth=10, max_features='sqrt', bootstrap=True, n_jobs=-1, random_state=0)
    elif strategy == 'Tree':
        self.__classifier = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=0, max_leaf_nodes=None, class_weight=None, presort=False)
    elif strategy == 'Bagging':
        self.__classifier = BaggingClassifier(base_estimator=None, n_estimators=500, max_samples=0.9, max_features=0.85, bootstrap=False, bootstrap_features=False, n_jobs=-1, random_state=0)
    elif strategy == 'AdaBoost':
        self.__classifier = AdaBoostClassifier(base_estimator=None, n_estimators=400, learning_rate=0.05, algorithm='SAMME.R', random_state=0)
    elif strategy == 'Linear':
        self.__classifier = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=0, solver='lbfgs', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=-1)
    else:
        raise ValueError(""Strategy invalid. Please choose between 'LightGBM', 'RandomForest', 'ExtraTrees', 'Tree', 'Bagging', 'AdaBoost' or 'Linear'"")",strategy == 'RandomForest',122,strategy == 'RandomForest',True,100.00000000000004,N/A
"def fit(self, df_train, y_train):
    """"""Fits Classifier.

        Parameters
        ----------
        df_train : pandas dataframe of shape = (n_train, n_features)
            The train dataset with numerical features.

        y_train : pandas series of shape = (n_train,)
            The numerical encoded target for classification tasks.

        Returns
        -------
        object
            self

        """"""
<mask>:
        raise ValueError('df_train must be a DataFrame')
    if type(y_train) != pd.core.series.Series:
        raise ValueError('y_train must be a Series')
    self.__classifier.fit(df_train.values, y_train)
    self.__col = df_train.columns
    self.__fitOK = True
    return self",type(df_train) != pd.SparseDataFrame and type(df_train) != pd.DataFrame,70,type(df_train) != pd.core.dataframe.DataFrame,False,39.480497923992814,N/A
"def feature_importances(self):
    """"""Compute feature importances.

        Classifier must be fitted before.

        Returns
        -------
        dict
            Dictionnary containing a measure of feature importance (value) for
            each feature (key).

        """"""
<mask>:
        if self.get_params()['strategy'] in ['Linear']:
            importance = {}
            f = np.mean(np.abs(self.get_estimator().coef_), axis=0)
            for i, col in enumerate(self.__col):
                importance[col] = f[i]
        elif self.get_params()['strategy'] in ['LightGBM', 'RandomForest', 'ExtraTrees', 'Tree']:
            importance = {}
            f = self.get_estimator().feature_importances_
            for i, col in enumerate(self.__col):
                importance[col] = f[i]
        elif self.get_params()['strategy'] in ['AdaBoost']:
            importance = {}
            norm = self.get_estimator().estimator_weights_.sum()
            try:
                f = sum((weight * est.feature_importances_ for weight, est in zip(self.get_estimator().estimator_weights_, self.get_estimator().estimators_))) / norm
            except:
                f = sum((weight * np.mean(np.abs(est.coef_), axis=0) for weight, est in zip(self.get_estimator().estimator_weights_, self.get_estimator().estimators_))) / norm
            for i, col in enumerate(self.__col):
                importance[col] = f[i]
        elif self.get_params()['strategy'] in ['Bagging']:
            importance = {}
            importance_bag = []
            for i, b in enumerate(self.get_estimator().estimators_):
                d = {}
                try:
                    f = b.feature_importances_
                except:
                    f = np.mean(np.abs(b.coef_), axis=0)
                for j, c in enumerate(self.get_estimator().estimators_features_[i]):
                    d[self.__col[c]] = f[j]
                importance_bag.append(d.copy())
            for i, col in enumerate(self.__col):
                list_filtered = filter(lambda x: x != 0, [k[col] if col in k else 0 for k in importance_bag])
                importance[col] = np.mean(list(list_filtered))
        else:
            importance = {}
        return importance
    else:
        raise ValueError('You must call the fit function before !')",self.__fitOK,192,self.get_params()['enable_bias'],False,8.054496384843702,N/A
"def __init__(self, base_estimators=[Classifier(strategy='LightGBM'), Classifier(strategy='RandomForest'), Classifier(strategy='ExtraTrees')], level_estimator=LogisticRegression(n_jobs=-1), n_folds=5, copy=False, drop_first=True, random_state=1, verbose=True):
    self.base_estimators = base_estimators
<mask>:
        raise ValueError('base_estimators must be a list')
    else:
        for i, est in enumerate(self.base_estimators):
            self.base_estimators[i] = make_copy(est)
    self.level_estimator = level_estimator
    self.n_folds = n_folds
    if type(self.n_folds) != int:
        raise ValueError('n_folds must be an integer')
    self.copy = copy
    if type(self.copy) != bool:
        raise ValueError('copy must be a boolean')
    self.drop_first = drop_first
    if type(self.drop_first) != bool:
        raise ValueError('drop_first must be a boolean')
    self.random_state = random_state
    if type(self.random_state) != int and self.random_state is not None:
        raise ValueError('random_state must be either None or an integer')
    self.verbose = verbose
    if type(self.verbose) != bool:
        raise ValueError('verbose must be a boolean')
    self.__fitOK = False
    self.__fittransformOK = False",type(self.base_estimators) != list,112,type(self.base_estimators) != list,True,100.00000000000004,N/A
"def set_params(self, **params):
    self.__fitOK = False
    self.__fittransformOK = False
    for k, v in params.items():
<mask>:
            warnings.warn('Invalid parameter a for stacking_classifier StackingClassifier. Parameter IGNORED. Check the list of available parameters with `stacking_classifier.get_params().keys()`')
        else:
            setattr(self, k, v)",k not in self.get_params(),35,k == 'stacking_classifier',False,4.955725306405571,N/A
"def fit_transform(self, df_train, y_train):
    """"""Creates meta-features for the training dataset.

        Parameters
        ----------
        df_train : pandas dataframe of shape = (n_samples, n_features)
            The training dataset.

        y_train : pandas series of shape = (n_samples, )
            The target.

        Returns
        -------
        pandas dataframe of shape = (n_samples, n_features*int(copy)+n_metafeatures)
            The transformed training dataset.
        """"""
<mask>:
        raise ValueError('df_train must be a DataFrame')
    if type(y_train) != pd.core.series.Series:
        raise ValueError('y_train must be a Series')
    cv = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
    preds = pd.DataFrame([], index=y_train.index)
    classes = y_train.value_counts()
    classes_to_drop = classes[classes < 2].index
    indexes_to_drop = y_train[y_train.apply(lambda x: x in classes_to_drop)].index
    if self.verbose:
        print('')
        print('[=============================================================================] LAYER [===================================================================================]')
        print('')
    for c, clf in enumerate(self.base_estimators):
        if self.verbose:
            print('> fitting estimator n°' + str(c + 1) + ' : ' + str(clf.get_params()) + ' ...')
            print('')
        y_pred = self.__cross_val_predict_proba(clf, df_train, y_train, cv)
        for i in range(0, y_pred.shape[1] - int(self.drop_first)):
            preds['est' + str(c + 1) + '_class' + str(i)] = y_pred[:, i]
        clf.fit(df_train.drop(indexes_to_drop), y_train.drop(indexes_to_drop))
    layer = 1
    columns = ['layer' + str(layer) + '_' + s for s in preds.columns]
    while len(np.intersect1d(df_train.columns, columns)) > 0:
        layer = layer + 1
        columns = ['layer' + str(layer) + '_' + s for s in preds.columns]
    preds.columns = ['layer' + str(layer) + '_' + s for s in preds.columns]
    self.__fittransformOK = True
    if self.copy:
        return pd.concat([df_train, preds], axis=1)
    else:
        return preds",type(df_train) != pd.SparseDataFrame and type(df_train) != pd.DataFrame,213,type(df_train) != pd.core.dataframe.DataFrame,False,39.480497923992814,N/A
"def transform(self, df_test):
    """"""Creates meta-features for the test dataset.

        Parameters
        ----------
        df_test : pandas dataframe of shape = (n_samples_test, n_features)
            The test dataset.

        Returns
        -------
        pandas dataframe of shape = (n_samples_test, n_features*int(copy)+n_metafeatures)
            The transformed test dataset.
        """"""
<mask>:
        raise ValueError('df_test must be a DataFrame')
    if self.__fittransformOK:
        preds_test = pd.DataFrame([], index=df_test.index)
        for c, clf in enumerate(self.base_estimators):
            y_pred_test = clf.predict_proba(df_test)
            for i in range(0, y_pred_test.shape[1] - int(self.drop_first)):
                idx_name = 'est' + str(c + 1) + '_class' + str(i)
                preds_test[idx_name] = y_pred_test[:, i]
        layer = 1
        columns = ['layer' + str(layer) + '_' + s for s in preds_test.columns]
        while len(np.intersect1d(df_test.columns, columns)) > 0:
            layer = layer + 1
            columns = ['layer' + str(layer) + '_' + s for s in preds_test.columns]
        preds_test.columns = ['layer' + str(layer) + '_' + s for s in preds_test.columns]
        if self.copy:
            return pd.concat([df_test, preds_test], axis=1)
        else:
            return preds_test
    else:
        raise ValueError('Call fit_transform before !')",type(df_test) != pd.SparseDataFrame and type(df_test) != pd.DataFrame,147,"not isinstance(df_test, pd.DataFrame)",False,11.785234207804194,N/A
"def fit(self, df_train, y_train):
    """"""Fits the first level estimators and the second level estimator on X.

        Parameters
        ----------
        df_train : pandas dataframe of shape (n_samples, n_features)
            Input data

        y_train : pandas series of shape = (n_samples, )
            The target

        Returns
        -------
        object
            self.
        """"""
    df_train = self.fit_transform(df_train, y_train)
<mask>:
        print('')
        print('[=========================================================================] PREDICTION LAYER [============================================================================]')
        print('')
        print('> fitting estimator : ')
        print(str(self.level_estimator.get_params()) + ' ...')
        print('')
    self.level_estimator.fit(df_train.values, y_train.values)
    self.__fitOK = True
    return self",self.verbose,72,self.__fitOK,False,21.3643503198117,N/A
"def set_params(self, **params):
    self.__fitOK = False
    for k, v in params.items():
<mask>:
            warnings.warn('Invalid parameter a for feature selectorReg_feature_selector. Parameter IGNORED. Check the list of available parameters with `feature_selector.get_params().keys()`')
        else:
            setattr(self, k, v)",k not in self.get_params(),32,k == 'feature_selector',False,4.955725306405571,N/A
"def fit(self, df_train, y_train):
    """"""Fits Reg_feature_selector.

        Parameters
        ----------
        df_train : pandas dataframe of shape = (n_train, n_features)
            The train dataset with numerical features and no NA

        y_train : pandas series of shape = (n_train, ).
            The target for regression task.

        Returns
        -------
        sobject
            self
        """"""
<mask>:
        raise ValueError('df_train must be a DataFrame')
    if type(y_train) != pd.core.series.Series:
        raise ValueError('y_train must be a Series')
    if self.strategy == 'variance':
        coef = df_train.std()
        abstract_threshold = np.percentile(coef, 100.0 * self.threshold)
        self.__to_discard = coef[coef < abstract_threshold].index
        self.__fitOK = True
    elif self.strategy == 'l1':
        model = Lasso(alpha=100.0, random_state=0)
        model.fit(df_train, y_train)
        coef = np.abs(model.coef_)
        abstract_threshold = np.percentile(coef, 100.0 * self.threshold)
        self.__to_discard = df_train.columns[coef < abstract_threshold]
        self.__fitOK = True
    elif self.strategy == 'rf_feature_importance':
        model = RandomForestRegressor(n_estimators=50, n_jobs=-1, random_state=0)
        model.fit(df_train, y_train)
        coef = model.feature_importances_
        abstract_threshold = np.percentile(coef, 100.0 * self.threshold)
        self.__to_discard = df_train.columns[coef < abstract_threshold]
        self.__fitOK = True
    else:
        raise ValueError(""Strategy invalid. Please choose between 'variance', 'l1' or 'rf_feature_importance'"")
    return self",type(df_train) != pd.SparseDataFrame and type(df_train) != pd.DataFrame,151,type(df_train) != pd.core.dataframe.DataFrame,False,39.480497923992814,N/A
"def transform(self, df):
    """"""Transforms the dataset

        Parameters
        ----------
        df : pandas dataframe of shape = (n, n_features)
            The dataset with numerical features and no NA

        Returns
        -------
        pandas dataframe of shape = (n_train, n_features*(1-threshold))
            The train dataset with relevant features
        """"""
<mask>:
        if (type(df) != pd.SparseDataFrame) & (type(df) != pd.DataFrame):
            raise ValueError('df must be a DataFrame')
        return df.drop(self.__to_discard, axis=1)
    else:
        raise ValueError('call fit or fit_transform function before')",self.__fitOK,67,self.__fit_transform is None,False,29.84745896009822,N/A
"def __init__(self, base_estimators=[Regressor(strategy='LightGBM'), Regressor(strategy='RandomForest'), Regressor(strategy='ExtraTrees')], level_estimator=LinearRegression(), n_folds=5, copy=False, random_state=1, verbose=True):
    """"""Init method for StackingRegressor.""""""
    self.base_estimators = base_estimators
<mask>:
        raise ValueError('base_estimators must be a list')
    else:
        for i, est in enumerate(self.base_estimators):
            self.base_estimators[i] = make_copy(est)
    self.level_estimator = level_estimator
    self.n_folds = n_folds
    if type(n_folds) != int:
        raise ValueError('n_folds must be an integer')
    self.copy = copy
    if type(copy) != bool:
        raise ValueError('copy must be a boolean')
    self.random_state = random_state
    if type(self.random_state) != int and self.random_state is not None:
        raise ValueError('random_state must be either None or an integer')
    self.verbose = verbose
    if type(self.verbose) != bool:
        raise ValueError('verbose must be a boolean')
    self.__fitOK = False
    self.__fittransformOK = False",type(base_estimators) != list,102,type(self.base_estimators) != list,False,63.15552371794039,N/A
"def set_params(self, **params):
    """"""Set parameters of a StackingRegressor object.""""""
    self.__fitOK = False
    self.__fittransformOK = False
    for k, v in params.items():
<mask>:
            warnings.warn('Invalid parameter a for stacking_regressor StackingRegressor. Parameter IGNORED. Check the list of available parameters with `stacking_regressor.get_params().keys()`')
        else:
            setattr(self, k, v)",k not in self.get_params(),41,k == 'fitOK',False,3.564186929405141,N/A
"def fit_transform(self, df_train, y_train):
    """"""Create meta-features for the training dataset.

        Parameters
        ----------
        df_train : pandas DataFrame of shape = (n_samples, n_features)
            The training dataset.

        y_train : pandas series of shape = (n_samples, )
            The target

        Returns
        -------
        pandas DataFrame of shape = (n_samples,
                                     n_features*int(copy)+n_metafeatures)
            The transformed training dataset.

        """"""
<mask>:
        raise ValueError('df_train must be a DataFrame')
    if type(y_train) != pd.core.series.Series:
        raise ValueError('y_train must be a Series')
    cv = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
    preds = pd.DataFrame([], index=y_train.index)
    if self.verbose:
        print('')
        print('[=============================================================================] LAYER [===================================================================================]')
        print('')
    for c, reg in enumerate(self.base_estimators):
        if self.verbose:
            print('> fitting estimator n°' + str(c + 1) + ' : ' + str(reg.get_params()) + ' ...')
            print('')
        y_pred = cross_val_predict(estimator=reg, X=df_train, y=y_train, cv=cv)
        preds['est' + str(c + 1)] = y_pred
        reg.fit(df_train, y_train)
    layer = 1
    columns = ['layer' + str(layer) + '_' + s for s in preds.columns]
    while len(np.intersect1d(df_train.columns, columns)) > 0:
        layer = layer + 1
        columns = ['layer' + str(layer) + '_' + s for s in preds.columns]
    preds.columns = ['layer' + str(layer) + '_' + s for s in preds.columns]
    self.__fittransformOK = True
    if self.copy:
        return pd.concat([df_train, preds], axis=1)
    else:
        return preds",(type(df_train) != pd.SparseDataFrame) & (type(df_train) != pd.DataFrame),186,type(df_train) != pd.core.dataframe.DataFrame,False,30.23923217273278,N/A
"def transform(self, df_test):
    """"""Create meta-features for the test dataset.

        Parameters
        ----------
        df_test : pandas DataFrame of shape = (n_samples_test, n_features)
            The test dataset.

        Returns
        -------
        pandas DataFrame of shape = (n_samples_test,
                                     n_features*int(copy)+n_metafeatures)
            The transformed test dataset.

        """"""
<mask>:
        raise ValueError('df_test must be a DataFrame')
    if self.__fittransformOK:
        preds_test = pd.DataFrame([], index=df_test.index)
        for c, reg in enumerate(self.base_estimators):
            y_pred_test = reg.predict(df_test)
            preds_test['est' + str(c + 1)] = y_pred_test
        layer = 1
        columns = ['layer' + str(layer) + '_' + s for s in preds_test.columns]
        while len(np.intersect1d(df_test.columns, columns)) > 0:
            layer = layer + 1
            columns = ['layer' + str(layer) + '_' + s for s in preds_test.columns]
        preds_test.columns = ['layer' + str(layer) + '_' + s for s in preds_test.columns]
        if self.copy:
            return pd.concat([df_test, preds_test], axis=1)
        else:
            return preds_test
    else:
        raise ValueError('Call fit_transform before !')",type(df_test) != pd.SparseDataFrame and type(df_test) != pd.DataFrame,132,"not isinstance(df_test, pd.DataFrame)",False,11.785234207804194,N/A
"def fit(self, df_train, y_train):
    """"""Fit the first level estimators and the second level estimator on X.

        Parameters
        ----------
        df_train : pandas DataFrame of shape (n_samples, n_features)
            Input data

        y_train : pandas series of shape = (n_samples, )
            The target

        Returns
        -------
        object
            self

        """"""
    df_train = self.fit_transform(df_train, y_train)
<mask>:
        print('')
        print('[=========================================================================] PREDICTION LAYER [============================================================================]')
        print('')
        print('> fitting estimator : ' + str(self.level_estimator.get_params()) + ' ...')
        print('')
    self.level_estimator.fit(df_train.values, y_train.values)
    self.__fitOK = True
    return self",self.verbose,73,self.__fitOK,False,21.3643503198117,N/A
"def __init__(self, **params):
    """"""Init Regressor object where user can pass a strategy.""""""
<mask>:
        self.__strategy = params['strategy']
    else:
        self.__strategy = 'LightGBM'
    self.__regress_params = {}
    self.__regressor = None
    self.__set_regressor(self.__strategy)
    self.__col = None
    self.set_params(**params)
    self.__fitOK = False",'strategy' in params,34,'strategy' in params,True,100.00000000000004,N/A
"def set_params(self, **params):
    """"""Set parameters of Regressor object.""""""
    self.__fitOK = False
<mask>:
        self.__set_regressor(params['strategy'])
        for k, v in self.__regress_params.items():
            if k not in self.get_params().keys():
                warnings.warn('Invalid parameter for regressor ' + str(self.__strategy) + '. Parameter IGNORED. Check the list of available parameters with `regressor.get_params().keys()`')
            else:
                setattr(self.__regressor, k, v)
    for k, v in params.items():
        if k == 'strategy':
            pass
        elif k not in self.__regressor.get_params().keys():
            warnings.warn('Invalid parameter for regressor ' + str(self.__strategy) + '. Parameter IGNORED. Check the list of available parameters with `regressor.get_params().keys()`')
        else:
            setattr(self.__regressor, k, v)
            self.__regress_params[k] = v",'strategy' in params.keys(),87,'search' in params,False,14.506309551249304,N/A
"def __set_regressor(self, strategy):
    """"""Set strategy of a regressor object.""""""
    self.__strategy = strategy
<mask>:
        self.__regressor = RandomForestRegressor(n_estimators=400, max_depth=10, max_features='sqrt', bootstrap=True, n_jobs=-1, random_state=0)
    elif strategy == 'LightGBM':
        self.__regressor = LGBMRegressor(n_estimators=500, learning_rate=0.05, colsample_bytree=0.8, subsample=0.9, nthread=-1, seed=0)
    elif strategy == 'ExtraTrees':
        self.__regressor = ExtraTreesRegressor(n_estimators=400, max_depth=10, max_features='sqrt', bootstrap=True, n_jobs=-1, random_state=0)
    elif strategy == 'Tree':
        self.__regressor = DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=0, max_leaf_nodes=None, presort=False)
    elif strategy == 'Bagging':
        self.__regressor = BaggingRegressor(base_estimator=None, n_estimators=500, max_samples=0.9, max_features=0.85, bootstrap=False, bootstrap_features=False, n_jobs=-1, random_state=0)
    elif strategy == 'AdaBoost':
        self.__regressor = AdaBoostRegressor(base_estimator=None, n_estimators=400, learning_rate=0.05, random_state=0)
    elif strategy == 'Linear':
        self.__regressor = Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=0)
    else:
        raise ValueError(""Strategy invalid. Please choose between 'LightGBM', 'RandomForest', 'ExtraTrees', 'Tree', 'Bagging', 'AdaBoost' or 'Linear'"")",strategy == 'RandomForest',114,strategy == 'RandomForest',True,100.00000000000004,N/A
"def fit(self, df_train, y_train):
    """"""Fits Regressor.

        Parameters
        ----------
        df_train : pandas dataframe of shape = (n_train, n_features)
            The train dataset with numerical features.

        y_train : pandas series of shape = (n_train, )
            The target for regression tasks.

        Returns
        -------
        object
            self

        """"""
<mask>:
        raise ValueError('df_train must be a DataFrame')
    if type(y_train) != pd.core.series.Series:
        raise ValueError('y_train must be a Series')
    self.__regressor.fit(df_train.values, y_train)
    self.__col = df_train.columns
    self.__fitOK = True
    return self",type(df_train) != pd.SparseDataFrame and type(df_train) != pd.DataFrame,69,type(df_train) != pd.core.dataframe.DataFrame,False,39.480497923992814,N/A
"def feature_importances(self):
    """"""Computes feature importances.

        Regressor must be fitted before.

        Returns
        -------
        dict
            Dictionnary containing a measure of feature importance (value)
            for each feature (key).

        """"""
<mask>:
        if self.get_params()['strategy'] in ['Linear']:
            importance = {}
            f = np.abs(self.get_estimator().coef_)
            for i, col in enumerate(self.__col):
                importance[col] = f[i]
        elif self.get_params()['strategy'] in ['LightGBM', 'RandomForest', 'ExtraTrees', 'Tree']:
            importance = {}
            f = self.get_estimator().feature_importances_
            for i, col in enumerate(self.__col):
                importance[col] = f[i]
        elif self.get_params()['strategy'] in ['AdaBoost']:
            importance = {}
            norm = self.get_estimator().estimator_weights_.sum()
            try:
                f = sum((weight * est.feature_importances_ for weight, est in zip(self.get_estimator().estimator_weights_, self.get_estimator().estimators_))) / norm
            except Exception:
                f = sum((weight * np.abs(est.coef_) for weight, est in zip(self.get_estimator().estimator_weights_, self.get_estimator().estimators_))) / norm
            for i, col in enumerate(self.__col):
                importance[col] = f[i]
        elif self.get_params()['strategy'] in ['Bagging']:
            importance = {}
            importance_bag = []
            for i, b in enumerate(self.get_estimator().estimators_):
                d = {}
                try:
                    f = b.feature_importances_
                except Exception:
                    f = np.abs(b.coef_)
                estimator = self.get_estimator()
                items = enumerate(estimator.estimators_features_[i])
                for j, c in items:
                    d[self.__col[c]] = f[j]
                importance_bag.append(d.copy())
            for i, col in enumerate(self.__col):
                list_filtered = filter(lambda x: x != 0, [k[col] if col in k else 0 for k in importance_bag])
                importance[col] = np.mean(list(list_filtered))
        else:
            importance = {}
        return importance
    else:
        raise ValueError('You must call the fit function before !')",self.__fitOK,197,self.get_params()['regressor'] is not None,False,6.837203339116283,N/A
"def fit_transform(self, df):
    """"""Fits and transforms train and test datasets

        Automatically drops ids and drifting variables between train and test datasets.
        The list of drift coefficients is available and saved as ""drifts.txt""

        Parameters
        ----------
        df : dict, defaut = None
            Dictionnary containing :

            - 'train' : pandas dataframe for train dataset
            - 'test' : pandas dataframe for test dataset
            - 'target' : pandas serie for the target on train set

        Returns
        -------
        dict
            Dictionnary containing :

            - 'train' : transformed pandas dataframe for train dataset
            - 'test' : transformed pandas dataframe for test dataset
            - 'target' : pandas serie for the target on train set
        """"""
<mask>:
        if self.verbose:
            print('')
            print('You have no test dataset...')
        return df
    else:
        start_time = time.time()
        ds = DriftThreshold(self.threshold)
        na = NA_encoder(numerical_strategy=0)
        ca = Categorical_encoder()
        pp = Pipeline([('na', na), ('ca', ca)])
        pp.fit(df['train'], None)
        if self.verbose:
            print('')
            print('computing drifts ...')
        ds.fit(pp.transform(df['train']), pp.transform(df['test']))
        if self.verbose:
            print('CPU time: %s seconds' % (time.time() - start_time))
            print('')
        self.__fitOK = True
        self.__Ddrifts = ds.drifts()
        drifts_top = sorted(ds.drifts().items(), key=lambda x: x[1], reverse=True)[:10]
        if self.verbose:
            print('> Top 10 drifts')
            print('')
            for d in range(len(drifts_top)):
                print(drifts_top[d])
        if self.verbose:
            print('')
            print('> Deleted variables : ' + str(ds.get_support(complement=True)))
        if self.to_path is not None:
            try:
                os.mkdir(self.to_path)
            except OSError:
                pass
            file = open(self.to_path + '/drifts.txt', 'w')
            file.write('\n')
            file.write('*******************************************  Drifts coefficients *******************************************\n')
            file.write('\n')
            for var, d in sorted(ds.drifts().items(), key=lambda x: x[1], reverse=True):
                file.write(str(var) + ' = ' + str(d) + '\n')
            file.close()
            if self.verbose:
                print('> Drift coefficients dumped into directory : ' + self.to_path)
        if self.inplace:
            df['train'] = ds.transform(df['train'])
            df['test'] = ds.transform(df['test'])
        else:
            return {'train': ds.transform(df['train']), 'test': ds.transform(df['test']), 'target': df['target']}",df['test'].shape[0] == 0,261,not df.has_dataset(),False,3.983253478176822,N/A
"def drifts(self):
    """"""Returns the univariate drifts for all variables.

        Returns
        -------
        dict
            Dictionnary containing the drifts for each feature
        """"""
<mask>:
        return self.__Ddrifts
    else:
        raise ValueError('Call the fit_transform function before !')",self.__fitOK,31,self.__Ddrifts is not None,False,34.57207846419409,N/A
"def convert_list(serie):
    """"""Converts lists in a pandas serie into a dataframe
    where which element of a list is a column

    Parameters
    ----------
    serie : pandas Serie
        The serie you want to cast into a dataframe

    Returns
    -------
    pandas DataFrame
        The converted dataframe
    """"""
    import numpy
    import pandas
<mask>:
        serie = serie.apply(lambda x: [x] if type(x) != list else x)
        cut = int(numpy.percentile(serie.apply(len), 90))
        serie = serie.apply(lambda x: x[:cut])
        return pandas.DataFrame(serie.tolist(), index=serie.index, columns=[serie.name + '_item' + str(i + 1) for i in range(cut)])
    else:
        return serie",serie.apply(lambda x: type(x) == list).sum() > 0,85,"isinstance(serie, pd.DataFrame)",False,1.5378506155406293,N/A
"def convert_float_and_dates(serie):
    """"""Converts into float if possible and converts dates.

    Creates timestamp from 01/01/2017, year, month, day, day_of_week and hour

    Parameters
    ----------
    serie : pandas Serie
        The serie you want to convert

    Returns
    -------
    pandas DataFrame
        The converted dataframe
    """"""
    import pandas
<mask>:
        df = pandas.DataFrame([], index=serie.index)
        df[serie.name + '_TIMESTAMP'] = (pandas.DatetimeIndex(serie) - pandas.datetime(2017, 1, 1)).total_seconds()
        df[serie.name + '_YEAR'] = pandas.DatetimeIndex(serie).year.astype(float)
        df[serie.name + '_MONTH'] = pandas.DatetimeIndex(serie).month.astype(float)
        df[serie.name + '_DAY'] = pandas.DatetimeIndex(serie).day.astype(float)
        df[serie.name + '_DAYOFWEEK'] = pandas.DatetimeIndex(serie).dayofweek.astype(float)
        df[serie.name + '_HOUR'] = pandas.DatetimeIndex(serie).hour.astype(float) + pandas.DatetimeIndex(serie).minute.astype(float) / 60.0 + pandas.DatetimeIndex(serie).second.astype(float) / 3600.0
        return df
    else:
        try:
            serie = serie.apply(float)
        except:
            pass
        if serie.dtype != 'object':
            return serie
        else:
            df = pandas.DataFrame([], index=serie.index)
            try:
                serie_to_df = pandas.DatetimeIndex(pd.to_datetime(serie))
                df[serie.name + '_TIMESTAMP'] = (serie_to_df - pandas.datetime(2017, 1, 1)).total_seconds()
                df[serie.name + '_YEAR'] = serie_to_df.year.astype(float)
                df[serie.name + '_MONTH'] = serie_to_df.month.astype(float)
                df[serie.name + '_DAY'] = serie_to_df.day.astype(float)
                df[serie.name + '_DAYOFWEEK'] = serie_to_df.dayofweek.astype(float)
                df[serie.name + '_HOUR'] = serie_to_df.hour.astype(float) + serie_to_df.minute.astype(float) / 60.0 + serie_to_df.second.astype(float) / 3600.0
                return df
            except:
                return serie",serie.dtype == 'datetime64[ns]',160,serie.dtype == 'object',False,39.01126486653949,N/A
"def set_params(self, **params):
<mask>:
        self.estimator = params['estimator']
    if 'n_folds' in params.keys():
        self.n_folds = params['n_folds']
    if 'stratify' in params.keys():
        self.stratify = params['stratify']
    if 'random_state' in params.keys():
        self.random_state = params['random_state']",'estimator' in params.keys(),28,'estimator' in params.keys(),True,100.00000000000004,N/A
"def fit(self, df_train, df_test):
    """"""
        Computes the drift between the two datasets

        Parameters
        ----------
        df_train : pandas dataframe of shape = (n_train, p)
            The train set

        df_test : pandas dataframe of shape = (n_test, p)
            The test set

        Returns
        -------
        self : object
            Returns self.
        """"""
    df_train['target'] = 0
    df_test['target'] = 1
    self.__target = pd.concat((df_train.target, df_test.target), ignore_index=True)
<mask>:
        self.__cv = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
    else:
        self.__cv = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
    X_tmp = pd.concat((df_train, df_test), ignore_index=True).drop(['target'], axis=1)
    self.__pred = cross_val_predict(estimator=self.estimator, X=X_tmp, y=self.__target, cv=self.__cv, method='predict_proba')[:, 1]
    del df_train['target']
    del df_test['target']
    self.__fitOK = True
    return self",self.stratify,92,self.is_stratified,False,21.3643503198117,N/A
"def score(self):
    """"""Returns the global drift measure between two datasets.

         0. = No drift. 1. = Maximal Drift

        Returns
        -------
        float
            The drift measure
        """"""
    S = []
<mask>:
        X_zeros = np.zeros(len(self.__target))
        for train_index, test_index in self.__cv.split(X=X_zeros, y=self.__target):
            S.append(roc_auc_score(self.__target.iloc[test_index], self.__pred[test_index]))
        return (max(np.mean(S), 1 - np.mean(S)) - 0.5) * 2
    else:
        raise ValueError('Call the fit function before !')",self.__fitOK,57,self.__cv is not None,False,34.57207846419409,N/A
"def predict(self):
    """"""Returns the probabilities that the sample belongs to the test dataset

        Returns
        -------
        Array of shape = (n_train+n_test,)
            The probabilities
        """"""
<mask>:
        return self.__pred
    else:
        raise ValueError('Call the fit function before !')",self.__fitOK,34,self.__pred is not None,False,34.57207846419409,N/A
"def set_params(self, **params):
    """"""Set parameters of a DriftThreshold object.""""""
<mask>:
        self.threshold = params['threshold']
    if 'subsample' in params.keys():
        self.subsample = params['subsample']
    if 'estimator' in params.keys():
        self.estimator = params['estimator']
    if 'n_folds' in params.keys():
        self.n_folds = params['n_folds']
    if 'stratify' in params.keys():
        self.stratify = params['stratify']
    if 'random_state' in params.keys():
        self.random_state = params['random_state']
    if 'n_jobs' in params.keys():
        self.n_jobs = params['n_jobs']",'threshold' in params.keys(),55,'threshold' in params.keys(),True,100.00000000000004,N/A
"def fit(self, df_train, df_test):
    """"""Compute the univariate drifts between df_train and df_test datasets.

        Parameters
        ----------
        df_train : pandas dataframe of shape = (n_train, p)
            The train set

        df_test : pandas dataframe of shape = (n_test, p)
            The test set

        Returns
        -------
        None

        """"""
    self.__Ddrifts = dict()
<mask>:
        Ldrifts = [sync_fit(df_train.sample(frac=self.subsample)[[col]], df_test.sample(frac=self.subsample)[[col]], self.estimator, self.n_folds, self.stratify, self.random_state) for col in df_train.columns]
    else:
        Ldrifts = Parallel(n_jobs=self.n_jobs)((delayed(sync_fit)(df_train.sample(frac=self.subsample)[[col]], df_test.sample(frac=self.subsample)[[col]], self.estimator, self.n_folds, self.stratify, self.random_state) for col in df_train.columns))
    for i, col in enumerate(df_train.columns):
        self.__Ddrifts[col] = Ldrifts[i]
    del Ldrifts
    self.__fitOK = True",sys.platform == 'win32',85,self.n_jobs == 1,False,12.22307556087252,N/A
"def transform(self, df):
    """"""Select the features with low drift.

        Parameters
        ----------
        df : pandas dataframe
            A dataset with the same features

        Returns
        -------
        pandas DataFrame
            The transformed dataframe

        """"""
<mask>:
        selected_col = []
        for i, col in enumerate(df.columns):
            if self.__Ddrifts[col] < self.threshold:
                selected_col.append(col)
        return df[selected_col]
    else:
        raise ValueError('Call the fit function before !')",self.__fitOK,53,self.__Ddrifts is not None,False,34.57207846419409,N/A
"def get_support(self, complement=False):
    """"""Return the variables kept or dropped.

        Parameters
        ----------
        complement : bool, default = True
            If True, returns the features to drop
            If False, returns the features to keep

        Returns
        -------
        list
            The list of features to keep or to drop.

        """"""
<mask>:
        keepList = []
        dropList = []
        for col in self.__Ddrifts:
            if self.__Ddrifts[col] < self.threshold:
                keepList.append(col)
            else:
                dropList.append(col)
        if complement:
            return dropList
        else:
            return keepList
    else:
        raise ValueError('Call the fit function before !')",self.__fitOK,77,self.__Ddrifts is not None,False,34.57207846419409,N/A
"def drifts(self):
    """"""Return the univariate drifts for all variables.

        Returns
        -------
        dict
            The dictionnary of drift measures for each features

        """"""
<mask>:
        return self.__Ddrifts
    else:
        raise ValueError('Call the fit function before !')",self.__fitOK,32,self.__Ddrifts is not None,False,34.57207846419409,N/A
"def set_params(self, **params):
    self.__fitOK = False
    for k, v in params.items():
<mask>:
            warnings.warn('Invalid parameter a for optimiser Optimiser. Parameter IGNORED. Check the list of available parameters with `optimiser.get_params().keys()`')
        else:
            setattr(self, k, v)",k not in self.get_params(),32,k == 'params',False,3.564186929405141,N/A
"def remove_screenshots(text):
    """"""
    Removes the section with screenshots since PyPI doesn't like them.
    """"""
    outputting = True
    new_text = ''
    for line in text.splitlines():
<mask>:
            outputting = False
            continue
        if len(line) and line[0] in string.ascii_letters:
            outputting = True
        if outputting:
            new_text += line + '\n'
    return new_text",line.startswith('Screenshots'),47,not line,False,6.7667641618306344,N/A
"def pushtx():
    print('\n### Pushing translations and sources to Transifex...')
    print('Warning: This might destroy existing translations. Probably you should pull first.')
<mask>:
        local('tx push -s -t')
    else:
        print('Aborting.')","confirm('Continue anyways?', default=False)",27,local('tx push -s -t'),False,4.410363736106611,N/A
"def parse_django_adminopt_node(env, sig, signode):
    """"""A copy of sphinx.directives.CmdoptionDesc.parse_signature()""""""
    from sphinx.domains.std import option_desc_re
    count = 0
    firstname = ''
    for m in option_desc_re.finditer(sig):
        optname, args = m.groups()
<mask>:
            signode += addnodes.desc_addname(', ', ', ')
        signode += addnodes.desc_name(optname, optname)
        signode += addnodes.desc_addname(args, args)
        if not count:
            firstname = optname
        count += 1
    if not count:
        for m in simple_option_desc_re.finditer(sig):
            optname, args = m.groups()
            if count:
                signode += addnodes.desc_addname(', ', ', ')
            signode += addnodes.desc_name(optname, optname)
            signode += addnodes.desc_addname(args, args)
            if not count:
                firstname = optname
            count += 1
    if not firstname:
        raise ValueError
    return firstname",count,93,count,True,100.00000000000004,N/A
"def __str__(self):
<mask>:
        return gettext('Added ""%(object)s"".') % {'object': self.object_repr}
    elif self.action_flag == self.CHANGE:
        return gettext('Changed ""%(object)s"" - %(changes)s') % {'object': self.object_repr, 'changes': self.change_message}
    elif self.action_flag == self.DELETION:
        return gettext('Deleted ""%(object)s.""') % {'object': self.object_repr}
    return gettext('LogEntry Object')",self.action_flag == self.ADDITION,36,self.action_flag == self.ADD,False,88.01117367933934,N/A
"@property
def action_type(self):
<mask>:
        return _('added')
    if self.is_change():
        return _('changed')
    if self.is_deletion():
        return _('deleted')
    return ''",self.is_addition(),16,self.is_added(),False,48.892302243490086,N/A
"def get_admin_url(self):
    """"""
        Returns the admin URL to edit the object represented by this log entry.
        This is relative to the Django admin index page.
        """"""
<mask>:
        return '{0.app_label}/{0.model}/{1}'.format(self.content_type, quote(self.object_id))
    return None",self.content_type and self.object_id,32,self.object_id,False,30.119421191220216,N/A
"def get_description(action):
<mask>:
        return action.description
    else:
        return capfirst(action.__name__.replace('_', ' '))","hasattr(action, 'description')",10,"hasattr(action, 'description')",True,100.00000000000004,N/A
"def __init__(self, queryset, *args, **kwargs):
    self.queryset = queryset
    self.model = queryset.model
    options = utils.model_options(self.model)
    self.app_label = options.app_label
    self.model_name = options.model_name
    self.item_count = len(queryset)
<mask>:
        objects_name = options.verbose_name
    else:
        objects_name = options.verbose_name_plural
    self.objects_name = force_str(objects_name)
    super().__init__(*args, **kwargs)",self.item_count <= 1,36,self.item_count > 1,False,55.780028607687655,N/A
"def get(self, request):
<mask>:
        return super().get(request)
    message = _(self.empty_message)
    messages.add_message(request, messages.INFO, message)
    return None",self.item_count > 0,14,self.is_active,False,15.848738972120703,N/A
"def post(self, request):
<mask>:
        message = ngettext(self.success_message, self.success_message_plural, self.item_count) % {'count': self.item_count, 'items': self.objects_name}
        messages.add_message(request, messages.INFO, message)
        return None",self.process_queryset() is None,19,self.item_count,False,10.62372743739878,N/A
"def register(self, model, model_admin=None, **kwargs):
    """"""
        Registers the given model with the given admin class. Once a model is
        registered in self.registry, we also add it to app registries in
        self.apps.

        If no model_admin is passed, it will use ModelAdmin2. If keyword
        arguments are given they will be passed to the admin class on
        instantiation.

        If a model is already registered, this will raise ImproperlyConfigured.
        """"""
<mask>:
        raise ImproperlyConfigured('%s is already registered in django-admin2' % model)
    if not model_admin:
        model_admin = types.ModelAdmin2
    self.registry[model] = model_admin(model, admin=self, **kwargs)
    app_label = utils.model_options(model).app_label
    if app_label in self.apps.keys():
        self.apps[app_label][model] = self.registry[model]
    else:
        self.apps[app_label] = {model: self.registry[model]}",model in self.registry,102,model in self.registry,True,100.00000000000004,N/A
"def deregister(self, model):
    """"""
        Deregisters the given model. Remove the model from the self.app as well

        If the model is not already registered, this will raise
        ImproperlyConfigured.
        """"""
    try:
        del self.registry[model]
    except KeyError:
        raise ImproperlyConfigured('%s was never registered in django-admin2' % model)
    app_label = utils.model_options(model).app_label
    del self.apps[app_label][model]
<mask>:
        del self.apps[app_label]",self.apps[app_label] is {},50,model in self.apps,False,11.976547020391715,N/A
"def register_app_verbose_name(self, app_label, app_verbose_name):
    """"""
        Registers the given app label with the given app verbose name.

        If a app_label is already registered, this will raise
        ImproperlyConfigured.
        """"""
<mask>:
        raise ImproperlyConfigured('%s is already registered in django-admin2' % app_label)
    self.app_verbose_names[app_label] = app_verbose_name",app_label in self.app_verbose_names,40,app_label in self.app_verbose_names,True,100.00000000000004,N/A
"def autodiscover(self):
    """"""
        Autodiscovers all admin2.py modules for apps in INSTALLED_APPS by
        trying to import them.
        """"""
    for app_name in [x for x in settings.INSTALLED_APPS]:
        try:
            import_module('%s.admin2' % app_name)
        except ImportError as e:
<mask>:
                continue
            raise e",str(e).startswith('No module named') and 'admin2' in str(e),37,e.name == 'ModuleNotFound',False,1.3063150080561918,N/A
"def get_admin_by_name(self, name):
    """"""
        Returns the admin instance that was registered with the passed in
        name.
        """"""
    for object_admin in self.registry.values():
<mask>:
            return object_admin
    raise ValueError('No object admin found with name {}'.format(repr(name)))",object_admin.name == name,32,object_admin.name == name,True,100.00000000000004,N/A
"def datetime_renderer(value, field):
    """"""
    Localize and format the specified date.

    :param value: The value to process.
    :type value: datetime.date or datetime.time or datetime.datetime
    :param field: The model field instance
    :type field: django.db.models.fields.Field
    :rtype: unicode

    """"""
<mask>:
        return formats.localize(timezone.template_localtime(value))
    elif isinstance(value, (date, time)):
        return ':'.join(formats.localize(value).split(':')[:2])
    else:
        return ':'.join(value.split(':')[:2])","isinstance(value, datetime)",47,"isinstance(value, datetime.datetime)",False,57.21248424548516,N/A
"def number_renderer(value, field):
    """"""
    Format a number.

    :param value: The value to process.
    :type value: float or long
    :param field: The model field instance
    :type field: django.db.models.fields.Field
    :rtype: unicode

    """"""
<mask>:
        return formats.number_format(value, field.decimal_places)
    return formats.number_format(value)","isinstance(field, models.DecimalField)",36,field.decimal_places,False,6.9717291216921975,N/A
"def __new__(cls, name, bases, attrs):
    new_class = super().__new__(cls, name, bases, attrs)
    view_list = []
    for key, value in attrs.items():
<mask>:
            if not value.name:
                value.name = key
            view_list.append(value)
    view_list.extend(getattr(new_class, 'views', []))
    new_class.views = view_list
    return new_class","isinstance(value, views.AdminView)",35,"isinstance(value, View)",False,38.49815007763549,N/A
"def __init__(self, model, admin, name=None, **kwargs):
    self.name = name
    self.model = model
    self.admin = admin
    model_options = utils.model_options(model)
    self.app_label = model_options.app_label
    self.model_name = model_options.object_name.lower()
<mask>:
        self.name = '{}_{}'.format(self.app_label, self.model_name)
    if self.verbose_name is None:
        self.verbose_name = model_options.verbose_name
    if self.verbose_name_plural is None:
        self.verbose_name_plural = model_options.verbose_name_plural",self.name is None,43,self.name is None,True,100.00000000000004,N/A
"def get_update_kwargs(self):
    kwargs = self.get_default_view_kwargs()
    form_class = self.update_form_class if self.update_form_class else self.form_class
<mask>:
        form_class = modelform_factory(self.model, fields='__all__')
    kwargs.update({'inlines': self.inlines, 'form_class': form_class})
    return kwargs",form_class is None,23,form_class is None,True,100.00000000000004,N/A
"def get_urls(self):
    pattern_list = []
    for admin_view in self.views:
        admin_view.model_admin = self
        get_kwargs = getattr(self, 'get_%s_kwargs' % admin_view.name, None)
<mask>:
            get_kwargs = admin_view.get_view_kwargs
        try:
            view_instance = admin_view.view.as_view(**get_kwargs())
        except Exception as e:
            trace = sys.exc_info()[2]
            new_exception = TypeError('Cannot instantiate admin view ""{}.{}"". The error that got raised was: {}'.format(self.__class__.__name__, admin_view.name, e))
            try:
                raise new_exception.with_traceback(trace)
            except AttributeError:
                raise (new_exception, None, trace)
        pattern_list.append(re_path(admin_view.url, view=view_instance, name=self.get_prefixed_view_name(admin_view.name)))
    return pattern_list",not get_kwargs,64,not get_kwargs,True,100.00000000000004,N/A
"def clean(self):
    username = self.cleaned_data.get('username')
    password = self.cleaned_data.get('password')
    message = ERROR_MESSAGE
<mask>:
        self.user_cache = authenticate(username=username, password=password)
        if self.user_cache is None:
            raise ValidationError(message % {'username': self.username_field.verbose_name})
        elif not self.user_cache.is_active or not self.user_cache.is_staff:
            raise ValidationError(message % {'username': self.username_field.verbose_name})
    return self.cleaned_data",username and password,38,username and password,True,100.00000000000004,N/A
"def lookup_needs_distinct(opts, lookup_path):
    """"""
    Returns True if 'distinct()' should be used to query the given lookup path.

    This is adopted from the Django core. django-admin2 mandates that code
    doesn't depend on imports from django.contrib.admin.

    https://github.com/django/django/blob/1.9.6/django/contrib/admin/utils.py#L22
    """"""
    lookup_fields = lookup_path.split(LOOKUP_SEP)
    for field_name in lookup_fields:
<mask>:
            field_name = opts.pk.name
        try:
            field = opts.get_field(field_name)
        except FieldDoesNotExist:
            continue
        else:
            if hasattr(field, 'get_path_info'):
                path_info = field.get_path_info()
                opts = path_info[-1].to_opts
                if any((path.m2m for path in path_info)):
                    return True
    return False",field_name == 'pk',74,field_name == 'pk',True,100.00000000000004,N/A
"def get_attr(obj, attr):
    """"""
    Get the right value for the attribute. Handle special cases like callables
    and the __str__ attribute.
    """"""
<mask>:
        from builtins import str as text
        value = text(obj)
    else:
        attribute = getattr(obj, attr)
        value = attribute() if callable(attribute) else attribute
    return value",attr == '__str__',45,attr == '__str__',True,100.00000000000004,N/A
"def collect(self, objs, source=None, source_attr=None, **kwargs):
    for obj in objs:
<mask>:
            related_name = source_attr % {'class': source._meta.model_name, 'app_label': source._meta.app_label}
            self.add_edge(getattr(obj, related_name), obj)
        else:
            self.add_edge(None, obj)
        self.model_objs[obj._meta.model].add(obj)
    try:
        return super().collect(objs, source_attr=source_attr, **kwargs)
    except ProtectedError as e:
        self.protected.update(e.protected_objects)",source_attr and (not source_attr.endswith('+')),36,source,False,1.1253517471925916e-05,N/A
"def related_objects(self, *args):
<mask>:
        related_model, related_fields, objs = args
        qs = super().related_objects(related_model, related_fields, objs)
        return qs.select_related(*[related_field.name for related_field in related_fields])
    elif len(args) == 2:
        related, objs = args
        qs = super(NestedObjects, self).related_objects(related, objs)
        return qs.select_related(related.field.name)",len(args) == 3,35,len(args) == 1,False,80.91067115702207,N/A
"def _nested(self, obj, seen, format_callback):
<mask>:
        return []
    seen.add(obj)
    children = []
    for child in self.edges.get(obj, ()):
        children.extend(self._nested(child, seen, format_callback))
    if format_callback:
        ret = [format_callback(obj)]
    else:
        ret = [obj]
    if children:
        ret.append(children)
    return ret",obj in seen,34,obj in seen,True,100.00000000000004,N/A
"def model_permission(permission):
    """"""
    This is actually a permission check factory. It means that it will return
    a function that can then act as a permission check. The returned callable
    will check if the user has the with ``permission`` provided model
    permission. You can use ``{app_label}`` and ``{model_name}`` as
    placeholders in the permission name. They will be replaced with the
    ``app_label`` and the ``model_name`` (in lowercase) of the model that the
    current view is operating on.

    Example:

    .. code-block:: python

        check_add_perm = model_permission('{app_label}.add_{model_name}')

        class ModelAddPermission(permissions.BasePermission):
            permissions = [check_add_perm]
    """"""

    def has_permission(request, view, obj=None):
        model_class = getattr(view, 'model', None)
        queryset = getattr(view, 'queryset', None)
<mask>:
            model_class = queryset.model
        assert model_class, 'Cannot apply model permissions on a view that does not have a `.model` or `.queryset` property.'
        try:
            model_name = model_class._meta.model_name
        except AttributeError:
            model_name = model_class._meta.module_name
        permission_name = permission.format(app_label=model_class._meta.app_label, model_name=model_name)
        return request.user.has_perm(permission_name, obj)
    return has_permission",model_class is None and queryset is not None,142,queryset and model_class is None,False,43.56435540068713,N/A
"def has_permission(self, request, view, obj=None):
<mask>:
        for permission_check in self.get_permission_checks(request, view):
            if not permission_check(request, view, obj):
                return False
        return True
    return False",request.user,22,"self.has_permission(request, view)",False,4.990049701936832,N/A
"def bind_admin(self, admin):
    """"""
        Return a clone of the permission wrapper with a new model_admin bind
        to it.
        """"""
<mask>:
        try:
            admin = self._model_admin.admin.get_admin_by_name(admin)
        except ValueError:
            return ''
    new_permissions = self.clone()
    new_permissions._view = None
    new_permissions._model_admin = admin
    return new_permissions","isinstance(admin, str)",39,self._model_admin.admin is not None,False,3.7477767366779213,N/A
"def bind_view(self, view):
    """"""
        Return a clone of the permission wrapper with a new view bind to it.
        """"""
<mask>:
        if view not in self.view_name_mapping:
            return ''
        view_name = self.view_name_mapping[view]
        view = getattr(self._model_admin, view_name).view
    else:
        return ''
    if isinstance(view, type):
        view = view(request=self._request, **self._model_admin.get_default_view_kwargs())
    new_permissions = self.clone()
    new_permissions._view = view
    return new_permissions","isinstance(view, str)",52,self._request.user.is_authenticated,False,0.0,N/A
"def __getitem__(self, key):
    match = self._has_named_permission_regex.match(key)
<mask>:
        view_name = match.groupdict()['name']
        return self.bind_view(view_name)
    try:
        admin_site = self._model_admin.admin
        model_admin = admin_site.get_admin_by_name(key)
    except ValueError:
        raise KeyError
    return self.bind_admin(model_admin)",match,25,match,True,100.00000000000004,N/A
"def get_serializer_class(self):
<mask>:
        model_class = self.get_model()

        class ModelAPISerilizer(Admin2APISerializer):
            _default_view_name = ':'.join((self.model_admin.admin.name, '%(app_label)s_%(model_name)s_api_detail'))

            class Meta:
                model = model_class
                fields = '__all__'
        return ModelAPISerilizer
    return super().get_serializer_class()",self.serializer_class is None,24,self.model_admin,False,15.848738972120703,N/A
"def build_list_filter(request, model_admin, queryset):
    """"""Builds :class:`~django_filters.FilterSet` instance
    for :attr:`djadmin2.ModelAdmin2.Meta.list_filter` option.

    If :attr:`djadmin2.ModelAdmin2.Meta.list_filter` is not
    sequence, it's considered to be class with interface like
    :class:`django_filters.FilterSet` and its instantiate wit
    `request.GET` and `queryset`.
    """"""
<mask>:
        return model_admin.list_filter(request.GET, queryset=queryset)
    filters = []
    for field_filter in model_admin.list_filter:
        if isinstance(field_filter, str):
            filters.append(get_filter_for_field_name(queryset.model, field_filter))
        else:
            filters.append(field_filter)
    filterset_dict = {}
    for field_filter in filters:
        filterset_dict[field_filter.field_name] = field_filter
    fields = list(filterset_dict.keys())
    filterset_dict['Meta'] = type(type_str('Meta'), (), {'model': queryset.model, 'fields': fields})
    return type(type_str('%sFilterSet' % queryset.model.__name__), (django_filters.FilterSet,), filterset_dict)(request.GET, queryset=queryset)","not isinstance(model_admin.list_filter, collections.abc.Iterable)",78,"django.VERSION < (1, 7)",False,2.771947612153099,N/A
"def has_permission(self, obj=None):
    """"""
        Return ``True`` if the permission for this view shall be granted,
        ``False`` otherwise. Supports object-level permission by passing the
        related object as first argument.
        """"""
    for permission in self.permissions:
<mask>:
            return False
    return True","not permission.has_permission(self.request, self, obj)",38,"not self.has_permission(permission, obj)",False,36.71035772322804,N/A
"def dispatch(self, request, *args, **kwargs):
<mask>:
        if self.raise_exception:
            raise PermissionDenied
        else:
            return redirect_to_login(request.get_full_path(), self.get_login_url(), self.get_redirect_field_name())
    return super().dispatch(request, *args, **kwargs)",not self.has_permission(),19,not request.user.is_authenticated,False,7.267884212102741,N/A
"def dispatch(self, request, *args, **kwargs):
<mask>:
        if request.path == reverse('admin2:logout'):
            return HttpResponseRedirect(self.index_path)
        if request.path == self.index_path:
            extra = {'next': request.GET.get('next', self.index_path)}
            return self.login_view().dispatch(request, *args, extra_context=extra, **kwargs)
    return super().dispatch(request, *args, **kwargs)",self.is_user(request),30,self.admin_enabled,False,12.975849993980741,N/A
"def post(self, request):
    action_name = request.POST['action']
    action_callable = self.get_actions()[action_name]['action_callable']
    selected_model_pks = request.POST.getlist('selected_model_pk')
<mask>:
        queryset = self.model.objects.filter(pk__in=selected_model_pks)
    else:
        queryset = self.model.objects.all()
    if hasattr(action_callable, 'process_queryset'):
        response = action_callable.as_view(queryset=queryset, model_admin=self.model_admin)(request)
    else:
        response = action_callable(request, queryset)
    if response is None:
        return HttpResponseRedirect(self.get_success_url())
    else:
        return response","getattr(action_callable, 'only_selected', True)",41,selected_model_pks,False,3.1325998243558226,N/A
"def get_search_results(self, queryset, search_term):

    def construct_search(field_name):
<mask>:
            return '%s__istartswith' % field_name[1:]
        elif field_name.startswith('='):
            return '%s__iexact' % field_name[1:]
        elif field_name.startswith('@'):
            return '%s__search' % field_name[1:]
        else:
            return '%s__icontains' % field_name
    use_distinct = False
    orm_lookups = [construct_search(str(search_field)) for search_field in self.model_admin.search_fields]
    for bit in search_term.split():
        or_queries = [models.Q(**{orm_lookup: bit}) for orm_lookup in orm_lookups]
        queryset = queryset.filter(reduce(operator.or_, or_queries))
    if not use_distinct:
        for search_spec in orm_lookups:
            opts = utils.model_options(self.get_model())
            if utils.lookup_needs_distinct(opts, search_spec):
                use_distinct = True
                break
    return (queryset, use_distinct)",field_name.startswith('^'),74,field_name.startswith('('),False,70.71067811865478,N/A
"def get_queryset(self):
    queryset = super().get_queryset()
    search_term = self.request.GET.get('q', None)
    search_use_distinct = False
<mask>:
        queryset, search_use_distinct = self.get_search_results(queryset, search_term)
    queryset = self._modify_queryset_for_ordering(queryset)
    if self.model_admin.list_filter:
        queryset = self.build_list_filter(queryset).qs
    if self.model_admin.date_hierarchy:
        queryset = self.build_date_filter(queryset, self.model_admin.date_hierarchy).qs
    queryset = self._modify_queryset_for_sort(queryset)
    if search_use_distinct:
        return queryset.distinct()
    else:
        return queryset",self.model_admin.search_fields and search_term,42,search_term,False,3.567399334725242,N/A
"def _modify_queryset_for_ordering(self, queryset):
    ordering = self.model_admin.get_ordering(self.request)
<mask>:
        queryset = queryset.order_by(*ordering)
    return queryset",ordering,12,ordering,True,100.00000000000004,N/A
"def _modify_queryset_for_sort(self, queryset):
    sort_by = self.request.GET.get('sort', None)
<mask>:
        if sort_by == '-__str__':
            queryset = queryset[::-1]
        try:
            field_exists = sort_by
            if field_exists[0] == '-':
                field_exists = field_exists[1:]
            options = utils.model_options(self.model)
            options.get_field(field_exists)
            queryset = queryset.order_by(sort_by)
        except FieldDoesNotExist:
            pass
    return queryset",sort_by,38,sort_by,True,100.00000000000004,N/A
"@register.filter
def for_admin(permissions, admin):
    """"""
    Only useful in the permission handling. This filter binds a new admin to
    the permission handler to allow checking views of an arbitrary admin.
    """"""
<mask>:
        return permissions
    return permissions.bind_admin(admin)",permissions == '',35,not admin,False,0.0,N/A
"@register.filter
def for_view(permissions, view):
    """"""
    Only useful in the permission handling. This filter binds a new view to
    the permission handler to check for view names that are not known during
    template compile time.
    """"""
<mask>:
        return permissions
    return permissions.bind_view(view)",permissions == '',40,view is None,False,0.0,N/A
"@register.filter
def for_object(permissions, obj):
    """"""
    Only useful in the permission handling. This filter binds a new object to
    the permission handler to check for object-level permissions.
    """"""
<mask>:
        return permissions
    return permissions.bind_object(obj)",permissions == '',32,obj is None,False,0.0,N/A
"@register.simple_tag(takes_context=True)
def render(context, model_instance, attribute_name):
    """"""
    This filter applies all renderers specified in admin2.py to the field.
    """"""
    value = utils.get_attr(model_instance, attribute_name)
    admin = context['view'].model_admin
    renderer = admin.field_renderers.get(attribute_name, False)
<mask>:
        return value
    if not renderer:
        if isinstance(value, bool):
            renderer = renderers.boolean_renderer
        elif isinstance(value, (date, time, datetime)):
            renderer = renderers.datetime_renderer
        elif isinstance(value, Number):
            renderer = renderers.number_renderer
        else:
            return value
    try:
        field = model_instance._meta.get_field(attribute_name)
    except FieldDoesNotExist:
        field = None
    return renderer(value, field)",renderer is None,70,not renderer,False,30.326532985631665,N/A
"def _copy_attributes(original, new_widget, attributes):
    for attr in attributes:
        original_value = getattr(original, attr)
        original_value = deepcopy(original_value)
        old_value_on_new_widget = getattr(new_widget.__class__, attr, None)
<mask>:
            setattr(new_widget, attr, original_value)","not isinstance(old_value_on_new_widget, property)",24,"old_value_on_new_widget and attr not in ['value', 'value_on_new_widget', 'value_on_new_widget', 'value_on_new_widget', 'value_on_new_widget', 'value_on_new_widget', 'value_on_new_widget', 'value_on_new_widget', 'value_on_new_widget', 'value_on_new_widget', '",False,8.911696651501536,N/A
"def _create_radioselect(original):
<mask>:
        return original
    create_new_widget = _create_widget(floppyforms.widgets.RadioSelect, ('choices', 'allow_multiple_selected'))
    return create_new_widget(original)",original.renderer is not django.forms.widgets.RadioFieldRenderer,12,"not isinstance(original, floppyforms.widgets.RadioSelect)",False,14.473710747837542,N/A
"def allow_floppify_widget_for_field(field):
    """"""
    We only allow to replace a widget with the floppyform counterpart if the
    original, by django determined widget is still in place. We don't want to
    override custom widgets that a user specified.
    """"""
<mask>:
        if field.widget.__class__ is django.forms.NumberInput:
            return True
    if field.widget.__class__ is field.__class__.widget:
        return True
    return False","isinstance(field, django.forms.IntegerField) and (not field.localize)",52,"django.VERSION < (1, 7)",False,4.929297364398415,N/A
"def floppify_widget(widget, field=None):
    """"""
    Get an instance of django.forms.widgets.Widget and return a new widget
    instance but using the corresponding floppyforms widget class.

    Only original django widgets will be replaced with a floppyforms version.
    The widget will be returned unaltered if it is not known, e.g. if it's a
    custom widget from a third-party app.

    The optional parameter ``field`` can be used to influence the widget
    creation further. This is useful since floppyforms supports more widgets
    than django does. For example is django using a ``TextInput`` for a
    ``EmailField``, but floppyforms has a better suiting widget called
    ``EmailInput``. If a widget is found specifically for the passed in
    ``field``, it will take precendence to the first parameter ``widget``
    which will effectively be ignored.
    """"""
<mask>:
        create_widget = _django_field_to_floppyform_widget.get(field.__class__)
        if create_widget is not None:
            if allow_floppify_widget_for_field(field):
                return create_widget(widget)
    create_widget = _django_to_floppyforms_widget.get(widget.__class__)
    if create_widget is not None:
        return create_widget(widget)
    return widget",field is not None,148,field is not None,True,100.00000000000004,N/A
"def initialize_options(self):
<mask>:
        return
    super().initialize_options()
    self.use_system_llhttp = False
    self.use_system_http_parser = False
    self.cython_always = False
    self.cython_annotate = None
    self.cython_directives = None","getattr(self, '_initialized', False)",20,self.use_system_llhttp and self.use_system_http_parser,False,2.719665272174911,N/A
"def finalize_options(self):
<mask>:
        return
    need_cythonize = self.cython_always
    cfiles = {}
    for extension in self.distribution.ext_modules:
        for i, sfile in enumerate(extension.sources):
            if sfile.endswith('.pyx'):
                prefix, ext = os.path.splitext(sfile)
                cfile = prefix + '.c'
                if os.path.exists(cfile) and (not self.cython_always):
                    extension.sources[i] = cfile
                else:
                    if os.path.exists(cfile):
                        cfiles[cfile] = os.path.getmtime(cfile)
                    else:
                        cfiles[cfile] = 0
                    need_cythonize = True
    if need_cythonize:
        try:
            import Cython
        except ImportError:
            raise RuntimeError('please install Cython to compile httptools from source')
        if Cython.__version__ < '0.29':
            raise RuntimeError('httptools requires Cython version 0.29 or greater')
        from Cython.Build import cythonize
        directives = {}
        if self.cython_directives:
            for directive in self.cython_directives.split(','):
                k, _, v = directive.partition('=')
                if v.lower() == 'false':
                    v = False
                if v.lower() == 'true':
                    v = True
                directives[k] = v
        self.distribution.ext_modules[:] = cythonize(self.distribution.ext_modules, compiler_directives=directives, annotate=self.cython_annotate)
    super().finalize_options()
    self._initialized = True","getattr(self, '_initialized', False)",123,self._initialized,False,4.238556455648295,N/A
"def build_extensions(self):
    mod_parser, mod_url_parser = self.distribution.ext_modules
<mask>:
        mod_parser.libraries.append('llhttp')
        if sys.platform == 'darwin' and os.path.exists('/opt/local/include'):
            mod_parser.include_dirs.append('/opt/local/include')
    else:
        mod_parser.include_dirs.append(str(ROOT / 'vendor' / 'llhttp' / 'include'))
        mod_parser.include_dirs.append(str(ROOT / 'vendor' / 'llhttp' / 'src'))
        mod_parser.sources.append('vendor/llhttp/src/api.c')
        mod_parser.sources.append('vendor/llhttp/src/http.c')
        mod_parser.sources.append('vendor/llhttp/src/llhttp.c')
    if self.use_system_http_parser:
        mod_url_parser.libraries.append('http_parser')
        if sys.platform == 'darwin' and os.path.exists('/opt/local/include'):
            mod_url_parser.include_dirs.append('/opt/local/include')
    else:
        mod_url_parser.include_dirs.append(str(ROOT / 'vendor' / 'http-parser'))
        mod_url_parser.sources.append('vendor/http-parser/http_parser.c')
    super().build_extensions()",self.use_system_llhttp,51,self.use_system_llhttp,True,100.00000000000004,N/A
"def _get_or_create_tracker_id(class_def):
    with _DYNAMIC_CLASS_TRACKER_LOCK:
        class_tracker_id = _DYNAMIC_CLASS_TRACKER_BY_CLASS.get(class_def)
<mask>:
            class_tracker_id = uuid.uuid4().hex
            _DYNAMIC_CLASS_TRACKER_BY_CLASS[class_def] = class_tracker_id
            _DYNAMIC_CLASS_TRACKER_BY_ID[class_tracker_id] = class_def
    return class_tracker_id",class_tracker_id is None,19,not class_tracker_id,False,64.31870218238025,N/A
"def _lookup_class_or_track(class_tracker_id, class_def):
<mask>:
        with _DYNAMIC_CLASS_TRACKER_LOCK:
            class_def = _DYNAMIC_CLASS_TRACKER_BY_ID.setdefault(class_tracker_id, class_def)
            _DYNAMIC_CLASS_TRACKER_BY_CLASS[class_def] = class_tracker_id
    return class_def",class_tracker_id is not None,15,class_def not in _DYNAMIC_CLASS_TRACKER_BY_ID,False,6.250381527944883,N/A
"def register_pickle_by_value(module):
    """"""Register a module to make its functions and classes picklable by value.

    By default, functions and classes that are attributes of an importable
    module are to be pickled by reference, that is relying on re-importing
    the attribute from the module at load time.

    If `register_pickle_by_value(module)` is called, all its functions and
    classes are subsequently to be pickled by value, meaning that they can
    be loaded in Python processes where the module is not importable.

    This is especially useful when developing a module in a distributed
    execution environment: restarting the client Python process with the new
    source code is enough: there is no need to re-install the new version
    of the module on all the worker nodes nor to restart the workers.

    Note: this feature is considered experimental. See the cloudpickle
    README.md file for more details and limitations.
    """"""
<mask>:
        raise ValueError(f'Input should be a module object, got {str(module)} instead')
    if module.__name__ not in sys.modules:
        raise ValueError(f'{module} was not imported correctly, have you used an `import` statement to access it?')
    _PICKLE_BY_VALUE_MODULES.add(module.__name__)","not isinstance(module, types.ModuleType)",172,"not isinstance(module, ModuleType)",False,53.137468984124546,N/A
"def unregister_pickle_by_value(module):
    """"""Unregister that the input module should be pickled by value.""""""
<mask>:
        raise ValueError(f'Input should be a module object, got {str(module)} instead')
    if module.__name__ not in _PICKLE_BY_VALUE_MODULES:
        raise ValueError(f'{module} is not registered for pickle by value')
    else:
        _PICKLE_BY_VALUE_MODULES.remove(module.__name__)","not isinstance(module, types.ModuleType)",39,"not isinstance(module, ModuleType)",False,53.137468984124546,N/A
"def _is_registered_pickle_by_value(module):
    module_name = module.__name__
<mask>:
        return True
    while True:
        parent_name = module_name.rsplit('.', 1)[0]
        if parent_name == module_name:
            break
        if parent_name in _PICKLE_BY_VALUE_MODULES:
            return True
        module_name = parent_name
    return False",module_name in _PICKLE_BY_VALUE_MODULES,30,module_name in _PICKLE_BY_VALUE_MODULES,True,100.00000000000004,N/A
"def subprocess_pickle_string(input_data, protocol=None, timeout=TIMEOUT, add_env=None):
    """"""Retrieve pickle string of an object generated by a child Python process

    Pickle the input data into a buffer, send it to a subprocess via
    stdin, expect the subprocess to unpickle, re-pickle that data back
    and send it back to the parent process via stdout for final unpickling.

    >>> testutils.subprocess_pickle_string([1, 'a', None], protocol=2)
    b'\x80\x02]q\x00(K\x01X\x01\x00\x00\x00aq\x01Ne.'

    """"""
    cmd = [sys.executable, '-W ignore', __file__, '--protocol', str(protocol)]
    cwd, env = _make_cwd_env()
<mask>:
        env.update(add_env)
    proc = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE, cwd=cwd, env=env, bufsize=4096)
    pickle_string = dumps(input_data, protocol=protocol)
    try:
        comm_kwargs = {}
        comm_kwargs['timeout'] = timeout
        out, err = proc.communicate(pickle_string, **comm_kwargs)
        if proc.returncode != 0 or len(err):
            message = 'Subprocess returned %d: ' % proc.returncode
            message += err.decode('utf-8')
            raise RuntimeError(message)
        return out
    except TimeoutExpired as e:
        proc.kill()
        out, err = proc.communicate()
        message = '\n'.join([out.decode('utf-8'), err.decode('utf-8')])
        raise RuntimeError(message) from e",add_env,137,add_env,True,100.00000000000004,N/A
"def _read_all_bytes(stream_in, chunk_size=4096):
    all_data = b''
    while True:
        data = stream_in.read(chunk_size)
        all_data += data
<mask>:
            break
    return all_data",len(data) < chunk_size,18,not data,False,2.489353418393197,N/A
"def pickle_echo(stream_in=None, stream_out=None, protocol=None):
    """"""Read a pickle from stdin and pickle it back to stdout""""""
<mask>:
        stream_in = sys.stdin
    if stream_out is None:
        stream_out = sys.stdout
    if hasattr(stream_in, 'buffer'):
        stream_in = stream_in.buffer
    if hasattr(stream_out, 'buffer'):
        stream_out = stream_out.buffer
    input_bytes = _read_all_bytes(stream_in)
    stream_in.close()
    obj = loads(input_bytes)
    repickled_bytes = dumps(obj, protocol=protocol)
    stream_out.write(repickled_bytes)
    stream_out.close()",stream_in is None,51,stream_in is None,True,100.00000000000004,N/A
"def run(self, func, *args, **kwargs):
    """"""Synchronous remote function call""""""
    input_payload = dumps((func, args, kwargs), protocol=self.protocol)
    result_payload = self.pool.submit(call_func, input_payload, self.protocol).result()
    result = loads(result_payload)
<mask>:
        raise result
    return result","isinstance(result, BaseException)",28,"isinstance(result, Exception)",False,53.7284965911771,N/A
"def memsize(self):
    workers_pids = [p.pid if hasattr(p, 'pid') else p for p in list(self.pool._processes)]
    num_workers = len(workers_pids)
<mask>:
        return 0
    elif num_workers > 1:
        raise RuntimeError('Unexpected number of workers: %d' % num_workers)
    return psutil.Process(workers_pids[0]).memory_info().rss",num_workers == 0,34,num_workers == 0,True,100.00000000000004,N/A
"def load_obj(filename, check_deprecation_warning='auto'):
<mask>:
        check_deprecation_warning = False
    pickle_filepath = PICKLE_DIRECTORY / filename
    if not pickle_filepath.exists():
        pytest.skip(f'Could not find {str(pickle_filepath)}')
    with open(str(pickle_filepath), 'rb') as f:
        if check_deprecation_warning:
            msg = 'A pickle file created using an old'
            with pytest.warns(UserWarning, match=msg):
                obj = pickle.load(f)
        else:
            obj = pickle.load(f)
    return obj",check_deprecation_warning == 'auto',47,check_deprecation_warning == 'auto',True,100.00000000000004,N/A
"def test_extract_class_dict():

    class A(int):
        """"""A docstring""""""

        def method(self):
            return 'a'

    class B:
        """"""B docstring""""""
        B_CONSTANT = 42

        def method(self):
            return 'b'

    class C(A, B):
        C_CONSTANT = 43

        def method_c(self):
            return 'c'
    clsdict = _extract_class_dict(C)
    expected_keys = ['C_CONSTANT', '__doc__', 'method_c']
<mask>:
        expected_keys.insert(2, '__firstlineno__')
    assert list(clsdict.keys()) == expected_keys
    assert clsdict['C_CONSTANT'] == 43
    assert clsdict['__doc__'] is None
    assert clsdict['method_c'](C()) == C().method_c()","sys.version_info >= (3, 13)",58,sys.version_info[0] == 2,False,33.5295721026861,N/A
"def test_empty_cell_preserved(self):

    def f():
<mask>:
            cell = None

        def g():
            cell
        return g
    g1 = f()
    with pytest.raises(NameError):
        g1()
    g2 = pickle_depickle(g1, protocol=self.protocol)
    with pytest.raises(NameError):
        g2()",False,26,self.protocol == 'cell',False,0.0,N/A
"def test_class_no_firstlineno_deletion_(self):

    class A:
        pass
<mask>:
        A_roundtrip = pickle_depickle(A, protocol=self.protocol)
        assert hasattr(A_roundtrip, '__firstlineno__')
        assert A_roundtrip.__firstlineno__ == A.__firstlineno__","hasattr(A, '__firstlineno__')",17,"hasattr(A, 'firstlineno')",False,19.765609300943975,N/A
"@pytest.mark.skipif(platform.python_implementation() == 'PyPy', reason='Skip numpy and scipy tests on PyPy')
def test_ufunc(self):
<mask>:
        self.assertEqual(pickle_depickle(np.add, protocol=self.protocol), np.add)
    else:
        pass
    if spp:
        self.assertEqual(pickle_depickle(spp.iv, protocol=self.protocol), spp.iv)
    else:
        pass",np,25,np,True,100.00000000000004,N/A
"def _check_dynamic_module(self, mod):
    mod = types.ModuleType('mod')
    code = '\n        x = 1\n        def f(y):\n            return x + y\n\n        class Foo:\n            def method(self, x):\n                return f(x)\n        '
    exec(textwrap.dedent(code), mod.__dict__)
    mod2 = pickle_depickle(mod, protocol=self.protocol)
    self.assertEqual(mod.x, mod2.x)
    self.assertEqual(mod.f(5), mod2.f(5))
    self.assertEqual(mod.Foo().method(5), mod2.Foo().method(5))
<mask>:
        mod3 = subprocess_pickle_echo(mod, protocol=self.protocol)
        self.assertEqual(mod.x, mod3.x)
        self.assertEqual(mod.f(5), mod3.f(5))
        self.assertEqual(mod.Foo().method(5), mod3.Foo().method(5))
    mod1, mod2 = pickle_depickle([mod, mod])
    self.assertEqual(id(mod1), id(mod2))
    try:
        sys.modules['mod'] = mod
        depickled_f = pickle_depickle(mod.f, protocol=self.protocol)
        self.assertEqual(mod.f(5), depickled_f(5))
    finally:
        sys.modules.pop('mod', None)",platform.python_implementation() != 'PyPy',69,sys.version_info[0] == 2,False,4.9323515694897075,N/A
"def _calculate_model_cv_score_(df, target, feature, task, cross_validation, random_seed, **kwargs):
    """"""Calculates the mean model score based on cross-validation""""""
    metric = task['metric_key']
    model = task['model']
    df = df.sample(frac=1, random_state=random_seed, replace=False)
<mask>:
        label_encoder = preprocessing.LabelEncoder()
        df[target] = label_encoder.fit_transform(df[target])
        target_series = df[target]
    else:
        target_series = df[target]
    if _dtype_represents_categories(df[feature]):
        one_hot_encoder = preprocessing.OneHotEncoder()
        array = df[feature].__array__()
        sparse_matrix = one_hot_encoder.fit_transform(array.reshape(-1, 1))
        feature_input = sparse_matrix
    else:
        array = df[feature].values
        if not isinstance(array, np.ndarray):
            array = array.to_numpy()
        feature_input = array.reshape(-1, 1)
    scores = cross_val_score(model, feature_input, target_series.to_numpy(), cv=cross_validation, scoring=metric)
    return scores.mean()",task['type'] == 'classification',80,_dtype_represents_categories(df[target]),False,4.02724819242185,N/A
"def _normalized_mae_score(model_mae, naive_mae):
    """"""Normalizes the model MAE score, given the baseline score""""""
<mask>:
        return 0
    else:
        return 1 - model_mae / naive_mae",model_mae > naive_mae,22,model_mae == 0,False,25.57539057896621,N/A
"def _normalized_f1_score(model_f1, baseline_f1):
    """"""Normalizes the model F1 score, given the baseline score""""""
<mask>:
        return 0
    else:
        scale_range = 1.0 - baseline_f1
        f1_diff = model_f1 - baseline_f1
        return f1_diff / scale_range",model_f1 < baseline_f1,30,baseline_f1 == 0,False,25.57539057896621,N/A
"def _determine_case_and_prepare_df(df, x, y, sample=5000, random_seed=123):
    """"""Returns str with the name of the determined case based on the columns x and y""""""
<mask>:
        return (df, 'predict_itself')
    df = df[[x, y]]
    df = df.dropna()
    if len(df) == 0:
        return (df, 'empty_dataframe_after_dropping_na')
    df = _maybe_sample(df, sample, random_seed=random_seed)
    if _feature_is_id(df, x):
        return (df, 'feature_is_id')
    category_count = df[y].value_counts().count()
    if category_count == 1:
        return (df, 'target_is_constant')
    if _dtype_represents_categories(df[y]) and category_count == len(df[y]):
        return (df, 'target_is_id')
    if _dtype_represents_categories(df[y]):
        return (df, 'classification')
    if is_numeric_dtype(df[y]):
        return (df, 'regression')
    if is_datetime64_any_dtype(df[y]) or is_timedelta64_dtype(df[y]):
        return (df, 'target_is_datetime')
    return (df, 'target_data_type_not_supported')",x == y,90,"_predict_is_itself(df, x, y)",False,3.673526562988939,N/A
"def _feature_is_id(df, x):
    """"""Returns Boolean if the feature column x is an ID""""""
<mask>:
        return False
    category_count = df[x].value_counts().count()
    return category_count == len(df[x])",not _dtype_represents_categories(df[x]),23,x not in df.columns,False,3.3264637832151163,N/A
"def expand_range_of_ips(start_ip, end_ip):
    """"""Takes an IP range and returns all the IPs in that range.
    # http://cmikavac.net/2011/09/11/how-to-generate-an-ip-range-list-in-python/
    """"""
    ip_range = []
<mask>:
        print('IPv6 IP range not supported in this function: {} - {}'.format(start_ip, end_ip))
        return ip_range
    start = list(map(int, start_ip.split('.')))
    end = list(map(int, end_ip.split('.')))
    temp = start
    ip_range.append(start_ip)
    while temp != end:
        start[3] += 1
        for i in (3, 2, 1):
            if temp[i] == 256:
                temp[i] = 0
                temp[i - 1] += 1
        ip_range.append('.'.join(map(str, temp)))
    return ip_range",ipaddress.ip_address(start_ip).version == 6 or ipaddress.ip_address(end_ip).version == 6,77,start_ip == end_ip,False,2.2161086144330966,N/A
"def __init__(self, secrets_file_location='./scantron_api_secrets.json', **kwargs):
    """"""Initialize a Scantron client.""""""
    SECRETS = {}
    try:
        with open(secrets_file_location) as config_file:
            SECRETS = json.loads(config_file.read())
    except OSError:
        print(f'Error: {secrets_file_location} does not exist.  Exiting...')
        sys.exit(1)
    try:
        self.host = SECRETS['scantron']['host']
        self.port = SECRETS['scantron']['port']
        self.token = SECRETS['scantron']['token']
    except KeyError:
        print(f'Error reading key-values in {secrets_file_location} file.  Exiting...')
        sys.exit(1)
    self.BASE_URL = f'https://{self.host}:{self.port}'
    self.user_agent = kwargs.get('user_agent', f'scantron-api-client-v{__version__}')
    self.headers = {'Content-Type': 'application/json', 'Accept': 'application/json', 'Authorization': f'Token {self.token}', 'User-Agent': self.user_agent}
    self.timeout = kwargs.get('timeout', 30)
    self.api_self_signed = kwargs.get('api_self_signed', True)
    self.max_attempts = kwargs.get('max_attempts', 3)
<mask>:
        urllib3.disable_warnings()
    self.debug_print = False",self.api_self_signed,83,self.api_self_signed,True,100.00000000000004,N/A
"def scantron_api_query(self, endpoint, **kwargs):
    """"""Executes a properly formatted API call to the Scantron API with the supplied arguments.""""""
    url = f'{self.BASE_URL}{endpoint}'
    headers = kwargs.get('headers', {})
<mask>:
        raise ValueError('headers keyword passed to scantron_api_query is not a valid dict object')
    headers = {**self.headers, **headers}
    method = kwargs.get('method', 'GET')
    method = method.upper()
    parameters = kwargs.get('params', {})
    if not isinstance(parameters, dict):
        raise ValueError('params keyword passed to scantron_api_query is not a valid dict object')
    payload = kwargs.get('payload', '{}')
    attempts = 0
    while True:
        try:
            if method == 'GET':
                response = requests.get(url, headers=headers, params=parameters, json=payload, verify=not self.api_self_signed, timeout=self.timeout)
                if response.status_code != 200:
                    debug_requests_response(response)
                break
            elif method == 'POST':
                response = requests.post(url, headers=headers, params=parameters, json=payload, verify=not self.api_self_signed, timeout=self.timeout)
                if response.status_code != 201:
                    debug_requests_response(response)
                break
            elif method == 'PATCH':
                response = requests.patch(url, headers=headers, params=parameters, json=payload, verify=not self.api_self_signed, timeout=self.timeout)
                if response.status_code != 200:
                    debug_requests_response(response)
                break
            elif method == 'PUT':
                response = requests.put(url, headers=headers, params=parameters, json=payload, verify=not self.api_self_signed, timeout=self.timeout)
                if response.status_code != 200:
                    debug_requests_response(response)
                break
            elif method == 'DELETE':
                response = requests.delete(url, headers=headers, params=parameters, json=payload, verify=not self.api_self_signed, timeout=self.timeout)
                if response.status_code != 204:
                    debug_requests_response(response)
                break
            else:
                print(f'Invalid HTTP method passed to scantron_api_query: {method}')
                raise ValueError(f'Invalid HTTP method passed to scantron_api_query: {method}')
        except (requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError):
            attempts += 1
            if self.max_attempts < attempts:
                print(f'Unable to reach Scantron API after {self.max_attempts} tries.  Consider increasing the timeout.')
                sys.exit(1)
            else:
                print('Packet loss when attempting to reach the Scantron API.')
    if self.debug_print:
        debug_requests_response(response)
    return response","not isinstance(headers, dict)",229,"not isinstance(headers, dict)",True,100.00000000000004,N/A
"def retrieve_server_time(self):
    """"""Retrieve the date and time on the server.  Useful when scheduling scans using the API.  Returns a string of
        Django's localtime().isoformat.""""""
    response = self.scantron_api_query('/api/server_time')
<mask>:
        server_time = response.json()['server_time']
    return server_time",response.status_code == 200,32,response.is_success(),False,12.600736402830258,N/A
"def retrieve_scan_results(self, scan_id, file_type, write_to_disk=False, **kwargs):
    """"""Returns a text blob of the scan results if they actually exist.  For .json files, the requests .json()
        method is called to return a Python dictionary object.""""""
    scan_results = None
    file_type = file_type.lower()
    file_name = f'scan_results_{scan_id}.{file_type}'
<mask>:
        print(f'Not a valid file type: {file_type}')
    else:
        response = self.scantron_api_query(f'/results/{scan_id}?file_type={file_type}', **kwargs)
        if response.status_code == 200 and file_type in ['nmap', 'xml']:
            scan_results = response.text
            if write_to_disk:
                with open(file_name, 'w') as fh:
                    fh.write(scan_results)
        elif response.status_code == 200 and file_type in ['json', 'pooled']:
            try:
                scan_results = response.json()
                if write_to_disk:
                    with open(file_name, 'w') as fh:
                        json.dump(scan_results, fh)
            except Exception as e:
                print(f'Exception decoding json for scan ID {scan_id}: {e}')
    return scan_results","file_type not in ['nmap', 'xml', 'json', 'pooled']",110,"file_type not in ['nmap', 'xml']",False,60.57025366576469,N/A
"def main(start_rank, end_rank, protocol='tcp'):
<mask>:
        port_file = 'nmap_top_ports_tcp.txt'
    elif protocol == 'udp':
        port_file = 'nmap_top_ports_udp.txt'
    else:
        print('This should never be reached.')
        exit()
    port_list = []
    with open(port_file, 'r') as fh:
        for index, port in enumerate(fh):
            port_list.append(port.strip())
    port_rank_list_temp = port_list[start_rank - 1:end_rank]
    port_rank_csv = ','.join(port_rank_list_temp)
    port_rank_list = []
    for rank in port_rank_list_temp:
        port_rank_list.append(int(rank))
    print(f'port_rank_list: {port_rank_list}')
    print(f'port_rank_csv: {port_rank_csv}')
    port_rank_dict = {'port_rank_list': port_rank_list, 'port_rank_csv': port_rank_csv}
    return port_rank_dict",protocol == 'tcp',64,protocol == 'tcp',True,100.00000000000004,N/A
"def build_masscan_command(scan_command, target_file, excluded_target_file, json_file, http_useragent):
    """"""Builds the masscan command.""""""
    file_options = f'-iL {target_file} -oJ {json_file} --http-user-agent {http_useragent}'
<mask>:
        file_options += f' --excludefile {excluded_target_file}'
    masscan_command = f'masscan {scan_command} {file_options}'
    return masscan_command",excluded_target_file,31,excluded_target_file,True,100.00000000000004,N/A
"def check_for_scan_jobs():
    """"""Check for new scans through the API.""""""
    console_address = engine.config_data['console_address']
    console_port = engine.config_data['console_port']
    scan_engine = engine.config_data['scan_engine']
    api_token = engine.config_data['api_token']
    url = f'{console_address}:{console_port}/api/scheduled_scans'
    ROOT_LOGGER.info(f'check_for_scans URL: {url}')
    headers = {'user-agent': scan_engine, 'Authorization': f'Token {api_token}'}
    try:
        request = urllib.request.Request(method='GET', url=url, headers=headers)
        response = urllib.request.urlopen(request)
        response_code = response.status
        response_data = response.read().decode('utf-8')
<mask>:
            json_data = json.loads(response_data)
            return json_data
        else:
            ROOT_LOGGER.error(f'Could not access {console_address}:{console_port}. HTTP status code: {response_code}')
            ROOT_LOGGER.error(f'Response content: {response_data}')
            return None
    except Exception as e:
        ROOT_LOGGER.error(f'check_for_scan_jobs() function exception: {e}')",response_code == 200,77,response_code == 200,True,100.00000000000004,N/A
"def update_scan_information(scan_job, update_info):
    """"""Update scan information using a PATCH API request.""""""
    console_address = engine.config_data['console_address']
    console_port = engine.config_data['console_port']
    scan_engine = engine.config_data['scan_engine']
    api_token = engine.config_data['api_token']
    scan_job_id = scan_job['id']
    url = f'{console_address}:{console_port}/api/scheduled_scans/{scan_job_id}'
    ROOT_LOGGER.info(f'update_scan_information URL: {url}')
    headers = {'user-agent': scan_engine, 'Authorization': f'Token {api_token}', 'Content-Type': 'application/json'}
    data = json.dumps(update_info).encode('utf-8')
    request = urllib.request.Request(method='PATCH', url=url, data=data, headers=headers)
    response = urllib.request.urlopen(request)
    response_code = response.status
    response_data = response.read().decode('utf-8')
<mask>:
        ROOT_LOGGER.info(f'Successfully updated scan information for scan ID {scan_job_id} with data {update_info}')
        update_scan_information_success = True
    else:
        ROOT_LOGGER.error(f'Could not access {console_address}:{console_port} or failed to update scan ID {scan_job_id}. HTTP status code: {response_code}')
        ROOT_LOGGER.error(f'Response content: {response_data}')
        update_scan_information_success = False
    return update_scan_information_success",response_code == 200,98,response_code == 200,True,100.00000000000004,N/A
"def load_config(self, config_file):
    """"""Load the engine_config.json file and return a JSON object.""""""
<mask>:
        with open(config_file) as fh:
            json_data = json.loads(fh.read())
            return json_data
    else:
        ROOT_LOGGER.error(f""'{config_file}' does not exist or contains no data."")
        sys.exit(0)",os.path.isfile(config_file),32,os.path.isfile(config_file),True,100.00000000000004,N/A
"def datetime_object_to_string_converter(datetime_object):
    """"""Convert a datetime object to a string.""""""
<mask>:
        return datetime_object.__str__()","isinstance(datetime_object, datetime.datetime)",12,"isinstance(datetime_object, datetime.datetime)",True,100.00000000000004,N/A
"def move_wildcard_files(wildcard_filename, source_directory, destination_directory):
    """"""Move files with supported fnmatch patterns (* and ?).""""""
    file_list = os.listdir(source_directory)
    for file_name in file_list:
<mask>:
            shutil.move(os.path.join(source_directory, file_name), os.path.join(destination_directory, file_name))","fnmatch.fnmatch(file_name, wildcard_filename)",25,"re.match(wildcard_filename, file_name)",False,24.08856270485351,N/A
"def main(number_of_days_in_the_future=5, scan_id=None):
    now_datetime = localtime()
    now_time = now_datetime.time()
<mask>:
        scans = django_connector.Scan.objects.filter(id=scan_id)
    else:
        scans = django_connector.Scan.objects.filter(enable_scan=True)
    for scan in scans:
        for index, exdate in enumerate(scan.recurrences.exdates):
            updated_exdate = localtime(exdate).replace(hour=now_time.hour).replace(minute=now_time.minute)
            scan.recurrences.exdates[index] = updated_exdate
        beginning_of_today = now_datetime.replace(hour=0).replace(minute=0).replace(second=0).replace(microsecond=0)
        future_end_datetime = beginning_of_today + datetime.timedelta(days=number_of_days_in_the_future)
        dtstart = localtime(scan.dtstart)
        scan_occurrences = scan.recurrences.between(beginning_of_today, future_end_datetime, dtstart=dtstart, inc=True)
        if scan_occurrences:
            print(f'Scan ID: {scan.id}')
            print(f'Scan start time: {scan.start_time}')
            print(f'{len(scan_occurrences)} total scans between {beginning_of_today} and {future_end_datetime}')
            for scan_occurrence in scan_occurrences:
                print(f'\t{scan_occurrence}')
            if scan.recurrences.exdates:
                print('exdates')
                for exdate in scan.recurrences.exdates:
                    print(f'\t{exdate}')
            print('=' * 20)",scan_id,81,scan_id,True,100.00000000000004,N/A
"def schedule_scan(scan_dict):
    """"""Given a scan dictionary, try and schedule the scan.""""""
    empty_scan_dict_value_detected = False
    for key, value in scan_dict.items():
<mask>:
            continue
        if not value:
            ROOT_LOGGER.error(f""scan_dict['{key}'] has an empty value."")
            empty_scan_dict_value_detected = True
    if empty_scan_dict_value_detected:
        return
    try:
        obj, created = django_connector.ScheduledScan.objects.get_or_create(**scan_dict)
        if created:
            ROOT_LOGGER.info(f'Adding to scheduled scans: {scan_dict}')
        else:
            ROOT_LOGGER.error(f'Scheduled scan not created: {scan_dict}')
    except Exception as e:
        ROOT_LOGGER.exception(f'Error adding scan: {scan_dict}.  Exception: {e}')","key in ['excluded_targets', 'pooled_scan_result_file_base_name', 'scan_binary_process_id']",63,key == 'timestamp',False,0.05084039605643275,N/A
"def is_ipv4_address(ip):
    """"""Returns True/False if a string is a valid IPv4 address.""""""
    ip = str(ip)
    try:
<mask>:
            return True
        else:
            return False
    except ValueError as e:
        print(f'{e}')",ipaddress.ip_address(ip).version == 4,27,ip in self.valid_ips,False,3.667862829704212,N/A
"def is_ipv6_address(ip):
    """"""Returns True/False if a string is a valid IPv6 address.""""""
    ip = str(ip)
    try:
<mask>:
            return True
        else:
            return False
    except ValueError as e:
        print(f'{e}')",ipaddress.ip_address(ip).version == 6,27,ip in self.valid_ips,False,3.667862829704212,N/A
"def retrieve_cloudflare_ip_networks(retrieve_new_data=False, cloudflare_filename='cloudflare_ip_networks.txt', write_to_disk=True):
    """"""Retrieve the IPv4 and IPv6 ranges for Cloudflare servers.

    https://www.cloudflare.com/ips/
    """"""
    cloudflare_dict = {'list_of_strings': set(), 'list_of_ipaddress_objects': set()}
<mask>:
        print(f'File already exists: {cloudflare_filename}')
        with open(cloudflare_filename, 'r') as fh:
            for ip_network in fh.readlines():
                cloudflare_dict['list_of_ipaddress_objects'].add(ipaddress.ip_network(ip_network.strip()))
    else:
        for ip_version in ['4', '6']:
            print(f'Retrieving Cloudflare IPv{ip_version} networks')
            url = f'https://www.cloudflare.com/ips-v{ip_version}'
            response = requests.get(url, timeout=2, verify=True)
            if response.status_code == 200:
                text = response.text
                for ip_network in text.strip().split('\n'):
                    cloudflare_dict['list_of_ipaddress_objects'].add(ipaddress.ip_network(ip_network))
            else:
                print('Cloudflare IP networks could not be retrieved.')
    cloudflare_dict['list_of_ipaddress_objects'] = sorted(cloudflare_dict['list_of_ipaddress_objects'], key=lambda obj: ipaddress.get_mixed_type_key(obj))
    cloudflare_dict['list_of_strings'] = [str(obj) for obj in cloudflare_dict['list_of_ipaddress_objects']]
    if write_to_disk and retrieve_new_data:
        print(f'Writing CloudFront IP networks to disk: {cloudflare_filename}')
        with open(cloudflare_filename, 'w') as fh:
            for ip_network in cloudflare_dict['list_of_strings']:
                fh.write(f'{ip_network}\n')
    return cloudflare_dict",os.path.exists(cloudflare_filename) and (not retrieve_new_data),110,os.path.exists(cloudflare_filename),False,40.656965974059936,N/A
"def retrieve_amazon_cloudfront_ip_ranges(retrieve_new_data=False, aws_cloudfront_filename='aws_cloudfront_ip_networks.txt', write_to_disk=True):
    """"""Retrieve the IPv4 and IPv6 ranges for AWS' CloudFront servers.

    https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/LocationsOfEdgeServers.html
    """"""
    cloudfront_dict = {'list_of_strings': set(), 'list_of_ipaddress_objects': set()}
<mask>:
        print(f'File already exists: {aws_cloudfront_filename}')
        with open(aws_cloudfront_filename, 'r') as fh:
            for ip_network in fh.readlines():
                cloudfront_dict['list_of_ipaddress_objects'].add(ipaddress.ip_network(ip_network.strip()))
    else:
        print(""Retrieving IPv4 and IPv6 network ranges for AWS' CloudFront servers."")
        url = 'https://ip-ranges.amazonaws.com/ip-ranges.json'
        response = requests.get(url, verify=True)
        if response.status_code == 200:
            json_data = response.json()
            for service in json_data['prefixes']:
                if service['service'] == 'CLOUDFRONT':
                    cloudfront_dict['list_of_ipaddress_objects'].add(ipaddress.ip_network(service['ip_prefix']))
            for service in json_data['ipv6_prefixes']:
                if service['service'] == 'CLOUDFRONT':
                    cloudfront_dict['list_of_ipaddress_objects'].add(ipaddress.ip_network(service['ipv6_prefix']))
        else:
            print('CloudFront IP networks could not be retrieved.')
    cloudfront_dict['list_of_ipaddress_objects'] = sorted(cloudfront_dict['list_of_ipaddress_objects'], key=lambda obj: ipaddress.get_mixed_type_key(obj))
    cloudfront_dict['list_of_strings'] = [str(obj) for obj in cloudfront_dict['list_of_ipaddress_objects']]
    if write_to_disk and retrieve_new_data:
        print(f'Writing CloudFront IP networks to disk: {aws_cloudfront_filename}')
        with open(aws_cloudfront_filename, 'w') as fh:
            for ip_network in cloudfront_dict['list_of_strings']:
                fh.write(f'{ip_network}\n')
    return cloudfront_dict",os.path.exists(aws_cloudfront_filename) and (not retrieve_new_data),124,os.path.exists(aws_cloudfront_filename),False,47.23665527410149,N/A
"def __init__(self, delimiter='', targets_string=None, targets_file=None, exclude_private_ips=False, sort_targets=False, exclude_cdn_ip_networks=False, retrieve_new_cdn_ip_data=False, write_to_disk=False):
    self.delimiter = delimiter
    self.targets_string = str(targets_string).strip()
    self.targets_file = targets_file
    self.exclude_private_ips = exclude_private_ips
    self.sort_targets = sort_targets
    self.exclude_cdn_ip_networks = exclude_cdn_ip_networks
    self.retrieve_new_cdn_ip_data = retrieve_new_cdn_ip_data
    self.write_to_disk = write_to_disk
<mask>:
        self.cdn_ip_networks = retrieve_cdn_ip_networks(self.retrieve_new_cdn_ip_data)
    if self.targets_file:
        with open(self.targets_file, 'r') as fh:
            self.targets_string = fh.read().strip()
    self.targets_dict = self.extract_targets(self.targets_string)",self.exclude_cdn_ip_networks,51,retrieve_new_cdn_ip_data,False,44.63236137853326,N/A
"def validate_string_of_email_addresses(string_of_email_addresses):
    """"""Given a comma deliminited string of email addresses, determine if they are all valid.  Returns a cleaned up
    version of string_of_email_addresses to be saved in the database.  Splits on commas because that is what
    console.utility.process_scan_status_change() does when sending email alerts.""""""
    cleaned_string_of_email_addresses = string_of_email_addresses.replace(' ', '').strip(',').strip()
    email_addresses = list(set(cleaned_string_of_email_addresses.split(',')))
    for email_address in email_addresses:
<mask>:
            raise ValidationError(f'Invalid email address found in string, enter comma delimited email addresses: {string_of_email_addresses}')
    valid_emails_string = ','.join(email_addresses)
    return valid_emails_string",check_email_address_validity(email_address) is False,73,not email_address.strip(),False,11.033017809693943,N/A
"def main(database_remove, file_remove, scan_retention_in_minutes, max_queryset_size_to_delete, disable_dryrun, verbosity):
    """"""Execute main function.""""""
    ROOT_LOGGER.setLevel((6 - verbosity) * 10)
    script_name = os.path.basename(os.path.abspath(__file__))
    log_file_handler = logging.FileHandler(f""{script_name.split('.py')[0]}.log"")
    log_file_handler.setFormatter(LOG_FORMATTER)
    ROOT_LOGGER.addHandler(log_file_handler)
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(LOG_FORMATTER)
    ROOT_LOGGER.addHandler(console_handler)
    ROOT_LOGGER.info(f'Starting {script_name} script.')
<mask>:
        ROOT_LOGGER.info('Scan retention is disabled.  Exiting...')
        return
    ROOT_LOGGER.info(f'Disable dryrun setting: {disable_dryrun}')
    now = django.utils.timezone.now()
    if not scan_retention_in_minutes:
        scan_retention_in_minutes = 60 * 24 * django_connector.Configuration.objects.all()[0].scan_retention_in_days
    ROOT_LOGGER.info(f'Removing scans older than {scan_retention_in_minutes} minutes.')
    datetime_retention_in_minutes = now - datetime.timedelta(minutes=scan_retention_in_minutes)
    scan_retention_dict = {}
    scans_older_than_retention_date = django_connector.ScheduledScan.objects.filter(scan_status__in=['cancelled', 'completed', 'error']).filter(completed_time__lt=datetime_retention_in_minutes)
    if file_remove:
        root_dir = '/home/scantron/console'
        target_files_dir = os.path.join(root_dir, 'target_files')
        complete_dir = os.path.join(root_dir, 'scan_results', 'complete')
        processed_dir = os.path.join(root_dir, 'scan_results', 'processed')
        cancelled_dir = os.path.join(root_dir, 'scan_results', 'cancelled')
        bigdata_analytics_dir = os.path.join(root_dir, 'for_bigdata_analytics')
        for scan in scans_older_than_retention_date:
            result_file_base_name = scan.result_file_base_name
            target_files = glob.glob(os.path.join(target_files_dir, f'{result_file_base_name}.*targets'))
            complete_dir_scans = glob.glob(os.path.join(complete_dir, f'{result_file_base_name}*'))
            processed_dir_scans = glob.glob(os.path.join(processed_dir, f'{result_file_base_name}*'))
            cancelled_dir_scans = glob.glob(os.path.join(cancelled_dir, f'{result_file_base_name}*'))
            bigdata_analytics_dir_csv_files = glob.glob(os.path.join(bigdata_analytics_dir, f'{result_file_base_name}.csv'))
            for file_to_delete in target_files + complete_dir_scans + processed_dir_scans + cancelled_dir_scans + bigdata_analytics_dir_csv_files:
                ROOT_LOGGER.debug(f'Deleting file: {file_to_delete}')
                if disable_dryrun:
                    try:
                        os.remove(file_to_delete)
                        ROOT_LOGGER.debug(f'Deleted file: {file_to_delete}')
                    except OSError:
                        ROOT_LOGGER.error(f'Could not delete file: {file_to_delete}')
    if database_remove:
        scans_older_than_retention_date_size = scans_older_than_retention_date.count()
        ROOT_LOGGER.info(f'{scans_older_than_retention_date_size} scans will be removed from the database.')
        if disable_dryrun:
            if scans_older_than_retention_date_size < max_queryset_size_to_delete + 1:
                scan_retention_dict['database'] = ()
                try:
                    database_result = scans_older_than_retention_date.delete()
                    scan_retention_dict['database'] = database_result
                    ROOT_LOGGER.info(f'Successfully deleted {scans_older_than_retention_date_size} scans from the database.')
                except Exception as e:
                    ROOT_LOGGER.exception(f'Problem deleting scans from database using .delete().  Exception: {e}')
            else:
                ROOT_LOGGER.warning(f'The number of scans to delete ({scans_older_than_retention_date_size}) is greater than the specified max_queryset_size_to_delete ({max_queryset_size_to_delete}).  Using an iterator for better memory management.')
                total_iterator_scans_deleted = 0
                for scan in scans_older_than_retention_date.iterator():
                    try:
                        scan_id = scan.id
                        scan.delete()
                        ROOT_LOGGER.debug(f'Scan ID successfully deleted: {scan_id}')
                        total_iterator_scans_deleted += 1
                    except Exception as e:
                        ROOT_LOGGER.exception(f'Problem deleting scan from database using iterator().  Exception: {e}')
                ROOT_LOGGER.info(f'Successfully deleted {total_iterator_scans_deleted} scans from the database.')
    ROOT_LOGGER.info(f'scan_retention_dict: {scan_retention_dict}')
    ROOT_LOGGER.info(f'{script_name} is done!')
    return scan_retention_dict",not django_connector.Configuration.objects.filter(id=1)[0].enable_scan_retention,278,not disable_dryrun,False,0.12799331444835488,N/A
"def write_results_to_csv_file(results_list, csv_file_name):
    """"""Writes results to a .csv file.  Attempts to extract column names, falls back to CSV_FIELD_NAMES.""""""
    print(f'Writing results to: {csv_file_name}')
    with open(csv_file_name, 'w') as csvfile:
        try:
            field_names = results_list[0].keys()
        except IndexError:
            field_names = CSV_FIELD_NAMES
        writer = csv.DictWriter(csvfile, fieldnames=field_names)
        writer.writeheader()
<mask>:
            for result in results_list:
                writer.writerow(result)
        else:
            writer.writerow({'openports': 'no'})
    print(f'Done writing results to: {csv_file_name}')",results_list,55,len(results_list) > 0,False,20.556680845025987,N/A
"def main():
    root_dir = '/home/scantron/console'
    complete_dir = os.path.join(root_dir, 'scan_results', 'complete')
    processed_dir = os.path.join(root_dir, 'scan_results', 'processed')
    bigdata_analytics_dir = os.path.join(root_dir, 'for_bigdata_analytics')
    json_scans = glob.glob(os.path.join(complete_dir, '*.json'))
    for scan in json_scans:
        try:
            base_scan_file_name = os.path.basename(scan).split('.json')[0]
            csv_file_name = f'{base_scan_file_name}.csv'
            results_list = []
<mask>:
                print(f'File is 0 bytes: {scan}')
            else:
                scan_file_name = os.path.basename(scan)
                site_name = scan_file_name.split('__')[0]
                engine = scan_file_name.split('__')[1]
                with open(scan, 'r') as fh:
                    scan_results = json.load(fh)
                    for result in scan_results:
                        try:
                            for port in result['ports']:
                                result_dict = {'openports': 'yes', 'start_time': result['timestamp'], 'end_time': result['timestamp'], 'site_name': site_name, 'engine': engine, 'scan_binary': 'masscan', 'target': result['ip'], 'protocol': port['proto'], 'port': port['port'], 'service_name': '', 'banner': '', 'state': 'open'}
                                if 'service' in port:
                                    if port['service']['banner'].startswith('MII'):
                                        result_dict['banner'] = 'custom_service_modification_TLS_cert'
                                    elif re.search('\\u003c', port['service']['banner']):
                                        result_dict['banner'] = 'custom_service_modification_html_source_blob'
                                    else:
                                        result_dict['banner'] = port['service']['banner'].replace('\n', ' ').replace('\r', ' ')
                                    result_dict['service_name'] = port['service']['name']
                                results_list.append(result_dict)
                        except Exception as e:
                            print(f'Issue with parsing scan: {result}.  Exception: {e}')
                            sys.exit(0)
            write_results_to_csv_file(results_list, os.path.join(bigdata_analytics_dir, csv_file_name))
            try:
                shutil.move(scan, processed_dir)
            except shutil.Error:
                os.remove(os.path.join(processed_dir, os.path.basename(scan)))
                shutil.move(scan, processed_dir)
        except Exception as e:
            print(f'Exception: {e}')",os.path.getsize(scan) == 0,153,len(scan) == 0,False,45.691722266180875,N/A
"def main(xml_files, merged_filename=''):
    """"""Given a list of XML files to merge, returns the string of the final merged XML file.""""""
    hosts = 0
<mask>:
        print('No XML files were found.')
        return ''
    xml_files = list(set(xml_files))
    try:
        if not merged_filename:
            now = time.localtime()
            timestamp = time.strftime('%Y%m%d_%H%M%S', now)
            merged_filename = f'nmap_merged_{timestamp}.xml'
        start_epoch_list = []
        finished_epoch_list = []
        for xml_file in xml_files:
            with open(xml_file, 'r') as fh:
                nmap_xml = ET.parse(fh)
                root = nmap_xml.getroot()
                start_epoch_list.append(int(root.get('start')))
                finished_epoch_list.append(int(nmap_xml.find('runstats')[0].get('time')))
        start_epoch = min(start_epoch_list)
        finished_epoch = max(finished_epoch_list)
        print(f'Earliest starting epoch is {start_epoch} from: {start_epoch_list}')
        print(f'Latest finishing epoch is {finished_epoch} from: {finished_epoch_list}')
        version = root.get('version')
        xmloutputversion = root.get('xmloutputversion')
        add_header(merged_filename, start_epoch, version, xmloutputversion)
        for xml_file in xml_files:
            if xml_file.endswith('.xml'):
                logging.debug(f'Parsing: {xml_file}')
                host_in_xml_file = merge_nmap_results(xml_file, merged_filename)
                hosts += host_in_xml_file
        add_footer(finished_epoch, merged_filename)
        print(f'Final merged file: {merged_filename}')
    except Exception as e:
        print(f'Exception: {e}')
        merged_filename = ''
    return merged_filename",not xml_files,133,not xml_files,True,100.00000000000004,N/A
"def main(xml_input_file, json_output_file, pretty_print_json):
    """"""Given an XML input file, create an output JSON file.  Also returns a dictionary object if used as a module.

    Arguments:
        input_file {string} -- Input XML file.
        output_file {string} -- Output XML file.
    """"""
    json_data = []
    try:
        with open(xml_input_file, 'r') as fh_xml:
            with open(json_output_file, 'w') as fh_json:
                xmljson_object = xmljson.XMLData()
<mask>:
                    json.dump(xmljson_object.data(parse(fh_xml).getroot()), fh_json, indent=4)
                else:
                    json.dump(xmljson_object.data(parse(fh_xml).getroot()), fh_json)
        with open(json_output_file, 'r') as fh:
            json_data = json.load(fh)
    except Exception as e:
        print(f'[-] Exception: {e}')
    return json_data",pretty_print_json,79,pretty_print_json,True,100.00000000000004,N/A
"def main():
    root_dir = '/home/scantron/console'
    complete_dir = os.path.join(root_dir, 'scan_results', 'complete')
    processed_dir = os.path.join(root_dir, 'scan_results', 'processed')
    bigdata_analytics_dir = os.path.join(root_dir, 'for_bigdata_analytics')
    xml_scans = glob.glob(os.path.join(complete_dir, '*.xml'))
    for scan in xml_scans:
        try:
            scan_file_name = os.path.basename(scan)
            site_name = scan_file_name.split('__')[0]
            engine = scan_file_name.split('__')[1]
            base_scan_file_name = os.path.basename(scan).split('.xml')[0]
            csv_file_name = f'{base_scan_file_name}.csv'
            results_list = []
            report = NmapParser.parse_fromfile(scan)
            for host in report.hosts:
<mask>:
                    for service in host.services:
                        result_dict = {'openports': 'yes', 'start_time': report.started, 'end_time': report.endtime, 'site_name': site_name, 'engine': engine, 'scan_binary': 'nmap', 'target': host.address, 'protocol': service.protocol, 'port': service.port, 'service_name': service.service, 'banner': '', 'state': service.state}
                        service_dict = service.service_dict
                        if 'product' in service_dict:
                            result_dict['banner'] += f""{service_dict['product']}""
                        if 'version' in service_dict:
                            result_dict['banner'] += f""{service_dict['version']}""
                        if 'extrainfo' in service_dict:
                            result_dict['banner'] += f""{service_dict['extrainfo']}""
                        results_list.append(result_dict)
            write_results_to_csv_file(results_list, os.path.join(bigdata_analytics_dir, csv_file_name))
            base_scan_files = glob.glob(os.path.join(complete_dir, f'{base_scan_file_name}*'))
            for scan_file in base_scan_files:
                shutil.move(scan_file, os.path.join(os.path.join(processed_dir, scan_file.split('/')[-1])))
        except Exception as e:
            print(f'Exception processing file: {scan}.  Exception: {e}')",len(host.services) != 0,133,host.address == 'nmap',False,10.89644800332157,N/A
"def main(json_files, merged_filename='', pretty_print_json=True):
    """"""Given a list of JSON files to merge, returns the string of the final merged JSON file.""""""
<mask>:
        print('No JSON files were found.')
        return ''
    json_files = list(set(json_files))
    try:
        if not merged_filename:
            now = time.localtime()
            timestamp = time.strftime('%Y%m%d_%H%M%S', now)
            merged_filename = f'masscan_merged_{timestamp}.json'
        final_json_list = []
        for json_file in json_files:
            if os.path.getsize(json_file) == 0:
                print(f'File is 0 bytes: {json_file}')
                continue
            with open(json_file, 'r') as fh_json_file:
                final_json_list.extend(json.load(fh_json_file))
        with open(merged_filename, 'w') as fh:
            if pretty_print_json:
                json.dump(final_json_list, fh, indent=4)
            else:
                json.dump(final_json_list, fh)
        print(f'Final merged file: {merged_filename}')
    except Exception as e:
        print(f'Exception: {e}')
        merged_filename = ''
    return merged_filename",not json_files,97,not json_files,True,100.00000000000004,N/A
"def get_secret(setting, secrets=SECRETS):
    """"""
    Get the secret variable or return explicit exception.
    """"""
    try:
<mask>:
            return secrets['local'][setting]
        else:
            return secrets['production'][setting]
    except KeyError:
        error_msg = f'Set the {setting} environment variable'
        raise ImproperlyConfigured(error_msg)",os.environ['DJANGO_SETTINGS_MODULE'] == 'config.settings.local',31,"setting in ['local', 'production']",False,1.8716386091619874,N/A
"@receiver(post_save, sender=settings.AUTH_USER_MODEL)
def create_auth_token(sender, instance=None, created=False, **kwargs):
    """"""Automatically generate an API key when a user is created, then create Engine.""""""
<mask>:
        api_token = Token.objects.create(user=instance)
        if instance.is_superuser is False:
            Engine.objects.create(scan_engine=instance, api_token=api_token)",created,30,created,True,100.00000000000004,N/A
"def clean(self):
    """"""Checks for any invalid IPs, IP subnets, or FQDNs in the globally_excluded_targets field.""""""
    target_extractor = extract_targets.TargetExtractor(targets_string=self.globally_excluded_targets, sort_targets=True)
    targets_dict = target_extractor.targets_dict
<mask>:
        invalid_targets = ','.join(target_extractor.targets_dict['invalid_targets'])
        raise ValidationError(f'Invalid globally excluded targets provided: {invalid_targets}')
    self.globally_excluded_targets = targets_dict['as_nmap']",targets_dict['invalid_targets'],36,'invalid_targets' in targets_dict,False,35.640264633541825,N/A
"def clean(self):
    """"""Checks for any invalid IPs, IP subnets, or FQDNs in the targets and excluded_targets fields.""""""
<mask>:
        raise ValidationError('Only select a single scan engine or scan engine pool.')
    if not self.scan_engine and (not self.scan_engine_pool):
        raise ValidationError('Select a single scan engine or scan engine pool.')
    target_extractor = extract_targets.TargetExtractor(targets_string=self.targets, sort_targets=True)
    targets_dict = target_extractor.targets_dict
    if targets_dict['invalid_targets']:
        invalid_targets = ','.join(target_extractor.targets_dict['invalid_targets'])
        raise ValidationError(f'Invalid targets provided: {invalid_targets}')
    self.targets = targets_dict['as_nmap']
    target_extractor = extract_targets.TargetExtractor(targets_string=self.excluded_targets, sort_targets=True)
    targets_dict = target_extractor.targets_dict
    if targets_dict['invalid_targets']:
        invalid_targets = ','.join(target_extractor.targets_dict['invalid_targets'])
        raise ValidationError(f'Invalid excluded targets provided: {invalid_targets}')
    self.excluded_targets = targets_dict['as_nmap']
    if self.email_scan_alerts and (not self.email_alert_addresses):
        raise ValidationError(""Provide an email address if enabling 'Email scan alerts'"")
    if self.email_alert_addresses:
        'Checks that email addresses are valid and returns a cleaned up string of them to save to the database.'
        self.email_alert_addresses = email_validation_utils.validate_string_of_email_addresses(self.email_alert_addresses)
    if self.email_scan_diff and (not self.email_scan_diff_addresses):
        raise ValidationError(""Provide an email address if enabling 'Email nmap scan diff'"")
    if self.email_scan_diff_addresses:
        'Checks that email addresses are valid and returns a cleaned up string of them to save to the database.'
        self.email_scan_diff_addresses = email_validation_utils.validate_string_of_email_addresses(self.email_scan_diff_addresses)",self.scan_engine and self.scan_engine_pool,165,not self.scan_engine and (not self.scan_engine_pool),False,63.019085559238604,N/A
"def clean(self):
    """"""Based off the current scan status, ensure the updated scan status is valid.""""""
    scan_status_allowed_state_update_dict = {'pending': ['started', 'error'], 'started': ['pause', 'cancel', 'completed', 'error'], 'pause': ['paused', 'error'], 'paused': ['pending', 'cancel', 'error'], 'cancel': ['cancelled', 'error'], 'cancelled': ['error'], 'completed': ['error'], 'error': ['pending']}
    scheduled_scan_dict = ScheduledScan.objects.get(pk=self.pk)
    current_scan_status = scheduled_scan_dict.scan_status
<mask>:
        valid_scan_states = ', '.join(scan_status_allowed_state_update_dict[current_scan_status])
        raise ValidationError(f""Invalid scan status change requested.  Scan status state '{current_scan_status}' can only transition to: {valid_scan_states}"")
    if current_scan_status == 'paused' and self.scan_status == 'cancel':
        self.scan_status = 'cancelled'",self.scan_status not in scan_status_allowed_state_update_dict[current_scan_status],78,current_scan_status not in scan_status_allowed_state_update_dict,False,67.78095780054507,N/A
"@api_view(http_method_names=['GET'])
@authentication_classes((SessionAuthentication, TokenAuthentication))
@permission_classes((IsAdminUser, IsAuthenticated))
@login_required(login_url='login')
def retrieve_scan_file(request, id):
    requested_scan = ScheduledScan.objects.get(id=id)
    file_type = request.GET.get('file_type', '')
<mask>:
        scan_file = requested_scan.pooled_scan_result_file_base_name
    else:
        result_file_base_name = requested_scan.result_file_base_name
        scan_file = f'{result_file_base_name}.{file_type}'
    response = HttpResponse()
    response['Content-Type'] = 'text/plain'
    response['Content-Disposition'] = f'inline; filename={scan_file}'
    response['X-Accel-Redirect'] = f'/protected/complete/{scan_file}'
    return response",file_type == 'pooled',42,file_type == 'pooled',True,100.00000000000004,N/A
"def validate(self, attrs):
    """"""Checks for any invalid IPs, IP subnets, or FQDNs in the globally_excluded_targets field.""""""
<mask>:
        globally_excluded_targets = attrs['globally_excluded_targets']
        target_extractor = extract_targets.TargetExtractor(targets_string=globally_excluded_targets, sort_targets=True)
        targets_dict = target_extractor.targets_dict
        if targets_dict['invalid_targets']:
            invalid_targets = ','.join(targets_dict['invalid_targets'])
            raise serializers.ValidationError(f'Invalid globally excluded targets provided: {invalid_targets}')
        attrs['globally_excluded_targets'] = targets_dict['as_nmap']
    return attrs",'globally_excluded_targets' in attrs,44,'globally_excluded_targets' in attrs,True,100.00000000000004,N/A
"def validate(self, attrs):
    """"""Checks for any invalid IPs, IP subnets, or FQDNs in the targets or excluded_targets fields.""""""
<mask>:
        targets = attrs['targets']
        target_extractor = extract_targets.TargetExtractor(targets_string=targets, sort_targets=True)
        targets_dict = target_extractor.targets_dict
        if targets_dict['invalid_targets']:
            invalid_targets = ','.join(targets_dict['invalid_targets'])
            raise serializers.ValidationError(f'Invalid targets provided: {invalid_targets}')
        attrs['targets'] = targets_dict['as_nmap']
    if 'excluded_targets' in attrs:
        excluded_targets = attrs['excluded_targets']
        target_extractor = extract_targets.TargetExtractor(targets_string=excluded_targets, sort_targets=True)
        targets_dict = target_extractor.targets_dict
        if targets_dict['invalid_targets']:
            invalid_targets = ','.join(targets_dict['invalid_targets'])
            raise serializers.ValidationError(f'Invalid excluded targets provided: {invalid_targets}')
        attrs['excluded_targets'] = targets_dict['as_nmap']
    if 'email_scan_alerts' in attrs and 'email_alert_addresses' in attrs:
        email_scan_alerts = attrs['email_scan_alerts']
        email_alert_addresses = attrs['email_alert_addresses']
        if email_scan_alerts and (not email_alert_addresses):
            raise serializers.ValidationError(""Provide an email address if enabling 'Email scan alerts'"")
    if 'email_alert_addresses' in attrs:
        'Checks that email addresses are valid and returns a cleaned up string of them to save to the database.'
        email_alert_addresses = attrs['email_alert_addresses']
        attrs['email_alert_addresses'] = email_validation_utils.validate_string_of_email_addresses(email_alert_addresses)
    if 'email_scan_diff' in attrs and 'email_scan_diff_addresses' in attrs:
        email_scan_diff = attrs['email_scan_diff']
        email_scan_diff_addresses = attrs['email_scan_diff_addresses']
        if email_scan_diff and (not email_scan_diff_addresses):
            raise serializers.ValidationError(""Provide an email address if enabling 'Email nmap scan diff'"")
    if 'email_scan_diff_addresses' in attrs:
        'Checks that email addresses are valid and returns a cleaned up string of them to save to the database.'
        email_scan_diff_addresses = attrs['email_scan_diff_addresses']
        attrs['email_scan_diff_addresses'] = email_validation_utils.validate_string_of_email_addresses(email_scan_diff_addresses)
    return attrs",'targets' in attrs,189,'target' in attrs,False,55.03212081491043,N/A
"def partial_update(self, request, pk=None, **kwargs):
    scan_status_allowed_state_update_dict = {'pending': ['started', 'error'], 'started': ['pause', 'cancel', 'completed', 'error'], 'pause': ['paused', 'error'], 'paused': ['pending', 'cancel', 'error'], 'cancel': ['cancelled', 'error'], 'cancelled': ['error'], 'completed': ['error'], 'error': ['pending']}
    try:
        body = self.request.data
        new_scan_status = body['scan_status']
<mask>:
            scheduled_scan_dict = ScheduledScan.objects.filter(scan_engine=request.user).filter(pk=pk).values()[0]
            current_scan_status = scheduled_scan_dict['scan_status']
            if new_scan_status not in scan_status_allowed_state_update_dict[current_scan_status]:
                valid_scan_states = ', '.join(scan_status_allowed_state_update_dict[current_scan_status])
                response_dict = {'detail': f""Invalid scan status change requested.  Scan status state '{current_scan_status}' can only transition to: {valid_scan_states}""}
                return JsonResponse(response_dict)
            scan_results_dir = '/home/scantron/console/scan_results'
            pending_files_dir = os.path.join(scan_results_dir, 'pending')
            completed_files_dir = os.path.join(scan_results_dir, 'complete')
            cancelled_files_dir = os.path.join(scan_results_dir, 'cancelled')
            if new_scan_status == 'cancelled':
                utility.move_wildcard_files(f""{scheduled_scan_dict['result_file_base_name']}*"", pending_files_dir, cancelled_files_dir)
                now_datetime = localtime()
                ScheduledScan.objects.filter(scan_engine=request.user).filter(pk=pk).update(completed_time=now_datetime)
            if new_scan_status == 'completed':
                utility.move_wildcard_files(f""{scheduled_scan_dict['result_file_base_name']}*"", pending_files_dir, completed_files_dir)
                now_datetime = localtime()
                ScheduledScan.objects.filter(scan_engine=request.user).filter(pk=pk).update(completed_time=now_datetime)
            scheduled_scan_dict['scan_status'] = new_scan_status
            redis_conn = redis.Redis(host='127.0.0.1', port=6379, db=0)
            q = rq.Queue(connection=redis_conn)
            job = q.enqueue(utility.process_scan_status_change, scheduled_scan_dict)
        else:
            raise Http404
    except ScheduledScan.DoesNotExist:
        raise Http404
    kwargs['partial'] = True
    return self.update(request, pk, **kwargs)","new_scan_status in ['started', 'pause', 'paused', 'cancel', 'cancelled', 'completed', 'error']",139,new_scan_status not in scan_status_allowed_state_update_dict,False,17.812506766310307,N/A
"def get_queryset(self):
    http_method = self.request.method
    user = self.request.user
    now_datetime = localtime()
    Engine.objects.filter(scan_engine=user).update(last_checkin=now_datetime)
<mask>:
        queryset = ScheduledScan.objects.all()
    elif http_method == 'GET':
        queryset = ScheduledScan.objects.filter(scan_engine=user).filter(scan_status__in=['pending', 'pause', 'cancel']).filter(start_datetime__lt=now_datetime)
    elif http_method in 'PATCH':
        queryset = ScheduledScan.objects.filter(scan_engine=user)
    else:
        queryset = []
    return queryset",user.is_superuser,38,http_method == 'GET',False,8.116697886877475,N/A
"def __init__(self, args=None):
<mask>:
        ap = argparse.ArgumentParser()
        ap.add_argument('-c', '--config', default=None)
        ap.add_argument('--nomerge', default=False, action='store_true')
        ap.add_argument('-d', '--delay', default=0)
        ap.add_argument('-f', '--file', default='-')
        ap.add_argument('-i', '--infile', default=None)
        ap.add_argument('-o', dest='output', default='N', choices=('D', 'J', 'N', 'S'))
        ap.add_argument('-O', '--otype', choices=('ipv4', 'ipv6', 'fqdn', 'email', 'sslfp', 'hash', 'url', 'mac'))
        ap.add_argument('-q', '--quiet', dest='verbose', default=True, action='store_false')
        ap.add_argument('-s', '--sites', default='default')
        ap.add_argument('-a', '--auth')
        ap.add_argument('-H', '--http-proxy', dest='http_proxy')
        ap.add_argument('targets', nargs=argparse.REMAINDER)
        ap.add_argument('-v', '--version', action='version', version='%(prog)s ' + __version__)
        modes = ap.add_mutually_exclusive_group()
        modes.add_argument('--dump-config', dest='mode', action='store_const', const='dump_config')
        modes.add_argument('--detect-otype', dest='mode', action='store_const', const='detect_otype')
        modes.add_argument('--list-sites', dest='mode', action='store_const', const='list_sites')
        args = ap.parse_args()
    self.args = args",args is None,82,args is None,True,100.00000000000004,N/A
"@property
def conf(self):
<mask>:
        path = None
        if self.args.config:
            path = self.args.config
        else:
            for possible_path in default_config_locations:
                if possible_path is None:
                    continue
                if os.path.exists(possible_path):
                    path = possible_path
                    break
        if path:
            with open(path, 'r') as f:
                conf = utils.safe_load(f)
        else:
            conf = {}
        if not self.args.nomerge:
            local_path = '/etc/machinae.local.yml'
            if os.path.exists(local_path):
                with open(local_path, 'r') as f:
                    local_conf = utils.safe_load(f)
                conf = dict_merge(conf, local_conf)
            local_path = os.path.expanduser('~/.machinae.yml')
            if os.path.exists(local_path):
                with open(local_path, 'r') as f:
                    local_conf = utils.safe_load(f)
                conf = dict_merge(conf, local_conf)
        self._conf = conf
    return self._conf",self._conf is None,84,self._conf is None,True,100.00000000000004,N/A
"@property
def results(self):
    creds = None
<mask>:
        with open(self.args.auth) as auth_f:
            creds = utils.safe_load(auth_f.read())
    proxies = {}
    if self.args.http_proxy:
        proxies['http'] = self.args.http_proxy
        proxies['https'] = self.args.http_proxy
    else:
        if 'HTTP_PROXY' in os.environ:
            proxies['http'] = os.environ['HTTP_PROXY']
        elif 'http_proxy' in os.environ:
            proxies['http'] = os.environ['http_proxy']
        if 'HTTPS_PROXY' in os.environ:
            proxies['https'] = os.environ['HTTPS_PROXY']
        elif 'https_proxy' in os.environ:
            proxies['https'] = os.environ['https_proxy']
    if 'http' in proxies:
        print('HTTP Proxy: {http}'.format(**proxies), file=sys.stderr)
    if 'https' in proxies:
        print('HTTPS Proxy: {https}'.format(**proxies), file=sys.stderr)
    for target_info in self.targets:
        target, otype, _ = target_info
        target_results = list()
        for site_name, site_conf in self.sites.items():
            if otype.lower() not in map(lambda x: x.lower(), site_conf['otypes']):
                continue
            site_conf['target'] = target
            site_conf['verbose'] = self.args.verbose
            scraper = Site.from_conf(site_conf, creds=creds, proxies=proxies)
            try:
                with stopit.SignalTimeout(15, swallow_exc=False):
                    run_results = list()
                    for r in scraper.run():
                        if 'value' not in r:
                            r = {'value': r, 'pretty_name': None}
                        run_results.append(Result(r['value'], r['pretty_name']))
            except stopit.TimeoutException:
                target_results.append(ErrorResult(target_info, site_conf, 'Timeout'))
            except Exception as e:
                target_results.append(ErrorResult(target_info, site_conf, e))
            else:
                target_results.append(SiteResults(site_conf, run_results))
        yield ResultSet(target_info, target_results)",self.args.auth and os.path.isfile(self.args.auth),149,self.args.auth,False,7.427357821433391,N/A
"@property
def sites(self):
<mask>:
        if self.args.sites.lower() == 'all':
            sites = self._conf.keys()
        elif self.args.sites.lower() == 'default':
            sites = [k for k, v in self.conf.items() if v.get('default', True)]
        else:
            sites = self.args.sites.lower().split(',')
        self._sites = OrderedDict([(k, v) for k, v in self.conf.items() if k in sites])
    return copy.deepcopy(self._sites)",self._sites is None,45,"not hasattr(self, '_sites')",False,5.669791110976001,N/A
"@property
def targets(self):
    targets = list()
<mask>:
        with open(self.args.infile, 'r') as f:
            targets.extend([line.strip() for line in f.readlines()])
    targets.extend(self.args.targets)
    for target in targets:
        otype, otype_detected = self.detect_otype(target)
        if otype == 'url' and (not (target.startswith('http://') or target.startswith('https://'))):
            target = 'http://{0}'.format(target)
        yield TargetInfo(target, otype, otype_detected)",self.args.infile,42,self.args.infile,True,100.00000000000004,N/A
"def get_target_type(target):
    try:
        getVer = ipaddress.ip_address(target)
<mask>:
            return 'ipv4'
        elif getVer.version == 6:
            return 'ipv6'
    except ValueError:
        pass
    if re.match('^[a-f0-9]{32}$', target, re.I):
        return 'hash'
    elif re.match('^[a-f0-9]{40}$', target, re.I):
        return 'hash.sha1'
    elif re.match('^[a-f0-9]{64}$', target, re.I):
        return 'hash.sha256'
    elif re.match('^[a-f0-9]{128}$', target, re.I):
        return 'hash.sha512'
    elif re.match('^https?://', target, re.I):
        return 'url'
    elif re.match('^.*?@.*?$', target, re.I):
        return 'email'
    elif re.match('^(?:[a-f0-9]{2}:){19}[a-f0-9]{2}$', target, flags=re.I):
        return 'sslfp'
    elif re.match('^([0-9a-fA-F][0-9a-fA-F][-:\\.]){5}([0-9a-fA-F][0-9a-fA-F])$', target, re.I):
        return 'mac'
    return 'fqdn'",getVer.version == 4,68,getVer.version == 4,True,100.00000000000004,N/A
"def dict_merge(d1, d2):
    d3 = d1.copy()
    for key in d2:
<mask>:
            d3[key] = dict_merge(d3[key], d2[key])
        elif hasattr(d2[key], 'items'):
            d3[key] = d2[key].copy()
        else:
            d3[key] = d2[key]
    return d3","key in d3 and hasattr(d3[key], 'items') and hasattr(d2[key], 'items')",27,"isinstance(d2[key], dict)",False,12.885393382770486,N/A
"def listsites(conf):
    rstr = '{0:40}{1:40}{2:40}{3}'.format('SITE', 'NAME', 'OTYPES', 'DEFAULT')
    rstr += '\n'
    for key in conf:
        d = 'True'
<mask>:
            d = str(conf[key]['default'])
        rstr += '{0:40}{1:40}{2:40}{3}'.format(key, conf[key]['name'], ', '.join(conf[key]['otypes']), d)
        rstr += '\n'
    return rstr",'default' in conf[key].keys(),34,'default' in conf[key],False,51.341711903259224,N/A
"@staticmethod
def get_formatter(format):
<mask>:
        return NormalOutput()
    elif format.upper() == 'J':
        return JsonOutput()
    elif format.upper() == 'D':
        return DotEscapedOutput()
    elif format.upper() == 'S':
        return ShortOutput()",format.upper() == 'N',24,format.upper() == 'A',False,84.08964152537145,N/A
"def run(self, result_sets: object):
    self.init_buffer()
    for row in result_sets:
        target, otype, otype_detected = row.target_info
        self.output_header(target, otype, otype_detected)
        self.print('')
        for item in row.results:
            site = item.site_info
<mask>:
                self.print('[!] Error from {0}: {1}'.format(site['name'], item.error_info))
                continue
            if not item.resultset:
                self.print('[-] No {0} Results'.format(site['name']))
            else:
                self.print('[+] {0} Results'.format(site['name']))
                for result in item.resultset:
                    labels = getattr(result[0], 'labels', None)
                    if len(result[0].values()) > 1 or labels is not None:
                        values = map(repr, result[0].values())
                        values = map(self.escape, values)
                        if labels is not None:
                            values = zip(labels, values)
                            values = ['{0}: {1}'.format(label, value) for label, value in values]
                            output = ', '.join(values)
                        if result[1] is not None:
                            output = '({0})'.format(', '.join(values))
                            output = defang(output)
                    else:
                        output = self.escape(list(result[0].values())[0])
                        output = defang(output)
                    if result[1] is not None:
                        output = '{1}: {0}'.format(output, result[1])
                        output = defang(output)
                    self.print('    [-] {0}'.format(output))
    return self._buffer.getvalue()","hasattr(item, 'error_info')",130,item.error_info is not None,False,6.567274736060395,N/A
"def run(self, result_sets):
    records = list()
    for row in result_sets:
        target, otype, otype_detected = row.target_info
        for item in row.results:
            output = dict()
            output['site'] = item.site_info['name']
            output['results'] = dict()
            output['observable'] = target
            output['observable type'] = otype
            output['observable type detected'] = otype_detected
<mask>:
                output['results'] = {'error_info': str(item.error_info)}
            elif item.resultset:
                for result in item.resultset:
                    if result.pretty_name not in output['results']:
                        output['results'][result.pretty_name] = list()
                    values = list(result.value.values())
                    if len(values) == 1:
                        output['results'][result.pretty_name].append(values[0])
                    elif len(values) > 1:
                        output['results'][result.pretty_name].append(values)
                for k, v in output['results'].items():
                    if len(v) == 1:
                        output['results'][k] = v[0]
            records.append(output)
    return records","hasattr(item, 'error_info')",87,item.error_info,False,6.9717291216921975,N/A
"def run(self, result_sets):
    self.init_buffer()
    for row in result_sets:
        target, otype, otype_detected = row.target_info
        self.print('[+] {0}'.format(target))
        for item in row.results:
            site = item.site_info
<mask>:
                self.print('    {0}: Error'.format(site['name']))
            elif not item.resultset:
                self.print('    {0}: No'.format(site['name']))
            else:
                self.print('    {0}: Yes'.format(site['name']))
    return self._buffer.getvalue()","hasattr(item, 'error_info')",38,item.error,False,5.197112497172873,N/A
"def run(self):
    r = self._req(self.conf['request'])
    body = r.text
    rss = feedparser.parse(body)
    parser = None
    for entry in rss.entries:
        for parser1 in self.conf['results']:
            result_dict = dict()
            for key, parser in parser1.items():
                print(parser)
                rex = re.compile(parser['regex'])
                fieldnames = parser['values']
<mask>:
                    fieldnames = [fieldnames]
                rss_value = getattr(entry, key)
                m = rex.search(rss_value)
                if m:
                    result_dict.update(dict(zip(fieldnames, m.groups())))
                else:
                    result_dict = None
                    break
            if result_dict is None:
                continue
            yield self.build_result(parser, result_dict)","not isinstance(fieldnames, list)",65,"isinstance(fieldnames, six.string_types)",False,27.77619034011791,N/A
"def get_html(self):
    r = super(HtmlSite, self).get_content()
    body = r.text
    cleanup = self.conf['request'].get('cleanup', {})
    strip_comments = str(cleanup.get('strip_comments', False)).lower()
<mask>:
        soup = BeautifulSoup(r.text, 'html5lib')
        for comment in soup.find_all(text=lambda _: isinstance(_, Comment)):
            comment.extract()
        body = str(soup)
    return html_unescape(body)","strip_comments in ('1', 'yes', 'true')",35,strip_comments,False,6.948345122280157,N/A
"@classmethod
def find_table(cls, html, headers):
    soup = BeautifulSoup(html, 'html5lib')
    for table in soup.find_all('table'):
        cells = cls.get_row_contents(table.find('tr'))
<mask>:
            return (table, cells)
    raise ValueError('No matching table found')","cls.compare_rows(cells, headers)",25,len(cells) == 0,False,9.469167282754096,N/A
"def run(self):
    body = self.get_html()
    for parser in self.conf['results']:
        table, columns = self.find_table(body, parser['map'].keys())
        for row in table.find_all('tr'):
            cells = self.get_row_contents(row)
<mask>:
                continue
            result_dict = dict(zip(columns, cells))
            yield self.build_result(parser, result_dict)","self.compare_rows(cells, columns)",30,not cells,False,0.9157819444367088,N/A
"def run(self):
    body = self.get_html()
<mask>:
        raise Exception('No parsing configuration found')
    for parser in self.conf['results']:
        rex = re.compile(parser['regex'], flags=re.I)
        for match in rex.finditer(body):
            result_dict = dict()
            for k, v in zip(parser['values'], match.groups()):
                result_dict[k] = v
            yield self.build_result(parser, result_dict)",'results' not in self.conf,38,not self.conf,False,38.75385825373298,N/A
"def kwargs_setter(self, kwargs):
<mask>:
        target = kwargs.pop('target')
        if 'target' in self.conf.get('request', {}):
            target_conf = self.conf['request']['target']
            ptr_style = str(target_conf.get('ptr', False)).lower()
            if ptr_style in ('1', 'yes', 'true'):
                target = '.'.join(reversed(target.split('.')))
            urlencode = str(target_conf.get('urlencode', False)).lower()
            if urlencode in ('1', 'yes', 'true'):
                target = urllib.parse.quote(target)
            elif urlencode == 'twice':
                target = urllib.parse.quote(urllib.parse.quote(target, safe=''))
            if 'format' in target_conf:
                target = target_conf['format'] % (target,)
        kwargs['target'] = target
    self._kwargs = kwargs",'target' in kwargs,64,'target' in kwargs,True,100.00000000000004,N/A
"@staticmethod
def from_conf(conf, *args, **kwargs):
    from . import csv, html, rss, json, ipwhois
<mask>:
        site_conf = conf.pop('webscraper')
        scraper = html.Webscraper(site_conf, *args, **kwargs)
    elif 'tablescraper' in conf:
        site_conf = conf.pop('tablescraper')
        scraper = html.TableScraper(site_conf, *args, **kwargs)
    elif 'json' in conf:
        site_conf = conf.pop('json')
        scraper = json.JsonApi(site_conf, *args, **kwargs)
    elif 'csv' in conf:
        site_conf = conf.pop('csv')
        scraper = csv.CsvSite(site_conf, *args, **kwargs)
    elif 'rss' in conf:
        site_conf = conf.pop('rss')
        scraper = rss.RssSite(site_conf, *args, **kwargs)
    elif 'ipwhois' in conf:
        site_conf = conf.pop('ipwhois')
        scraper = ipwhois.IpWhois(site_conf, *args, **kwargs)
    else:
        raise NotImplementedError(conf.keys())
    scraper.kwargs = conf.copy()
    return scraper",'webscraper' in conf,90,'webscraper' in conf,True,100.00000000000004,N/A
"@property
def dialect(self):
<mask>:
        return 'excel'

    class DelimDialect(csv.excel):
        delimiter = str(self.delim)
        skipinitialspace = True
    return DelimDialect()",'pattern' not in self.conf,16,self.delim == '',False,16.233395773754953,N/A
"def get_content(self):
    r = super(CsvSite, self).get_content()
    body = r.text
<mask>:
        body = re.sub(self.conf['pattern'], '|', body)
        self._delim = '|'
    buf = io.StringIO(body)
    csvfile = csv.reader(buf, dialect=self.dialect)
    return csvfile",len(self.delim) > 1,27,self.conf['pattern'],False,11.631736348831648,N/A
"def run(self):
    r = self._req(self.conf['request'])
    body = r.text
<mask>:
        body = re.sub(self.conf['pattern'], '|', body)
        self._delim = '|'
    buf = io.StringIO(body)
    csvfile = csv.reader(buf, dialect=self.dialect)
    for lineno, row in enumerate(csvfile):
        for parser in self.conf['results']:
            start = parser.get('start', 1)
            stop = parser.get('end', None)
            if lineno < start or len(row) == 0 or row[0].startswith('#'):
                continue
            elif stop is not None and lineno > stop:
                break
            if 'match' in parser:
                rex = re.compile(parser['match']['regex'])
                col = int(parser['match']['column'])
                if not rex.search(row[col]):
                    continue
            row = [item.strip() for item in row]
            result_dict = dict(zip(parser['values'], row))
            yield self.build_result(parser, result_dict)",len(self.delim) > 1,90,self.conf['pattern'],False,11.631736348831648,N/A
"@staticmethod
def get_cidr(network):
    networks = [str(net) for net in summarize_address_range(ip_address(network['start_address']), ip_address(network['end_address']))]
<mask>:
        networks = networks[0]
    return networks",len(networks) == 1,17,len(networks) == 1,True,100.00000000000004,N/A
"@staticmethod
def get_value(data, key, default=None):
<mask>:
        return data
    ret = data
    key_parts = key.split('.')
    for key_part in key_parts:
        if key_part not in ret:
            return default
        ret = ret[key_part]
    return ret",key == '@' or data is None,30,not key,False,0.9157819444367088,N/A
"def get_json(self, url=None):
    r = self.get_content(url=url)
    ignored_status_codes = [int(sc) for sc in self.conf['request'].get('ignored_status_codes', [])]
<mask>:
        return []
    if not self.conf.get('multi_json', False):
        return r.json()
    if r.status_code in ignored_status_codes:
        return []
    results = list()
    for json_line in r.text.split('\n'):
        if not json_line:
            break
        results.append(json.loads(json_line))
    return results",r.status_code in ignored_status_codes,43,r.text is None,False,6.434818657591886,N/A
"def run(self):
    data = self.get_json()
<mask>:
        next_url = None
        if self.conf.get('paginated', False):
            next_url = data.get('next', None)
        data = [data]
        while next_url:
            next_data = self.get_json(url=next_url)
            next_url = next_data.get('next', None)
            data.append(next_data)
    if 'results' not in self.conf:
        return
    for row in data:
        for parser in self.conf['results']:
            for _ in self.parse_dict(row, parser):
                yield _","hasattr(data, 'items')",50,data,False,0.673794699908547,N/A
"@classmethod
def get_result_dicts(cls, data, parser, mm_key=None, onlyif=None):
<mask>:
        parser = {'key': parser}
    if 'key' not in parser:
        yield data
        return
    key = parser['key']
    rex = None
    if 'regex' in parser:
        rex = re.compile(parser['regex'], flags=re.I)
    if key == '@' and mm_key is not None:
        yield {key: mm_key}
        return
    values = cls.get_value(data, key)
    if values is None:
        return
    if not parser.get('match_all', False):
        values = [values]
    for val in values:
        result_dict = OrderedDict()
        if rex:
            m = rex.search(val)
            if not m:
                return
            if len(m.groups()) > 0:
                val = m.groups()
                if len(val) == 1:
                    val = val[0]
        urldecode = str(parser.get('urldecode', False)).lower()
        if urldecode in ('1', 'yes', 'true'):
            val = urllib.parse.unquote(val)
        elif urldecode == 'twice':
            val = urllib.parse.unquote(urllib.parse.unquote(val))
        if 'format' in parser:
            if parser['format'] == 'as_list':
                val = ', '.join(map(str, val))
            elif parser['format'] == 'as_time':
                try:
                    dt = datetime.datetime.fromtimestamp(float(val))
                except:
                    dt = parse(val)
                val = dt.isoformat()
        result_dict[key] = val
        yield result_dict","not hasattr(parser, 'items')",146,parser is not None,False,8.9730240870212,N/A
"@classmethod
def multi_match_generator(cls, data, parser, mm_key):
<mask>:
        for item in data:
            for _ in cls.multi_match_generator(item, parser, mm_key='@'):
                yield _
        return
    onlyif = parser.get('onlyif', None)
    if onlyif is not None and (not hasattr(onlyif, 'items')):
        onlyif = {'key': onlyif}
    if mm_key == '@' or parser.get('match_all', False):
        data = [(None, data)]
    else:
        data = data.items()
    for k, v in data:
        if onlyif is not None:
            if not hasattr(onlyif, 'items'):
                onlyif = {'key': onlyif}
            value = cls.get_value(v, onlyif['key'], None)
            if value is None:
                continue
            elif 'regex' in onlyif:
                rex = re.compile(onlyif['regex'], re.I)
                if not rex.search(value):
                    continue
            elif 'maxage' in onlyif:
                age = parse(value)
                if not onlyif['maxage'].startswith('-'):
                    print('\x1b[91m' + 'WARNING: maxage must be prepended with ""-"" Please correct this in your configuration file.' + '\x1b[0m')
                    onlyif['maxage'] = '-%s' % onlyif['maxage']
                ageout = timeParser(onlyif['maxage']).replace(tzinfo=None)
                if age < ageout:
                    continue
            elif not bool(value):
                continue
        result_dict = OrderedDict()
        for mm_parser in parser['keys']:
            for mm_result_dict in cls.get_result_dicts(v, mm_parser, mm_key=k, onlyif=onlyif):
                result_dict.update(mm_result_dict)
        if result_dict:
            result_dict.labels = parser.get('labels', None)
            yield result_dict","not hasattr(data, 'items')",160,"parser.get('match_all', False)",False,5.522397783539471,N/A
"@property
def session(self):
<mask>:
        self._session = requests.Session()
        self._session.headers.update({'User-Agent': 'Vor/1.0 (Like CIF/2.0)'})
        if self.proxies:
            self._session.proxies = self.proxies
    return self._session",self._session is None,18,not self._session,False,54.75182535069452,N/A
"@staticmethod
def unzip_content(r, *args, **kwargs):
    content = r.content
    mime = magic.from_buffer(content, mime=True)
<mask>:
        zip_buffer = io.BytesIO(content)
        with zipfile.ZipFile(zip_buffer) as zf:
            fn = zf.namelist()[0]
            with zf.open(fn) as f:
                r._content = f.read()
    elif mime == 'application/x-gzip':
        gz_buffer = io.BytesIO(content)
        with gzip.GzipFile(fileobj=gz_buffer) as gz:
            r._content = gz.read()
    else:
        r._content = content
    return r",mime == 'application/zip',50,mime == 'application/zip',True,100.00000000000004,N/A
"def _req(self, conf, url=None):
<mask>:
        url = conf.get('url', '')
        if url == '':
            return
        url = url.format(**self.kwargs)
    method = conf.get('method', 'get').upper()
    kwargs = dict()
    headers = conf.get('headers', {})
    if headers:
        kwargs['headers'] = headers
    verify_ssl = conf.get('verify_ssl', True)
    params = conf.get('params', {}).copy()
    for k, v in params.items():
        if hasattr(v, 'items'):
            conf = params.pop(k)
            if 'relatime' in conf:
                dt = relatime.timeParser(conf['relatime'], timezone=str(get_localzone()))
                target_tz = pytz.timezone(conf.get('timezone', 'UTC'))
                dt = dt.astimezone(target_tz)
                dt = dt.replace(tzinfo=None)
                time_format = conf.get('format', '%Y-%m-%dT%H:%M:%S.%fZ')
                if time_format.lower() == 'as_epoch':
                    params[k] = str(int(dt.timestamp()))
                else:
                    params[k] = dt.strftime(time_format)
        else:
            params[k] = str(v).format(**self.kwargs)
    if params:
        kwargs['params'] = params
    data = conf.get('data', {})
    for k, v in data.items():
        data[k] = v.format(**self.kwargs)
    if data:
        kwargs['data'] = data
    if conf.get('auth') and self.creds and self.creds.get(conf['auth']):
        kwargs['auth'] = tuple(self.creds[conf['auth']])
    if conf.get('decompress', False):
        kwargs['hooks'] = {'response': self.unzip_content}
    raw_req = requests.Request(method, url, **kwargs)
    req = self.session.prepare_request(raw_req)
    if self.kwargs.get('verbose', False):
        print('[.] Requesting {0} ({1})'.format(req.url, req.method))
    with warnings.catch_warnings():
        if not verify_ssl:
            warnings.simplefilter('ignore', exceptions.InsecureRequestWarning)
        return self.session.send(req, verify=verify_ssl)",url is None,153,url is None,True,100.00000000000004,N/A
"def get_content(self, conf=None, url=None):
<mask>:
        conf = self.conf['request']
    r = self._req(conf, url)
    ignored_status_codes = [int(sc) for sc in conf.get('ignored_status_codes', [])]
    if r.status_code not in ignored_status_codes:
        r.raise_for_status()
    return r",conf is None,28,conf is None,True,100.00000000000004,N/A
"def build_result(self, parser, result_dict):
    defaults_dict = parser.get('defaults', {})
    result = OrderedDict()
    result.update(defaults_dict)
    result.update(result_dict)
    result.pop(None, None)
<mask>:
        for old, new in parser['map'].items():
            if new is None:
                result.pop(old)
            elif old in result:
                result[new] = result.pop(old)
    if 'defaults' in parser:
        for k, v in parser['defaults'].items():
            result[k] = v
    if 'pretty_name' in parser:
        result = OrderedDict([('value', result), ('pretty_name', parser['pretty_name'])])
    if hasattr(result_dict, 'labels'):
        result.labels = result_dict.labels
    return result",'map' in parser,63,'map' in parser,True,100.00000000000004,N/A
"@property
def total_count(self):
    """"""
        Returns total count of available resources in Redmine, this is known only after ResourceSet evaluation.
        """"""
<mask>:
        if self._resources is None:
            raise exceptions.ResultSetTotalCountError
        else:
            self._total_count = len(self)
    return self._total_count",self._total_count is None,33,self._total_count is None,True,100.00000000000004,N/A
"def export(self, fmt, savepath=None, filename=None, columns=None, encoding='UTF-8'):
    """"""
        Exports all resources from resource set to requested format if Resource supports that.

        :param string fmt: (required). Format to use for export, e.g. atom, csv, txt, pdf, html etc.
        :param string savepath: (optional). Path where to save the file.
        :param string filename: (optional). Name that will be used for the file.
        :param columns: (optional). Iterable of column names, ""all_gui"" for GUI behaviour or ""all"" for all columns.
        :param encoding: (optional). Encoding that will be used by Redmine for the result file.
        :type columns: iterable or string
        """"""
<mask>:
        raise exceptions.ExportNotSupported
    url = self.manager.redmine.url + self.manager.resource_class.query_all_export.format(format=fmt, **self.manager.params)
    params = dict(self.manager.resource_class.query_all_export.formatter.unused_kwargs, encoding=encoding)
    if columns is not None:
        if columns == 'all':
            columns = ['all', 'all_inline'] + self.manager.resource_class.extra_export_columns
            if self.manager.redmine.ver is not None and self.manager.redmine.ver < (3, 4, 0):
                params.update(dict.fromkeys(self.manager.resource_class.extra_export_columns, 1), columns='all')
        elif 'all_gui' in columns:
            if columns == 'all_gui':
                columns = ['all', 'all_inline']
                if self.manager.redmine.ver is not None and self.manager.redmine.ver < (3, 4, 0):
                    params['columns'] = 'all'
            else:
                if self.manager.redmine.ver is not None and self.manager.redmine.ver < (3, 4, 0):
                    params.update(dict.fromkeys(columns, 1), columns='all')
                columns = list(columns) + ['all', 'all_inline']
        params['c[]'] = columns
    try:
        return self.manager.redmine.download(url, savepath, filename, params=params)
    except exceptions.UnknownError as e:
        if e.status_code == 406:
            raise exceptions.ExportFormatNotSupportedError
        raise e",self.manager.resource_class.query_all_export is None,205,not self.supports_all_export,False,17.13859898988808,N/A
"def __getitem__(self, item):
    """"""
        Sets limit and offset or returns a Resource by requested index.
        """"""
<mask>:
        self.limit = item.stop
        self.offset = item.start
        self._is_sliced = True
    elif isinstance(item, int):
        try:
            return next(itertools.islice(self, item, item + 1))
        except StopIteration:
            raise exceptions.ResourceSetIndexError
    if self._resources is not None and self._is_sliced:
        return self._resource_cls(self.__class__, [resource for resource in BaseResourceSet.__iter__(self)])
    return self","isinstance(item, slice)",56,"isinstance(item, slice)",True,100.00000000000004,N/A
"def __iter__(self):
    """"""
        Returns requested resources in a lazy fashion.
        """"""
<mask>:
        self.manager.params.setdefault('limit', self.limit)
        self.manager.params.setdefault('offset', self.offset)
        try:
            self._resources, self._total_count = self.manager.redmine.engine.bulk_request('get', self.manager.url, self.manager.container, **self.manager.params)
        except exceptions.ResourceNotFoundError as e:
            if self.manager.resource_class.requirements:
                raise exceptions.ResourceRequirementsError(self.manager.resource_class.requirements)
            raise e
        resources = self._resources
    elif self._is_sliced:
        offset = self.offset or None
        if not self.limit:
            limit = None
        elif self.limit and (not self.offset):
            limit = self.limit
        else:
            limit = self.limit + self.offset
        resources = self._resources[offset:limit]
    else:
        resources = self._resources
    self._is_sliced = False
    return (resource for resource in resources)",self._resources is None,80,self._is_lazy,False,32.46679154750991,N/A
"def get(self, resource_id, default=None):
    """"""
        Returns a single Resource from a ResourceSet by resource id.

        :param resource_id: (required). Resource id.
        :type resource_id: int or string
        :param none default: (optional). What to return if Resource wasn't found.
        """"""
    for resource in super().__iter__():
<mask>:
            return self.manager.to_resource(resource)
    return default",resource_id == resource[self.manager.resource_class.internal_id_key],46,resource.resource_id == resource_id,False,23.252183803755873,N/A
"def __init__(self, url, **kwargs):
    """"""
        :param string url: (required). Redmine location.
        :param string key: (optional). API key used for authentication.
        :param string version: (optional). Redmine version.
        :param string username: (optional). Username used for authentication.
        :param string password: (optional). Password used for authentication.
        :param dict requests: (optional). Connection options.
        :param string impersonate: (optional). Username to impersonate.
        :param string date_format: (optional). Formatting directives for date format.
        :param string datetime_format: (optional). Formatting directives for datetime format.
        :param raise_attr_exception: (optional). Control over resource attribute access exception raising.
        :type raise_attr_exception: bool or tuple
        :param timezone: (optional). Whether to convert a naive datetime to a specific timezone aware one.
        :type timezone: str or cls
        :param cls engine: (optional). Engine that will be used to make requests to Redmine.
        """"""
    self.url = url.rstrip('/')
    self.ver = kwargs.pop('version', None)
<mask>:
        self.ver = utilities.versiontuple(self.ver)
    self.timezone = kwargs.pop('timezone', None)
    if self.timezone is not None and (not isinstance(self.timezone, datetime.tzinfo)):
        try:
            self.timezone = datetime.datetime.strptime(self.timezone, '%z').tzinfo
        except (TypeError, ValueError):
            raise exceptions.TimezoneError
    self.date_format = kwargs.pop('date_format', '%Y-%m-%d')
    self.datetime_format = kwargs.pop('datetime_format', '%Y-%m-%dT%H:%M:%SZ')
    self.raise_attr_exception = kwargs.pop('raise_attr_exception', True)
    engine = kwargs.pop('engine', engines.DefaultEngine)
    if not inspect.isclass(engine) or not issubclass(engine, engines.BaseEngine):
        raise exceptions.EngineClassError
    self.engine = engine(**kwargs)",self.ver is not None,185,"self.ver is not None and (not isinstance(self.ver, tuple))",False,26.46015952359329,N/A
"def __getattr__(self, resource_name):
    """"""
        Returns a ResourceManager object for the requested resource.

        :param string resource_name: (required). Resource name.
        """"""
<mask>:
        raise AttributeError
    resource_name = ''.join((word[0].upper() + word[1:] for word in str(resource_name).split('_')))
    try:
        resource_class = resources.registry[resource_name]['class']
    except KeyError:
        raise exceptions.ResourceError
    if self.ver is not None and self.ver < resource_class.redmine_version:
        raise exceptions.ResourceVersionMismatchError
    return resource_class.manager_class(self, resource_class)",resource_name.startswith('_'),53,resource_name is None,False,14.628187563941408,N/A
"def upload(self, f, filename=None):
    """"""
        Uploads file from file path / file stream to Redmine and returns an assigned token.

        :param f: (required). File path / stream that will be uploaded.
        :type f: string or file-like object
        :param filename: (optional). Filename for the file that will be uploaded.
        """"""
<mask>:
        raise exceptions.VersionMismatchError('File uploading')
    if hasattr(f, 'close'):
        try:
            c = f.read(0)
        except (AttributeError, TypeError):
            raise exceptions.FileObjectError
        if isinstance(c, str):
            warnings.warn('File-like object contains unicode, hence an additional step is performed to convert its content to bytes, please consider switching to bytes to eliminate this warning', exceptions.PerformanceWarning)
            f = io.BytesIO(f.read().encode('utf-8'))
        stream = f
        close = False
    else:
        if not os.path.isfile(f) or os.path.getsize(f) == 0:
            raise exceptions.NoFileError
        if not filename:
            filename = os.path.basename(f)
        stream = open(f, 'rb')
        close = True
    url = f'{self.url}/uploads.json'
    headers = {'Content-Type': 'application/octet-stream'}
    params = {'filename': filename or ''}
    response = self.engine.request('post', url, params=params, data=stream, headers=headers)
    if close:
        stream.close()
    return response['upload']","self.ver is not None and self.ver < (1, 4, 0)",151,self.version != 2,False,2.1969512149331583,N/A
"def download(self, url, savepath=None, filename=None, params=None):
    """"""
        Downloads file from Redmine and saves it to savepath or returns a response directly
        for maximum control over file processing.

        :param string url: (required). URL of the file that will be downloaded.
        :param string savepath: (optional). Path where to save the file.
        :param string filename: (optional). Name that will be used for the file.
        :param dict params: (optional). Params to send in the query string.
        """"""
    with self.session(requests={'stream': True}, return_raw_response=True):
        response = self.engine.request('get', url, params=params or {})
<mask>:
        return response
    from urllib.parse import urlsplit
    if filename is None:
        filename = urlsplit(url)[2].split('/')[-1]
        if not filename:
            raise exceptions.FileUrlError
    savepath = os.path.join(savepath, filename)
    with open(savepath, 'wb') as f:
        for chunk in response.iter_content(1024):
            f.write(chunk)
    return savepath",savepath is None,119,response.status_code == 200,False,0.0,N/A
"def search(self, query, **options):
    """"""
        Interface to Redmine Search API

        :param string query: (required). What to search.
        :param dict options: (optional). Dictionary of search options.
        """"""
<mask>:
        raise exceptions.VersionMismatchError('Search functionality')
    container_map, manager_map, results = ({}, {}, {'unknown': {}})
    for resource in options.pop('resources', []):
        options[resource] = True
    options['q'] = query
    for name, details in resources.registry.items():
        if details['class'].search_hints is not None:
            container = details['class'].container_all or details['class'].container_filter
            for hint in details['class'].search_hints:
                container_map[hint] = container
            manager_map[container] = getattr(self, name)
    raw_resources, _ = self.engine.bulk_request('get', f'{self.url}/search.json', 'results', **options)
    for resource in raw_resources:
        if resource['type'] in container_map:
            container = container_map[resource['type']]
            if container not in results:
                results[container] = []
            results[container].append(resource)
        else:
            if resource['type'] not in results['unknown']:
                results['unknown'][resource['type']] = []
            results['unknown'][resource['type']].append(resource)
        del resource['type']
    if not results['unknown']:
        del results['unknown']
    for container in results:
        if container in manager_map:
            results[container] = manager_map[container].to_resource_set(results[container])
    return results or None","self.ver is not None and self.ver < (3, 0, 0)",134,self.version != 2,False,2.1969512149331583,N/A
"def __init__(self, requirements):
    reqs = []
    for req in requirements:
<mask>:
            reqs.append(' >= '.join([req[0], '.'.join(map(str, req[1]))]))
        else:
            reqs.append(req)
    super().__init__(f""The following requirements must be installed for resource to function: {', '.join(reqs)}"")","isinstance(req, (list, tuple))",30,"isinstance(req, list)",False,28.871566309219904,N/A
"def versiontuple(version):
    """"""
    Converts numeric SemVer version string to tuple.

    :param string version: (required). Version string to convert.
    """"""
    parts = str(version).split('.')
<mask>:
        raise exceptions.VersionFormatError(version)
    return tuple((int(part) for part in parts))",len(parts) != 3 or not all((part.isnumeric() for part in parts)),31,len(parts) != 2,False,8.228727908938593,N/A
"def merge_dicts(a, b):
    """"""
    Merges dicts a and b recursively into a new dict.

    :param dict a: (required).
    :param dict b: (required).
    """"""
    result = copy.deepcopy(a)
    for key, value in b.items():
<mask>:
            result[key] = merge_dicts(value, a.get(key, {}))
        else:
            result[key] = value
    return result","isinstance(value, dict)",43,"isinstance(value, dict)",True,100.00000000000004,N/A
"def check_unused_args(self, used_args, args, kwargs):
    for item in used_args:
<mask>:
            self.used_kwargs[item] = kwargs.pop(item)
    self.unused_kwargs = kwargs",item in kwargs,16,item in args,False,55.03212081491043,N/A
"@classmethod
def encode(cls, attr, value, manager):
<mask>:
        return (attr, [module['name'] for module in value])
    return super().encode(attr, value, manager)",attr == 'enabled_modules',18,"isinstance(value, list)",False,0.0,N/A
"def __getattr__(self, attr):
<mask>:
        return lambda: getattr(self.manager, attr)(self.internal_id)
    return super().__getattr__(attr)","attr in ('close', 'reopen', 'archive', 'unarchive')",10,"attr in ('_id', '_manager', '_internal_id')",False,10.123734869668828,N/A
"def __init__(self, issue):
    self._redmine = issue.manager.redmine
    self._issue_id = issue.internal_id
<mask>:
        raise exceptions.ResourceVersionMismatchError","self._redmine.ver is not None and self._redmine.ver < (2, 3, 0)",12,self._issue_id != self._redmine.resource_version,False,18.572266535605454,N/A
"def __getattr__(self, attr):
<mask>:
        return Issue.Watcher(self)
    if attr == 'version':
        attr = 'fixed_version'
    return super().__getattr__(attr)",attr == 'watcher',15,attr == 'watcher',True,100.00000000000004,N/A
"def __setattr__(self, attr, value):
<mask>:
        attr = 'fixed_version_id'
    super().__setattr__(attr, value)",attr == 'version_id',10,attr == 'version_id',True,100.00000000000004,N/A
"def __new__(mcs, name, bases, attrs):
    cls = super().__new__(mcs, name, bases, mcs.bulk_update_attrs(attrs))
    mcs.bulk_update_cls_attrs(cls, attrs)
<mask>:
        return cls
    if name not in registry:
        registry[name] = {}
    for attr in ('_attach_includes', '_attach_relations', '_attach_includes_map'):
        class_attr_name = attr[7:]
        registry_attr_name = attr[1:]
        if registry_attr_name in registry[name]:
            if attr == '_attach_includes_map':
                mcs.update_cls_attr(cls, class_attr_name, dict(registry[name][registry_attr_name].keys()))
            else:
                mcs.update_cls_attr(cls, class_attr_name, registry[name][registry_attr_name].keys())
                mcs.update_cls_attr(cls, '_resource_set_map', registry[name][registry_attr_name])
        if not isinstance(getattr(cls, attr), dict):
            continue
        for resource_name, value in getattr(cls, attr).items():
            if resource_name not in registry:
                registry[resource_name] = {}
            if registry_attr_name not in registry[resource_name]:
                registry[resource_name][registry_attr_name] = {}
            registry[resource_name][registry_attr_name][value] = name
            if 'class' in registry[resource_name]:
                if attr == '_attach_includes_map':
                    mcs.update_cls_attr(registry[resource_name]['class'], class_attr_name, dict([value]))
                else:
                    mcs.update_cls_attr(registry[resource_name]['class'], class_attr_name, [value])
                    mcs.update_cls_attr(registry[resource_name]['class'], '_resource_set_map', {value: name})
    return registry[name].setdefault('class', cls)",name.startswith('Base'),107,bases is None,False,0.0,N/A
"@staticmethod
def bulk_update_attrs(attrs):
    """"""
        Updates attrs with specific features and/or actualizes their content before a class is created.

        :param dict attrs: (required). Attributes to work with.
        """"""
    for attr, value in attrs.items():
<mask>:
            attrs[attr] = utilities.ResourceQueryStr(value)
    return attrs",attr.startswith('query_') and value is not None,38,"isinstance(value, str)",False,3.3264637832151163,N/A
"@classmethod
def bulk_update_cls_attrs(mcs, cls, attrs):
    """"""
        Updates attrs with specific features and/or actualizes their content after a class is created.

        :param any cls: (required). Resource class.
        :param dict attrs: (required). Attributes to work with.
        """"""
    properties = []
    for attr, value in attrs.items():
<mask>:
            properties.append(attr)
    mcs.update_cls_attr(cls, '_members', properties)","not attr.startswith('_') and isinstance(value, property)",48,value is not None,False,0.9457497807469653,N/A
"@staticmethod
def update_cls_attr(cls, name, value):
    """"""
        Updates class attribute's value by first copying the current value and then updating it with
        new value. We need that to be sure that each resource class has its own copy of the value.

        :param any cls: (required). Resource class.
        :param string name: (required). Attribute name.
        :param any value: (required). Attribute value.
        """"""
    attr = getattr(cls, name, None)
<mask>:
        value = list(set().union(attr, value))
    elif isinstance(attr, dict):
        value = dict(attr, **value)
    else:
        return
    setattr(cls, name, value)","isinstance(attr, list)",81,"isinstance(attr, list)",True,100.00000000000004,N/A
"def __init__(self, manager, attributes):
    """"""
        :param managers.ResourceManager manager: (required). Manager object.
        :param dict attributes: (required). Resource attributes.
        """"""
    relations_includes = self._relations + self._includes
    self.manager = manager
    self._create_readonly = self._create_readonly[:] + relations_includes
    self._update_readonly = self._update_readonly[:] + relations_includes
    self._decoded_attrs = dict(dict.fromkeys(relations_includes), **attributes)
    self._encoded_attrs = {}
    self._changes = {}
<mask>:
        self._relations_name = self.__class__.__name__.lower()",self._relations_name is None,50,"self.__class__.__name__.lower() in ('patch', 'patch')",False,7.7075324887553816,N/A
"def __getattr__(self, attr):
<mask>:
        if self.redmine.ver is not None and self.redmine.ver < (5, 0, 0):
            raise exceptions.VersionMismatchError(f'Project {attr}')
        return lambda resource_id: self.redmine.engine.request('put', f'{self.redmine.url}{self.resource_class.query_one.format(resource_id)[:-5]}/{attr}.json')
    raise AttributeError(f""'{self.__class__.__name__}' object has no attribute '{attr}'"")","attr in ('close', 'reopen', 'archive', 'unarchive')",30,"attr in ['redmine_version', 'redmine_version_string', 'redmine_version_string', 'redmine_ver', 'redmine_ver_string', 'redmine_ver_string', 'redmine_ver_string', 'redmine_ver_string', 'redmine_ver_string', 'redmine_ver_string', 'redmine_ver_string', 'redmine_ver_string', 'redmine_",False,1.2437041056909581,N/A
"def copy(self, issue_id, link_original=True, include=(), **fields):
    fields['_copy'] = {'copy_from': issue_id}
<mask>:
        fields['_copy']['link_copy'] = '1'
    if include is not None:
        for i in include or ('subtasks', 'attachments'):
            fields['_copy'][f'copy_{i}'] = '1'
    return self.create(**fields)",link_original,31,link_original,True,100.00000000000004,N/A
"def _process_create_response(self, request, response):
<mask>:
        response = {self.container: {'id': int(request[self.container]['token'].split('.')[0])}}
    return super()._process_create_response(request, response)",response is True,13,self.container,False,0.0,N/A
"def _process_create_response(self, request, response):
<mask>:
        raise exceptions.ValidationError('Resource already exists')
    return super()._process_create_response(request, response)",response is True,12,self.resource_exists(),False,0.0,N/A
"def get(self, resource_id, **params):
    """"""
        Returns a Resource object from Redmine by resource id.

        :param resource_id: (required). Resource id.
        :type resource_id: int or string
        :param dict params: (optional). Parameters used for resource retrieval.
        """"""
<mask>:
        operation = self.all if self.resource_class.query_all else self.filter
        resource = operation(**params).get(resource_id, None)
        if resource is None:
            raise exceptions.ResourceNotFoundError
        return resource
    try:
        self.url = self._construct_get_url(self.resource_class.query_one.format(resource_id, **params))
    except KeyError as e:
        raise exceptions.ValidationError(f'{e} argument is required')
    self.params = self._prepare_get_request(params)
    self.container = self.resource_class.container_one
    try:
        response = self.redmine.engine.request('get', self.url, params=self.params)
    except exceptions.ResourceNotFoundError as e:
        if self.resource_class.requirements:
            raise exceptions.ResourceRequirementsError(self.resource_class.requirements)
        raise e
    return self._process_get_response(self.params, response)",self.resource_class.query_one is None or self.resource_class.container_one is None,93,"isinstance(resource_id, int)",False,1.6937742579984036,N/A
"def all(self, **params):
    """"""
        Returns a ResourceSet object with all Resource objects.

        :param dict params: (optional). Parameters used for resources retrieval.
        """"""
<mask>:
        raise exceptions.ResourceBadMethodError
    self.url = self.redmine.url + self.resource_class.query_all
    self.params = self.resource_class.bulk_decode(params, self)
    self.container = self.resource_class.container_all
    return resultsets.ResourceSet(self)",self.resource_class.query_all is None or self.resource_class.container_all is None,39,self.resource_class.method not in self.resource_class.methods,False,38.90730303940564,N/A
"def filter(self, **filters):
    """"""
        Returns a ResourceSet object with Resource objects filtered by a dict of filters.

        :param dict filters: (optional). Filters used for resources retrieval.
        """"""
<mask>:
        raise exceptions.ResourceBadMethodError
    if not filters:
        raise exceptions.ResourceNoFiltersProvidedError
    try:
        self.url = self.redmine.url + self.resource_class.query_filter.format(**filters)
        self.container = self.resource_class.container_filter.format(**filters)
    except KeyError:
        raise exceptions.ResourceFilterError
    self.params = self.resource_class.bulk_decode(filters, self)
    return resultsets.ResourceSet(self)",self.resource_class.query_filter is None or self.resource_class.container_filter is None,54,not self.resource_class.method,False,10.433887534096735,N/A
"def create(self, **fields):
    """"""
        Creates a new resource in Redmine and returns created Resource object on success.

        :param dict fields: (optional). Fields used for resource creation.
        """"""
<mask>:
        raise exceptions.ResourceBadMethodError
    if not fields:
        raise exceptions.ResourceNoFieldsProvidedError
    try:
        url = self._construct_create_url(self.resource_class.query_create.format(**fields))
    except KeyError as e:
        raise exceptions.ValidationError(f'{e} field is required')
    self.params = self.resource_class.query_create.formatter.used_kwargs
    self.container = self.resource_class.container_create
    request = self._prepare_create_request(self.resource_class.query_create.formatter.unused_kwargs)
    response = self.redmine.engine.request(self.resource_class.http_method_create, url, data=request)
    if response is None:
        return None
    resource = self._process_create_response(request, response)
    if self.resource_class.query_one is not None:
        self.url = self.redmine.url + self.resource_class.query_one.format(resource.internal_id, **fields)
    return resource",self.resource_class.query_create is None or self.resource_class.container_create is None,85,not self.resource_class.http_method_create,False,19.323525572409448,N/A
"def update(self, resource_id, **fields):
    """"""
        Updates a Resource object by resource id.

        :param resource_id: (required). Resource id.
        :type resource_id: int or string
        :param dict fields: (optional). Fields that will be updated for the resource.
        """"""
<mask>:
        raise exceptions.ResourceBadMethodError
    if not fields:
        raise exceptions.ResourceNoFieldsProvidedError
    try:
        query_update = self.resource_class.query_update.format(resource_id, **fields)
    except KeyError as e:
        param = e.args[0]
        if param in self.params:
            fields[param] = self.params[param]
            query_update = self.resource_class.query_update.format(resource_id, **fields)
        else:
            raise exceptions.ValidationError(f'{e} argument is required')
    self.params.update(self.resource_class.query_update.formatter.used_kwargs)
    self.container = self.resource_class.container_update
    url = self._construct_update_url(query_update)
    request = self._prepare_update_request(self.resource_class.query_update.formatter.unused_kwargs)
    response = self.redmine.engine.request(self.resource_class.http_method_update, url, data=request)
    if response is None:
        return None
    return self._process_update_response(request, response)",self.resource_class.query_update is None or self.resource_class.container_update is None,96,not self.resource_class.http_method_update,False,19.323525572409448,N/A
"def __init__(self, **options):
    """"""
        :param string key: (optional). API key used for authentication.
        :param string username: (optional). Username used for authentication.
        :param string password: (optional). Password used for authentication.
        :param dict requests: (optional). Connection options.
        :param string impersonate: (optional). Username to impersonate.
        :param bool ignore_response (optional). If True no response processing will be done at all.
        :param bool return_response (optional). Whether to return response or None.
        :param bool return_raw_response (optional). Whether to return raw or json encoded responses.
        """"""
    self.ignore_response = options.pop('ignore_response', False)
    self.return_response = options.pop('return_response', True)
    self.return_raw_response = options.pop('return_raw_response', False)
    self.requests = dict(dict(headers={}, params={}), **options.get('requests', {}))
<mask>:
        self.requests['stream'] = True
    if options.get('impersonate') is not None:
        self.requests['headers']['X-Redmine-Switch-User'] = options['impersonate']
    if options.get('key') is not None:
        self.requests['headers']['X-Redmine-API-Key'] = options['key']
    elif options.get('username') is not None and options.get('password') is not None:
        self.requests['auth'] = (options['username'], options['password'])
    self.session = self.create_session(**self.requests)",self.ignore_response,134,options.get('stream') is None,False,5.522397783539471,N/A
"def construct_request_kwargs(self, method, headers, params, data):
    """"""
        Constructs kwargs that will be used in all requests to Redmine.

        :param string method: (required). HTTP verb to use for the request.
        :param dict headers: (required). HTTP headers to send with the request.
        :param dict params: (required). Params to send in the query string.
        :param data: (required). Data to send in the body of the request.
        :type data: dict, bytes or file-like object
        """"""
    kwargs = dict(self.requests, **{'data': data or {}, 'params': params or {}, 'headers': headers or {}})
<mask>:
        kwargs['data'] = json.dumps(data)
        kwargs['headers']['Content-Type'] = 'application/json'
    return kwargs","method in ('post', 'put', 'patch') and 'Content-Type' not in kwargs['headers']",95,method == 'POST',False,0.6193628179172647,N/A
"def bulk_request(self, method, url, container, **params):
    """"""
        Makes needed preparations before launching the active engine's request process.

        :param string method: (required). HTTP verb to use for the request.
        :param string url: (required). URL of the request.
        :param string container: (required). Key in the response that should be used to access retrieved resources.
        :param dict params: (optional). Params that should be used for resource retrieval.
        """"""
    limit = params.get('limit') or 0
    offset = params.get('offset') or 0
    response = self.request(method, url, params=dict(params, limit=limit or self.chunk, offset=offset))
<mask>:
        total_count = response['total_count']
        results = response[container]
        limit = limit or total_count
        if limit > self.chunk:
            bulk_params = []
            for num in range(limit - self.chunk, 0, -self.chunk):
                offset += self.chunk
                limit -= self.chunk
                bulk_params.append(dict(params, offset=offset, limit=limit))
            if len(bulk_params) == 1:
                results.extend(self.request(method, url, params=bulk_params[0])[container])
            else:
                results.extend(self.process_bulk_request(method, url, container, bulk_params))
    else:
        total_count = len(response[container])
        results = response[container][offset:None if limit == 0 else limit + offset]
    return (results, total_count)","all((response.get(param) is not None for param in ('total_count', 'limit', 'offset')))",150,container in self.container_map,False,0.572619575460528,N/A
"def process_response(self, response):
    """"""
        Processes response received from Redmine.

        :param obj response: (required). Response object with response details.
        """"""
<mask>:
        return None
    if response.history:
        r = response.history[0]
        if 300 <= r.status_code <= 399:
            url1, url2 = (str(r.request.url), str(response.request.url))
            if url1[:5] == 'http:' and url2[:6] == 'https:' or (url1[:6] == 'https:' and url2[:5] == 'http:'):
                raise exceptions.HTTPProtocolError
            else:
                warnings.warn('Redirect detected during request-response, normally there should be no redirects, so please check your Redmine URL for things like prepending www which redirects to a no www domain and vice versa or using an old domain which redirects to a new one', exceptions.PerformanceWarning)
    status_code = response.status_code
    if status_code in (200, 201, 204):
        if not self.return_response:
            return None
        elif self.return_raw_response:
            return response
        elif not response.content.strip():
            return True
        else:
            try:
                return response.json()
            except (ValueError, TypeError):
                raise exceptions.JSONDecodeError(response)
    elif status_code == 401:
        raise exceptions.AuthError
    elif status_code == 403:
        raise exceptions.ForbiddenError
    elif status_code == 404:
        raise exceptions.ResourceNotFoundError
    elif status_code == 409:
        raise exceptions.ConflictError
    elif status_code == 412:
        raise exceptions.ImpersonateError
    elif status_code == 413:
        raise exceptions.RequestEntityTooLargeError
    elif status_code == 422:
        errors = response.json()['errors']
        raise exceptions.ValidationError(', '.join((': '.join(e) if isinstance(e, list) else e for e in errors)))
    elif status_code == 500:
        raise exceptions.ServerError
    raise exceptions.UnknownError(status_code)",self.ignore_response,196,not response,False,11.15650800742149,N/A
"def __init__(self, trials: int=100, epsilon: float=0.005, ext_EMD=None, parallel: bool=False, **kwargs):
    self.trials = trials
    self.epsilon = epsilon
    self.noise_scale = float(kwargs.get('noise_scale', 1.0))
    self.range_thr = float(kwargs.get('range_thr', 0.01))
    self.total_power_thr = float(kwargs.get('total_power_thr', 0.05))
    self.beta_progress = bool(kwargs.get('beta_progress', True))
    self.random = np.random.RandomState(seed=kwargs.get('seed'))
    self.noise_kind = kwargs.get('noise_kind', 'normal')
    self._max_imf = int(kwargs.get('max_imf', 100))
    self.parallel = parallel
    self.processes = kwargs.get('processes')
<mask>:
        self.logger.warning('Passed value for process has no effect when `parallel` is False.')
    self.all_noise_EMD = []
    if ext_EMD is None:
        from PyEMD import EMD
        self.EMD = EMD(**kwargs)
    else:
        self.EMD = ext_EMD
    self.C_IMF = None
    self.residue = None",self.processes is not None and (not self.parallel),85,self.parallel,False,3.567399334725242,N/A
"def __getstate__(self) -> Dict:
    self_dict = self.__dict__.copy()
<mask>:
        del self_dict['pool']
    return self_dict",'pool' in self_dict,12,'pool' in self_dict,True,100.00000000000004,N/A
"def generate_noise(self, scale: float, size: Union[int, Sequence[int]]) -> np.ndarray:
    """"""
        Generate noise with specified parameters.
        Currently supported distributions are:

        * *normal* with std equal scale.
        * *uniform* with range [-scale/2, scale/2].

        Parameters
        ----------

        scale : float
            Width for the distribution.
        size : int or shape
            Shape of the noise that is added. In case of `int` an array of that len is generated.

        Returns
        -------

        noise : numpy array
            Noise sampled from selected distribution.
        """"""
<mask>:
        noise = self.random.normal(loc=0, scale=scale, size=size)
    elif self.noise_kind == 'uniform':
        noise = self.random.uniform(low=-scale / 2, high=scale / 2, size=size)
    else:
        raise ValueError('Unsupported noise kind. Please assigned `noise_kind` to be one of these: {0}'.format(str(self.noise_kinds_all)))
    return noise",self.noise_kind == 'normal',110,self.noise_kind == 'normal',True,100.00000000000004,N/A
"def ceemdan(self, S: np.ndarray, T: Optional[np.ndarray]=None, max_imf: int=-1, progress: bool=False) -> np.ndarray:
    """"""Perform CEEMDAN decomposition.

        Parameters
        ----------
        S : numpy array
            Original signal on which CEEMDAN is to perform.
        T : Optional(numpy array) (default: None)
            Time (x) values for the signal. If not passed, i.e. `T = None`, then assumes equidistant values.
        max_imf : int (default: -1)
            Maximum number of components to extract.
        progress : bool (default: False)
            Whether to print out '.' every 1s to indicate progress.

        Returns
        -------
        components : np.ndarray
            CEEMDAN components.
        """"""
    scale_s = np.std(S)
    S = S / scale_s
    self.all_noises = self.generate_noise(self.noise_scale, (self.trials, S.size))
    self.logger.debug('Decomposing all noises')
    self.all_noise_EMD = self._decompose_noise()
    last_imf = self._eemd(S, T, max_imf=1, progress=progress)[0]
    res = np.empty(S.size)
    all_cimfs = last_imf.reshape((-1, last_imf.size))
    prev_res = S - last_imf
    self.logger.debug('Starting CEEMDAN')
    total = max_imf - 1 if max_imf != -1 else None
    it = iter if not progress else lambda x: tqdm(x, desc='cIMF decomposition', total=total)
    for _ in it(range(self._max_imf)):
<mask>:
            self.logger.debug('End Condition - Pass')
            break
        imfNo = all_cimfs.shape[0]
        beta = self.epsilon * np.std(prev_res)
        local_mean = np.zeros(S.size)
        for trial in range(self.trials):
            noise_imf = self.all_noise_EMD[trial]
            res = prev_res.copy()
            if len(noise_imf) > imfNo:
                res += beta * noise_imf[imfNo]
            imfs = self.emd(res, T, max_imf=1)
            local_mean += imfs[-1] / self.trials
        last_imf = prev_res - local_mean
        all_cimfs = np.vstack((all_cimfs, last_imf))
        prev_res = local_mean.copy()
    res = S - np.sum(all_cimfs, axis=0)
    all_cimfs = np.vstack((all_cimfs, res))
    all_cimfs = all_cimfs * scale_s
    del self.all_noise_EMD[:]
    self.C_IMF = all_cimfs
    self.residue = S * scale_s - np.sum(self.C_IMF, axis=0)
    return all_cimfs","self.end_condition(S, all_cimfs, max_imf)",241,self.all_noise_EMD.empty,False,6.550847048803501,N/A
"def end_condition(self, S: np.ndarray, cIMFs: np.ndarray, max_imf: int) -> bool:
    """"""Test for end condition of CEEMDAN.

        Procedure stops if:

        * number of components reach provided `max_imf`, or
        * last component is close to being pure noise (range or power), or
        * set of provided components reconstructs sufficiently input.

        Parameters
        ----------
        S : numpy array
            Original signal on which CEEMDAN was performed.
        cIMFs : numpy 2D array
            Set of cIMFs where each row is cIMF.
        max_imf : int
            The maximum number of imfs to extract.

        Returns
        -------
        end : bool
            Whether to stop CEEMDAN.
        """"""
    imfNo = cIMFs.shape[0]
<mask>:
        return True
    R = S - np.sum(cIMFs, axis=0)
    _test_imf = self.emd(R, None, max_imf=1)
    if _test_imf.shape[0] == 1:
        self.logger.debug('Not enough extrema')
        return True
    if np.max(R) - np.min(R) < self.range_thr:
        self.logger.debug('FINISHED -- RANGE')
        return True
    if np.sum(np.abs(R)) < self.total_power_thr:
        self.logger.debug('FINISHED -- SUM POWER')
        return True
    return False",0 < max_imf <= imfNo,143,imfNo == 2,False,6.988198185490689,N/A
"def __init__(self, emd_instance=None):
    self.emd_instance = emd_instance
    self.imfs = None
    self.residue = None
<mask>:
        self.imfs, self.residue = self.emd_instance.get_imfs_and_residue()",emd_instance is not None,17,self.emd_instance,False,32.555630133216134,N/A
"def _check_imfs(self, imfs, residue, include_residue):
    """"""Checks for passed imfs and residue.""""""
    imfs = imfs if imfs is not None else self.imfs
    residue = residue if residue is not None else self.residue
<mask>:
        raise AttributeError('No imfs passed to plot')
    if include_residue and residue is None:
        raise AttributeError('Requested to plot residue but no residue provided')
    return (imfs, residue)",imfs is None,56,imfs is None,True,100.00000000000004,N/A
"def plot_imfs(self, imfs=None, residue=None, t=None, include_residue=True):
    """"""Plots and shows all IMFs.

        All parameters are optional since the `emd` object could have been passed when instantiating this object.

        The residual is an optional and can be excluded by setting `include_residue=False`.
        """"""
    imfs, residue = self._check_imfs(imfs, residue, include_residue)
    num_rows, t_length = imfs.shape
    num_rows += include_residue is True
    t = t if t is not None else range(t_length)
    fig, axes = plt.subplots(num_rows, 1, figsize=(self.PLOT_WIDTH, num_rows * self.PLOT_HEIGHT_PER_IMF))
<mask>:
        axes = list(axes)
    axes[0].set_title('Time series')
    for num, imf in enumerate(imfs):
        ax = axes[num]
        ax.plot(t, imf)
        ax.set_ylabel('IMF ' + str(num + 1))
    if include_residue:
        ax = axes[-1]
        ax.plot(t, residue)
        ax.set_ylabel('Res')
    plt.tight_layout()",num_rows == 1,105,include_residue,False,10.122592925934278,N/A
"def plot_instant_freq(self, t, imfs=None, order=False, alpha=None):
    """"""Plots and shows instantaneous frequencies for all provided imfs.

        The necessary parameter is `t` which is the time array used to compute the EMD.
        One should pass `imfs` if no `emd` instances is passed when creating the Visualisation object.

        Parameters
        ----------

        order : bool (default: False)
            Represents whether the finite difference scheme is
            low-order (1st order forward scheme) or high-order (6th order
            compact scheme). The default value is False (low-order)

        alpha : float (default: None)
            Filter intensity. Default value is None, which
            is equivalent to `alpha` = 0.5, meaning that no filter is applied.
            The `alpha` values must be in between -0.5 (fully active) and 0.5
            (no filter).
        """"""
<mask>:
        assert -0.5 < alpha < 0.5, '`alpha` must be in between -0.5 and 0.5'
    imfs, _ = self._check_imfs(imfs, None, False)
    num_rows = imfs.shape[0]
    imfs_inst_freqs = self._calc_inst_freq(imfs, t, order=order, alpha=alpha)
    fig, axes = plt.subplots(num_rows, 1, figsize=(self.PLOT_WIDTH, num_rows * self.PLOT_HEIGHT_PER_IMF))
    if num_rows == 1:
        axes = fig.axes
    axes[0].set_title('Instantaneous frequency')
    for num, imf_inst_freq in enumerate(imfs_inst_freqs):
        ax = axes[num]
        ax.plot(t, imf_inst_freq)
        ax.set_ylabel('IMF {} [Hz]'.format(num + 1))
    plt.tight_layout()",alpha is not None,179,alpha is not None,True,100.00000000000004,N/A
"def _calc_inst_phase(self, sig, alpha):
    """"""Extract analytical signal through the Hilbert Transform.""""""
    analytic_signal = hilbert(sig)
<mask>:
        assert -0.5 < alpha < 0.5, '`alpha` must be in between -0.5 and 0.5'
        real_part = np.array([filt6(row.real, alpha) for row in analytic_signal])
        imag_part = np.array([filt6(row.imag, alpha) for row in analytic_signal])
        analytic_signal = real_part + 1j * imag_part
    phase = np.unwrap(np.angle(analytic_signal))
    if alpha is not None:
        phase = np.array([filt6(row, alpha) for row in phase])
    return phase",alpha is not None,70,alpha is not None,True,100.00000000000004,N/A
"def __init__(self, **config):
    self.mse_thr = 0.01
    self.mean_thr = 0.01
    self.FIXE = 0
    self.FIXE_H = 0
    self.MAX_ITERATION = 1000
    for key in config.keys():
<mask>:
            self.__dict__[key] = config[key]",key in self.__dict__.keys(),26,key not in self.__dict__,False,57.97215869131433,N/A
"@classmethod
def end_condition(cls, image, IMFs):
    """"""Determins whether decomposition should be stopped.

        Parameters
        ----------
        image : numpy 2D array
            Input image which is decomposed.
        IMFs : numpy 3D array
            Array for which first dimensions relates to respective IMF,
            i.e. (numIMFs, imageX, imageY).
        """"""
    rec = np.sum(IMFs, axis=0)
<mask>:
        return True
    return False","np.allclose(image, rec)",51,rec > 0,False,5.197112497172873,N/A
"def check_proto_imf(self, proto_imf, proto_imf_prev, mean_env):
    """"""Check whether passed (proto) IMF is actual IMF.
        Current condition is solely based on checking whether the mean is below threshold.

        Parameters
        ----------
        proto_imf : numpy 2D array
            Current iteration of proto IMF.
        proto_imf_prev : numpy 2D array
            Previous iteration of proto IMF.
        mean_env : numpy 2D array
            Local mean computed from top and bottom envelopes.

        Returns
        -------
        boolean
            Whether current proto IMF is actual IMF.
        """"""
<mask>:
        return True
    if np.allclose(proto_imf, proto_imf_prev):
        return True
    if np.mean(np.abs(proto_imf)) < self.mean_thr:
        return True
    mse_proto_imf = np.mean(proto_imf * proto_imf)
    if mse_proto_imf < self.mse_thr:
        return True
    return False",np.all(np.abs(mean_env - mean_env.mean()) < self.mean_thr),99,"np.allclose(proto_imf, proto_imf_env)",False,3.7741809796952825,N/A
"def emd(self, image, max_imf=-1):
    """"""Performs EMD on input image with specified parameters.

        Parameters
        ----------
        image : numpy 2D array,
            Image which will be decomposed.
        max_imf : int, (default: -1)
            IMF number to which decomposition should be performed.
            Negative value means *all*.

        Returns
        -------
        IMFs : numpy 3D array
            Set of IMFs in form of numpy array where the first dimension
            relates to IMF's ordinary number.
        """"""
    image_min, image_max = (np.min(image), np.max(image))
    offset = image_min
    scale = image_max - image_min
    image_s = (image - offset) / scale
    imf = np.zeros(image.shape)
    imf_old = imf.copy()
    imfNo = 0
    IMF = np.empty((imfNo,) + image.shape)
    notFinished = True
    while notFinished:
        self.logger.debug('IMF -- ' + str(imfNo))
        res = image_s - np.sum(IMF[:imfNo], axis=0)
        imf = res.copy()
        mean_env = np.zeros(image.shape)
        stop_sifting = False
        n = 0
        n_h = 0
        while not stop_sifting and n < self.MAX_ITERATION:
            n += 1
            self.logger.debug('Iteration: ' + str(n))
            min_peaks, max_peaks = self.find_extrema(imf)
            self.logger.debug('min_peaks = %i  |  max_peaks = %i', len(min_peaks[0]), len(max_peaks[0]))
<mask>:
                imf_old = imf.copy()
                imf = imf - mean_env
                min_env, max_env = self.extract_max_min_spline(imf)
                mean_env = 0.5 * (min_env + max_env)
                imf_old = imf.copy()
                imf = imf - mean_env
                if self.FIXE:
                    if n >= self.FIXE + 1:
                        stop_sifting = True
                elif self.FIXE_H:
                    if n == 1:
                        continue
                    if self.check_proto_imf(imf, imf_old, mean_env):
                        n_h += 1
                    else:
                        n_h = 0
                    if n_h >= self.FIXE_H:
                        stop_sifting = True
                elif self.check_proto_imf(imf, imf_old, mean_env):
                    stop_sifting = True
            else:
                notFinished = False
                stop_sifting = True
        IMF = np.vstack((IMF, imf.copy()[None, :]))
        imfNo += 1
        if self.end_condition(image, IMF) or (max_imf > 0 and imfNo >= max_imf):
            notFinished = False
            break
    res = image_s - np.sum(IMF[:imfNo], axis=0)
    if not np.allclose(res, 0):
        IMF = np.vstack((IMF, res[None, :]))
        imfNo += 1
    IMF = IMF * scale
    IMF[-1] += offset
    return IMF",len(min_peaks[0]) > 4 and len(max_peaks[0]) > 4,287,min_peaks == -1,False,1.7770664037976964,N/A
"def extract_max_min_spline(self, T: np.ndarray, S: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """"""
        Extracts top and bottom envelopes based on the signal,
        which are constructed based on maxima and minima, respectively.

        Parameters
        ----------
        T : numpy array
            Position or time array.
        S : numpy array
            Input data S(T).

        Returns
        -------
        max_spline : numpy array
            Spline spanned on S maxima.
        min_spline : numpy array
            Spline spanned on S minima.
        max_extrema : numpy array
            Points indicating local maxima.
        min_extrema : numpy array
            Points indicating local minima.
        """"""
    ext_res = self.find_extrema(T, S)
    max_pos, max_val = (ext_res[0], ext_res[1])
    min_pos, min_val = (ext_res[2], ext_res[3])
<mask>:
        return [-1] * 4
    max_extrema, min_extrema = self.prepare_points(T, S, max_pos, max_val, min_pos, min_val)
    _, max_spline = self.spline_points(T, max_extrema)
    _, min_spline = self.spline_points(T, min_extrema)
    return (max_spline, min_spline, max_extrema, min_extrema)",len(max_pos) + len(min_pos) < 3,127,max_pos == -1 or min_pos == -1,False,16.580932597529678,N/A
"def prepare_points(self, T: np.ndarray, S: np.ndarray, max_pos: np.ndarray, max_val: np.ndarray, min_pos: np.ndarray, min_val: np.ndarray):
    """"""
        Performs extrapolation on edges by adding extra extrema, also known
        as mirroring signal. The number of added points depends on *nbsym*
        variable.

        Parameters
        ----------
        T : numpy array
            Position or time array.
        S : numpy array
            Input signal.
        max_pos : iterable
            Sorted time positions of maxima.
        max_val : iterable
            Signal values at max_pos positions.
        min_pos : iterable
            Sorted time positions of minima.
        min_val : iterable
            Signal values at min_pos positions.

        Returns
        -------
        max_extrema : numpy array (2 rows)
            Position (1st row) and values (2nd row) of minima.
        min_extrema : numpy array (2 rows)
            Position (1st row) and values (2nd row) of maxima.
        """"""
<mask>:
        return self.prepare_points_parabol(T, S, max_pos, max_val, min_pos, min_val)
    elif self.extrema_detection == 'simple':
        return self.prepare_points_simple(T, S, max_pos, max_val, min_pos, min_val)
    else:
        msg = ""Incorrect extrema detection type. Please try: 'simple' or 'parabol'.""
        raise ValueError(msg)",self.extrema_detection == 'parabol',151,self.extrema_detection == 'parabol',True,100.00000000000004,N/A
"def spline_points(self, T: np.ndarray, extrema: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """"""
        Constructs spline over given points.

        Parameters
        ----------
        T : numpy array
            Position or time array.
        extrema : numpy array
            Position (1st row) and values (2nd row) of points.

        Returns
        -------
        T : numpy array
            Position array (same as input).
        spline : numpy array
            Spline array over given positions T.
        """"""
    kind = self.spline_kind.lower()
    t = T[np.r_[T >= extrema[0, 0]] & np.r_[T <= extrema[0, -1]]]
<mask>:
        return (t, akima(extrema[0], extrema[1], t))
    elif kind == 'cubic':
        if extrema.shape[1] > 3:
            return (t, cubic(extrema[0], extrema[1], t))
        else:
            return cubic_spline_3pts(extrema[0], extrema[1], t)
    elif kind == 'pchip':
        return (t, pchip(extrema[0], extrema[1], t))
    elif kind == 'cubic_hermite':
        return (t, cubic_hermite(extrema[0], extrema[1], t))
    elif kind in ['slinear', 'quadratic', 'linear']:
        return (T, interp1d(extrema[0], extrema[1], kind=kind)(t).astype(self.DTYPE))
    else:
        raise ValueError('No such interpolation method!')",kind == 'akima',133,kind == 'akima',True,100.00000000000004,N/A
"@classmethod
def end_condition(cls, image, IMFs):
    """"""Determines whether decomposition should be stopped.

        Parameters
        ----------
        image : numpy 2D array
            Input image which is decomposed.
        IMFs : numpy 3D array
            Array for which first dimensions relates to respective IMF,
            i.e. (numIMFs, imageX, imageY).
        """"""
    rec = np.sum(IMFs, axis=0)
<mask>:
        return True
    return False","np.allclose(image, rec)",51,rec > 0,False,5.197112497172873,N/A
"def check_proto_imf(self, proto_imf, proto_imf_prev, mean_env):
    """"""Check whether passed (proto) IMF is actual IMF.
        Current condition is solely based on checking whether the mean is below threshold.

        Parameters
        ----------
        proto_imf : numpy 2D array
            Current iteration of proto IMF.
        proto_imf_prev : numpy 2D array
            Previous iteration of proto IMF.
        mean_env : numpy 2D array
            Local mean computed from top and bottom envelopes.

        Returns
        -------
        boolean
            Whether current proto IMF is actual IMF.
        """"""
<mask>:
        return True
    if np.allclose(proto_imf, proto_imf_prev, rtol=0.01):
        return True
    if np.mean(np.abs(proto_imf)) < self.mean_thr:
        return True
    mse_proto_imf = np.mean(proto_imf * proto_imf)
    if mse_proto_imf > self.mse_thr:
        return False
    return False",np.all(np.abs(mean_env - mean_env.mean()) < self.mean_thr),100,"np.allclose(proto_imf, proto_imf_prev, rtol=0.01)",False,3.563457678516677,N/A
"def bemd(self, image, max_imf=-1):
    """"""Performs bidimensional EMD (BEMD) on grey-scale image with specified parameters.

        Parameters
        ----------
        image : numpy 2D array,
            Grey-scale image.
        max_imf : int, (default: -1)
            IMF number to which decomposition should be performed.
            Negative value means *all*.

        Returns
        -------
        IMFs : numpy 3D array
            Set of IMFs in form of numpy array where the first dimension
            relates to IMF's ordinary number.
        """"""
    image_s = image.copy()
    imf = np.zeros(image.shape)
    imf_old = imf.copy()
    imfNo = 0
    IMF = np.empty((imfNo,) + image.shape)
    notFinished = True
    while notFinished:
        self.logger.debug('IMF -- ' + str(imfNo))
        res = image_s - np.sum(IMF[:imfNo], axis=0)
        imf = res.copy()
        mean_env = np.zeros(image.shape)
        stop_sifting = False
        n = 0
        n_h = 0
        while not stop_sifting and n < self.MAX_ITERATION:
            n += 1
            self.logger.debug('Iteration: %i', n)
            min_peaks_pos, max_peaks_pos = self.find_extrema_positions(imf)
            self.logger.debug('min_peaks_pos = %i  |  max_peaks_pos = %i', len(min_peaks_pos[0]), len(max_peaks_pos[0]))
<mask>:
                min_env, max_env = self.extract_max_min_spline(imf, min_peaks_pos, max_peaks_pos)
                mean_env = 0.5 * (min_env + max_env)
                imf_old = imf.copy()
                imf = imf - mean_env
                if self.FIXE:
                    if n >= self.FIXE + 1:
                        stop_sifting = True
                elif self.FIXE_H:
                    if n == 1:
                        continue
                    if self.check_proto_imf(imf, imf_old, mean_env):
                        n_h += 1
                    else:
                        n_h = 0
                    if n_h >= self.FIXE_H:
                        stop_sifting = True
                elif self.check_proto_imf(imf, imf_old, mean_env):
                    stop_sifting = True
            else:
                stop_sifting = True
        IMF = np.vstack((IMF, imf.copy()[None, :]))
        imfNo += 1
        if self.end_condition(image, IMF) or (max_imf > 0 and imfNo >= max_imf):
            notFinished = False
            break
    res = image_s - np.sum(IMF[:imfNo], axis=0)
    if not np.allclose(res, 0):
        IMF = np.vstack((IMF, res[None, :]))
        imfNo += 1
    return IMF",len(min_peaks_pos[0]) > 1 and len(max_peaks_pos[0]) > 1,251,min_peaks_pos != -1 and max_peaks_pos != -1,False,24.35056746619584,N/A
"def get_timeline(range_max: int, dtype: Optional[np.dtype]=None) -> np.ndarray:
    """"""Returns timeline array for requirements.

    Parameters
    ----------
    range_max : int
        Largest value in range. Assume `range(range_max)`. Commonly that's length of the signal.
    dtype : np.dtype
        Minimal definition type. Returned timeline will have dtype that's the same or with higher byte size.

    """"""
    timeline = np.arange(0, range_max, dtype=dtype)
<mask>:
        inclusive_dtype = smallest_inclusive_dtype(timeline.dtype, range_max)
        timeline = np.arange(0, range_max, dtype=inclusive_dtype)
    return timeline",timeline[-1] != range_max - 1,66,not dtype,False,0.0,N/A
"def smallest_inclusive_dtype(ref_dtype: np.dtype, ref_value) -> np.dtype:
    """"""Returns a numpy dtype with the same base as reference dtype (ref_dtype)
    but with the range that includes reference value (ref_value).

    Parameters
    ----------
    ref_dtype : dtype
         Reference dtype. Used to select the base, i.e. int or float, for returned type.
    ref_value : value
        A value which needs to be included in returned dtype. Value will be typically int or float.

    """"""
<mask>:
        for dtype in [np.uint16, np.uint32, np.uint64]:
            if ref_value < np.iinfo(dtype).max:
                return dtype
        max_val = np.iinfo(np.uint32).max
        raise ValueError(""Requested too large integer range. Exceeds max( uint64 ) == '{}."".format(max_val))
    if np.issubdtype(ref_dtype, np.floating):
        for dtype in [np.float16, np.float32, np.float64]:
            if ref_value < np.finfo(dtype).max:
                return dtype
        max_val = np.finfo(np.float64).max
        raise ValueError(""Requested too large integer range. Exceeds max( float64 ) == '{}."".format(max_val))
    raise ValueError(""Unsupported dtype '{}'. Only intX and floatX are supported."".format(ref_dtype))","np.issubdtype(ref_dtype, np.integer)",135,"np.issubdtype(ref_dtype, np.uinting)",False,82.651681837938,N/A
"@cache
def deduce_common_type(xtype: np.dtype, ytype: np.dtype) -> np.dtype:
<mask>:
        return xtype
    if np.version.version[0] == '1':
        dtype = np.find_common_type([xtype, ytype], [])
    else:
        dtype = np.promote_types(xtype, ytype)
    return dtype",xtype == ytype,27,xtype.dtype != ytype.dtype,False,12.22307556087252,N/A
"def unify_types(x: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    dtype = deduce_common_type(x.dtype, y.dtype)
<mask>:
        x = x.astype(dtype)
    if y.dtype != dtype:
        y = y.astype(dtype)
    return (x, y)",x.dtype != dtype,26,x.dtype != dtype,True,100.00000000000004,N/A
"def extractMaxMinSpline(self, T, S):
    """"""
        Input:
        -----------------
            S - Input signal array. Should be 1D.
            T - Time array. If none passed numpy arange is created.

        Output:
        -----------------
            maxSpline - Upper envelope of signal S.
            minSpline - Bottom envelope of signal S.
            maxExtrema - Position (1st row) and values (2nd row) of maxima.
            minExtrema - Position (1st row) and values (2nd row) of minima.
        """"""
    maxPos, maxVal, minPos, minVal, _ = self.findExtrema(T, S)
<mask>:
        return [-1] * 4
    maxExtrema, minExtrema = self.preparePoints(S, T, maxPos, maxVal, minPos, minVal)
    _, maxSpline = self.splinePoints(T, maxExtrema, self.splineKind)
    _, minSpline = self.splinePoints(T, minExtrema, self.splineKind)
    return (maxSpline, minSpline, maxExtrema, minExtrema)",len(maxPos) + len(minPos) < 3,104,maxPos is None or minPos is None,False,4.410363736106611,N/A
"def splinePoints(self, T, extrema, splineKind):
    """"""
        Constructs spline over given points.

        Input:
        ---------
            T: Time array.
            extrema: Position (1st row) and values (2nd row) of points.
            splineKind: Type of spline.

        Output:
        ---------
            T: Position array.
            spline: Spline over the given points.
        """"""
    kind = splineKind.lower()
    t = T[np.r_[T >= extrema[0, 0]] & np.r_[T <= extrema[0, -1]]]
<mask>:
        self.logger.error('t.dtype: ' + str(t.dtype))
    if extrema.dtype != self.DTYPE:
        self.logger.error('extrema.dtype: ' + str(extrema.dtype))
    if kind == 'akima':
        return (t, akima(extrema[0], extrema[1], t))
    elif kind == 'cubic':
        if extrema.shape[1] > 3:
            return (t, interp1d(extrema[0], extrema[1], kind=kind)(t).astype(self.DTYPE))
        else:
            return self.cubicSpline_3points(T, extrema)
    elif kind in ['slinear', 'quadratic', 'linear']:
        return (T, interp1d(extrema[0], extrema[1], kind=kind)(t).astype(self.DTYPE))
    else:
        raise ValueError('No such interpolation method!')",t.dtype != self.DTYPE,112,t.dtype != self.DTYPE,True,100.00000000000004,N/A
"@classmethod
def findExtrema(cls, t, s):
    """"""
        Finds extrema and zero-crossings.

        Input:
        ---------
            S: Signal.
            T: Time array.

        Output:
        ---------
            localMaxPos: Time positions of maxima.
            localMaxVal: Values of signal at localMaxPos positions.
            localMinPos: Time positions of minima.
            localMinVal: Values of signal at localMinPos positions.
            indzer: Indexes of zero crossings.
        """"""
    s1, s2 = (s[:-1], s[1:])
    indzer = np.nonzero(s1 * s2 < 0)[0]
<mask>:
        iz = np.nonzero(s == 0)[0]
        indz = []
        if np.any(np.diff(iz) == 1):
            zer = s == 0
            dz = np.diff(np.append(np.append(0, zer), 0))
            debz = np.nonzero(dz == 1)[0]
            finz = np.nonzero(dz == -1)[0] - 1
            indz = np.round((debz + finz) / 2)
        else:
            indz = iz
        indzer = np.sort(np.append(indzer, indz))
    d = np.diff(s)
    d1, d2 = (d[:-1], d[1:])
    indmin = np.nonzero(np.r_[d1 * d2 < 0] & np.r_[d1 < 0])[0] + 1
    indmax = np.nonzero(np.r_[d1 * d2 < 0] & np.r_[d1 > 0])[0] + 1
    if np.any(d == 0):
        imax, imin = ([], [])
        bad = d == 0
        dd = np.diff(np.append(np.append(0, bad), 0))
        debs = np.nonzero(dd == 1)[0]
        fins = np.nonzero(dd == -1)[0]
        if debs[0] == 1:
            if len(debs) > 1:
                debs, fins = (debs[1:], fins[1:])
            else:
                debs, fins = ([], [])
        if len(debs) > 0:
            if fins[-1] == len(s) - 1:
                if len(debs) > 1:
                    debs, fins = (debs[:-1], fins[:-1])
                else:
                    debs, fins = ([], [])
        lc = len(debs)
        if lc > 0:
            for k in range(lc):
                if d[debs[k] - 1] > 0:
                    if d[fins[k]] < 0:
                        imax.append(round((fins[k] + debs[k]) / 2.0))
                elif d[fins[k]] > 0:
                    imin.append(round((fins[k] + debs[k]) / 2.0))
        if len(imax) > 0:
            indmax = indmax.tolist()
            for x in imax:
                indmax.append(int(x))
            indmax.sort()
        if len(imin) > 0:
            indmin = indmin.tolist()
            for x in imin:
                indmin.append(int(x))
            indmin.sort()
    localMaxPos = t[indmax]
    localMaxVal = s[indmax]
    localMinPos = t[indmin]
    localMinVal = s[indmin]
    return (localMaxPos, localMaxVal, localMinPos, localMinVal, indzer)",np.any(s == 0),297,np.any(np.diff(indzer)) == 1,False,21.97281387499715,N/A
"def stop_sifting(self, imf, envMax, envMin, mean, extNo):
    """"""
        Criterion for stopping sifting process.
        Based on conditions presented in [1].

        [1] G. Rilling, P. Flandrin and P. Goncalves
            ""On Empirical Mode Decomposition and its
            algorithms"", 2003

        Input:
        ---------
            imf: Current imf.
            envMax: Upper envelope of imf.
            envMin: Bottom envelope of imf.
            mean: Mean of envelopes.
            extNo: Number of extrema.

        Output:
        ---------
            boolean: True if stopping criteria are meet.
        """"""
    amp = np.abs(envMax - envMin) / 2.0
    sx = np.abs(mean) / amp
    f1 = np.mean(sx > self.stop1) > self.stop3
    f2 = np.any(sx > self.stop2)
    f3 = extNo > 2
<mask>:
        return True
    else:
        return False",not (f1 or f2) and f3,103,f1 and f2 and f3,False,13.943458243384402,N/A
"def __init__(self, trials: int=100, noise_width: float=0.05, ext_EMD=None, parallel: bool=False, **kwargs):
    self.trials = trials
    self.noise_width = noise_width
    self.separate_trends = bool(kwargs.get('separate_trends', False))
    self.random = np.random.RandomState()
    self.noise_kind = kwargs.get('noise_kind', 'normal')
    self.parallel = parallel
    self.processes = kwargs.get('processes')
<mask>:
        self.logger.warning('Passed value for process has no effect when `parallel` is False.')
    if ext_EMD is None:
        from PyEMD import EMD
        self.EMD = EMD(**kwargs)
    else:
        self.EMD = ext_EMD
    self.E_IMF = None
    self.residue = None
    self._all_imfs = {}",self.processes is not None and (not self.parallel),69,self.parallel,False,3.567399334725242,N/A
"def generate_noise(self, scale: float, size: Union[int, Sequence[int]]) -> np.ndarray:
    """"""
        Generate noise with specified parameters.
        Currently supported distributions are:

        * *normal* with std equal scale.
        * *uniform* with range [-scale/2, scale/2].

        Parameters
        ----------
        scale : float
            Width for the distribution.
        size : int
            Number of generated samples.

        Returns
        -------
        noise : numpy array
            Noise sampled from selected distribution.
        """"""
<mask>:
        noise = self.random.normal(loc=0, scale=scale, size=size)
    elif self.noise_kind == 'uniform':
        noise = self.random.uniform(low=-scale / 2, high=scale / 2, size=size)
    else:
        raise ValueError('Unsupported noise kind. Please assigned `noise_kind` to be one of these: {0}'.format(str(self.noise_kinds_all)))
    return noise",self.noise_kind == 'normal',94,self.noise_kind == 'normal',True,100.00000000000004,N/A
"def eemd(self, S: np.ndarray, T: Optional[np.ndarray]=None, max_imf: int=-1, progress: bool=False) -> np.ndarray:
    """"""
        Performs EEMD on provided signal.

        For a large number of iterations defined by `trials` attr
        the method performs :py:meth:`emd` on a signal with added white noise.

        Parameters
        ----------
        S : numpy array,
            Input signal on which EEMD is performed.
        T : numpy array or None, (default: None)
            If none passed samples are numerated.
        max_imf : int, (default: -1)
            Defines up to how many IMFs each decomposition should
            be performed. By default (negative value) it decomposes
            all IMFs.

        Returns
        -------
        eIMF : numpy array
            Set of ensemble IMFs produced from input signal. In general,
            these do not have to be, and most likely will not be, same as IMFs
            produced using EMD.
        """"""
<mask>:
        T = get_timeline(len(S), S.dtype)
    scale = self.noise_width * np.abs(np.max(S) - np.min(S))
    self._S = S
    self._T = T
    self._N = len(S)
    self._scale = scale
    self.max_imf = max_imf
    if self.parallel:
        pool = Pool(processes=self.processes)
        map_pool = pool.map
    else:
        map_pool = map
    all_IMFs = map_pool(self._trial_update, range(self.trials))
    if self.parallel:
        pool.close()
    self._all_imfs = defaultdict(list)
    it = iter if not progress else lambda x: tqdm(x, desc='EEMD', total=self.trials)
    for imfs, trend in it(all_IMFs):
        if trend is not None:
            self._all_imfs[-1].append(trend)
        for imf_num, imf in enumerate(imfs):
            self._all_imfs[imf_num].append(imf)
    self._all_imfs = dict(self._all_imfs)
    if -1 in self._all_imfs:
        self._all_imfs[len(self._all_imfs)] = self._all_imfs.pop(-1)
    for imf_num in self._all_imfs.keys():
        self._all_imfs[imf_num] = np.array(self._all_imfs[imf_num])
    self.E_IMF = self.ensemble_mean()
    self.residue = S - np.sum(self.E_IMF, axis=0)
    return self.E_IMF",T is None,231,T is None,True,100.00000000000004,N/A
"def _trial_update(self, trial) -> Tuple[np.ndarray, Optional[np.ndarray]]:
    """"""A single trial evaluation, i.e. EMD(signal + noise).

        *Note*: Although `trial` argument isn't used it's needed for the (multiprocessing) map method.
        """"""
    noise = self.generate_noise(self._scale, self._N)
    imfs = self.emd(self._S + noise, self._T, self.max_imf)
    trend = None
<mask>:
        imfs, trend = self.EMD.get_imfs_and_trend()
    return (imfs, trend)",self.separate_trends,50,self.max_imf is not None,False,12.22307556087252,N/A
"def get_imfs_and_residue(self) -> Tuple[np.ndarray, np.ndarray]:
    """"""
        Provides access to separated imfs and residue from recently analysed signal.

        Returns
        -------
        (imfs, residue) : (np.ndarray, np.ndarray)
            Tuple that contains all imfs and a residue (if any).

        """"""
<mask>:
        raise ValueError('No IMF found. Please, run EMD method or its variant first.')
    return (self.E_IMF, self.residue)",self.E_IMF is None or self.residue is None,51,self.E_IMF is None,False,42.43728456769501,N/A
"@nb.jit(nb.types.UniTuple(float64[:], 5)(float64[:], float64[:]), nopython=True)
def _find_extrema_simple(T: np.ndarray, S: np.ndarray) -> FindExtremaOutput:
    S1, S2 = (S[:-1], S[1:])
    indzer = np.nonzero(S1 * S2 < 0)[0]
<mask>:
        indz = np.nonzero(S == 0)[0]
        if np.any(nb_diff(indz) == 1):
            zer = S == 0
            dz = nb_diff(np.append(np.append(0, zer), 0))
            debz = np.nonzero(dz == 1)[0].astype(np.float64)
            finz = (np.nonzero(dz == -1)[0] - 1).astype(np.float64)
            y = np.empty_like(debz)
            np_round((debz + finz) / 2, 0, y)
            indz = y.astype(np.int64)
        indzer = np.sort(np.append(indzer, indz))
    d = S2 - S1
    d1, d2 = (d[:-1], d[1:])
    indmin = np.nonzero(np.logical_and(d1 * d2 < 0, d1 < 0))[0] + 1
    indmax = np.nonzero(np.logical_and(d1 * d2 < 0, d1 > 0))[0] + 1
    indmin = indmin.astype(np.int64)
    indmax = indmax.astype(np.int64)
    if np.any(d == 0):
        imax, imin = ([], [])
        same_values = d == 0
        dd = nb_diff(np.append(np.append(0, same_values), 0))
        _left_idx = np.nonzero(dd == 1)[0]
        _right_idx = np.nonzero(dd == -1)[0]
        if _left_idx[0] == 1:
            _left_idx = _left_idx[1:]
            _right_idx = _right_idx[1:]
        if len(_left_idx) > 0:
            if _right_idx[-1] == len(S) - 1:
                _left_idx = _left_idx[:-1]
                _right_idx = _right_idx[:-1]
        for k in range(len(_left_idx)):
            _left_value = d[_left_idx[k] - 1]
            _right_value = d[_right_idx[k]]
            _mid_value = round((_left_idx[k] + _right_idx[k]) / 2.0)
            if _left_value > 0 and _right_value < 0:
                imax.append(_mid_value)
            elif _left_value < 0 and _right_value > 0:
                imin.append(_mid_value)
        if len(imax) > 0:
            indmax = np.append(indmax, np.array(imax)).astype(np.int64)
            indmax.sort()
        if len(imin) > 0:
            indmin = np.append(indmin, np.array(imin)).astype(np.int64)
            indmin.sort()
    local_max_pos = T[indmax].astype(S.dtype)
    local_max_val = S[indmax].astype(S.dtype)
    local_min_pos = T[indmin].astype(S.dtype)
    local_min_val = S[indmin].astype(S.dtype)
    return (local_max_pos, local_max_val, local_min_pos, local_min_val, indzer.astype(S.dtype))",np.any(S == 0),240,np.any(indzer == 1),False,36.88939732334405,N/A
"@nb.jit(nb.types.Tuple((float64[:], float64[:], float64[:], float64[:], float64[:]))(float64[:], float64[:], unicode_type), nopython=True)
def find_extrema(T: np.ndarray, S: np.ndarray, extrema_detection: str) -> FindExtremaOutput:
    """"""
    Returns extrema (minima and maxima) for given signal S.
    Detection and definition of the extrema depends on
    ``extrema_detection`` variable, set on initiation of EMD.

    Parameters
    ----------
    T : numpy array
        Position or time array.
    S : numpy array
        Input data S(T).

    Returns
    -------
    local_max_pos : numpy array
        Position of local maxima.
    local_max_val : numpy array
        Values of local maxima.
    local_min_pos : numpy array
        Position of local minima.
    local_min_val : numpy array
        Values of local minima.
    """"""
<mask>:
        return _find_extrema_parabol(T, S)
    elif extrema_detection == 'simple':
        return _find_extrema_simple(T, S)
    else:
        raise ValueError(""Incorrect extrema detection type. Please try: 'simple' or 'parabol'."")",extrema_detection == 'parabol',116,extrema_detection == 'parabol',True,100.00000000000004,N/A
"@nb.jit(nopython=True)
def prepare_points(T: np.ndarray, S: np.ndarray, max_pos: np.ndarray, max_val: np.ndarray, min_pos: np.ndarray, min_val: np.ndarray, extrema_detection, nbsym):
    """"""
    Performs extrapolation on edges by adding extra extrema, also known
    as mirroring signal. The number of added points depends on *nbsym*
    variable.

    Parameters
    ----------
    T : numpy array
        Position or time array.
    S : numpy array
        Input signal.
    max_pos : iterable
        Sorted time positions of maxima.
    max_val : iterable
        Signal values at max_pos positions.
    min_pos : iterable
        Sorted time positions of minima.
    min_val : iterable
        Signal values at min_pos positions.

    Returns
    -------
    max_extrema : numpy array (2 rows)
        Position (1st row) and values (2nd row) of minima.
    min_extrema : numpy array (2 rows)
        Position (1st row) and values (2nd row) of maxima.
    """"""
<mask>:
        return _prepare_points_parabol(T, S, max_pos, max_val, min_pos, min_val, nbsym)
    elif extrema_detection == 'simple':
        return _prepare_points_simple(T, S, max_pos, min_pos, nbsym)
    else:
        msg = ""Incorrect extrema detection type. Please try: 'simple' or 'parabol'.""
        raise ValueError(msg)",extrema_detection == 'parabol',153,extrema_detection == 'parabol',True,100.00000000000004,N/A
"def test_ceemdan_noiseSeed(self):
    T = np.linspace(0, 1, 100)
    S = np.sin(2 * np.pi * T + 4 ** T) + np.cos((T - 0.4) ** 2)

    def cmpMachEps(x, y):
        return np.abs(x - y) <= 2 * np.finfo(x.dtype).eps
    ceemdan = CEEMDAN(trials=10)
    cIMF1 = ceemdan(S)
    ceemdan.noise_seed(12345)
    cIMF2 = ceemdan(S)
    msg_false = 'Different seeds, expected different outcomes'
<mask>:
        self.assertFalse(np.all(cmpMachEps(cIMF1, cIMF2)), msg_false)
    ceemdan.noise_seed(12345)
    cIMF3 = ceemdan(S)
    msg_true = 'Used same seed, expected same results'
    self.assertTrue(np.all(cmpMachEps(cIMF2, cIMF3)), msg_true)",cIMF1.shape == cIMF2.shape,71,"not np.all(cmpMachEps(cIMF1, cIMF2))",False,4.456882760699063,N/A
"def test_eemd_noiseSeed(self):
    T = np.linspace(0, 1, 100)
    S = np.sin(2 * np.pi * T + 4 ** T) + np.cos((T - 0.4) ** 2)

    def cmpMachEps(x, y):
        return np.abs(x - y) <= 2 * np.finfo(x.dtype).eps
    config = {'processes': 1}
    eemd = EEMD(trials=10, **config)
    eIMF1 = eemd(S)
    eemd.noise_seed(12345)
    eIMF2 = eemd(S)
    msg_false = 'Different seeds, expected different outcomes'
<mask>:
        self.assertFalse(np.all(cmpMachEps(eIMF1, eIMF2)), msg_false)
    eemd.noise_seed(12345)
    eIMF3 = eemd(S)
    msg_true = 'Used same seed, expected same results'
    self.assertTrue(np.all(cmpMachEps(eIMF2, eIMF3)), msg_true)",eIMF1.shape == eIMF2.shape,76,"not np.all(cmpMachEps(eIMF1, eIMF2))",False,4.456882760699063,N/A
"@property
def valid_temperature_range(self) -> Tuple[int, int]:
    """"""Return the device-specific white temperature range (in Kelvin).

        :return: White temperature range in Kelvin (minimun, maximum)
        :rtype: tuple
        """"""
<mask>:
        return (0, 0)
    for model, temp_range in TPLINK_KELVIN.items():
        if re.match(model, self.sys_info['model']):
            return temp_range
    return (0, 0)",not self.is_variable_color_temp,42,not self.sys_info['model'],False,17.0653267718276,N/A
"@property
def hsv(self) -> Tuple[int, int, int]:
    """"""Return the current HSV state of the bulb.

        :return: hue, saturation and value (degrees, %, %)
        :rtype: tuple
        """"""
<mask>:
        raise SmartDeviceException('Bulb does not support color.')
    light_state = self.get_light_state()
    if not self.is_on:
        hue = light_state['dft_on_state']['hue']
        saturation = light_state['dft_on_state']['saturation']
        value = light_state['dft_on_state']['brightness']
    else:
        hue = light_state['hue']
        saturation = light_state['saturation']
        value = light_state['brightness']
    return (hue, saturation, value)",not self.is_color,62,not self.supports_color,False,37.99178428257963,N/A
"def _raise_for_invalid_brightness(self, value):
<mask>:
        raise ValueError('Invalid brightness value: {} (valid range: 0-100%)'.format(value))","not isinstance(value, int) or not 0 <= value <= 100",12,value < 0 or value > 100,False,4.778778209871407,N/A
"def set_hsv(self, hue: int, saturation: int, value: int):
    """"""Set new HSV.

        :param tuple state: hue, saturation and value (degrees, %, %)
        """"""
<mask>:
        raise SmartDeviceException('Bulb does not support color.')
    if not isinstance(hue, int) or not 0 <= hue <= 360:
        raise ValueError('Invalid hue value: {} (valid range: 0-360)'.format(hue))
    if not isinstance(saturation, int) or not 0 <= saturation <= 100:
        raise ValueError('Invalid saturation value: {} (valid range: 0-100%)'.format(saturation))
    self._raise_for_invalid_brightness(value)
    light_state = {'hue': hue, 'saturation': saturation, 'brightness': value, 'color_temp': 0}
    self.set_light_state(light_state)",not self.is_color,79,not self.is_bulb(),False,51.697315395717055,N/A
"@property
def color_temp(self) -> int:
    """"""Return color temperature of the device.

        :return: Color temperature in Kelvin
        :rtype: int
        """"""
<mask>:
        raise SmartDeviceException('Bulb does not support colortemp.')
    light_state = self.get_light_state()
    if not self.is_on:
        return int(light_state['dft_on_state']['color_temp'])
    else:
        return int(light_state['color_temp'])",not self.is_variable_color_temp,37,not self.supports_colortemp,False,16.669006580554246,N/A
"def raise_for_index(self, index: int):
    """"""
        Raises SmartStripException if the plug index is out of bounds

        :param index: plug index to check
        :raises SmartStripException: index out of bounds
        """"""
<mask>:
        raise SmartStripException('plug index of %d is out of bounds' % index)",index not in range(self.num_children),40,index < self.plug_index_size,False,9.080027618567454,N/A
"@property
@deprecated(details='use is_on, get_is_on()')
def state(self) -> bool:
<mask>:
        return self.STATE_ON
    return self.STATE_OFF",self.is_on,13,self.state == self.STATE_ON,False,9.287528999566801,N/A
"def get_state(self, *, index=-1) -> Dict[int, str]:
    """"""Retrieve the switch state

        :returns: list with the state of each child plug
                  STATE_ON
                  STATE_OFF
        :rtype: dict
        """"""

    def _state_for_bool(b):
        return SmartPlug.STATE_ON if b else SmartPlug.STATE_OFF
    is_on = self.get_is_on(index=index)
<mask>:
        return _state_for_bool(is_on)
    print(is_on)
    return {k: _state_for_bool(v) for k, v in self.get_is_on().items()}","isinstance(is_on, bool)",48,self.is_on_enabled(),False,20.164945583740657,N/A
"@state.setter
@deprecated(details='use turn_on(), turn_off()')
def state(self, value: str):
    """"""Sets the state of all plugs in the strip

        :param value: one of
                    STATE_ON
                    STATE_OFF
        :raises ValueError: on invalid state
        :raises SmartDeviceException: on error
        """"""
<mask>:
        raise ValueError('State must be str, not of %s.', type(value))
    elif value.upper() == SmartPlug.STATE_ON:
        self.turn_on()
    elif value.upper() == SmartPlug.STATE_OFF:
        self.turn_off()
    else:
        raise ValueError('State %s is not valid.', value)","not isinstance(value, str)",61,"not isinstance(value, str)",True,100.00000000000004,N/A
"def set_state(self, value: str, *, index: int=-1):
    """"""Sets the state of a plug on the strip

        :param value: one of
                    STATE_ON
                    STATE_OFF
        :param index: plug index (-1 for all)
        :raises ValueError: on invalid state
        :raises SmartDeviceException: on error
        :raises SmartStripException: index out of bounds
        """"""
<mask>:
        self.state = value
    else:
        self.raise_for_index(index)
        self.plugs[index].state = value",index < 0,54,index == -1,False,15.97357760615681,N/A
"@property
@deprecated(details='use is_on()')
def state(self) -> str:
    """"""Retrieve the switch state.

        :returns: one of
                  STATE_ON
                  STATE_OFF
        :rtype: str
        """"""
<mask>:
        return self.STATE_ON
    return self.STATE_OFF",self.is_on,24,self.state == 'on',False,16.233395773754953,N/A
"@state.setter
@deprecated(details='use turn_on() and turn_off()')
def state(self, value: str):
    """"""Set the new switch state.

        :param value: one of
                    STATE_ON
                    STATE_OFF
        :raises ValueError: on invalid state
        :raises SmartDeviceException: on error
        """"""
<mask>:
        raise ValueError('State must be str, not of %s.' % type(value))
    if value.upper() == self.STATE_ON:
        return self.turn_on()
    elif value.upper() == self.STATE_OFF:
        return self.turn_off()
    raise ValueError('State %s is not valid.' % value)","not isinstance(value, str)",61,"not isinstance(value, str)",True,100.00000000000004,N/A
"@property
def brightness(self) -> int:
    """"""Return current brightness on dimmers.

        Will return a range between 0 - 100.

        :returns: integer
        :rtype: int
        """"""
<mask>:
        raise SmartDeviceException('Device is not dimmable.')
    return int(self.sys_info['brightness'])",not self.is_dimmable,31,not self.is_dimmable,True,100.00000000000004,N/A
"def set_brightness(self, value: int):
    """"""Set the new dimmer brightness level.

        Note:
        When setting brightness, if the light is not
        already on, it will be turned on automatically.

        :param value: integer between 1 and 100

        """"""
<mask>:
        raise SmartDeviceException('Device is not dimmable.')
    if not isinstance(value, int):
        raise ValueError('Brightness must be integer, not of %s.', type(value))
    elif 0 < value <= 100:
        self.turn_on()
        self._query_helper('smartlife.iot.dimmer', 'set_brightness', {'brightness': value})
    else:
        raise ValueError('Brightness value %s is not valid.' % value)",not self.is_dimmable,75,not self.is_dimmable(),False,68.037493331712,N/A
"@property
def on_since(self) -> datetime.datetime:
    """"""Return pretty-printed on-time.

        :return: datetime for on since
        :rtype: datetime
        """"""
<mask>:
        for plug in self.sys_info['children']:
            if plug['id'] == self.context:
                on_time = plug['on_time']
                break
    else:
        on_time = self.sys_info['on_time']
    return datetime.datetime.now() - datetime.timedelta(seconds=on_time)",self.context,37,self.context,True,100.00000000000004,N/A
"@staticmethod
def discover(protocol: TPLinkSmartHomeProtocol=None, target: str='255.255.255.255', port: int=9999, timeout: int=3, discovery_packets=3, return_raw=False) -> Dict[str, SmartDevice]:
    """"""
        Sends discovery message to 255.255.255.255:9999 in order
        to detect available supported devices in the local network,
        and waits for given timeout for answers from devices.

        :param protocol: Protocol implementation to use
        :param target: The target broadcast address (e.g. 192.168.xxx.255).
        :param timeout: How long to wait for responses, defaults to 3
        :param port: port to send broadcast messages, defaults to 9999.
        :rtype: dict
        :return: Array of json objects {""ip"", ""port"", ""sys_info""}
        """"""
<mask>:
        protocol = TPLinkSmartHomeProtocol()
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    sock.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)
    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    sock.settimeout(timeout)
    req = json.dumps(Discover.DISCOVERY_QUERY)
    _LOGGER.debug('Sending discovery to %s:%s', target, port)
    encrypted_req = protocol.encrypt(req)
    for i in range(discovery_packets):
        sock.sendto(encrypted_req[4:], (target, port))
    devices = {}
    _LOGGER.debug('Waiting %s seconds for responses...', timeout)
    try:
        while True:
            data, addr = sock.recvfrom(4096)
            ip, port = addr
            info = json.loads(protocol.decrypt(data))
            device_class = Discover._get_device_class(info)
            if return_raw:
                devices[ip] = info
            elif device_class is not None:
                devices[ip] = device_class(ip)
    except socket.timeout:
        _LOGGER.debug('Got socket timeout, which is okay.')
    except Exception as ex:
        _LOGGER.error('Got exception %s', ex, exc_info=True)
    _LOGGER.debug('Found %s devices: %s', len(devices), devices)
    return devices",protocol is None,185,protocol is None,True,100.00000000000004,N/A
"@staticmethod
def discover_single(host: str, protocol: TPLinkSmartHomeProtocol=None) -> Optional[SmartDevice]:
    """"""Discover a single device by the given IP address.

        :param host: Hostname of device to query
        :param protocol: Protocol implementation to use
        :rtype: SmartDevice
        :return: Object for querying/controlling found device.
        """"""
<mask>:
        protocol = TPLinkSmartHomeProtocol()
    info = protocol.query(host, Discover.DISCOVERY_QUERY)
    device_class = Discover._get_device_class(info)
    if device_class is not None:
        return device_class(host)
    return None",protocol is None,59,protocol is None,True,100.00000000000004,N/A
"@staticmethod
def _get_device_class(info: dict) -> Optional[Type[SmartDevice]]:
    """"""Find SmartDevice subclass for device described by passed data.""""""
<mask>:
        sysinfo = info['system']['get_sysinfo']
        if 'type' in sysinfo:
            type_ = sysinfo['type']
        elif 'mic_type' in sysinfo:
            type_ = sysinfo['mic_type']
        else:
            raise SmartDeviceException('Unable to find the device type field!')
    else:
        raise SmartDeviceException(""No 'system' nor 'get_sysinfo' in response"")
    if 'smartplug' in type_.lower() and 'children' in sysinfo:
        return SmartStrip
    elif 'smartplug' in type_.lower():
        return SmartPlug
    elif 'smartbulb' in type_.lower():
        return SmartBulb
    return None",'system' in info and 'get_sysinfo' in info['system'],74,'system' in info,False,4.9787068367863965,N/A
"@staticmethod
def query(host: str, request: Union[str, Dict], port: int=DEFAULT_PORT) -> Any:
    """"""Request information from a TP-Link SmartHome Device.

        :param str host: host name or ip address of the device
        :param int port: port on the device (default: 9999)
        :param request: command to send to the device (can be either dict or
        json string)
        :return: response dict
        """"""
<mask>:
        request = json.dumps(request)
    timeout = TPLinkSmartHomeProtocol.DEFAULT_TIMEOUT
    sock = None
    try:
        sock = socket.create_connection((host, port), timeout)
        _LOGGER.debug('> (%i) %s', len(request), request)
        sock.send(TPLinkSmartHomeProtocol.encrypt(request))
        buffer = bytes()
        length = -1
        while True:
            chunk = sock.recv(4096)
            if length == -1:
                length = struct.unpack('>I', chunk[0:4])[0]
            buffer += chunk
            if length > 0 and len(buffer) >= length + 4 or not chunk:
                break
    finally:
        try:
            if sock:
                sock.shutdown(socket.SHUT_RDWR)
        except OSError:
            pass
        finally:
            if sock:
                sock.close()
    response = TPLinkSmartHomeProtocol.decrypt(buffer[4:])
    _LOGGER.debug('< (%i) %s', len(response), response)
    return json.loads(response)","isinstance(request, dict)",137,"isinstance(request, dict)",True,100.00000000000004,N/A
"@click.group(invoke_without_command=True)
@click.option('--ip', envvar='PYHS100_IP', required=False, help='The IP address of the device to connect to. This option is deprecated and will be removed in the future; use --host instead.')
@click.option('--host', envvar='PYHS100_HOST', required=False, help='The host name or IP address of the device to connect to.')
@click.option('--alias', envvar='PYHS100_NAME', required=False, help='The device name, or alias, of the device to connect to.')
@click.option('--target', default='255.255.255.255', required=False, help='The broadcast address to be used for discovery.')
@click.option('--debug/--normal', default=False)
@click.option('--bulb', default=False, is_flag=True)
@click.option('--plug', default=False, is_flag=True)
@click.option('--strip', default=False, is_flag=True)
@click.pass_context
def cli(ctx, ip, host, alias, target, debug, bulb, plug, strip):
    """"""A cli tool for controlling TP-Link smart home plugs.""""""
<mask>:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)
    if ctx.invoked_subcommand == 'discover':
        return
    if ip is not None and host is None:
        host = ip
    if alias is not None and host is None:
        click.echo('Alias is given, using discovery to find host %s' % alias)
        host = find_host_from_alias(alias=alias, target=target)
        if host:
            click.echo('Found hostname is {}'.format(host))
        else:
            click.echo('No device with name {} found'.format(alias))
            return
    if host is None:
        click.echo('No host name given, trying discovery..')
        ctx.invoke(discover)
        return
    else:
        if not bulb and (not plug) and (not strip):
            click.echo('No --strip nor --bulb nor --plug given, discovering..')
            dev = Discover.discover_single(host)
        elif bulb:
            dev = SmartBulb(host)
        elif plug:
            dev = SmartPlug(host)
        elif strip:
            dev = SmartStrip(host)
        else:
            click.echo('Unable to detect type, use --strip or --bulb or --plug!')
            return
        ctx.obj = dev
    if ctx.invoked_subcommand is None:
        ctx.invoke(state)",debug,225,debug,True,100.00000000000004,N/A
"@cli.command()
@click.option('--timeout', default=3, required=False)
@click.option('--discover-only', default=False)
@click.option('--dump-raw', is_flag=True)
@click.pass_context
def discover(ctx, timeout, discover_only, dump_raw):
    """"""Discover devices in the network.""""""
    target = ctx.parent.params['target']
    click.echo('Discovering devices for %s seconds' % timeout)
    found_devs = Discover.discover(target=target, timeout=timeout, return_raw=dump_raw).items()
<mask>:
        for ip, dev in found_devs:
            if dump_raw:
                click.echo(dev)
                continue
            ctx.obj = dev
            ctx.invoke(state)
            print()
    return found_devs",not discover_only,51,discover_only,False,71.65313105737896,N/A
"def find_host_from_alias(alias, target='255.255.255.255', timeout=1, attempts=3):
    """"""Discover a device identified by its alias.""""""
    host = None
    click.echo('Trying to discover %s using %s attempts of %s seconds' % (alias, attempts, timeout))
    for attempt in range(1, attempts):
        click.echo('Attempt %s of %s' % (attempt, attempts))
        found_devs = Discover.discover(target=target, timeout=timeout).items()
        for ip, dev in found_devs:
<mask>:
                host = dev.host
                return host
    return None",dev.alias.lower() == alias.lower(),58,ip == alias,False,4.880803219926451,N/A
"@cli.command()
@pass_dev
@click.pass_context
def state(ctx, dev):
    """"""Print out device state and versions.""""""
    click.echo(click.style('== %s - %s ==' % (dev.alias, dev.model), bold=True))
    click.echo(click.style('Device state: %s' % ('ON' if dev.is_on else 'OFF'), fg='green' if dev.is_on else 'red'))
<mask>:
        is_on = dev.get_is_on()
        aliases = dev.get_alias()
        for child in range(dev.num_children):
            click.echo(click.style('  * %s state: %s' % (aliases[child], 'ON' if is_on[child] else 'OFF'), fg='green' if is_on[child] else 'red'))
    click.echo('Host/IP: %s' % dev.host)
    for k, v in dev.state_information.items():
        click.echo('%s: %s' % (k, v))
    click.echo(click.style('== Generic information ==', bold=True))
    click.echo('Time:         %s' % dev.time)
    click.echo('Hardware:     %s' % dev.hw_info['hw_ver'])
    click.echo('Software:     %s' % dev.hw_info['sw_ver'])
    click.echo('MAC (rssi):   %s (%s)' % (dev.mac, dev.rssi))
    click.echo('Location:     %s' % dev.location)
    ctx.invoke(emeter)",dev.num_children > 0,106,dev.is_on,False,15.848738972120703,N/A
"@cli.command()
@pass_dev
@click.argument('new_alias', required=False, default=None)
def alias(dev, new_alias):
    """"""Get or set the device alias.""""""
<mask>:
        click.echo('Setting alias to %s' % new_alias)
        dev.alias = new_alias
    click.echo('Alias: %s' % dev.alias)",new_alias is not None,28,new_alias,False,36.78794411714425,N/A
"def __getitem__(self, item):
    valid_keys = ['voltage_mv', 'power_mw', 'current_ma', 'energy_wh', 'total_wh', 'voltage', 'power', 'current', 'total', 'energy']
<mask>:
        return super().__getitem__(item)
    else:
        if item not in valid_keys:
            raise KeyError(item)
        if '_' in item:
            return super().__getitem__(item[:item.find('_')]) * 10 ** 3
        else:
            for i in super().keys():
                if i.startswith(item):
                    return self.__getitem__(i) / 10 ** 3
            raise SmartDeviceException(""Unable to find a value for '%s'"" % item)",item in super().keys(),59,item in valid_keys,False,10.62372743739878,N/A
"def __init__(self, host: str, protocol: Optional[TPLinkSmartHomeProtocol]=None, context: str=None, cache_ttl: int=3) -> None:
    """"""Create a new SmartDevice instance.

        :param str host: host name or ip address on which the device listens
        :param context: optional child ID for context in a parent device
        """"""
    self.host = host
<mask>:
        protocol = TPLinkSmartHomeProtocol()
    self.protocol = protocol
    self.emeter_type = 'emeter'
    self.context = context
    self.num_children = 0
    self.cache_ttl = timedelta(seconds=cache_ttl)
    _LOGGER.debug('Initializing %s using context %s and cache ttl %s', self.host, self.context, self.cache_ttl)
    self.cache = defaultdict(lambda: defaultdict(lambda: None))
    self._device_type = DeviceType.Unknown",protocol is None,84,protocol is None,True,100.00000000000004,N/A
"def _result_from_cache(self, target, cmd) -> Optional[Dict]:
    """"""Return query result from cache if still fresh.
        Only results from commands starting with `get_` are considered cacheable.

        :param target: Target system
        :param cmd: Command
        :rtype: query result or None if expired.
        """"""
    _LOGGER.debug('Checking cache for %s %s', target, cmd)
<mask>:
        return None
    cached = self.cache[target][cmd]
    if cached and cached['last_updated'] is not None:
        if cached['last_updated'] + self.cache_ttl > datetime.utcnow() and cmd.startswith('get_'):
            _LOGGER.debug('Got cached %s %s', target, cmd)
            return self.cache[target][cmd]
        else:
            _LOGGER.debug('Invalidating the cache for %s cmd %s', target, cmd)
            for cache_entry in self.cache[target].values():
                cache_entry['last_updated'] = datetime.utcfromtimestamp(0)
    return None",cmd not in self.cache[target],94,target not in self.cache,False,48.23560797692261,N/A
"def _query_helper(self, target: str, cmd: str, arg: Optional[Dict]=None) -> Any:
    """"""Handle result unwrapping and error handling.

        :param target: Target system {system, time, emeter, ..}
        :param cmd: Command to execute
        :param arg: JSON object passed as parameter to the command
        :return: Unwrapped result for the call.
        :rtype: dict
        :raises SmartDeviceException: if command was not executed correctly
        """"""
<mask>:
        request = {target: {cmd: arg}}
    else:
        request = {'context': {'child_ids': [self.context]}, target: {cmd: arg}}
    try:
        response = self._result_from_cache(target, cmd)
        if response is None:
            _LOGGER.debug('Got no result from cache, querying the device.')
            response = self.protocol.query(host=self.host, request=request)
            self._insert_to_cache(target, cmd, response)
    except Exception as ex:
        raise SmartDeviceException('Communication error on %s:%s' % (target, cmd)) from ex
    if target not in response:
        raise SmartDeviceException('No required {} in response: {}'.format(target, response))
    result = response[target]
    if 'err_code' in result and result['err_code'] != 0:
        raise SmartDeviceException('Error on {}.{}: {}'.format(target, cmd, result))
    if cmd not in result:
        raise SmartDeviceException('No command in response: {}'.format(response))
    result = result[cmd]
    if 'err_code' in result and result['err_code'] != 0:
        raise SmartDeviceException('Error on {} {}: {}'.format(target, cmd, result))
    if 'err_code' in result:
        del result['err_code']
    return result",self.context is None,178,self.context is None,True,100.00000000000004,N/A
"@property
def location(self) -> Dict:
    """"""Return geographical location.

        :return: latitude and longitude
        :rtype: dict
        """"""
    info = self.sys_info
    loc = {'latitude': None, 'longitude': None}
<mask>:
        loc['latitude'] = info['latitude']
        loc['longitude'] = info['longitude']
    elif 'latitude_i' in info and 'longitude_i' in info:
        loc['latitude'] = info['latitude_i']
        loc['longitude'] = info['longitude_i']
    else:
        _LOGGER.warning('Unsupported device location.')
    return loc",'latitude' in info and 'longitude' in info,51,'latitude' in info and 'longitude' in info,True,100.00000000000004,N/A
"def filter_model(filter):
    print(filter)
    filtered = list()
    for dev in SUPPORTED_DEVICES:
        for filt in filter:
<mask>:
                filtered.append(dev)
    return filtered",filt in basename(dev),18,filt in dev,False,23.174952587773145,N/A
"@pytest.fixture(params=SUPPORTED_DEVICES)
def dev(request):
    file = request.param
    ip = request.config.getoption('--ip')
<mask>:
        d = Discover.discover_single(ip)
        print(d.model)
        if d.model in file:
            return d
        return
    with open(file) as f:
        sysinfo = json.load(f)
        model = basename(file)
        params = {'host': '123.123.123.123', 'protocol': FakeTransportProtocol(sysinfo), 'cache_ttl': 0}
        if 'LB' in model or 'KL' in model:
            p = SmartBulb(**params)
        elif 'HS300' in model:
            p = SmartStrip(**params)
        elif 'HS' in model:
            p = SmartPlug(**params)
        else:
            raise Exception('No tests for %s' % model)
        yield p",ip,74,ip,True,100.00000000000004,N/A
"def pytest_collection_modifyitems(config, items):
<mask>:
        print('Testing against fixtures.')
        return
    else:
        print('Running against ip %s' % config.getoption('--ip'))",not config.getoption('--ip'),15,config.getoption('--fixture') == 'fixture',False,31.55984539112946,N/A
"def get_monthstat(obj, x, *args):
<mask>:
        return {'month_list': []}
    return {'month_list': [{'year': 2016, 'month': 11, 'energy': 1.089}, {'year': 2016, 'month': 12, 'energy': 1.582}]}",x['year'] < 2016,22,obj is None,False,0.0,N/A
"@turn_on
def test_state(dev, turn_on):
    handle_turn_on(dev, turn_on)
    orig_state = dev.is_on
<mask>:
        dev.turn_off()
        assert not dev.is_on
        assert dev.is_off
        dev.turn_on()
        assert dev.is_on
        assert not dev.is_off
    else:
        dev.turn_on()
        assert dev.is_on
        assert not dev.is_off
        dev.turn_off()
        assert not dev.is_on
        assert dev.is_off",orig_state,35,orig_state,True,100.00000000000004,N/A
"@has_emeter
def test_get_emeter_realtime(dev):
<mask>:
        pytest.skip('Disabled for HS300 temporarily')
    assert dev.has_emeter
    current_emeter = dev.get_emeter_realtime()
    CURRENT_CONSUMPTION_SCHEMA(current_emeter)",dev.is_strip,14,not HS300,False,0.0,N/A
"@has_emeter
def test_get_emeter_daily(dev):
<mask>:
        pytest.skip('Disabled for HS300 temporarily')
    assert dev.has_emeter
    assert dev.get_emeter_daily(year=1900, month=1) == {}
    d = dev.get_emeter_daily()
    assert len(d) > 0
    k, v = d.popitem()
    assert isinstance(k, int)
    assert isinstance(v, float)
    d = dev.get_emeter_daily(kwh=False)
    k2, v2 = d.popitem()
    assert v * 1000 == v2",dev.is_strip,45,not HS300,False,0.0,N/A
"@has_emeter
def test_get_emeter_monthly(dev):
<mask>:
        pytest.skip('Disabled for HS300 temporarily')
    assert dev.has_emeter
    assert dev.get_emeter_monthly(year=1900) == {}
    d = dev.get_emeter_monthly()
    assert len(d) > 0
    k, v = d.popitem()
    assert isinstance(k, int)
    assert isinstance(v, float)
    d = dev.get_emeter_monthly(kwh=False)
    k2, v2 = d.popitem()
    assert v * 1000 == v2",dev.is_strip,44,not HS300,False,0.0,N/A
"@has_emeter
def test_emeter_status(dev):
<mask>:
        pytest.skip('Disabled for HS300 temporarily')
    assert dev.has_emeter
    d = dev.get_emeter_realtime()
    with pytest.raises(KeyError):
        assert d['foo']
    assert d['power_mw'] == d['power'] * 1000
    if not dev.is_bulb:
        assert d['voltage_mv'] == d['voltage'] * 1000
        assert d['current_ma'] == d['current'] * 1000
        assert d['total_wh'] == d['total'] * 1000",dev.is_strip,44,not dev.is_hs300,False,50.81327481546149,N/A
"@bp.route('/login_user', methods=['POST'])
def login():
    """"""
    GET -> Returns login page
    POST -> Try authenticating user

    """"""
    username = request.json['username']
    password = request.json['password']
    db_conn = get_db()
    user = get_user(db_conn, username)
<mask>:
        return jsonify({'logged_in': False})
    login_user(user, remember=True)
    return jsonify({'logged_in': True})","not user or not check_password_hash(user.password, password)",38,"not user or not check_password(user, password)",False,48.97109464253322,N/A
"@bp.route('/logout')
def logout():
    """"""
    Logout current user

    """"""
<mask>:
        logout_user()
        return jsonify({'logged_out': True})
    return jsonify({'logged_out': False})",current_user.is_authenticated,16,current_user.is_authenticated,True,100.00000000000004,N/A
"def init_db():
    """"""
    Create database and tables if it isn't present

    """"""
<mask>:
        try:
            db = get_db()
            c = db.cursor()
            c.execute('ALTER TABLE blogs ADD COLUMN automatically_added INTEGER')
        except Exception:
            logging.warning(""[W0002] Column 'automatically_added' is already exists. No need to add it."")
        db.commit()
        db.close()
        return
    db = get_db()
    c = db.cursor()
    c.execute('CREATE TABLE users (id INTEGER PRIMARY KEY, username TEXT UNIQUE, password TEXT)')
    c.execute('CREATE TABLE public_repos (id INTEGER PRIMARY KEY, primary_language TEXT, primary_language_color TEXT, stars TEXT, title TEXT, description TEXT, readme TEXT, latest_commit TEXT, url TEXT, image_url TEXT, timestamp TEXT, visible INTEGER)')
    c.execute('CREATE TABLE blogs (id INTEGER PRIMARY KEY, title TEXT, description TEXT, url TEXT, image_url TEXT, timestamp TEXT, automatically_added INTEGER, file_type INTEGER, file_name TEXT)')
    c.execute('CREATE TABLE publications (id INTEGER PRIMARY KEY, title TEXT, description TEXT, url TEXT, image_url TEXT, timestamp TEXT, visible INTEGER)')
    c.execute('INSERT INTO users (username, password) VALUES (?, ?)', (os.getenv('USERNAME'), generate_password_hash(os.getenv('PASSWORD'))))
    db.commit()
    db.close()",os.path.exists(database_path),144,not os.path.exists(db_path),False,58.77283725105324,N/A
"@bp.route('/public_repos', methods=['GET', 'POST'])
def public_repos():
    """"""
    List all public repos from github

    Returns (JSON): GET  -> a list of repos and updated timestamp
                    POST -> set visibility of projects

    """"""
<mask>:
        db_conn = db.get_db()
        repos, updated = database.get_public_repos(db_conn)
        db_conn.close()
        return jsonify({'repos': repos, 'updated': updated})
    if current_user.is_authenticated:
        db_conn = db.get_db()
        selected = request.json['projects']
        for s in selected:
            entry_id = s['id']
            visible = s['visible']
            database.update_visibility('public_repos', db_conn, entry_id, visible)
        db_conn.close()
        return jsonify({'INFO': 'Updated'})
    return jsonify({'ERROR': 'Unauthenticated'})",request.method == 'GET',73,request.method == 'GET',True,100.00000000000004,N/A
"@bp.route('/blogs', methods=['GET', 'POST'])
def blogs():
    """"""
    GET all published blogs and external blog links

    Returns (JSON): GET  -> a list of all blogs
                    POST -> INFO message

    """"""
<mask>:
        db_conn = db.get_db()
        all_blogs, updated = database.get_articles(db_conn)
        db_conn.close()
        fm: FileManager = current_app.config['FILE_MANAGER']
        published_files = fm.list(as_dict=True, file_type=FileType.PUBLISHED)
        published_files = list(published_files.values())
        all_blogs.extend(published_files)
        if current_user.is_authenticated:
            fm: FileManager = current_app.config['FILE_MANAGER']
            unpublished_files = fm.list(as_dict=True, file_type=FileType.UNPUBLISHED)
            unpublished_files = list(unpublished_files.values())
            all_blogs.extend(unpublished_files)
        return jsonify({'blogs': all_blogs, 'updated': updated})",request.method == 'GET',68,current_user.is_authenticated,False,6.567274736060395,N/A
"@bp.route('/blogs/external_link', methods=['POST'])
def add_external_blog_link():
<mask>:
        db_conn = db.get_db()
        title = request.json['title']
        description = request.json['description']
        url = request.json['url']
        image_url = request.json['image_url']
        time_stamp = request.json['time_stamp'] + ' 00:00:00'
        database.add_entry('blogs', db_conn, title, description, url, image_url, time_stamp)
        db_conn.close()
        return jsonify({'INFO': 'Blog added'})
    return (jsonify({'ERROR': 'Unauthenticated'}), 401)",current_user.is_authenticated,42,request.method == 'POST',False,6.870636427700047,N/A
"@bp.route('/publications', methods=['GET', 'POST'])
def publications():
    """"""
    GET or POST publications

    Returns (JSON): GET  -> a list of all publications
                    POST -> INFO message

    """"""
<mask>:
        db_conn = db.get_db()
        all_blogs = database.get_entries('publications', db_conn)
        db_conn.close()
        return jsonify({'publications': all_blogs})
    if current_user.is_authenticated:
        db_conn = db.get_db()
        title = request.json['title']
        description = request.json['description']
        url = request.json['url']
        image_url = request.json['image_url']
        time_stamp = request.json['time_stamp'] + ' 00:00:00'
        database.add_entry('publications', db_conn, title, description, url, image_url, time_stamp)
        db_conn.close()
        return jsonify({'INFO': 'Publication added'})
    return (jsonify({'ERROR': 'Unauthenticated'}), 401)",request.method == 'GET',75,current_user.is_authenticated,False,6.567274736060395,N/A
"@bp.route('/markdown_content', methods=['GET', 'POST'])
def markdown_content():
    file_name: str = request.args.get('path')
    file_type: FileType = FileType(int(request.args.get('file_type')))
    fm: FileManager = current_app.config['FILE_MANAGER']
<mask>:
        return (jsonify({'INFO': 'Invalid file name'}), 550)
    if file_type == FileType.UNPUBLISHED and (not current_user.is_authenticated):
        return (jsonify({'ERROR': 'Unauthenticated'}), 401)
    if request.method == 'GET':
        if request.args.get('version'):
            content = fm.read_version(file_name, request.args.get('version'), file_type)
        else:
            content = fm.read(file_name, file_type)
        return jsonify({'INFO': 'Document found', 'markdown': content})
    elif current_user.is_authenticated:
        content = request.json['markdown']
        fm.write(file_name, content, file_type)
        return jsonify({'INFO': 'Document written', 'time': str(datetime.datetime.now())})
    else:
        return (jsonify({'ERROR': 'Unauthenticated'}), 401)",'..' in file_name or '~' in file_name or '/' in file_name,76,file_name is None,False,0.8895410018567118,N/A
"def get_user(db_conn, username):
    """"""
    Returns UserMixin object if the username matches an entry in the data
    Args:
        db_conn: sqlite3 db connection object
        username (str): username string

    Returns (User): UserMixin Object with id set to username

    """"""
    c = db_conn.cursor()
    c.execute('SELECT * FROM users WHERE username=?', (username,))
    user_row = c.fetchall()
<mask>:
        return None
    user = User()
    query_user = dict(user_row[0])
    user.id = query_user['username']
    user.password = query_user['password']
    return user",not user_row,66,not user_row,True,100.00000000000004,N/A
"def get_public_repos(db_conn):
    """"""
    Returns a list of public repos from github
    Args:
        db_conn: sqlite3 db connection object

    Returns (repos, update_time): a list of repos and the last updated time

    """"""
    c = db_conn.cursor()
    start_time = current_app.config['CACHED_TIME']
<mask>:
        current_app.config.from_mapping(CACHED_TIME=time.time())
        update_public_repos(db_conn)
    updated_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))
    c.execute('SELECT * FROM public_repos')
    all_rows = c.fetchall()
    repos = [dict(row) for row in all_rows[::-1]]
    return (repos, updated_time)",time.time() - start_time > cache_rate,61,not current_app.config.get('CACHE_ENABLED'),False,4.6192151051305474,N/A
"def get_articles(db_conn):
    """"""
    Returns a list of article from medium
    Args:
        db_conn: sqlite3 db connection object
    """"""
    c = db_conn.cursor()
    start_time = current_app.config['CACHED_TIME']
<mask>:
        current_app.config.from_mapping(CACHED_TIME=time.time())
        with open(current_app.config['THEME_DIR'], 'r') as f:
            data = json.load(f)
        medium_url = data['medium_url']
        update_articles(db_conn, medium_url)
    updated_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))
    c.execute('SELECT * FROM blogs')
    all_rows = c.fetchall()
    blogs = [dict(row) for row in all_rows[::-1]]
    return (blogs, updated_time)",time.time() - start_time > cache_rate,60,"not current_app.config.get('THEME_DIR', None)",False,3.9297193407553004,N/A
"def add_entry(table, db_conn, title, description, url, image_url, time_stamp):
    """"""
    Adds entry as another row
    Args:
        table (str): blogs, publications
        db_conn: sqlite3 db connection object
        title (str): blog title
        description (str): blog description
        url (str): blog link
        image_url (str): image link
        time_stamp (str): timestamp of the post

    Returns: True  -> entry added
             False -> error occurred

    """"""
    c = db_conn.cursor()
    try:
<mask>:
            c.execute(f'INSERT INTO {table} (title, description, url, image_url, timestamp, file_type) VALUES (?, ?, ?, ?, ?, ?)', (title, description, url, image_url, time_stamp, 1))
        else:
            c.execute(f'INSERT INTO {table} (title, description, url, image_url, timestamp) VALUES (?, ?, ?, ?, ?)', (title, description, url, image_url, time_stamp))
        db_conn.commit()
        db_conn.close()
    except Exception as e:
        print(f'ERROR: {e}')
        return False
    return True",table == 'blogs',115,'file_type' in url,False,0.0,N/A
"def max_times(times):
    """"""
    Returns the index of the max datetime
    Args:
        times (list): a list of datetimes in format 'YYYY-MM-DD HH:MM:SS'

    Returns: index of latest time

    """"""
    date_times = []
    for t in times:
<mask>:
            date_times.append(datetime.datetime.now().replace(1970, 1, 1, 0, 0, 0))
            continue
        try:
            dt = []
            t = t.split('-')
            t = [i for i in t]
            for i in t:
                if ':' in i:
                    i = i.split(' ')
                    dt.append(i[0])
                    dt.extend([j.strip() for j in i[1].split(':')])
                else:
                    dt.append(i.strip())
            dt = list(map(int, dt))
            date_times.append(datetime.datetime.now().replace(*dt))
        except ValueError:
            logging.error('[E0007] Error when parsing time. It is possible that the input time format is wrong.')
            date_times.append(datetime.datetime.now().replace(1970, 1, 1, 0, 0, 0))
    return date_times.index(max(date_times))",t is None,106,t == 'now',False,15.97357760615681,N/A
"def retrieving_posts(medium_url, db_conn):
    """"""
    Util for updating database the list of articles which belongs to the user

    Args:
        medium_url: user's Meidum profile url
        db_conn: sqlite3 db connection

    Returns (dict): (no return)

    """"""
    api = medium_url + '/?format=json&limit=10'
    res = requests.get(api)
<mask>:
        raise Exception('[E0001] An error occured when crawling data by page')
    content = json.loads(res.text.split('])}while(1);</x>')[1])
    posts = content['payload']['references']['Post']
    c = db_conn.cursor()
    for idx, post in enumerate(posts):
        post = posts[post]
        title = post['title']
        description = post['content']['subtitle']
        url = medium_url + '/' + post['uniqueSlug']
        previewImage = post['virtuals']['previewImage']
        if previewImage['imageId']:
            image_url = 'https://miro.medium.com/fit/c/%s/%s/%s' % (previewImage['originalWidth'], previewImage['originalHeight'], previewImage['imageId'])
        else:
            image_url = '/static/img/personal-website-logo.webp'
        try:
            time_stamp = datetime.fromtimestamp(int(post['createdAt'] / 1000))
        except Exception as e:
            time_stamp = ''
            logging.error(f'[E0006] {e}')
        try:
            c.execute(f'INSERT INTO blogs (title, description, url, image_url, timestamp, automatically_added, file_type) VALUES (?, ?, ?, ?, ?, ?, ?)', (title, description, url, image_url, time_stamp, 1, 1))
            db_conn.commit()
        except Exception as e:
            logging.error(f'[E0004] {e}')",not res,145,res.status_code != 200,False,5.522397783539471,N/A
"def extract_first_image_url(readme):
    s = re.search('(http(s?):)([/|.|\\w|\\s|-])*\\.(?:jpg|gif|png)', readme)
<mask>:
        return ''
    return s.group()",s is None,11,s is None,True,100.00000000000004,N/A
"def run_query(query):
    """"""
    A simple function to use requests.post to make the API call. Note the json= section.

    Args:
        query (str): GraphQL query to run

    Returns (JSON): response

    Reference: https://gist.github.com/gbaman/b3137e18c739e0cf98539bf4ec4366ad

    """"""
    request = requests.post('https://api.github.com/graphql', json={'query': query}, headers=headers)
<mask>:
        return request.json()
    else:
        raise Exception('Query failed to run. Error code received {}: {}'.format(request.status_code, query))",request.status_code == 200,52,request.status_code == 200,True,100.00000000000004,N/A
"def update_public_repos(db_conn):
    """"""
    List all public repos along with their links and descriptions from the authenticated user and update db if needed

    Args:
        db_conn: sqlite3 db connection

    Returns (dict): See query below for the data format

    """"""
    query = '\n    {\n      viewer {\n        repositories(privacy: PUBLIC, first: 100, orderBy: {field: PUSHED_AT, direction: ASC}) {\n          totalCount\n          nodes {\n            name\n            url\n            description\n            primaryLanguage {\n              name\n            }\n            stargazers{\n              totalCount\n            }\n            defaultBranchRef {\n              target {\n                ... on Commit {\n                  history(first: 1) {\n                    nodes {\n                      committedDate\n                      message\n                    }\n                  }\n                }\n              }\n            }\n            object(expression: ""master:README.md"") {\n              ... on Blob {\n                text\n              }\n            }\n          }\n        }\n      }\n    }\n    '
    result = run_query(query)
    c = db_conn.cursor()
    c.execute('SELECT * FROM public_repos')
    projects = c.fetchall()
    projects = {p['url']: dict(p) for p in projects}
    c.execute('DELETE FROM public_repos')
    try:
        for node in result['data']['viewer']['repositories']['nodes']:
            title = node['name']
            try:
                primary_language = node['primaryLanguage']['name']
                primary_language_color = github_language_colors[primary_language]['color']
            except Exception as e:
                primary_language = 'None'
                primary_language_color = '#FFFFFF'
            stars = node['stargazers']['totalCount']
            description = node['description']
            readme = '' if node['object'] is None else node['object']['text']
            image_url = extract_first_image_url(readme)
            url = node['url']
            try:
                latest_commit = node['defaultBranchRef']['target']['history']['nodes'][0]['message']
                timestamp = node['defaultBranchRef']['target']['history']['nodes'][0]['committedDate'].replace('T', ' ').replace('Z', '')
            except TypeError:
                latest_commit = ''
                timestamp = 0
<mask>:
                visible = projects[url]['visible']
            else:
                visible = 1
            c.execute('INSERT INTO public_repos (title, primary_language, primary_language_color, stars, description, readme, latest_commit, url, image_url, timestamp, visible) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', (title, primary_language, primary_language_color, stars, description, readme, latest_commit, url, image_url, timestamp, visible))
        db_conn.commit()
    except Exception as e:
        print(f'ERROR: {e}')",url in projects,241,node['id'] is None,False,0.0,N/A
"def __init__(self, file_src_dir: str, symbolic_link_dst: str):
    self.repo = Repo.init(file_src_dir)
    self.file_src_dir = pathlib.Path(file_src_dir)
    self.data_dir = pathlib.Path(symbolic_link_dst)
    self.published_dir = self.file_src_dir / 'published'
    self.unpublished_dir = self.file_src_dir / 'unpublished'
<mask>:
        os.mkdir(self.file_src_dir / 'published')
    if not os.path.exists(self.file_src_dir / 'unpublished'):
        os.mkdir(self.file_src_dir / 'unpublished')
    if not os.path.exists(symbolic_link_dst):
        subprocess.run(['ln', '-s', file_src_dir, symbolic_link_dst])",not os.path.exists(self.file_src_dir / 'published'),44,not os.path.exists(self.file_src_dir / 'published'),True,100.00000000000004,N/A
"def read(self, file_name: str, file_type: FileType) -> str:
    data_dir = self._get_dir(file_type)
    file_path = data_dir / file_name
<mask>:
        with open(str(file_path), 'r') as f:
            content = f.read()
        return content",file_path.exists(),27,file_path.exists(),True,100.00000000000004,N/A
"def publish(self, file_name: str) -> bool:
<mask>:
        content = self.read(file_name, FileType.UNPUBLISHED)
        file_path = self.published_dir / file_name
        with open(str(file_path), 'w') as f:
            f.write(content)
        self._commit_file(file_name, FileType.PUBLISHED)
        return True
    return False","file_name in self.list(as_dict=False, file_type=FileType.UNPUBLISHED)",28,os.path.exists(self.published_dir / file_name),False,9.112810809071439,N/A
"def unpublish(self, file_name: str) -> bool:
<mask>:
        try:
            file_path = self.published_dir / file_name
            os.remove(file_path)
            return True
        except FileNotFoundError:
            return False
    return False","file_name in self.list(as_dict=False, file_type=FileType.PUBLISHED)",22,file_name in self.published_dir,False,14.399672813486239,N/A
"def list(self, as_dict: bool, file_type: FileType):
    data_dir = self._get_dir(file_type)
<mask>:
        return {str(_.name): self.get_meta(_.name, file_type) for _ in data_dir.glob('*')}
    return {str(_.name) for _ in data_dir.glob('*')}",as_dict,24,as_dict,True,100.00000000000004,N/A
"@classmethod
def from_url(cls, url, **options):
    """"""Alternative constructor that extracts the information from a URL.

        :param url: String containing a URL

        Usage::

          >> from pusher import Pusher
          >> p =
            Pusher.from_url(""http://mykey:mysecret@api.pusher.com/apps/432"")
        """"""
    m = pusher_url_re.match(ensure_text(url, 'url'))
<mask>:
        raise Exception('Unparsable url: %s' % url)
    ssl = m.group(1) == 'https'
    options_ = {'key': m.group(2), 'secret': m.group(3), 'host': m.group(4), 'app_id': m.group(5), 'ssl': ssl}
    options_.update(options)
    return cls(**options_)",not m,62,not m,True,100.00000000000004,N/A
"@classmethod
def from_env(cls, env='PUSHER_URL', **options):
    """"""Alternative constructor that extracts the information from an URL
        stored in an environment variable. The pusher heroku addon will set
        the PUSHER_URL automatically when installed for example.

        :param env: Name of the environment variable

        Usage::

          >> from pusher import Pusher
          >> c = Pusher.from_env(""PUSHER_URL"")
        """"""
    val = os.environ.get(env)
<mask>:
        raise Exception('Environment variable %s not found' % env)
    return cls.from_url(val, **options)",not val,65,not val,True,100.00000000000004,N/A
"@asyncio.coroutine
def send_request(self, request):
    session = response = None
    try:
        session = aiohttp.ClientSession()
        response = (yield from session.request(request.method, '%s%s' % (request.base_url, request.path), params=request.query_params, data=request.body, headers=request.headers, timeout=self.client.timeout))
        body = (yield from response.text('utf-8'))
        return process_response(response.status, body)
    finally:
<mask>:
            response.close()
        if session is not None:
            yield from session.close()",response is not None,45,response is not None,True,100.00000000000004,N/A
"@request_method
def trigger(self, channels, event_name, data, socket_id=None):
    """"""Trigger an event on one or more channels, see:

        http://pusher.com/docs/rest_api#method-post-event
        """"""
<mask>:
        channels = [channels]
    if isinstance(channels, dict) or not isinstance(channels, (collections.Sized, collections.Iterable)):
        raise TypeError('Expected a single or a list of channels')
    if len(channels) > 100:
        raise ValueError('Too many channels')
    event_name = ensure_text(event_name, 'event_name')
    if len(event_name) > 200:
        raise ValueError('event_name too long')
    data = data_to_string(data, self._json_encoder)
    if sys.getsizeof(data) > 30720:
        raise ValueError('Too much data')
    channels = list(map(validate_channel, channels))
    if len(channels) > 1:
        for chan in channels:
            if is_encrypted_channel(chan):
                raise ValueError('You cannot trigger to multiple channels when using encrypted channels')
    if is_encrypted_channel(channels[0]):
        data = json.dumps(encrypt(channels[0], data, self._encryption_master_key), ensure_ascii=False)
    params = {'name': event_name, 'channels': channels, 'data': data}
    if socket_id:
        params['socket_id'] = validate_socket_id(socket_id)
    return Request(self, POST, '/apps/%s/events' % self.app_id, params)","isinstance(channels, six.string_types)",124,"not isinstance(channels, (list, tuple))",False,24.808415001701817,N/A
"@request_method
def trigger_batch(self, batch=[], already_encoded=False):
    """"""Trigger multiple events with a single HTTP call.

        http://pusher.com/docs/rest_api#method-post-batch-events
        """"""
<mask>:
        for event in batch:
            validate_channel(event['channel'])
            event_name = ensure_text(event['name'], 'event_name')
            if len(event['name']) > 200:
                raise ValueError('event_name too long')
            event['data'] = data_to_string(event['data'], self._json_encoder)
            if sys.getsizeof(event['data']) > 10240:
                raise ValueError('Too much data')
            if is_encrypted_channel(event['channel']):
                event['data'] = json.dumps(encrypt(event['channel'], event['data'], self._encryption_master_key), ensure_ascii=False)
    params = {'batch': batch}
    return Request(self, POST, '/apps/%s/batch_events' % self.app_id, params)",not already_encoded,64,already_encoded,False,71.65313105737896,N/A
"@request_method
def channels_info(self, prefix_filter=None, attributes=[]):
    """"""Get information on multiple channels, see:

        http://pusher.com/docs/rest_api#method-get-channels
        """"""
    params = {}
<mask>:
        params['info'] = join_attributes(attributes)
    if prefix_filter:
        params['filter_by_prefix'] = ensure_text(prefix_filter, 'prefix_filter')
    return Request(self, GET, six.text_type('/apps/%s/channels') % self.app_id, params)",attributes,33,attributes,True,100.00000000000004,N/A
"@request_method
def channel_info(self, channel, attributes=[]):
    """"""Get information on a specific channel, see:

        http://pusher.com/docs/rest_api#method-get-channel
        """"""
    validate_channel(channel)
    params = {}
<mask>:
        params['info'] = join_attributes(attributes)
    return Request(self, GET, '/apps/%s/channels/%s' % (self.app_id, channel), params)",attributes,30,attributes,True,100.00000000000004,N/A
"def authenticate(self, channel, socket_id, custom_data=None):
    """"""Used to generate delegated client subscription token.

        :param channel: name of the channel to authorize subscription to
        :param socket_id: id of the socket that requires authorization
        :param custom_data: used on presence channels to provide user info
        """"""
    channel = validate_channel(channel)
<mask>:
        raise ValueError('Channel should be a valid channel, got: %s' % channel)
    socket_id = validate_socket_id(socket_id)
    if custom_data:
        custom_data = json.dumps(custom_data, cls=self._json_encoder)
    string_to_sign = '%s:%s' % (socket_id, channel)
    if custom_data:
        string_to_sign += ':%s' % custom_data
    signature = sign(self.secret, string_to_sign)
    auth = '%s:%s' % (self.key, signature)
    response_payload = {'auth': auth}
    if is_encrypted_channel(channel):
        shared_secret = generate_shared_secret(ensure_binary(channel, 'channel'), self._encryption_master_key)
        shared_secret_b64 = base64.b64encode(shared_secret)
        response_payload['shared_secret'] = shared_secret_b64
    if custom_data:
        response_payload['channel_data'] = custom_data
    return response_payload",not channel_name_re.match(channel),113,not channel,False,1.110899653824231,N/A
"def validate_webhook(self, key, signature, body):
    """"""Used to validate incoming webhook messages. When used it guarantees
        that the sender is Pusher and not someone else impersonating it.

        :param key: key used to sign the body
        :param signature: signature that was given with the body
        :param body: content that needs to be verified
        """"""
    key = ensure_text(key, 'key')
    signature = ensure_text(signature, 'signature')
    body = ensure_text(body, 'body')
<mask>:
        return None
    if not verify(self.secret, body, signature):
        return None
    try:
        body_data = json.loads(body, cls=self._json_decoder)
    except ValueError:
        return None
    time_ms = body_data.get('time_ms')
    if not time_ms:
        return None
    if abs(time.time() * 1000 - time_ms) > 300000:
        return None
    return body_data",key != self.key,103,"not verify(self.key, key, signature)",False,14.991106946711685,N/A
"def ensure_text(obj, name):
<mask>:
        return obj
    if isinstance(obj, six.string_types):
        return six.text_type(obj)
    if isinstance(obj, six.binary_type):
        return bytes(obj).decode('utf-8')
    raise TypeError('%s should be %s instead it is a %s' % (name, text, type(obj)))","isinstance(obj, six.text_type)",30,"isinstance(obj, text)",False,28.871566309219904,N/A
"def ensure_binary(obj, name):
    """"""
    ensure_binary() ensures that the value is a
    python2 str or python3 bytes
    more on this here: https://pythonhosted.org/six/#six.binary_type
    """"""
<mask>:
        return obj
    if isinstance(obj, six.text_type) or isinstance(obj, six.string_types):
        return obj.encode('utf-8')
    raise TypeError('%s should be %s instead it is a %s' % (name, byte_type, type(obj)))","isinstance(obj, six.binary_type)",47,"isinstance(obj, byte_type)",False,38.940039153570254,N/A
"def validate_user_id(user_id):
    user_id = ensure_text(user_id, 'user_id')
    length = len(user_id)
<mask>:
        raise ValueError('User id is empty')
    if length > 200:
        raise ValueError(""User id too long: '{}'"".format(user_id))
    if not channel_name_re.match(user_id):
        raise ValueError(""Invalid user id: '{}'"".format(user_id))
    return user_id",length == 0,35,len(user_id) == 0,False,17.747405280050266,N/A
"def validate_channel(channel):
    channel = ensure_text(channel, 'channel')
<mask>:
        raise ValueError('Channel too long: %s' % channel)
    if channel.startswith(SERVER_TO_USER_PREFIX):
        if not server_to_user_channel_re.match(channel):
            raise ValueError('Invalid server to user Channel: %s' % channel)
    elif not channel_name_re.match(channel):
        raise ValueError('Invalid Channel: %s' % channel)
    return channel",len(channel) > 200,39,len(channel) > MAX_CHANNEL_SIZE,False,39.281465090051306,N/A
"def validate_socket_id(socket_id):
    socket_id = ensure_text(socket_id, 'socket_id')
<mask>:
        raise ValueError('Invalid socket ID: %s' % socket_id)
    return socket_id",not socket_id_re.match(socket_id),16,not validate_socket_id(socket_id),False,41.36817680097793,N/A
"def __init__(self, app_id, key, secret, ssl=True, host=None, port=None, timeout=5, cluster=None, encryption_master_key=None, encryption_master_key_base64=None, json_encoder=None, json_decoder=None, backend=None, **backend_options):
<mask>:
        from .requests import RequestsBackend
        backend = RequestsBackend
    self._app_id = ensure_text(app_id, 'app_id')
    if not app_id_re.match(self._app_id):
        raise ValueError('Invalid app id')
    self._key = ensure_text(key, 'key')
    self._secret = ensure_text(secret, 'secret')
    if not isinstance(ssl, bool):
        raise TypeError('SSL should be a boolean')
    self._ssl = ssl
    if host:
        self._host = ensure_text(host, 'host')
    elif cluster:
        self._host = six.text_type('api-%s.pusher.com') % ensure_text(cluster, 'cluster')
    else:
        self._host = six.text_type('api.pusherapp.com')
    if port and (not isinstance(port, six.integer_types)):
        raise TypeError('port should be an integer')
    self._port = port or (443 if ssl else 80)
    if not (isinstance(timeout, six.integer_types) or isinstance(timeout, float)):
        raise TypeError('timeout should be an integer or a float')
    self._timeout = timeout
    self._json_encoder = json_encoder
    self._json_decoder = json_decoder
    self._encryption_master_key = parse_master_key(encryption_master_key, encryption_master_key_base64)
    self.http = backend(self, **backend_options)",backend is None,128,backend is None,True,100.00000000000004,N/A
"def is_encrypted_channel(channel):
    """"""
    is_encrypted_channel() checks if the channel is encrypted by verifying the prefix
    """"""
<mask>:
        return True
    return False",channel.startswith(ENCRYPTED_PREFIX),20,channel.startswith('['),False,36.55552228545123,N/A
"def parse_master_key(encryption_master_key, encryption_master_key_base64):
    """"""
    parse_master_key validates, parses and returns the bytes of the encryption master key
    from the constructor arguments.
    At present there is a deprecated ""raw"" key and a suggested base64 encoding.
    """"""
<mask>:
        raise ValueError('Do not provide both encryption_master_key and encryption_master_key_base64. ' + 'encryption_master_key is deprecated, provide only encryption_master_key_base64')
    if encryption_master_key is not None:
        warnings.warn('`encryption_master_key` is deprecated, please use `encryption_master_key_base64`')
        if len(encryption_master_key) == 32:
            return ensure_binary(encryption_master_key, 'encryption_master_key')
        else:
            raise ValueError('encryption_master_key must be 32 bytes long')
    if encryption_master_key_base64 is not None:
        if is_base64(encryption_master_key_base64):
            decoded = base64.b64decode(encryption_master_key_base64)
            if len(decoded) == 32:
                return decoded
            else:
                raise ValueError('encryption_master_key_base64 must be a base64 string which decodes to 32 bytes')
        else:
            raise ValueError('encryption_master_key_base64 must be valid base64')
    return None",encryption_master_key is not None and encryption_master_key_base64 is not None,115,"not all((isinstance(encryption_master_key, bytes) for encryption_master_key in [None, None]))",False,26.696378876165934,N/A
"def generate_shared_secret(channel, encryption_master_key):
    """"""
    generate_shared_secret() takes a six.binary_type (python2 str or python3 bytes) channel name and encryption_master_key
    and returns the sha256 hash in six.binary_type format
    """"""
<mask>:
        raise ValueError('No master key was provided for use with encrypted channels. Please provide encryption_master_key_base64 as an argument to the Pusher SDK')
    hashable = channel + encryption_master_key
    return hashlib.sha256(hashable).digest()",encryption_master_key is None,55,not encryption_master_key,False,64.31870218238025,N/A
"def encrypt(channel, data, encryption_master_key, nonce=None):
    """"""
    encrypt() encrypts the provided payload specified in the 'data' parameter
    """"""
    channel = ensure_binary(channel, 'channel')
    shared_secret = generate_shared_secret(channel, encryption_master_key)
    box = nacl.secret.SecretBox(shared_secret)
<mask>:
        nonce = nacl.utils.random(nacl.secret.SecretBox.NONCE_SIZE)
    else:
        nonce = ensure_binary(nonce, 'nonce')
    nonce_b64 = base64.b64encode(nonce)
    encrypted = box.encrypt(data.encode('utf-8'), nonce)
    cipher_text = encrypted.ciphertext
    cipher_text_b64 = base64.b64encode(cipher_text)
    return {'nonce': nonce_b64.decode('utf-8'), 'ciphertext': cipher_text_b64.decode('utf-8')}",nonce is None,55,nonce is None,True,100.00000000000004,N/A
"def send_request(self, request):
    method = request.method
    data = request.body
    headers = {'Content-Type': 'application/json'}
    future = Future()

    def process_response_future(response):
<mask>:
            future.set_exception(response.exception())
        else:
            result = response.result()
            code = result.code
            body = (result.body or b'').decode('utf8')
            future.set_result(process_response(code, body))
    request = tornado.httpclient.HTTPRequest(request.url, method=method, body=data, headers=headers, request_timeout=self.client.timeout, **self.options)
    response_future = self.http.fetch(request, raise_error=False)
    response_future.add_done_callback(process_response_future)
    return future",response.exception() is not None,49,response.exception(),False,54.88116360940266,N/A
"def process_response(status, body):
<mask>:
        return json.loads(body)
    elif status == 400:
        raise PusherBadRequest(body)
    elif status == 401:
        raise PusherBadAuth(body)
    elif status == 403:
        raise PusherForbidden(body)
    else:
        raise PusherBadStatus('%s: %s' % (status, body))",status == 200 or status == 202,31,status == 200,False,28.650479686019022,N/A
"def __init__(self, client, method, path, params=None):
<mask>:
        params = {}
    self.client = client
    self.method = method
    self.path = path
    self.params = copy.copy(params)
    if method == POST:
        self.body = six.text_type(json.dumps(params)).encode('utf8')
        self.query_params = {}
    elif method == GET:
        self.body = bytes()
        self.query_params = params
    else:
        raise NotImplementedError('Only GET and POST supported')
    self._generate_auth()",params is None,50,params is None,True,100.00000000000004,N/A
"@property
def headers(self):
    hdrs = {'X-Pusher-Library': 'pusher-http-python ' + VERSION}
<mask>:
        hdrs['Content-Type'] = 'application/json'
    return hdrs",self.method == POST,16,self.json,False,20.24518585186855,N/A
"def __init__(self, client, **options):
    self.client = client
    self.options = options
<mask>:
        self.options.update({'verify': CERT_PATH})
    self.session = requests.Session()",self.client.ssl,16,self.options.get('verify') is None,False,9.287528999566801,N/A
"def compare_digest(a, b):
<mask>:
        return False
    return reduce(lambda x, y: x | y, [ord(x) ^ ord(y) for x, y in zip(a, b)]) == 0",len(a) != len(b),24,len(a) != len(b),True,100.00000000000004,N/A
"def test_x_pusher_library_header(self):
    conf = Pusher.from_url(u'http://key:secret@somehost/apps/4')
    req = Request(conf._pusher_client, u'GET', u'/some/obscure/api', {u'foo': u'bar'})
    self.assertTrue('X-Pusher-Library' in req.headers)
    pusherLib = req.headers['X-Pusher-Library']
    regex = '^pusher-http-python \\d+(\\.\\d+)+(rc\\d+)?$'
<mask>:
        self.assertRegexpMatches(pusherLib, regex)
    else:
        self.assertRegex(pusherLib, regex)","sys.version_info < (3,)",28,six.PY3,False,2.6682865255867765,N/A
"def setUp(self):
<mask>:
        import warnings
        warnings.filterwarnings('ignore', category=ResourceWarning, message='unclosed file <_io.BufferedRandom name*')
    self.pusher = Pusher.from_url(u'http://key:secret@api.pusherapp.com/apps/4')",sys.version_info[0] >= 3,14,os.path.exists(u'/io/BufferedRandom name*'),False,2.627961710408444,N/A
"def setUp(self):

    class JSONEncoder(json.JSONEncoder):

        def default(self, o):
<mask>:
                return str(o)
            return super(JSONEncoder, self).default(o)
    constants = {'NaN': 99999}

    class JSONDecoder(json.JSONDecoder):

        def __init__(self, **kwargs):
            super(JSONDecoder, self).__init__(parse_constant=constants.__getitem__)
    self.authentication_client = AuthenticationClient('4', 'key', 'secret', host='somehost', json_encoder=JSONEncoder, json_decoder=JSONDecoder)","isinstance(o, Decimal)",32,"isinstance(o, int)",False,53.7284965911771,N/A
"def new_task(name, cls, workflow_task, **kwargs):
    """"""
    Instantiate a new task. Not supposed to be used by the end-user
    (use WorkflowTask.new_task() instead).
    """"""
    slurminfo = None
    for key, val in [(key, val) for key, val in kwargs.items()]:
<mask>:
            raise Exception('Key in kwargs to new_task is not string. Must be string: %s' % key)
        if isinstance(val, sciluigi.slurm.SlurmInfo):
            slurminfo = val
            kwargs[key] = val
        elif not isinstance(val, str):
            try:
                kwargs[key] = json.dumps(val)
            except TypeError:
                kwargs[key] = str(val)
    kwargs['instance_name'] = name
    kwargs['workflow_task'] = workflow_task
    kwargs['slurminfo'] = slurminfo
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', category=UserWarning, message='Parameter ""workflow_task"".*is not of type string')
        newtask = cls.from_str_params(kwargs)
        if slurminfo is not None:
            newtask.slurminfo = slurminfo
        return newtask","not isinstance(key, str)",106,"not isinstance(val, str)",False,41.11336169005196,N/A
"def ex_local(self, command):
    """"""
        Execute command locally (not through resource manager).
        """"""
<mask>:
        command = sub.list2cmdline(command)
    log.info('Executing command: ' + str(command))
    proc = sub.Popen(command, shell=True, stdout=sub.PIPE, stderr=sub.PIPE, text=True)
    stdout, stderr = proc.communicate()
    retcode = proc.returncode
    if len(stderr) > 0:
        log.debug('Stderr from command: %s', stderr)
    if retcode != 0:
        errmsg = 'Command failed (retcode {ret}): {cmd}\nCommand output: {out}\nCommand stderr: {err}'.format(ret=retcode, cmd=command, out=stdout, err=stderr)
        log.error(errmsg)
        raise Exception(errmsg)
    return (retcode, stdout, stderr)","isinstance(command, list)",69,"hasattr(sub, 'list2cmdline')",False,10.682175159905853,N/A
"def _add_auditinfo(self, instance_name, infotype, infoval):
    """"""
        Save audit information in a designated file, specific for this task.
        """"""
    dirpath = self.workflow_task.get_auditdirpath()
<mask>:
        time.sleep(random.random())
        if not os.path.isdir(dirpath):
            os.makedirs(dirpath)
    auditfile = os.path.join(dirpath, instance_name)
    if not os.path.exists(auditfile):
        with open(auditfile, 'w') as afile:
            afile.write('[%s]\n' % self.instance_name)
    with open(auditfile, 'a') as afile:
        afile.write('%s: %s\n' % (infotype, infoval))",not os.path.isdir(dirpath),52,not os.path.exists(dirpath),False,59.694917920196445,N/A
"def get_instance_name(self):
    """"""
        Return the luigi instance_name
        """"""
    instance_name = None
<mask>:
        instance_name = self.instance_name
    else:
        instance_name = self.task_id
    return instance_name",self.instance_name is not None,21,self.instance_name,False,54.88116360940266,N/A
"@luigi.Task.event_handler(luigi.Event.START)
def save_start_time(self):
    """"""
        Log start of execution of task.
        """"""
<mask>:
        msg = 'Task {task} started'.format(task=self.get_instance_name())
        log.info(msg)","hasattr(self, 'workflow_task') and self.workflow_task is not None",18,self.is_running(),False,4.142617217945258,N/A
"@luigi.Task.event_handler(luigi.Event.PROCESSING_TIME)
def save_end_time(self, task_exectime_sec):
    """"""
        Log end of execution of task, with execution time.
        """"""
<mask>:
        msg = 'Task {task} finished after {proctime:.3f}s'.format(task=self.get_instance_name(), proctime=task_exectime_sec)
        log.info(msg)
        self.add_auditinfo('task_exectime_sec', '%.3f' % task_exectime_sec)
        for paramname, paramval in self.param_kwargs.items():
            if paramname not in ['workflow_task']:
                self.add_auditinfo(paramname, paramval)","hasattr(self, 'workflow_task') and self.workflow_task is not None",41,task_exectime_sec > self.task_exectime_sec,False,5.710992792957473,N/A
"def parse(self, x):
<mask>:
        return x
    else:
        raise Exception('Parameter is not instance of SlurmInfo: %s' % x)","isinstance(x, SlurmInfo)",17,"isinstance(x, SlurmInfo)",True,100.00000000000004,N/A
"def ex(self, command):
    """"""
        Execute either locally or via SLURM, depending on config
        """"""
<mask>:
        command = ' '.join(command)
    if self.slurminfo.runmode == RUNMODE_LOCAL:
        log.info('Executing command in local mode: %s', command)
        self.ex_local(command)
    elif self.slurminfo.runmode == RUNMODE_HPC:
        log.info('Executing command in HPC mode: %s', command)
        self.ex_hpc(command)
    elif self.slurminfo.runmode == RUNMODE_MPI:
        log.info('Executing command in MPI mode: %s', command)
        self.ex_mpi(command)","isinstance(command, list)",55,"isinstance(command, list)",True,100.00000000000004,N/A
"def ex_hpc(self, command):
    """"""
        Execute command in HPC mode
        """"""
<mask>:
        command = sub.list2cmdline(command)
    fullcommand = 'salloc %s %s' % (self.slurminfo.get_argstr_hpc(), command)
    retcode, stdout, stderr = self.ex_local(fullcommand)
    self.log_slurm_info(stderr)
    return (retcode, stdout, stderr)","isinstance(command, list)",32,sub.is_list_commandline(command),False,9.980099403873663,N/A
"def ex_mpi(self, command):
    """"""
        Execute command in HPC mode with MPI support (multi-node, message passing interface).
        """"""
<mask>:
        command = sub.list2cmdline(command)
    fullcommand = 'salloc %s %s' % (self.slurminfo.get_argstr_mpi(), command)
    retcode, stdout, stderr = self.ex_local(fullcommand)
    self.log_slurm_info(stderr)
    return (retcode, stdout, stderr)","isinstance(command, list)",39,sub.is_mpi_supported(command),False,9.287528999566801,N/A
"def assert_matches_character_class(self, char_class, a_string):
    """"""
        Helper method, that tests whether a string matches a regex character class.
        """"""
<mask>:
        raise Exception('String {s} does not match character class {cc}'.format(s=a_string, cc=char_class))","not bool(re.match('^{c}+$'.format(c=char_class), a_string))",29,"not re.match(char_class, a_string)",False,16.007065335012292,N/A
"def _ensure_timestamp(self):
    """"""
        Make sure that there is a time stamp for when the workflow started.
        """"""
<mask>:
        self._wfstart = datetime.datetime.utcnow().strftime('%Y%m%d_%H%M%S_%f')",self._wfstart == '',21,not self._wfstart,False,44.82700320176826,N/A
"def get_wflogpath(self):
    """"""
        Get the path to the workflow-speicfic log file.
        """"""
<mask>:
        self._ensure_timestamp()
        clsname = self.__class__.__name__.lower()
        logpath = 'log/workflow_' + clsname + '_started_{t}.log'.format(t=self._wfstart)
        self._wflogpath = logpath
    return self._wflogpath",self._wflogpath == '',29,not self._wflogpath,False,44.82700320176826,N/A
"def requires(self):
    """"""
        Implementation of Luigi API method.
        """"""
<mask>:
        wflog_formatter = logging.Formatter(sciluigi.interface.LOGFMT_STREAM, sciluigi.interface.DATEFMT)
        wflog_file_handler = logging.FileHandler(self.output()['log'].path)
        wflog_file_handler.setLevel(logging.INFO)
        wflog_file_handler.setFormatter(wflog_formatter)
        log.addHandler(wflog_file_handler)
        luigilog = logging.getLogger('luigi-interface')
        luigilog.addHandler(wflog_file_handler)
        self._hasaddedhandler = True
    clsname = self.__class__.__name__
    if not self._hasloggedstart:
        log.info('-' * 80)
        log.info('SciLuigi: %s Workflow Started (logging to %s)', clsname, self.get_wflogpath())
        log.info('-' * 80)
        self._hasloggedstart = True
    workflow_output = self.workflow()
    if workflow_output is None:
        clsname = self.__class__.__name__
        raise Exception('Nothing returned from workflow() method in the %s Workflow task. Forgot to add a return statement at the end?' % clsname)
    return workflow_output",not self._hasaddedhandler,85,not self._hasaddedhandler,True,100.00000000000004,N/A
"def run(self):
    """"""
        Implementation of Luigi API method
        """"""
<mask>:
        errmsg = 'Audit file already exists, when trying to create it: %s' % self.output()['audit'].path
        log.error(errmsg)
        raise Exception(errmsg)
    else:
        with self.output()['audit'].open('w') as auditfile:
            for taskname in sorted(self._tasks):
                taskaudit_path = os.path.join(self.get_auditdirpath(), taskname)
                if os.path.exists(taskaudit_path):
                    auditfile.write(open(taskaudit_path).read() + '\n')
    clsname = self.__class__.__name__
    if not self._hasloggedfinish:
        log.info('-' * 80)
        log.info('SciLuigi: %s Workflow Finished (workflow log at %s)', clsname, self.get_wflogpath())
        log.info('-' * 80)
        self._hasloggedfinish = True",self.output()['audit'].exists(),70,os.path.exists(self.output()['audit'].path),False,56.35190098079901,N/A
"def _upstream_tasks(self):
    """"""
        Extract upstream tasks from the TargetInfo objects
        or functions returning those (or lists of both the earlier)
        for use in luigi's requires() method.
        """"""
    upstream_tasks = set()
    for attrname, attrval in self.__dict__.items():
<mask>:
            upstream_tasks = self._add_upstream_tasks(upstream_tasks, attrval)
    return list(upstream_tasks)",attrname.startswith('in_'),42,"isinstance(attrval, (list, tuple))",False,4.990049701936832,N/A
"def _add_upstream_tasks(self, tasks, new_tasks):
    """"""
        Recursively loop through lists of TargetInfos, or
        callables returning TargetInfos, or lists of ...
        (repeat recursively) ... and return all tasks.
        """"""
<mask>:
        new_tasks = new_tasks()
    if isinstance(new_tasks, TargetInfo):
        tasks.add(new_tasks.task)
    elif isinstance(new_tasks, list):
        for new_task in new_tasks:
            tasks = self._add_upstream_tasks(tasks, new_task)
    elif isinstance(new_tasks, dict):
        for _, new_task in new_tasks.items():
            tasks = self._add_upstream_tasks(tasks, new_task)
    else:
        raise Exception('Input value is neither callable, TargetInfo, nor list: %s' % val)
    return tasks",callable(new_tasks),73,"not isinstance(new_tasks, (callable, TargetInfo))",False,21.401603033752977,N/A
"def _output_targets(self):
    """"""
        Extract output targets from the TargetInfo objects
        or functions returning those (or lists of both the earlier)
        for use in luigi's output() method.
        """"""
    output_targets = []
    for attrname in dir(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=DeprecationWarning, message='Use of param_args has been deprecated')
            attrval = getattr(self, attrname)
<mask>:
                output_targets = self._parse_outputitem(attrval, output_targets)
    return output_targets",attrname.startswith('out_'),55,"isinstance(attrval, dict)",False,6.916271812933183,N/A
"def _parse_outputitem(self, val, targets):
    """"""
        Recursively loop through lists of TargetInfos, or
        callables returning TargetInfos, or lists of ...
        (repeat recursively) ... and return all targets.
        """"""
<mask>:
        val = val()
    if isinstance(val, TargetInfo):
        targets.append(val.target)
    elif isinstance(val, list):
        for valitem in val:
            targets = self._parse_outputitem(valitem, targets)
    elif isinstance(val, dict):
        for _, valitem in val.items():
            targets = self._parse_outputitem(valitem, targets)
    else:
        raise Exception('Input item is neither callable, TargetInfo, nor list: %s' % val)
    return targets",callable(val),73,callable(val),True,100.00000000000004,N/A
"def workflow(self):
    t1a = self.new_task('t1a', T1, text='hej_hopp')
    t1b = self.new_task('t1b', T1, text='hopp_hej')
    mrg1 = self.new_task('mrg1', Merge)
    mrg2 = self.new_task('mrg2', Merge)
    mrg1.in_data1 = t1a.out_data1
    mrg1.in_data2 = t1b.out_data1
    mrg2.in_data1 = t1b.out_data1
    mrg2.in_data2 = t1a.out_data1
    for name, instance in locals().items():
<mask>:
            log.info(f'{name}, task id: {instance.task_id}\n{name}, hash: {instance.__hash__()}')
    return locals()[self.task]","issubclass(type(instance), sl.Task)",46,instance.task_id,False,3.8261660656802645,N/A
"def run(self):
    resp = requests.get('http://nbis.se')
<mask>:
        log.error('Web request failed!')
    else:
        with self.out_doneflag().open('w') as flagfile:
            flagfile.write('Web Request Task Done!')",resp.status_code != 200,18,resp.status_code != 200,True,100.00000000000004,N/A
"def edges_ignoring_elsewhere(G, data=False):
<mask>:
        return [(a, b, data) for a, b, data in G.edges(data=True) if not (a.startswith('from') or b.startswith('from') or a.startswith('to') or b.startswith('to'))]
    else:
        return [(a, b) for a, b in G.edges(data=False) if not (a.startswith('from') or b.startswith('from') or a.startswith('to') or b.startswith('to'))]",data,41,data,True,100.00000000000004,N/A
"def nodes_ignoring_elsewhere(G, data=False):
<mask>:
        return [(u, data) for u, data in G.nodes(data=True) if not (u.startswith('from') or u.startswith('to'))]
    else:
        return [u for u in G.nodes(data=False) if not (u.startswith('from') or u.startswith('to'))]",data,29,data,True,100.00000000000004,N/A
"def to_json(self, filename=None, format=None):
    """"""Convert data to JSON-ready dictionary.""""""
<mask>:
        data = {'nodes': [n.to_json(format) for n in self.nodes], 'links': [l.to_json(format) for l in self.links], 'order': self.ordering.layers, 'groups': self.groups}
    else:
        data = {'format': 'sankey-v2', 'metadata': {'title': 'A Sankey diagram', 'authors': [], 'layers': self.ordering.layers}, 'nodes': [n.to_json(format) for n in self.nodes], 'links': [l.to_json(format) for l in self.links], 'groups': self.groups}
    if filename is None:
        return data
    else:
        with open(filename, 'wt') as f:
            json.dump(data, f)",format == 'widget',70,format is None,False,19.716118825581447,N/A
"def to_widget(self, width=700, height=500, margins=None, align_link_types=False, link_label_format='', link_label_min_width=5, debugging=False):
<mask>:
        raise RuntimeError('ipysankeywidget is required')
    if margins is None:
        margins = {'top': 25, 'bottom': 10, 'left': 130, 'right': 130}
    value = self.to_json(format='widget')
    widget = SankeyWidget(nodes=value['nodes'], links=value['links'], order=value['order'], groups=value['groups'], align_link_types=align_link_types, linkLabelFormat=link_label_format, linkLabelMinWidth=link_label_min_width, layout=Layout(width=str(width), height=str(height)), margins=margins)
    if debugging:
        output = Output()

        def callback(_, d):
            with output:
                clear_output()
            if not d:
                return
            if d['source'].startswith('__from_elsewhere_'):
                d['source'] = None
            elif d['target'].startswith('__to_elsewhere_'):
                d['target'] = None
            link = [l for l in self.links + [l for n in self.nodes for l in n.from_elsewhere_links] + [l for n in self.nodes for l in n.to_elsewhere_links] if l.source == d['source'] and l.target == d['target'] and (l.type == d['type'])]
            assert len(link) == 1
            link = link[0]
            with output:
                display('Flows in dataset contributing to this link:')
                if self.dataset:
                    display(self.dataset._table.loc[link.original_flows])
                else:
                    display(link.original_flows)
        widget.on_link_clicked(callback)
        return VBox([widget, output])
    else:
        return widget",SankeyWidget is None,135,"not hasattr(self, 'ipysankeywidget')",False,0.0,N/A
"def to_json(self, format=None):
    """"""Convert node to JSON-ready dictionary.""""""
<mask>:
        return {'id': self.id, 'title': self.title if self.title is not None else self.id, 'direction': self.direction.lower(), 'hidden': self.hidden is True or self.title == '', 'type': self.style if self.style is not None else 'default', 'fromElsewhere': [l.to_json(format) for l in self.from_elsewhere_links], 'toElsewhere': [l.to_json(format) for l in self.to_elsewhere_links]}
    else:
        return {'id': self.id, 'title': self.title if self.title is not None else self.id, 'style': {'direction': self.direction.lower(), 'hidden': self.hidden is True or self.title == '', 'type': self.style if self.style is not None else 'default'}}",format == 'widget',85,format is not None,False,15.97357760615681,N/A
"def _validate_opacity(instance, attr, value):
<mask>:
        raise ValueError('opacity must be a number')
    if value < 0 or value > 1:
        raise ValueError('opacity must be between 0 and 1')","not isinstance(value, float)",27,"not isinstance(value, (int, float))",False,45.384078730076126,N/A
"def to_json(self, format=None):
    """"""Convert link to JSON-ready dictionary.""""""
<mask>:
        return {'source': self.source, 'target': self.target, 'type': self.type, 'time': self.time, 'value': self.link_width, 'title': self.title, 'color': self.color, 'opacity': self.opacity, 'data': self.data}
    else:
        return {'source': self.source, 'target': self.target, 'type': self.type, 'title': self.title, 'time': self.time, 'link_width': self.link_width, 'data': self.data, 'style': {'color': self.color, 'opacity': self.opacity}}",format == 'widget',49,self.link_width is None,False,0.0,N/A
"def _validate_query(instance, attribute, value):
<mask>:
        if not all((isinstance(x, tuple) and len(x) == 2 for x in value)):
            raise ValueError('All elements of query should be 2-tuples')",value,25,attribute == 'values',False,0.0,N/A
"@classmethod
def Simple(cls, dimension, values):

    def make_group(v):
<mask>:
            label, items = v
        else:
            label, items = (v, (v,))
        return Group(label, [(dimension, tuple(items))])
    groups = [make_group(v) for v in values]
    seen_values = set()
    for i, group in enumerate(groups):
        for j, value in enumerate(group.query[0][1]):
            if value in seen_values:
                raise ValueError('Duplicate value ""{}"" in partition (value {} of group {})'.format(value, j, i))
            seen_values.add(value)
    return cls(groups)","isinstance(v, tuple)",62,"isinstance(v, tuple)",True,100.00000000000004,N/A
"def elsewhere_bundles(sankey_definition, add_elsewhere_waypoints=True):
    """"""Find new bundles and waypoints needed, so that every process group has a
    bundle to Elsewhere and a bundle from Elsewhere.

    If `add_elsewhere_waypoints` is True (the default), then new Waypoints are
    created for these Bundles to flow through. Otherwise, the Bundles are
    created without Waypoints, which will result in them being rendered as short
    ""stubs"" on the nodes.

    """"""
    has_to_elsewhere = set()
    has_from_elsewhere = set()
    for bundle in sankey_definition.bundles.values():
        assert not (bundle.source is Elsewhere and bundle.target is Elsewhere)
<mask>:
            has_to_elsewhere.add(bundle.source)
        if bundle.source is Elsewhere:
            has_from_elsewhere.add(bundle.target)
    R = len(sankey_definition.ordering.layers)
    new_waypoints = {}
    new_bundles = {}
    no_bundles = len(sankey_definition.bundles) == 0
    for u, process_group in sankey_definition.nodes.items():
        if not isinstance(process_group, ProcessGroup):
            continue
        waypoint_title = '→' if process_group.direction == 'R' else '←'
        d_rank = +1 if process_group.direction == 'R' else -1
        r, _, _ = sankey_definition.ordering.indices(u)
        if no_bundles or (0 <= r + d_rank < R and u not in has_to_elsewhere):
            dummy_id = '__{}>'.format(u)
            assert dummy_id not in sankey_definition.nodes
            if add_elsewhere_waypoints:
                new_waypoints[dummy_id] = Waypoint(direction=process_group.direction, title=waypoint_title)
                new_bundles[dummy_id] = Bundle(u, Elsewhere, waypoints=[dummy_id])
            else:
                new_bundles[dummy_id] = Bundle(u, Elsewhere)
        if no_bundles or (0 <= r - d_rank < R and u not in has_from_elsewhere):
            dummy_id = '__>{}'.format(u)
            assert dummy_id not in sankey_definition.nodes
            if add_elsewhere_waypoints:
                new_waypoints[dummy_id] = Waypoint(direction=process_group.direction, title=waypoint_title)
                new_bundles[dummy_id] = Bundle(Elsewhere, u, waypoints=[dummy_id])
            else:
                new_bundles[dummy_id] = Bundle(Elsewhere, u)
    return (new_waypoints, new_bundles)",bundle.target is Elsewhere,216,bundle.source is Elsewhere and bundle.target is Elsewhere,False,35.08439695638686,N/A
"def augment(G, new_waypoints, new_bundles):
    """"""Add waypoints for new_bundles to layered graph G.
    """"""
    for v in new_waypoints.values():
        assert isinstance(v, Waypoint)
    G = G.copy()
    R = len(G.ordering.layers)
    for k, bundle in sorted(new_bundles.items(), reverse=True):
<mask>:
            continue
        assert len(bundle.waypoints) == 1
        w = bundle.waypoints[0]
        if bundle.to_elsewhere:
            u = G.nodes[bundle.source]['node']
            r, _, _ = G.ordering.indices(bundle.source)
            d_rank = +1 if u.direction == 'R' else -1
            G.add_node(w, node=new_waypoints[w])
            r, G.ordering = check_order_edges(G.ordering, r, d_rank)
            this_rank = G.ordering.layers[r + d_rank]
            prev_rank = G.ordering.layers[r]
            G.add_edge(bundle.source, w, bundles=[k])
            i, j = new_node_indices(G, this_rank, prev_rank, w, side='below')
            G.ordering = G.ordering.insert(r + d_rank, i, j, w)
        elif bundle.from_elsewhere:
            u = G.nodes[bundle.target]['node']
            r, _, _ = G.ordering.indices(bundle.target)
            d_rank = +1 if u.direction == 'R' else -1
            G.add_node(w, node=new_waypoints[w])
            r, G.ordering = check_order_edges(G.ordering, r, -d_rank)
            this_rank = G.ordering.layers[r - d_rank]
            prev_rank = G.ordering.layers[r]
            G.add_edge(w, bundle.target, bundles=[k])
            i, j = new_node_indices(G, this_rank, prev_rank, w, side='below')
            G.ordering = G.ordering.insert(r - d_rank, i, j, w)
        else:
            assert False, 'Should not call augment() with non-elsewhere bundle'
    return G",not bundle.waypoints,161,len(bundle.bundles) == 0,False,9.535414040914192,N/A
"def check_order_edges(ordering, r, dr):
    layers = ordering.layers
    nb = len(layers[0]) if layers else 1
<mask>:
        layers = layers + tuple((() for i in range(nb)))
    elif r + dr < 0:
        layers = tuple((() for i in range(nb))) + layers
        r += 1
    return (r, Ordering(layers))",r + dr >= len(layers),45,r < 0,False,3.7238938287986976,N/A
"def weave(sankey_definition, dataset, measures='value', link_width=None, link_color=None, palette=None, add_elsewhere_waypoints=True):
<mask>:
        dataset = Dataset(dataset)
    GV = view_graph(sankey_definition)
    new_waypoints, new_bundles = elsewhere_bundles(sankey_definition, add_elsewhere_waypoints)
    GV2 = augment(GV, new_waypoints, new_bundles)
    bundles2 = dict(sankey_definition.bundles, **new_bundles)
    bundle_flows, unused_flows = dataset.apply_view(sankey_definition.nodes, bundles2, sankey_definition.flow_selection)
    GR, groups = results_graph(GV2, bundle_flows, flow_partition=sankey_definition.flow_partition, time_partition=sankey_definition.time_partition, measures=measures)
    if link_width is None:
        if not isinstance(measures, str):
            raise ValueError('If you set a complicated measure function, you need to set link_width too.')
        link_width = measures
    if callable(link_width):
        get_value = lambda link, measures: link_width(measures)
    elif isinstance(link_width, str):
        get_value = lambda link, measures: float(measures[link_width])
    else:
        raise ValueError('link_width must be a str or callable')
    if link_color is None:
        link_color = CategoricalScale('type', palette=palette)
    elif isinstance(link_color, str):
        link_color = CategoricalScale(link_color, palette=palette)
    elif not callable(link_color):
        raise TypeError('link_color must be a str or callable')
    if hasattr(link_color, 'set_domain_from'):
        link_color.set_domain_from([data['measures'] for _, _, data in GR.edges(data=True)])
    links = [make_link(get_value, link_color, v, w, m, t, data) for v, w, (m, t), data in GR.edges(keys=True, data=True)]
    nodes = [make_node(get_value, link_color, u, data) for u, data in GR.nodes(data=True)]
    result = SankeyData(nodes, links, groups, GR.ordering.layers, dataset)
    return result","isinstance(dataset, pd.DataFrame)",168,"isinstance(dataset, Dataset)",False,38.49815007763549,N/A
"def prep_qualitative_palette(G, palette):
<mask>:
        palette = 'Pastel1_8'
    if isinstance(palette, str):
        try:
            palette = getattr(qualitative, palette).hex_colors
        except AttributeError:
            raise ValueError('No qualitative palette called {}'.format(palette)) from None
    if not isinstance(palette, dict):
        materials = sorted(set([m for v, w, (m, t) in G.edges(keys=True)]))
        palette = {m: v for m, v in zip(materials, itertools.cycle(palette))}",palette is None,49,palette is None,True,100.00000000000004,N/A
"def serialise_data(value, version='2'):
    """"""Serialise `value` returned by graph_to_sankey.""""""
<mask>:
        raise ValueError('Only version 2 is supported')
    return _value_version_2(value)",version != '2',17,version != '2',True,100.00000000000004,N/A
"def no_default_vals_in_repr(cls):
    """"""Class decorator on top of attr.s that omits attributes from repr that
    have their default value""""""
    defaults = OrderedDict()
    for attribute in cls.__attrs_attrs__:
<mask>:
            assert attribute.default.takes_self == False, 'not implemented'
            defaults[attribute.name] = attribute.default.factory()
        else:
            defaults[attribute.name] = attribute.default

    def repr_(self):
        real_cls = self.__class__
        qualname = getattr(real_cls, '__qualname__', None)
        if qualname is not None:
            class_name = qualname.rsplit('>.', 1)[-1]
        else:
            class_name = real_cls.__name__
        attributes = defaults.keys()
        return '{0}({1})'.format(class_name, ', '.join((name + '=' + repr(getattr(self, name)) for name in attributes if getattr(self, name) != defaults[name])))
    cls.__repr__ = repr_
    return cls","isinstance(attribute.default, attr.Factory)",88,"hasattr(attribute, 'default')",False,9.911450612811139,N/A
"def _convert_bundles_to_dict(bundles):
<mask>:
        bundles = {k: v for k, v in enumerate(bundles)}
    return bundles","not isinstance(bundles, dict)",14,"isinstance(bundles, list)",False,45.48019047027906,N/A
"def _validate_bundles(instance, attribute, bundles):
    for k, b in bundles.items():
<mask>:
            if b.source not in instance.nodes:
                raise ValueError('Unknown source ""{}"" in bundle {}'.format(b.source, k))
            if not isinstance(instance.nodes[b.source], ProcessGroup):
                raise ValueError('Source of bundle {} is not a process group'.format(k))
        if not b.to_elsewhere:
            if b.target not in instance.nodes:
                raise ValueError('Unknown target ""{}"" in bundle {}'.format(b.target, k))
            if not isinstance(instance.nodes[b.target], ProcessGroup):
                raise ValueError('Target of bundle {} is not a process group'.format(k))
        for u in b.waypoints:
            if u not in instance.nodes:
                raise ValueError('Unknown waypoint ""{}"" in bundle {}'.format(u, k))
            if not isinstance(instance.nodes[u], Waypoint):
                raise ValueError('Waypoint ""{}"" of bundle {} is not a waypoint'.format(u, k))",not b.from_elsewhere,99,not b.from_elsewhere,True,100.00000000000004,N/A
"def _validate_ordering(instance, attribute, ordering):
    for layer_bands in ordering.layers:
        for band_nodes in layer_bands:
            for u in band_nodes:
<mask>:
                    raise ValueError('Unknown node ""{}"" in ordering'.format(u))",u not in instance.nodes,23,u not in instance.nodes,True,100.00000000000004,N/A
"def __call__(self, *nodes):
    """"""Return process IDs below the given nodes in the tree""""""
    s = set()
    for node in nodes:
<mask>:
            return None
        s.update(self._leaves_below(node))
    if len(s) == 1:
        query = '{} == ""{}""'.format(self.column, s.pop())
    else:
        query = '{} in {}'.format(self.column, repr(sorted(s)))
    return query",self.tree.in_degree(node) == 0,43,node == self.column,False,7.559185144683521,N/A
"def __getitem__(self, k):
<mask>:
        col = k
    elif k == 'id':
        col = self.column
    else:
        col = '{}.{}'.format(self.column, k)
    return self.df[col]",not self.column,21,"isinstance(k, int)",False,0.0,N/A
"def eval_selection(df, column, sel):
<mask>:
        return df[column].isin(sel)
    elif isinstance(sel, str):
        resolver = Resolver(df, column)
        return df.eval(sel, local_dict={}, global_dict={}, resolvers=(resolver,))
    else:
        raise TypeError('Unknown selection type: %s' % type(sel))","isinstance(sel, (list, tuple))",27,"isinstance(sel, (list, tuple))",True,100.00000000000004,N/A
"def __init__(self, flows, dim_process=None, dim_material=None, dim_time=None):
<mask>:
        raise ValueError('dim_process index not unique')
    if dim_material is not None and (not dim_material.index.is_unique):
        raise ValueError('dim_material index not unique')
    if dim_time is not None and (not dim_time.index.is_unique):
        raise ValueError('dim_time index not unique')
    flows = flows.reset_index(drop=True)
    self._flows = flows
    self._dim_process = dim_process
    self._dim_material = dim_material
    self._dim_time = dim_time
    self._table = flows
    if dim_process is not None:
        self._table = self._table.join(dim_process.add_prefix('source.'), on='source').join(dim_process.add_prefix('target.'), on='target')
    if dim_material is not None:
        self._table = self._table.join(dim_material.add_prefix('material.'), on='material')
    if dim_time is not None:
        self._table = self._table.join(dim_time.add_prefix('time.'), on='time')",dim_process is not None and (not dim_process.index.is_unique),84,dim_process is not None and (not dim_process.index.is_unique),True,100.00000000000004,N/A
"def partition(self, dimension, processes=None):
    """"""Partition of all values of `dimension` within `processes`""""""
<mask>:
        q = self._table.source.isin(processes) | self._table.target.isin(processes)
        values = self._table.loc[q, dimension].unique()
    else:
        values = self._table[dimension].unique()
    return Partition.Simple(dimension, values)",processes,29,processes,True,100.00000000000004,N/A
"def save(self, filename):
    with pd.HDFStore(filename) as store:
        store['flows'] = self._flows
<mask>:
            store['dim_process'] = self._dim_process
        if self._dim_material is not None:
            store['dim_material'] = self._dim_material
        if self._dim_time is not None:
            store['dim_time'] = self._dim_time",self._dim_process is not None,30,self._dim_process is not None,True,100.00000000000004,N/A
"def nodes_from_partition(u, partition):
<mask>:
        return [('{}^*'.format(u), '*')]
    else:
        return [('{}^{}'.format(u, value), value) for value in partition.labels + ['_']]",partition is None,18,partition.labels == [],False,6.567274736060395,N/A
"def group_flows(flows, v, partition1, w, partition2, flow_partition, time_partition, measures):
<mask>:
        data = measures
    elif isinstance(measures, str):
        data = agg_one_group({measures: 'sum'})
    elif isinstance(measures, list):
        data = agg_one_group({k: 'sum' for k in measures})
    elif isinstance(measures, dict):
        data = agg_one_group(measures)
    else:
        raise ValueError('measure must be str, list, dict or callable')
    e = flows.copy()
    set_partition_keys(e, partition1, 'k1', v + '^', process_side='source')
    set_partition_keys(e, partition2, 'k2', w + '^', process_side='target')
    set_partition_keys(e, flow_partition, 'k3', '')
    set_partition_keys(e, time_partition, 'k4', '')
    grouped = e.groupby(['k1', 'k2', 'k3', 'k4'])
    return [(source, target, (material, time), {'measures': data(group), 'original_flows': list(group.index)}) for (source, target, material, time), group in grouped]",callable(measures),95,"isinstance(measures, dict)",False,17.965205598154213,N/A
"def set_partition_keys(df, partition, key_column, prefix, process_side=None):
<mask>:
        partition = Partition([Group('*', [])])
    df[key_column] = prefix + '_'
    seen = df.index != df.index
    for group in partition.groups:
        q = df.index == df.index
        for dim, values in group.query:
            if dim.startswith('process') and process_side:
                dim = process_side + dim[7:]
            q = q & df[dim].isin(values)
        if any(q & seen):
            dup = df[q & seen]
            raise ValueError('Duplicate values in group {} ({}): {}'.format(group, process_side, ', '.join(['{}-{}'.format(e.source, e.target) for _, e in dup.iterrows()])))
        df.loc[q, key_column] = prefix + str(group.label)
        seen = seen | q",partition is None,86,not partition,False,30.326532985631665,N/A
"def add_dummy_nodes(G, v, w, bundle_key, bundle_index=0, node_kwargs=None):
<mask>:
        node_kwargs = {}
    V = G.get_node(v)
    W = G.get_node(w)
    H = G.copy()
    rv, iv, jv = H.ordering.indices(v)
    rw, iw, jw = H.ordering.indices(w)
    if rw > rv:
        p = rv if V.direction == 'L' else rv + 1
        q = rw if W.direction == 'L' else rw - 1
        new_ranks = list(range(p, q + 1))
        d = 'R'
    elif rv > rw:
        p = rv if V.direction == 'R' else rv - 1
        q = rw if W.direction == 'R' else rw + 1
        new_ranks = list(range(p, q - 1, -1))
        d = 'L'
    else:
        new_ranks = []
    if not new_ranks:
        _add_edge(H, v, w, bundle_key)
        return H
    u = v
    for r in new_ranks:
        idr = '__{}_{}_{}'.format(v, w, r)
        if idr not in H.nodes:
            _add_edge(H, u, idr, bundle_key)
            if r == rv:
                i, j = (iv, jv + (+1 if V.direction == 'R' else -1))
            else:
                prev_rank = H.ordering.layers[r + 1 if d == 'L' else r - 1]
                i, j = new_node_indices(H, H.ordering.layers[r], prev_rank, idr, side='below' if d == 'L' else 'above')
            H.ordering = H.ordering.insert(r, i, j, idr)
            H.add_node(idr, node=Waypoint(direction=d, **node_kwargs))
        else:
            _add_edge(H, u, idr, bundle_key)
        u = idr
    _add_edge(H, u, w, bundle_key)
    return H",node_kwargs is None,203,node_kwargs is None,True,100.00000000000004,N/A
"def _add_edge(G, v, w, bundle_key):
<mask>:
        G[v][w]['bundles'].append(bundle_key)
    else:
        G.add_edge(v, w, bundles=[bundle_key])","G.has_edge(v, w)",11,v in G[v][w]['bundles'],False,4.456882760699063,N/A
"def _convert_layers(layers):
    """"""Wrap nodes in a single band, if none are specified.""""""
    for item in layers:
<mask>:
            return tuple(((tuple(layer_nodes),) for layer_nodes in layers))
    return tuple((tuple((tuple(band_nodes) for band_nodes in layer_bands)) for layer_bands in layers))","any((isinstance(x, str) for x in item))",33,item == 'B',False,1.0211566521809647,N/A
"def indices(self, value):
    for r, bands in enumerate(self.layers):
        for i, rank in enumerate(bands):
<mask>:
                return (r, i, rank.index(value))
    raise ValueError('node ""{}"" not in ordering'.format(value))",value in rank,24,value in rank,True,100.00000000000004,N/A
"def band_index(idx, i):
    for iband, i0 in reversed(list(enumerate(idx))):
<mask>:
            return iband
    return len(idx)",i >= i0,13,i0 == idx[iband],False,7.809849842300637,N/A
"def new_node_indices(G, this_bands, other_bands, new_process_group, side='below'):
    assert side in ('above', 'below')
    this_layer, this_idx = flatten_bands(this_bands)
    other_layer, other_idx = flatten_bands(other_bands)
    new_pos = median_value(neighbour_positions(G, other_layer, new_process_group))
<mask>:
        return (0, 0)
    new_band = band_index(other_idx, new_pos)
    existing_pos = [median_value(neighbour_positions(G, other_layer, u)) for u in this_layer]
    existing_pos = fill_unknown(existing_pos, side)
    candidates = [pos for pos in existing_pos if band_index(other_idx, pos) == new_band]
    index = bisect.bisect_right(candidates, new_pos) if side == 'below' else bisect.bisect_left(candidates, new_pos)
    return (new_band, index)",new_pos == -1,71,new_pos == 0,False,75.98356856515926,N/A
"def median_value(positions):
    N = len(positions)
    m = N // 2
<mask>:
        return -1
    elif N % 2 == 1:
        return positions[m]
    elif N == 2:
        return (positions[0] + positions[1]) / 2
    else:
        left = positions[m - 1] - positions[0]
        right = positions[-1] - positions[m]
        return (positions[m - 1] * right + positions[m] * left) / (left + right)",N == 0,58,m == 0,False,59.460355750136046,N/A
"def rgb2hex(rgb):
    """"""Given an rgb or rgba sequence of 0-1 floats, return the hex string""""""
<mask>:
        return rgb
    else:
        return '#%02x%02x%02x' % tuple([int(np.round(val * 255)) for val in rgb[:3]])","isinstance(rgb, str)",29,len(rgb) == 4,False,14.535768424205482,N/A
"def __call__(self, link, measures):
    palette = self.get_palette()
    value = self.get_value(link, measures)
<mask>:
        return self.lookup[value]
    elif len(self.lookup) >= len(self.palette) and self.default:
        return self.default
    else:
        if self._next >= len(self.palette):
            self._next = 0
        color = self.palette[self._next]
        self._next += 1
        self.lookup[value] = color
        return color",value in self.lookup,41,value in self.lookup,True,100.00000000000004,N/A
"def get_value(self, link, measures):
<mask>:
        return getattr(link, self.attr)
    else:
        return measures[self.attr]","self.attr in ('source', 'target', 'type', 'time')",11,self.attr in measures,False,13.501633901742348,N/A
"def prep_qualitative_palette(palette):
<mask>:
        palette = 'Pastel1_8'
    if isinstance(palette, str):
        try:
            palette = getattr(qualitative, palette).hex_colors
        except AttributeError:
            raise ValueError('No qualitative palette called {}'.format(palette)) from None
    if isinstance(palette, dict):
        return (list(palette.values()), palette)
    else:
        return (palette, {})",palette is None,34,palette is None,True,100.00000000000004,N/A
"def __init__(self, attr, palette=None, intensity=None, domain=None):
<mask>:
        palette = self.default_palette_name
    self.attr = attr
    self.palette = self.lookup_palette_name(palette) if isinstance(palette, str) else palette
    self.domain = domain
    self.intensity = intensity",palette is None,27,palette is None,True,100.00000000000004,N/A
"def _add_bundles_to_graph(G, bundles, sort_key):
    for k, bundle in sorted(bundles.items(), key=sort_key):
<mask>:
            G.nodes[bundle.target].setdefault('from_elsewhere_bundles', []).append(k)
        elif not bundle.waypoints and bundle.source is not Elsewhere and (bundle.target is Elsewhere):
            G.nodes[bundle.source].setdefault('to_elsewhere_bundles', []).append(k)
        else:
            nodes = (bundle.source,) + bundle.waypoints + (bundle.target,)
            for iw, (a, b) in enumerate(pairwise(nodes)):
                if a is not Elsewhere and b is not Elsewhere:
                    G = add_dummy_nodes(G, a, b, k, iw, _dummy_kw(bundle))
    for v, w, data in G.edges(data=True):
        flow_partitions = list({bundles[b].flow_partition for b in data['bundles']})
        if len(flow_partitions) > 1:
            raise ValueError('Multiple flow partitions in bundles: {}'.format(', '.join((str(b) for b in data['bundles']))))
        if flow_partitions[0]:
            data['flow_partition'] = flow_partitions[0]
    return G",not bundle.waypoints and bundle.source is Elsewhere and (bundle.target is not Elsewhere),95,bundle.waypoints and bundle.source is Elsewhere and (bundle.target is Elsewhere),False,81.2130229944724,N/A
"def _bundle_order(sankey_definition):

    def keyfunc(item):
        k, bundle = item
<mask>:
            return (2, 0)
        r0, _, _ = sankey_definition.ordering.indices(bundle.source)
        r1, _, _ = sankey_definition.ordering.indices(bundle.target)
        if r1 > r0:
            return (0, r1 - r0)
        else:
            return (1, r1 - r0)
    return keyfunc",bundle.to_elsewhere or bundle.from_elsewhere,39,k != 'bundle',False,0.0,N/A
"def create(name, mro=(object,), extra_methods={}, *args, **kwargs):
    """"""
    create(name, mro=(object,), extra_methods={}, ...) -> Sentinel instance

    Creates a new sentinel instance. This is a singleton instance kind
    of like the builtin None, and Ellipsis.

    Method resolution order (MRO) for the anonymous class can be
    specified (i.e., it can be a subclass). Provide the mro as tuple of
    all classes that it inherits from. If only one class, provide a
    1-tuple: e.g., (Cls,).

    Additionally extra class attributes, such as methods can be provided
    in the extra_methods dict. The following methods are provided, but
    can be overridden:

        __repr__()
            Returns the class name, similar to None and Ellipsis.
        __copy__()
        __deepcopy__()
            Always return the same singleton instance such that
            ``copy(Sentinel) is Sentinel`` is true.
        __reduce__()
            Provided for proper pickling prowess. That is,
            ``pickle.loads(pickle.dumps(Sentinel)) is Sentinel`` is
            true.

    Finally, the remain arguments and keyword arguments are passed to
    the super class's __init__().  This is helpful when for
    instantiating base classes such as a tuple.
    """"""
    cls_dict = {}
    cls_dict.update(__repr__=lambda self: name, __deepcopy__=lambda self, _memo: self, __copy__=lambda self: self, __reduce__=lambda self: name)
<mask>:
        cls_dict.update(__slots__=())
    cls_dict.update(extra_methods)
    anon_type = type(name, mro, cls_dict)
    anon_type.__module__ = get_caller_module()
    return anon_type(*args, **kwargs)","mro == (object,)",188,__slots__ is not None,False,0.0,N/A
"def test_defects(self):
    mail = mailparser.parse_from_file(mail_malformed_1)
    self.assertTrue(mail.has_defects)
    self.assertEqual(1, len(mail.defects))
    self.assertEqual(1, len(mail.defects_categories))
    self.assertIn('defects', mail.mail)
    self.assertIn('StartBoundaryNotFoundDefect', mail.defects_categories)
    self.assertIsInstance(mail.mail_json, six.text_type)
    result = len(mail.attachments)
    self.assertEqual(1, result)
    mail = mailparser.parse_from_file(mail_test_1)
<mask>:
        self.assertFalse(mail.has_defects)
        self.assertNotIn('defects', mail.mail)
    elif six.PY3:
        self.assertTrue(mail.has_defects)
        self.assertEqual(1, len(mail.defects))
        self.assertEqual(1, len(mail.defects_categories))
        self.assertIn('defects', mail.mail)
        self.assertIn('CloseBoundaryNotFoundDefect', mail.defects_categories)",six.PY2,39,six.PY2,True,100.00000000000004,N/A
"@classmethod
def from_file(cls, fp, is_outlook=False):
    """"""
        Init a new object from a file path.

        Args:
            fp (string): file path of raw email
            is_outlook (boolean): if True is an Outlook email

        Returns:
            Instance of MailParser
        """"""
    log.debug('Parsing email from file {!r}'.format(fp))
    with ported_open(fp) as f:
        message = email.message_from_file(f)
<mask>:
        log.debug('Removing temp converted Outlook email {!r}'.format(fp))
        os.remove(fp)
    return cls(message)",is_outlook,57,is_outlook,True,100.00000000000004,N/A
"@classmethod
def from_bytes(cls, bt):
    """"""
        Init a new object from bytes.

        Args:
            bt (bytes-like object): raw email as bytes-like object

        Returns:
            Instance of MailParser
        """"""
    log.debug('Parsing email from bytes')
<mask>:
        raise MailParserEnvironmentError('Parsing from bytes is valid only for Python 3.x version')
    message = email.message_from_bytes(bt)
    return cls(message)",six.PY2,46,"sys.version_info < (3, 0)",False,3.7477767366779213,N/A
"def _append_defects(self, part, part_content_type):
    """"""
        Add new defects and defects categories to object attributes.

        The defects are a list of all the problems found
        when parsing this message.

        Args:
            part (string): mail part
            part_content_type (string): content type of part
        """"""
    part_defects = {}
    for e in part.defects:
        defects = '{}: {}'.format(e.__class__.__name__, e.__doc__)
        self._defects_categories.add(e.__class__.__name__)
        part_defects.setdefault(part_content_type, []).append(defects)
        log.debug('Added defect {!r}'.format(defects))
<mask>:
        self._has_defects = True
        self._defects.append(part_defects)",part_defects,63,not self._has_defects,False,13.134549472120788,N/A
"def _make_mail(self, complete=True):
    """"""
        This method assigns the right values to all tokens of email.
        Returns a parsed object

        Keyword Arguments:
            complete {bool} -- If True returns all mails parts
                                (default: {True})

        Returns:
            dict -- Parsed email object
        """"""
    mail = {}
    keys = get_mail_keys(self.message, complete)
    for i in keys:
        log.debug('Getting header or part {!r}'.format(i))
        value = getattr(self, i)
<mask>:
            mail[i] = value
    mail['has_defects'] = self.has_defects
    if self.has_defects:
        mail['defects'] = self.defects
        mail['defects_categories'] = list(self.defects_categories)
    return mail",value,76,value is not None,False,15.97357760615681,N/A
"def custom_log(level='WARNING', name=None):
    """"""
    This function returns a custom logger.
    :param level: logging level
    :type level: str
    :param name: logger name
    :type name: str
    :return: logger
    """"""
<mask>:
        log = logging.getLogger(name)
    else:
        log = logging.getLogger()
    log.setLevel(level)
    ch = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter('%(asctime)s | %(name)s | %(module)s | %(funcName)s | %(lineno)d | %(levelname)s | %(message)s')
    ch.setFormatter(formatter)
    log.addHandler(ch)
    return log",name,58,name,True,100.00000000000004,N/A
"@sanitize
def ported_string(raw_data, encoding='utf-8', errors='ignore'):
    """"""
    Give as input raw data and output a str in Python 3
    and unicode in Python 2.

    Args:
        raw_data: Python 2 str, Python 3 bytes or str to porting
        encoding: string giving the name of an encoding
        errors: his specifies the treatment of characters
            which are invalid in the input encoding

    Returns:
        str (Python 3) or unicode (Python 2)
    """"""
<mask>:
        return six.text_type()
    if isinstance(raw_data, six.text_type):
        return raw_data
    if six.PY2:
        try:
            return six.text_type(raw_data, encoding, errors)
        except LookupError:
            return six.text_type(raw_data, 'utf-8', errors)
    if six.PY3:
        try:
            return six.text_type(raw_data, encoding)
        except (LookupError, UnicodeDecodeError):
            return six.text_type(raw_data, 'utf-8', errors)",not raw_data,100,raw_data is None,False,39.76353643835252,N/A
"def decode_header_part(header):
    """"""
    Given an raw header returns an decoded header

    Args:
        header (string): header to decode

    Returns:
        str (Python 3) or unicode (Python 2)
    """"""
<mask>:
        return six.text_type()
    output = six.text_type()
    try:
        for d, c in decode_header(header):
            c = c if c else 'utf-8'
            output += ported_string(d, c, 'ignore')
    except (HeaderParseError, UnicodeError):
        log.error('Failed decoding header part: {}'.format(header))
        output += header
    return output.strip()",not header,63,header is None,False,27.516060407455225,N/A
"def ported_open(file_):
<mask>:
        return open(file_)
    elif six.PY3:
        return open(file_, encoding='utf-8', errors='ignore')",six.PY2,11,six.PY2,True,100.00000000000004,N/A
"def fingerprints(data):
    """"""
    This function return the fingerprints of data.

    Args:
        data (string): raw data

    Returns:
        namedtuple: fingerprints md5, sha1, sha256, sha512
    """"""
    hashes = namedtuple('Hashes', 'md5 sha1 sha256 sha512')
<mask>:
        data = data.encode('utf-8')
    md5 = hashlib.md5()
    md5.update(data)
    md5 = md5.hexdigest()
    sha1 = hashlib.sha1()
    sha1.update(data)
    sha1 = sha1.hexdigest()
    sha256 = hashlib.sha256()
    sha256.update(data)
    sha256 = sha256.hexdigest()
    sha512 = hashlib.sha512()
    sha512.update(data)
    sha512 = sha512.hexdigest()
    return hashes(md5, sha1, sha256, sha512)","not isinstance(data, six.binary_type)",67,"isinstance(data, str)",False,23.350308364304226,N/A
"def get_parser(args):
    """"""
    Get the correct parser based on the input source.
    :param args: argparse.Namespace
    :type args: argparse.Namespace
    :return: MailParser
    :rtype: mailparser.core.MailParser
    """"""
<mask>:
        return parse_file(args)
    elif args.string:
        log.debug('Start analysis by string mail')
        return mailparser.parse_from_string(args.string)
    elif args.stdin:
        return parse_stdin(args)
    else:
        raise ValueError('No input source provided')",args.file,45,args.file,True,100.00000000000004,N/A
"def parse_file(args):
    """"""
    Parse the file based on the arguments provided.
    :param args: argparse.Namespace
    :type args: argparse.Namespace
    :return: MailParser
    :rtype: mailparser.core.MailParser
    """"""
    log.debug('Start analysis by file mail')
<mask>:
        log.debug('Start analysis by Outlook msg')
        return mailparser.parse_from_file_msg(args.file)
    else:
        log.debug('Start analysis by raw mail')
        return mailparser.parse_from_file(args.file)",args.outlook,43,args.outlook_msg,False,39.76353643835252,N/A
"def parse_stdin(args):
    """"""
    Parse the stdin based on the arguments provided.
    :param args: argparse.Namespace
    :type args: argparse.Namespace
    :return: MailParser
    :rtype: mailparser.core.MailParser
    """"""
    log.debug('Start analysis by stdin mail')
<mask>:
        raise MailParserOutlookError(""You can't use stdin with msg Outlook"")
    return mailparser.parse_from_file_obj(sys.stdin)",args.outlook,38,args.outlook,True,100.00000000000004,N/A
"def process_output(args, parser):
    """"""
    Process the output based on the arguments provided.
    :param args: argparse.Namespace
    :type args: argparse.Namespace
    :param parser: MailParser
    :type parser: mailparser.core.MailParser
    :param log: logger
    :type log: logging.Logger
    """"""
<mask>:
        safe_print(parser.mail_json)
    if args.body:
        safe_print(parser.body)
    if args.headers:
        safe_print(parser.headers_json)
    if args.to:
        safe_print(parser.to_json)
    if args.delivered_to:
        safe_print(parser.delivered_to_json)
    if args.from_:
        safe_print(parser.from_json)
    if args.subject:
        safe_print(parser.subject)
    if args.receiveds:
        safe_print(parser.received_json)
    if args.defects:
        print_defects(parser)
    if args.senderip:
        print_sender_ip(parser, args)
    if args.attachments or args.attachments_hash:
        print_attachments_details(parser, args)
    if args.mail_hash:
        log.debug('Printing also mail fingerprints')
        print_mail_fingerprints(parser.body.encode('utf-8'))
    if args.store_attachments:
        log.debug('Store attachments on disk')
        write_attachments(parser.attachments, args.attachments_path)",args.json,82,args.mail_json,False,23.643540225079384,N/A
"def __init__(self, root_dir=None, force_download=False, prefix='', name='AIHub_KsponSpeech'):
    super().__init__(description, license)
<mask>:
        root_dir = os.path.join(default_korpora_path, 'AIHub_KsponSpeech_scripts', prefix)
    elif isinstance(root_dir, str) and os.path.isdir(root_dir):
        root_dir = os.path.join(root_dir, 'AIHub_KsponSpeech_scripts', prefix)
    paths = find_corpus_paths(root_dir)
    self.train = KorpusData(f'{name}.train', load_aihub_kspon_speech_scripts(paths))",root_dir is None,31,root_dir is None and default_korpora_path,False,35.08439695638686,N/A
"def find_corpus_paths(root_dir, suffix='.trn'):

    def match(path):
        return path[-4:] == suffix
<mask>:
        paths = sorted(glob(f'{root_dir}/*{suffix}') + glob(root_dir))
    else:
        paths = root_dir
    paths = [path for path in paths if match(path)]
    if not paths:
        raise ValueError('Not found corpus files. Check `root_dir`')
    return paths","isinstance(root_dir, str)",40,os.path.isdir(f'{root_dir}/*{suffix}'),False,8.562365224473284,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__()
    paths = ModuKorpus.get_corpus_path(root_dir, 'NIKL_WRITTEN', find_corpus_paths)
<mask>:
        raise ValueError('Not found corpus files. Check `root_dir`')
    self.train = KorpusData('모두의_문어_말뭉치.train', load_modu_written(paths))",not paths,22,not paths,True,100.00000000000004,N/A
"def find_corpus_paths(root_dir_or_paths):
    prefix_pattern = re.compile('W[ABCZ]RW')

    def match(path):
        prefix = path.split(os.path.sep)[-1][:4]
        return prefix_pattern.match(prefix)
<mask>:
        paths = sorted(glob(f'{root_dir_or_paths}/*.json') + glob(root_dir_or_paths))
    else:
        paths = root_dir_or_paths
    paths = [path for path in paths if match(path)]
    return paths","isinstance(root_dir_or_paths, str)",33,"isinstance(root_dir_or_paths, list)",False,82.651681837938,N/A
"def fetch(args):
    corpus_names = args.corpus
<mask>:
        corpus_names = 'all'
        Korpora.fetch(corpus_names, args.root, args.force_download)
        return
    if isinstance(corpus_names, str):
        corpus_names = [corpus_names]
    for name in corpus_names:
        if name not in KORPUS_DESCRIPTION:
            print(f'Korpora does not provide `{name}` corpus')
            continue
        Korpora.fetch(name, args.root, args.force_download)",corpus_names == 'all' or corpus_names[0] == 'all',38,corpus_names == 'all',False,18.887560283756194,N/A
"@classmethod
def get_corpus_path(cls, root_dir=None, prefix='', finder=None):
<mask>:
        root_dir = os.path.join(default_korpora_path, prefix)
    alternative_root_dir = os.path.join(root_dir, prefix)
    if os.path.exists(alternative_root_dir):
        root_dir = alternative_root_dir
    paths = []
    if callable(finder):
        paths = finder(root_dir)
    return paths",root_dir is None,30,root_dir is None,True,100.00000000000004,N/A
"def __init__(self, root_dir=None, force_download=False, load_light=True):
    super().__init__(force_download)
    paths = ModuKorpus.get_corpus_path(root_dir, 'NIKL_NEWSPAPER', find_corpus_paths)
<mask>:
        raise ValueError('Not found corpus files. Check `root_dir`')
    if load_light:
        self.train = ModuNewsDataLight('모두의_뉴스_말뭉치(light).train', load_modu_news(paths, load_light))
    else:
        self.train = ModuNewsData('모두의_뉴스_말뭉치.train', load_modu_news(paths, load_light))
    self.row_to_documentid = [news.document_id for news in self.train]
    self.documentid_to_row = {document_id: idx for idx, document_id in enumerate(self.row_to_documentid)}",not paths,48,not paths,True,100.00000000000004,N/A
"def find_corpus_paths(root_dir_or_paths):
    prefix_pattern = re.compile('N[WLPIZ]RW')

    def match(path):
        prefix = path.split(os.path.sep)[-1][:4]
        return prefix_pattern.match(prefix)
<mask>:
        paths = sorted(glob(f'{root_dir_or_paths}/*.json') + glob(root_dir_or_paths))
    else:
        paths = root_dir_or_paths
    paths = [path for path in paths if match(path)]
    return paths","isinstance(root_dir_or_paths, str)",33,"isinstance(root_dir_or_paths, list)",False,82.651681837938,N/A
"def cleaning(self, raw_documents: List[str], **kargs):
    examples = [example.split('\t') for example in raw_documents]
    for i_sent, columns in enumerate(examples):
<mask>:
            print(columns)
            raise ValueError(f'Found some errors in line {i_sent}: {examples[i_sent]}')
    texts, pairs, labels = zip(*examples)
    return (texts, pairs, labels)",len(columns) != 3,36,i_sent >= len(examples),False,11.339582221952005,N/A
"def cleaning(self, raw_documents: List[str], **kargs):
    examples = [example.split('\t') for example in raw_documents]
    for i_sent, columns in enumerate(examples):
<mask>:
            print(columns)
            raise ValueError(f'Found some errors in line {i_sent}: {examples[i_sent]}')
    texts, labels = zip(*examples)
    return (texts, labels)",len(columns) != 2,34,i_sent >= len(examples),False,11.339582221952005,N/A
"def cleaning(self, raw_documents: List[str], **kargs):
    examples = [example.split('\t') for example in raw_documents]
    for i_sent, columns in enumerate(examples):
<mask>:
            print(columns)
            raise ValueError(f'Found some errors in line {i_sent}: {examples[i_sent]}')
    texts, pairs = zip(*examples)
    return (texts, pairs)",len(columns) != 2,34,i_sent >= len(examples),False,11.339582221952005,N/A
"def get_attribute_name(path):
<mask>:
        raise ValueError('File must be format of tsv')
    name = os.path.basename(path)[:-4].replace('.', '_')
    return name",path[-4:] != '.tsv',16,not os.path.isfile(path),False,5.0735520042259505,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__()
    paths = ModuKorpus.get_corpus_path(root_dir, 'NIKL_MESSENGER', find_corpus_paths)
<mask>:
        raise ValueError('Not found corpus files. Check `root_dir`')
    self.train = KorpusData('모두의_메신저_말뭉치(conversation).train', load_modu_messenger(paths))",not paths,22,not paths,True,100.00000000000004,N/A
"def find_corpus_paths(root_dir_or_paths):
    prefix_pattern = re.compile('M[DM]RW')

    def match(path):
        prefix = path.split(os.path.sep)[-1][:4]
        return prefix_pattern.match(prefix)
<mask>:
        paths = sorted(glob(f'{root_dir_or_paths}/*.json') + glob(root_dir_or_paths))
    else:
        paths = root_dir_or_paths
    paths = [path for path in paths if match(path)]
    return paths","isinstance(root_dir_or_paths, str)",33,"isinstance(root_dir_or_paths, list)",False,82.651681837938,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__(description, license)
<mask>:
        root_dir = default_korpora_path
    fetch_nsmc(root_dir, force_download)
    for info in NSMC_FETCH_INFORMATION:
        local_path = os.path.join(os.path.abspath(root_dir), info['destination'])
        text, labels = self.cleaning(load_text(local_path, num_heads=1))
        if 'train' in info['destination']:
            self.train = LabeledSentenceKorpusData('NSMC.train', text, labels)
        else:
            self.test = LabeledSentenceKorpusData('NSMC.test', text, labels)",root_dir is None,40,root_dir is None,True,100.00000000000004,N/A
"def cleaning(self, raw_lines: List[str]):
    separated_lines = [line.split('\t') for line in raw_lines]
    for i_sent, separated_line in enumerate(separated_lines):
<mask>:
            raise ValueError(f'Found some errors in line {i_sent}: {separated_line}')
    _, texts, labels = zip(*separated_lines)
    labels = [int(label) for label in labels]
    return (texts, labels)",len(separated_line) != 3,40,i_sent not in self.errors,False,4.8734989388136185,N/A
"def __str__(self):
    attributes = ''
    for var_name, var in self.__dict__.items():
<mask>:
            attributes += f'  - {self.name}.{var_name} : list[{var[0].__class__.__name__}]\n'
    s = f'{self.name}: size={len(self.texts)}\n{attributes}'
    return s","var_name not in {'name', 'description', 'self'}",24,var[0].__class__.__name__ != 'Text',False,4.480836160121357,N/A
"def __init__(self, name, texts, labels):
<mask>:
        raise ValueError('All two arguments must be same length')
    super().__init__(name, texts)
    self.labels = labels",not len(texts) == len(labels),19,len(texts) != len(labels),False,59.54165059120785,N/A
"def __init__(self, name, texts, pairs):
<mask>:
        raise ValueError('All two arguments must be same length')
    super().__init__(name, texts)
    self.pairs = pairs",not len(texts) == len(pairs),19,len(texts) != 2,False,24.549475440235113,N/A
"def __init__(self, name, texts, pairs, labels):
<mask>:
        raise ValueError('All three arguments must be same length')
    super().__init__(name, texts)
    self.pairs = pairs
    self.labels = labels",not len(texts) == len(pairs) == len(labels),23,len(texts) != 3,False,10.41813075244897,N/A
"def __init__(self, name, texts, words, tags):
<mask>:
        raise ValueError('All three arguments must be same length')
    super().__init__(name, texts)
    self.words = words
    self.tags = tags",not len(texts) == len(words) == len(tags),23,len(texts) != 3,False,10.41813075244897,N/A
"def __init__(self, dataname, contents, categories, begins, ends, num_agrees, titles):
<mask>:
        raise ValueError('All length of input arguments must be same.')
    super().__init__(dataname, texts=contents)
    self.categories = categories
    self.num_agrees = num_agrees
    self.begins = begins
    self.ends = ends
    self.titles = titles",not len(contents) == len(categories) == len(begins) == len(ends) == len(num_agrees) == len(titles),36,len(contents) != len(begins),False,4.422371442196615,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__(description, license)
<mask>:
        root_dir = default_korpora_path
    fetch_korean_petitions(root_dir, force_download)
    contents, categories, begins, ends, num_agrees, titles = ([], [], [], [], [], [])
    for info in KOREAN_PETITIONS_FETCH_INFORMATION:
        local_path = os.path.join(os.path.abspath(root_dir), info['destination'])
        con, cat, b, e, num, tit = self.cleaning(load_text(local_path))
        categories += cat
        contents += con
        begins += b
        ends += e
        num_agrees += num
        titles += tit
    self.train = KoreanPetitionsData('KoreanPetitions.train', contents, categories, begins, ends, num_agrees, titles)",root_dir is None,68,root_dir is None,True,100.00000000000004,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__(description, license)
<mask>:
        root_dir = default_korpora_path
    train_info, dev_info, test_info = KOREAN_PARALLEL_KOEN_NEWS_FETCH_INFORMATION
    self.train = self.fetch_and_load('train', root_dir, train_info, force_download)
    self.dev = self.fetch_and_load('dev', root_dir, dev_info, force_download)
    self.test = self.fetch_and_load('test', root_dir, test_info, force_download)",root_dir is None,33,root_dir is None,True,100.00000000000004,N/A
"def fetch_and_load(self, mode, root_dir, fetch_info, force_download):
    dataname = f'koennews.{mode}'
    source_path = f'{root_dir}/korean_parallel/korean-english-park.{mode}.ko'
    target_path = f'{root_dir}/korean_parallel/korean-english-park.{mode}.en'
<mask>:
        local_path = os.path.join(os.path.abspath(root_dir), fetch_info['destination'])
        fetch(fetch_info['url'], local_path, 'korean_parallel', force_download, fetch_info['method'])
    sources, targets = load_parallel_text(source_path, target_path)
    return SentencePairKorpusData(dataname, sources, targets)",force_download or not os.path.exists(source_path) or (not os.path.exists(target_path)),34,fetch_info,False,0.004739153159129942,N/A
"def __init__(self, dataname, texts, pairs, labels, genres, filenames, years):
    super().__init__(dataname, texts, pairs, labels)
<mask>:
        raise ValueError('All length of `texts`, `pairs`, `labels`, `genres`, `filenames`, `years` should be same')
    self.genres = genres
    self.filenames = filenames
    self.years = years",not len(labels) == len(genres) == len(filenames) == len(years),36,len(texts) != len(pairs),False,5.822464699665066,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__(description, license)
<mask>:
        root_dir = default_korpora_path
    fetch_korsts(root_dir, force_download)
    for info in KORSTS_FETCH_INFORMATION:
        local_path = os.path.join(os.path.abspath(root_dir), info['destination'])
        returns = self.cleaning(load_text(local_path, num_heads=1))
        texts, pairs, labels, genres, filenames, years = returns
        if 'train' in info['destination']:
            self.train = KorSTSData('KorSTS.train', texts, pairs, labels, genres, filenames, years)
        elif 'dev' in info['destination']:
            self.dev = KorSTSData('KorSTS.dev', texts, pairs, labels, genres, filenames, years)
        elif 'test' in info['destination']:
            self.test = KorSTSData('KorSTS.test', texts, pairs, labels, genres, filenames, years)
        else:
            raise ValueError('Check `KORSTS_FETCH_INFORMATION`')",root_dir is None,75,root_dir is None,True,100.00000000000004,N/A
"def cleaning(self, raw_lines: List[str]):
    separated_lines = [line.split('\t') for line in raw_lines]
    for i_sent, separated_line in enumerate(separated_lines):
<mask>:
            raise ValueError(f'Found some errors in line {i_sent}: {separated_line}')
    genres, filenames, years, _, labels, texts, pairs = zip(*separated_lines)
    return (texts, pairs, labels, genres, filenames, years)",len(separated_line) != 7,41,i_sent not in self.errors,False,4.8734989388136185,N/A
"def __init__(self, dataname, texts, titles, gender_biases, biases, hates):
    super().__init__(dataname, texts)
<mask>:
        raise ValueError('All five arguments must be same length')
    self.titles = titles
    self.gender_biases = gender_biases
    self.biases = biases
    self.hates = hates",not len(texts) == len(titles) == len(gender_biases) == len(biases) == len(hates),31,len(texts) != five,False,1.4099406761787456,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__(description, license)
<mask>:
        root_dir = default_korpora_path
    fetch_korean_hate_speech(root_dir, force_download)
    self.train = load_train(root_dir)
    self.dev = load_dev(root_dir)
    self.unlabeled = load_unlabeled(root_dir)
    self.test = load_test(root_dir)",root_dir is None,24,root_dir is None,True,100.00000000000004,N/A
"def load_train(root_dir):
    text_labels = load_text(f'{root_dir}/korean_hate_speech/labeled/train.tsv', num_heads=1)
    titles = load_text(f'{root_dir}/korean_hate_speech/news_title/train.news_title.txt')
<mask>:
        raise ValueError(f'Found len(train.texts) != len(train.pairs). Do `fetch(force_download=True)`')
    texts, gender_biases, biases, hates = zip(*[line.split('\t') for line in text_labels])
    return KoreanHateSpeechLabeledData('KoreanHateSpeech.train', texts, titles, gender_biases, biases, hates)",len(text_labels) != len(titles),34,len(text_labels) != len(train.texts),False,69.30977286178778,N/A
"def load_dev(root_dir):
    text_labels = load_text(f'{root_dir}/korean_hate_speech/labeled/dev.tsv', num_heads=1)
    titles = load_text(f'{root_dir}/korean_hate_speech/news_title/dev.news_title.txt')
<mask>:
        raise ValueError(f'Found len(dev.texts) != len(dev.pairs). Do `fetch(force_download=True)`')
    texts, gender_biases, biases, hates = zip(*[line.split('\t') for line in text_labels])
    return KoreanHateSpeechLabeledData('KoreanHateSpeech.dev', texts, titles, gender_biases, biases, hates)",len(text_labels) != len(titles),34,len(text_labels) != len(dev.texts),False,69.30977286178778,N/A
"def load_unlabeled(root_dir):
    text_paths = [f'{root_dir}/korean_hate_speech/news_title/unlabeled_comments.news_title_{i}.txt' for i in range(1, 6)]
    title_paths = [f'{root_dir}/korean_hate_speech/news_title/unlabeled_comments.news_title_{i}.txt' for i in range(1, 6)]
    texts = []
    titles = []
    for text_path, title_path in zip(text_paths, title_paths):
        texts_ = load_text(text_path)
        titles_ = load_text(title_path)
<mask>:
            raise ValueError(f'Found some errors. not equal num of texts and titles. Do `fetch(force_download=True)`')
        texts += texts_
        titles += titles_
    return SentencePairKorpusData('KoreanHateSpeech.unlabeled', texts, titles)",len(texts_) != len(titles_),60,len(texts_) != len(titles_),True,100.00000000000004,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__(description, license)
<mask>:
        root_dir = default_korpora_path
    fetch_naverchangwon_ner(root_dir, force_download)
    info = NAVER_CHANGWON_NER_FETCH_INFORMATION[0]
    local_path = os.path.join(os.path.abspath(root_dir), info['destination'])
    self.train = WordTagKorpusData('NaverChangwonNER.train', *self.cleaning(load_text(local_path, num_heads=0)))",root_dir is None,24,root_dir is None,True,100.00000000000004,N/A
"def cleaning(self, raw_lines: List[str]):
    separated_lines = [line.split('\t') for line in raw_lines]
    all_texts, all_words, all_tags, text, words, tags = ([], [], [], '', [], [])
    for separated_line in separated_lines:
<mask>:
            _, word, tag = separated_line
            text += word + ' '
            words.append(word)
            tags.append(tag)
        else:
            all_texts.append(text)
            all_words.append(words)
            all_tags.append(tags)
            text, words, tags = ('', [], [])
    return (all_texts, all_words, all_tags)",len(separated_line) == 3,57,len(separated_line) > 1,False,60.042877124855906,N/A
"def create_lmdata(args):
    os.makedirs(os.path.abspath(args.output_dir), exist_ok=True)
    sampling_ratio = args.sampling_ratio
<mask>:
        sampling_ratio = float(sampling_ratio)
        if not 0 < sampling_ratio < 1:
            raise ValueError('`sampling_ratio` must be None or (0, 1) float')
    n_first_samples = args.n_first_samples
    np.random.seed(args.seed)
    selector = Selector(sampling_ratio, args.min_length, args.max_length)
    root_dir = args.root_dir
    if root_dir is None:
        root_dir = default_korpora_path
    force_download = args.force_download
    multilingual = args.multilingual
    corpus_names = check_corpus(root_dir, args.corpus)
    status = [['', name, ' - ', ''] for name in corpus_names]
    for i_corpus, name in enumerate(corpus_names):
        if not args.save_each and i_corpus > 0:
            mode = 'a'
        else:
            mode = 'w'
        filename = f'{name}.train' if args.save_each else 'all.train'
        lmdata_path = f'{args.output_dir}/{filename}'
        sent_iterator = tqdm(ITERATE_TEXTS[name](root_dir, force_download, multilingual), desc=f'Create train data from {name}')
        print_status(status)
        n_sampled = 0
        with open(lmdata_path, mode, encoding='utf-8') as f:
            for i_sent, sent in enumerate(sent_iterator):
                if not selector.use(sent):
                    continue
                f.write(f'{sent}\n')
                n_sampled += 1
                if n_first_samples is not None and n_first_samples <= n_sampled:
                    break
        status[i_corpus][0] = ' x '
        status[i_corpus][2] = n_sampled
        status[i_corpus][3] = filename
    print_status(status)",sampling_ratio is not None,152,sampling_ratio is not None,True,100.00000000000004,N/A
"def __init__(self, sampling_ratio, min_length, max_length):
<mask>:
        min_length = None
    if isinstance(max_length, int) and max_length < 0:
        max_length = None
    self.sampling_ratio = sampling_ratio
    self.min_length = min_length
    self.max_length = max_length","isinstance(min_length, int) and min_length < 0",28,"isinstance(min_length, int) and min_length < 0",True,100.00000000000004,N/A
"def use(self, text):
    length = len(text)
<mask>:
        return False
    if self.max_length is not None and length > self.max_length:
        return False
    if self.sampling_ratio is None:
        return True
    return np.random.rand() < self.sampling_ratio",self.min_length is not None and length < self.min_length,30,len(text) < self.min_length,False,28.372088243992376,N/A
"def check_corpus(root_dir, corpus_names):
<mask>:
        corpus_names = list(ITERATE_TEXTS)
    if isinstance(corpus_names, str):
        corpus_names = [corpus_names]
    available = []
    for name in corpus_names:
        if name not in ITERATE_TEXTS:
            print(f'Not provide {name} corpus. Check the `corpus` argument')
            continue
        if Korpora.exists(name, root_dir=root_dir):
            available.append(name)
    if not available:
        raise ValueError('Not found any proper corpus name. Check the `corpus` argument')
    return available",corpus_names == 'all' or corpus_names[0] == 'all',54,corpus_names is None,False,4.405925425025207,N/A
"def print_status(status):
    max_len = max(max((len(row[3]) for row in status)), 9)
    form = '| {:4} | {:25} | {:10} | {} |'
    print('\n\n' + form.format('Done', 'Corpus name', 'Num sents', 'File name' + ' ' * (max_len - 9)))
    print(form.format('-' * 4, '-' * 25, '-' * 10, '-' * max_len))
    for finish, name, num_sent, filename in status:
<mask>:
            filename = ' ' * max_len
        else:
            filename += ' ' * (max_len - len(filename))
        print(form.format(finish, name, num_sent, filename))",not filename,76,filename is None,False,27.516060407455225,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__(description, license)
<mask>:
        root_dir = default_korpora_path
    fetch_open_subtitles(root_dir, force_download)
    sources, targets = ([], [])
    for info in OPEN_SUBTITLES_FETCH_INFORMATION:
        local_path = os.path.join(os.path.abspath(root_dir), info['destination'])[:-3]
        sources_, targets_ = parse_xtm(local_path)
        sources += sources_
        targets += targets_
    self.train = SentencePairKorpusData('OpenSubtitle.train', targets, sources)",root_dir is None,40,root_dir is None,True,100.00000000000004,N/A
"def parse_xtm(path):
    pattern = re.compile('<seg>[\\S ]+</seg>')

    def parse_segment(line):
        seg = pattern.findall(line)[0]
        return seg[5:-6]
    sources = []
    targets = []
    mode = 0
    source, target = (None, None)
    with open(path, encoding='utf-8') as f:
        for line in f:
            line = line.strip()
<mask>:
                mode += 1
                continue
            elif line[:5] == '</tu>':
                mode = 0
                if source is not None and target is not None:
                    sources.append(source)
                    targets.append(target)
                source, target = (None, None)
                continue
            try:
                if mode == 1:
                    source = parse_segment(line)
                    mode += 1
                    continue
                if mode == 2:
                    target = parse_segment(line)
                    continue
            except:
                mode = 0
    return (sources, targets)",line[:4] == '<tu>',96,line == '<tu>',False,52.734307450329375,N/A
"def show_arguments(args):
    print('## Arguments of Korpora CLI ##')
    for name, var in sorted(vars(args).items()):
<mask>:
            print(f'  - {name} : {var.__name__}')
        else:
            print(f'  - {name} : {var}')",callable(var),25,"isinstance(var, str)",False,17.965205598154213,N/A
"def check_exists(corpus_name, informations, root_dir=None):
<mask>:
        root_dir = default_korpora_path
    all_is_ok = True
    for information in informations:
        local_installed_path = os.path.join(root_dir, information['destination'])
        if not os.path.exists(local_installed_path):
            print(f'Not found {local_installed_path}')
            all_is_ok = False
    if not all_is_ok:
        print(f'Install corpus using `Korpora.fetch(""{corpus_name}"")`')
    return all_is_ok",root_dir is None,37,root_dir is None,True,100.00000000000004,N/A
"def load_text(path, num_heads=0, num_samples=-1):
    lines = []
    with open(path, encoding='utf-8') as f:
<mask>:
            for _ in range(num_heads):
                next(f)
        for i, line in enumerate(f):
            if num_samples > 0 and i >= num_samples:
                break
            lines.append(line.rstrip('\n'))
    return lines",num_heads > 0,35,num_heads > 0,True,100.00000000000004,N/A
"def load_parallel_text(source_path, target_path, num_heads=0, num_samples=-1):
    sources = load_text(source_path, num_heads, num_samples)
    targets = load_text(target_path, num_heads, num_samples)
<mask>:
        raise ValueError('Parallel corpus must have same length two files')
    return (sources, targets)",len(sources) != len(targets),28,len(sources) != len(targets),True,100.00000000000004,N/A
"def load_wikitext(path, num_lines=-1):
    """"""
    Wikitext format

         = Head1 =

        text ...
        text ...

         = = 2ead = =

        text ...
        text ...
    """"""
<mask>:
        with open(path, encoding='utf-8') as f:
            texts = f.read().split('\n =')
    else:
        lines = []
        with open(path, encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= num_lines:
                    break
                lines.append(line)
        texts = ''.join(lines).split('\n =')
    texts = [texts[0]] + [f' ={text}' for text in texts[1:]]
    return texts",num_lines <= 0,69,os.path.isfile(path),False,0.0,N/A
"@classmethod
def load(cls, corpus_names, root_dir=None, force_download=False):
    return_single = isinstance(corpus_names, str)
<mask>:
        corpus_names = [corpus_names]
    corpora = [KORPUS[corpus_name](root_dir, force_download) for corpus_name in corpus_names]
    if return_single:
        return corpora[0]
    return corpora",return_single,28,"not isinstance(corpus_names, list)",False,4.767707020457095,N/A
"@classmethod
def fetch(cls, corpus_name, root_dir=None, force_download=False):
<mask>:
        corpus_name = sorted(FETCH.keys())
    if isinstance(corpus_name, str):
        corpus_name = [corpus_name]
    corpus_name = [name for name in corpus_name if name[:5] != 'modu_' and name[:6] != 'aihub_']
    for name in corpus_name:
        if name not in FETCH:
            raise ValueError(f'Support only f{sorted(FETCH.keys())}')
    if root_dir is None:
        root_dir = default_korpora_path
    for name in corpus_name:
        fetch_func = FETCH[name]
        fetch_func(root_dir, force_download)",corpus_name.lower() == 'all',60,corpus_name is None,False,14.628187563941408,N/A
"@classmethod
def exists(cls, corpus_name, root_dir=None, return_by_each_corpus=False):
<mask>:
        corpus_name = sorted(KORPUS.keys())
    elif isinstance(corpus_name, str):
        corpus_name = [corpus_name]
    if root_dir is None:
        root_dir = default_korpora_path
    corpora = [KORPUS[name].exists(root_dir=root_dir) for name in corpus_name]
    if return_by_each_corpus:
        return corpora
    return all(corpora)",corpus_name == 'all' or corpus_name[0] == 'all',36,corpus_name is None,False,4.405925425025207,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__()
    paths = ModuKorpus.get_corpus_path(root_dir, 'NIKL_MP', find_corpus_paths)
<mask>:
        raise ValueError('Not found corpus files. Check `root_dir`')
    self.train = KorpusData('모두의_형태분석_말뭉치.train', load_modu_morpheme(paths))
    self.tagmap = {'JKS': '주격조사', 'JKC': '보격조사', 'JKG': '관형격조사', 'JKO': '목적격조사', 'JKB': '부사격조사', 'JKV': '호격조사', 'JKQ': '인용격조사', 'JX': '보조사', 'JC': '접속조사', 'EP': '선어말어미', 'EF': '종결어미', 'EC': '연결어미', 'ETN': '명사형전성어미', 'ETM': '관형형전성어미', 'XPN': '체언접두사', 'XSN': '명사파생접미사', 'XSV': '동사파생접미사', 'XSA': '형용사파생접미사', 'XR': '어근', 'SF': '마침표, 물음표, 느낌표', 'SP': '쉼표, 가운뎃점, 콜론, 빗금', 'SS': '따옴표, 괄호표, 줄표', 'SE': '줄임표', 'SO': '붙임표(물결)', 'SW': '기타 기호', 'SL': '외국어', 'SH': '한자', 'SN': '숫자', 'NA': '분석불능범주', 'NF': '명사추정범주', 'NV': '용언추정범주', 'NNG': '일반명사', 'NNP': '고유명사', 'NNB': '의존명사', 'NP': '대명사', 'NR': '수사', 'VV': '동사', 'VA': '형용사', 'VX': '보조용언', 'VCP': '긍정지정사', 'VCN': '부정지정사', 'MMA': '성상 관형사', 'MMD': '지시 관형사', 'MMN': '수 관형사', 'MAG': '일반부사', 'MAJ': '접속부사', 'IC': '감탄사', 'NAP': '이름과 같은 개인정보'}",not paths,133,not paths,True,100.00000000000004,N/A
"def find_corpus_paths(root_dir_or_paths):
    prefix_pattern = re.compile('[NS]XMP')

    def match(path):
        prefix = path.split(os.path.sep)[-1][:4]
        return prefix_pattern.match(prefix)
<mask>:
        paths = sorted(glob(f'{root_dir_or_paths}/*.json') + glob(root_dir_or_paths))
    else:
        paths = root_dir_or_paths
    paths = [path for path in paths if match(path)]
    return paths","isinstance(root_dir_or_paths, str)",33,"isinstance(root_dir_or_paths, list)",False,82.651681837938,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__(description, license)
<mask>:
        root_dir = default_korpora_path
    fetch_chatbot(root_dir, force_download)
    local_path = os.path.join(os.path.abspath(root_dir), KOREAN_CHATBOT_FETCH_INFORMATION[0]['destination'])
    with open(local_path, 'r', encoding='utf-8') as f:
        questions, answers, labels = self.cleaning(csv.reader(f, delimiter=','))
    self.train = LabeledSentencePairKorpusData('KoreanChatbot.train', questions, answers, labels)",root_dir is None,34,root_dir is None,True,100.00000000000004,N/A
"def cleaning(self, examples):
    next(examples)
    examples = [example for example in examples]
    for i_sent, example in enumerate(examples):
<mask>:
            raise ValueError(f'Found some errors in line {i_sent}: {example}')
    questions, answers, labels = zip(*examples)
    labels = [int(label) for label in labels]
    return (questions, answers, labels)",len(example) != 3,41,i_sent >= self.max_i_sent,False,3.3864985683445354,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__(description, license)
<mask>:
        root_dir = default_korpora_path
    fetch_kowikitext(root_dir, force_download)
    for information in KOWIKI_FETCH_INFORMATION:
        destination = information['destination']
        local_path = os.path.join(os.path.abspath(root_dir), destination[:-4])
        if 'train' in destination:
            response = input('kowikiText.train text file is large (1.6G).\nIf you want to load text in your memory, please insert `yes`\nIf the `INPUT` is integer, it loads only first `INPUT` sentences\n').lower()
            if len(response) == 1 and response == 'y' or response == 'yes':
                texts, titles = self.load(local_path)
                self.train = SentencePairKorpusData('KowikiText.train', texts, titles)
            elif response.isdigit():
                texts, titles = self.load(local_path, num_lines=int(response))
                self.train = SentencePairKorpusData('KowikiText.train', texts, titles)
            else:
                dirname = os.path.abspath(f'{root_dir}/kowikitext')
                self.train = f'kowikitext corpus is downloaded. Open local directory {dirname}'
                print('Continue to load `dev` and `test`')
            continue
        texts, titles = self.load(local_path)
        if 'dev' in destination:
            self.dev = SentencePairKorpusData('KowikiText.dev', texts, titles)
        elif 'test' in destination:
            self.test = SentencePairKorpusData('KowikiText.test', texts, titles)
        else:
            raise ValueError(f'Check local files')",root_dir is None,137,root_dir is None,True,100.00000000000004,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__(description, license)
<mask>:
        root_dir = default_korpora_path
    fetch_questionpair(root_dir, force_download)
    for info in QUESTION_PAIR_FETCH_INFORMATION:
        local_path = os.path.join(os.path.abspath(root_dir), info['destination'])
        is_train = 'train' in info['destination']
        with open(local_path, 'r', encoding='utf-8') as f:
            texts, pairs, labels = self.cleaning(csv.reader(f, delimiter=','), is_train)
            dataname = 'QuestionPair.train' if is_train else 'QuestionPair.test'
            data = LabeledSentencePairKorpusData(dataname, texts, pairs, labels)
        if is_train:
            self.train = data
        else:
            self.test = data",root_dir is None,60,root_dir is None,True,100.00000000000004,N/A
"def cleaning(self, examples, is_train):
    next(examples)
    examples = [example for example in examples]
    for i, example in enumerate(examples):
<mask>:
            raise ValueError(f'Found some errors in line {i}: {example}')
    if is_train:
        _, _, _, texts, pairs, labels = zip(*examples)
    else:
        _, texts, pairs, labels, _ = zip(*examples)
    return (texts, pairs, labels)",is_train and len(example) != 6 or (not is_train and len(example) != 5),48,len(examples) > self.max_line_size,False,2.6520945228289934,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__()
    paths = ModuKorpus.get_corpus_path(root_dir, 'NIKL_WEB', find_corpus_paths)
<mask>:
        raise ValueError('Not found corpus files. Check `root_dir`')
    self.train = KorpusData('모두의_웹_말뭉치.train', load_modu_web(paths))",not paths,22,not paths,True,100.00000000000004,N/A
"def find_corpus_paths(root_dir_or_paths):
    prefix_pattern = re.compile('E[BPSR]RW')

    def match(path):
        prefix = path.split(os.path.sep)[-1][:4]
        return prefix_pattern.match(prefix)
<mask>:
        paths = sorted(glob(f'{root_dir_or_paths}/*.json') + glob(root_dir_or_paths))
    else:
        paths = root_dir_or_paths
    paths = [path for path in paths if match(path)]
    return paths","isinstance(root_dir_or_paths, str)",33,"isinstance(root_dir_or_paths, list)",False,82.651681837938,N/A
"def __init__(self, root_dir=None, force_download=False, prefix='', name='AIHub_translation'):
    super().__init__(description, license)
    paths = AIHubTranslationKorpus.get_corpus_path(root_dir, prefix, find_corpus_paths)
<mask>:
        raise ValueError('Not found corpus files. Check `root_dir`')
    self.train = SentencePairKorpusData(f'{name}.train', *load_aihub_translation(paths, name))",not paths,26,not paths,True,100.00000000000004,N/A
"@classmethod
def get_corpus_path(cls, root_dir=None, prefix='', finder=None):
<mask>:
        root_dir = os.path.join(default_korpora_path, 'AIHub_Translation', prefix)
    elif isinstance(root_dir, str) and os.path.isdir(root_dir):
        root_dir = os.path.join(root_dir, 'AIHub_Translation', prefix)
    paths = []
    if callable(finder):
        paths = finder(root_dir)
    return paths",root_dir is None,32,root_dir is None and default_korpora_path,False,35.08439695638686,N/A
"def find_corpus_paths(root_dir, suffix='200226.xlsx'):

    def match(path):
        return path[-11:] == suffix
<mask>:
        paths = sorted(glob(f'{root_dir}/*{suffix}') + glob(root_dir))
    else:
        paths = root_dir
    paths = [path for path in paths if match(path)]
    return paths","isinstance(root_dir, str)",30,os.path.isdir(f'{root_dir}/*{suffix}'),False,8.562365224473284,N/A
"def load_aihub_translation(paths, name):
    sources, targets = ([], [])
    for i_path, path in enumerate(tqdm(paths, desc=f'Loading {name}', total=len(paths))):
        workbook = openpyxl.load_workbook(path)
        sheet = workbook[workbook.sheetnames[0]]
        header = sheet[1]
<mask>:
            raise ValueError(f'The second last column and last column in header must be (""원문"", ""번역문"")')
        for row in sheet.iter_rows(min_row=2):
            sources.append(row[-2].value.strip())
            targets.append(row[-1].value.strip())
    return (sources, targets)",not (header[-2].value.strip() == '원문' and header[-1].value.strip() == '번역문'),49,header.column != 2,False,0.19565086277504187,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__(description, license)
<mask>:
        root_dir = default_korpora_path
    fetch_namuwikitext(root_dir, force_download)
    for information in NAMUWIKI_FETCH_INFORMATION:
        destination = information['destination']
        local_path = os.path.join(os.path.abspath(root_dir), destination[:-4])
        if 'train' in destination:
            response = input('NamuwikiText.train text file is large (5.3G).\nIf you want to load text in your memory, please insert `yes`\nIf the `INPUT` is integer, it loads only first `INPUT` sentences\n').lower()
            if len(response) == 1 and response == 'y' or response == 'yes':
                texts, titles = self.load(local_path)
                self.train = SentencePairKorpusData('NamuwikiText.train', texts, titles)
            elif response.isdigit():
                texts, titles = self.load(local_path, num_lines=int(response))
                self.train = SentencePairKorpusData('NamuwikiText.train', texts, titles)
            else:
                dirname = os.path.abspath(f'{root_dir}/namiwiki')
                self.train = f'Namuwikitext corpus is downloaded. Open local directory {dirname}'
                print('Continue to load `dev` and `test`')
            continue
        texts, titles = self.load(local_path)
        if 'dev' in destination:
            self.dev = SentencePairKorpusData('NamuwikiText.dev', texts, titles)
        elif 'test' in destination:
            self.test = SentencePairKorpusData('NamuwikiText.test', texts, titles)
        else:
            raise ValueError(f'Check local files')",root_dir is None,137,root_dir is None,True,100.00000000000004,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__(description, license)
<mask>:
        root_dir = default_korpora_path
    fetch_kornli(root_dir, force_download)
    for info in KORNLI_FETCH_INFORMATION:
        local_path = os.path.join(os.path.abspath(root_dir), info['destination'])
        if 'multinli.train' in info['destination']:
            self.multinli_train = LabeledSentencePairKorpusData('KorNLI.multinli_train', *self.cleaning(load_text(local_path, num_heads=1)))
        elif 'snli_1.0_train' in info['destination']:
            self.snli_train = LabeledSentencePairKorpusData('KorNLI.snli_1.0_train', *self.cleaning(load_text(local_path, num_heads=1)))
        elif 'xnli.dev' in info['destination']:
            self.xnli_dev = LabeledSentencePairKorpusData('KorNLI.xnli_dev', *self.cleaning(load_text(local_path, num_heads=1)))
        else:
            self.xnli_test = LabeledSentencePairKorpusData('KorNLI.xnli_test', *self.cleaning(load_text(local_path, num_heads=1)))",root_dir is None,53,root_dir is None,True,100.00000000000004,N/A
"def cleaning(self, raw_lines: List[str]):
    separated_lines = [line.split('\t') for line in raw_lines]
    for i_sent, separated_line in enumerate(separated_lines):
<mask>:
            raise ValueError(f'Found some errors in line {i_sent}: {separated_line}')
    texts, pairs, labels = zip(*separated_lines)
    return (texts, pairs, labels)",len(separated_line) != 3,34,i_sent not in self.errors,False,4.8734989388136185,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__()
    paths = ModuKorpus.get_corpus_path(root_dir, 'NIKL_SPOKEN', find_corpus_paths)
<mask>:
        raise ValueError('Not found corpus files. Check `root_dir`')
    self.train = KorpusData('모두의_구어_말뭉치.train', load_modu_spoken(paths))",not paths,22,not paths,True,100.00000000000004,N/A
"def find_corpus_paths(root_dir_or_paths):
    prefix_pattern = re.compile('S[ABDE]RW')

    def match(path):
        prefix = path.split(os.path.sep)[-1][:4]
        return prefix_pattern.match(prefix)
<mask>:
        paths = sorted(glob(f'{root_dir_or_paths}/*.json') + glob(root_dir_or_paths))
    else:
        paths = root_dir_or_paths
    paths = [path for path in paths if match(path)]
    return paths","isinstance(root_dir_or_paths, str)",33,"isinstance(root_dir_or_paths, list)",False,82.651681837938,N/A
"def document_to_texts(document):
    utterance = document['utterance']
    texts = []
    buffer = []
    speaker_id = -1
    for u in utterance:
        form = u['original_form']
        speaker_id_u = u['speaker_id']
<mask>:
            texts.append(' '.join(buffer))
            buffer = []
        buffer.append(form)
        speaker_id = speaker_id_u
    texts.append(' '.join(buffer))
    return texts",speaker_id != speaker_id_u and buffer,38,speaker_id == -1,False,11.943865131127652,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__()
    paths = ModuKorpus.get_corpus_path(root_dir, 'NIKL_NE', find_corpus_paths)
<mask>:
        raise ValueError('Not found corpus files. Check `root_dir`')
    self.train = KorpusData('모두의_개체명_말뭉치.train', load_modu_ne(paths))
    self.tagmap = {'PS': 'PERSON', 'LC': 'LOCATION', 'OG': 'ORGANIZATION', 'AF': 'ARTIFACT', 'DT': 'DATE', 'TI': 'TIME', 'CV': 'CIVILIZATION', 'AM': 'ANIMAL', 'PT': 'PLANT', 'QT': 'QUANTITY', 'FD': 'STUDY_FIELD', 'TR': 'THEORY', 'EV': 'EVENT', 'MT': 'MATERIAL', 'TM': 'TERM'}",not paths,54,not paths,True,100.00000000000004,N/A
"def find_corpus_paths(root_dir_or_paths):
    prefix_pattern = re.compile('[NS]XNE')

    def match(path):
        prefix = path.split(os.path.sep)[-1][:4]
        return prefix_pattern.match(prefix)
<mask>:
        paths = sorted(glob(f'{root_dir_or_paths}/*.json') + glob(root_dir_or_paths))
    else:
        paths = root_dir_or_paths
    paths = [path for path in paths if match(path)]
    return paths","isinstance(root_dir_or_paths, str)",33,"isinstance(root_dir_or_paths, list)",False,82.651681837938,N/A
"def __init__(self, root_dir=None, force_download=False):
    super().__init__(description, license)
<mask>:
        root_dir = default_korpora_path
    fetch_kcbert(root_dir, force_download)
    response = input('KcBERT text file is large (12G).\nIf you want to load text in your memory, please insert `yes`\nIf the `INPUT` is integer, it loads only first `INPUT` sentences\n').lower()
    if len(response) == 1 and response == 'y' or response == 'yes':
        self.train = KorpusData('KcBERT.train', load_text(f'{root_dir}/kcbert/20190101_20200611_v2.txt'))
    elif response.isdigit():
        self.train = KorpusData('KcBERT.train', load_text(f'{root_dir}/kcbert/20190101_20200611_v2.txt', num_samples=int(response)))
    else:
        dirname = os.path.abspath(f'{root_dir}/kcbert')
        self.train = f'KcBERT corpus is downloaded. Open local directory {dirname}'",root_dir is None,78,root_dir is None,True,100.00000000000004,N/A
"def fetch_kcbert(root_dir, force_download):
    for info in KCBERT_FETCH_INFORMATION:
        local_path = os.path.join(os.path.abspath(root_dir), info['destination'])
        fetch(info['url'], local_path, 'kcbert', force_download)
<mask>:
        return None
    if platform.system().lower() == 'windows':
        print(f'Korpora does not support KcBERT fetch in Windows OS.Please open local directory {root_dir} and unzip manually tar files')
        return None
    cwd = os.getcwd()
    print('Unzip tar. It needs a few minutes ... ', end='')
    os.chdir(f'{root_dir}/kcbert/')
    os.system('cat kcbert-train.tar.gz* | tar -zxvpf -')
    print('done')
    os.chdir(cwd)",os.path.exists(f'{root_dir}/kcbert/20190101_20200611_v2.txt'),64,not os.path.exists(local_path),False,16.666838052420022,N/A
"def fetch_test(args):
    exclusive_fetch_test = {'aihub_conversation_translation', 'aihub_decree_translation', 'aihub_government_website_translation', 'aihub_korean_culture_translation', 'aihub_news_translation', 'aihub_spoken_translation', 'aihub_translation', 'modu_messenger', 'modu_mp', 'modu_ne', 'modu_news', 'modu_spoken', 'modu_web', 'modu_written'}
    for corpus_name in corpus_list:
<mask>:
            continue
        Korpora.fetch(corpus_name, root_dir=args.root_dir)
        time.sleep(0.5)",corpus_name in exclusive_fetch_test,27,corpus_name in exclusive_fetch_test,True,100.00000000000004,N/A
"def load_small_test(args):
    exclusive_load_test = {'kcbert', 'kowikitext', 'namuwikitext', 'modu_messenger', 'modu_mp', 'modu_ne', 'modu_news', 'modu_spoken', 'modu_web', 'modu_written'}
    for corpus_name in corpus_list:
<mask>:
            continue
        with suppress_stdout():
            corpus = Korpora.load(corpus_name, root_dir=args.root_dir)
        bar = '=' * 80
        print(corpus, end=f'\n\n{bar}\n\n', flush=True)
        time.sleep(0.5)",corpus_name in exclusive_load_test,35,corpus_name in exclusive_load_test,True,100.00000000000004,N/A
"def print_reply(reply):
<mask>:
        print('Access accepted')
    else:
        print('Access denied')
    print('Attributes returned by server:')
    for i in reply.keys():
        print('%s: %s' % (i, reply[i]))",reply.code == AccessAccept,21,reply.get('accepted'),False,16.233395773754953,N/A
"def test_auth1():
    global client
    try:
        loop.run_until_complete(asyncio.ensure_future(client.initialize_transports(enable_auth=True, local_addr='127.0.0.1', local_auth_port=8000, enable_acct=True, enable_coa=True)))
        req = client.CreateAuthPacket(User_Name='wichert')
        req['NAS-IP-Address'] = '192.168.1.10'
        req['NAS-Port'] = 0
        req['Service-Type'] = 'Login-User'
        req['NAS-Identifier'] = 'trillian'
        req['Called-Station-Id'] = '00-04-5F-00-0F-D1'
        req['Calling-Station-Id'] = '00-01-24-80-B3-9C'
        req['Framed-IP-Address'] = '10.0.0.100'
        future = client.SendPacket(req)
        loop.run_until_complete(asyncio.ensure_future(asyncio.gather(future, return_exceptions=True)))
<mask>:
            print('EXCEPTION ', future.exception())
        else:
            reply = future.result()
            if reply.code == AccessAccept:
                print('Access accepted')
            else:
                print('Access denied')
            print('Attributes returned by server:')
            for i in reply.keys():
                print('%s: %s' % (i, reply[i]))
        loop.run_until_complete(asyncio.ensure_future(client.deinitialize_transports()))
        print('END')
        del client
    except Exception as exc:
        print('Error: ', exc)
        print('\n'.join(traceback.format_exc().splitlines()))
        loop.run_until_complete(asyncio.ensure_future(client.deinitialize_transports()))
    loop.close()",future.exception(),83,future.exception(),True,100.00000000000004,N/A
"def test_multi_auth():
    global client
    try:
        loop.run_until_complete(asyncio.ensure_future(client.initialize_transports(enable_auth=True, local_addr='127.0.0.1', local_auth_port=8000, enable_acct=True, enable_coa=True)))
        reqs = []
        for i in range(255):
            req = create_request(client, 'user%s' % i)
            future = client.SendPacket(req)
            reqs.append(future)
        loop.run_until_complete(asyncio.ensure_future(asyncio.gather(*reqs, return_exceptions=True)))
        for future in reqs:
<mask>:
                print('EXCEPTION ', future.exception())
            else:
                reply = future.result()
                print_reply(reply)
        loop.run_until_complete(asyncio.ensure_future(client.deinitialize_transports()))
        print('END')
        del client
    except Exception as exc:
        print('Error: ', exc)
        print('\n'.join(traceback.format_exc().splitlines()))
        loop.run_until_complete(asyncio.ensure_future(client.deinitialize_transports()))
    loop.close()",future.exception(),56,future.exception(),True,100.00000000000004,N/A
"def testSubTlvParsing(self):
    for attr, _, _ in self.simple_dict_values:
<mask>:
            self.assertEqual(self.dict[attr].is_sub_attribute, True)
            self.assertEqual(self.dict[attr].parent, self.dict['Test-Tlv'])
        else:
            self.assertEqual(self.dict[attr].is_sub_attribute, False)
            self.assertEqual(self.dict[attr].parent, None)
    full_dict = Dictionary(os.path.join(self.path, 'full'))
    self.assertEqual(full_dict['Simplon-Tlv-Str'].is_sub_attribute, True)
    self.assertEqual(full_dict['Simplon-Tlv-Str'].parent, full_dict['Simplon-Tlv'])
    self.assertEqual(full_dict['Simplon-Tlv-Int'].is_sub_attribute, True)
    self.assertEqual(full_dict['Simplon-Tlv-Int'].parent, full_dict['Simplon-Tlv'])",attr.startswith('Test-Tlv-'),30,attr == 'Test-Tlv',False,9.688464563433238,N/A
"def __init__(self, domain, type, data=None):
    self.domain = domain
    self.type = type
    self.closed = False
    self.options = []
    self.address = None
    self.output = []
<mask>:
        self.read_end, self.write_end = os.pipe()
        fcntl.fcntl(self.write_end, fcntl.F_SETFL, os.O_NONBLOCK)
        os.write(self.write_end, data)
        self.data = data
    else:
        self.read_end = 1
        self.write_end = None",data is not None,43,data,False,4.9787068367863965,N/A
"def MockClassMethod(klass, name, myfunc=None):

    def func(self, *args, **kwargs):
<mask>:
            self.called = []
        self.called.append((name, args, kwargs))
    key = origkey(klass)
    if not hasattr(klass, key):
        setattr(klass, key, {})
    getattr(klass, key)[name] = getattr(klass, name)
    if myfunc is None:
        setattr(klass, name, func)
    else:
        setattr(klass, name, myfunc)","not hasattr(self, 'called')",41,self.called is None,False,7.16047614494885,N/A
"def UnmockClassMethods(klass):
    key = origkey(klass)
<mask>:
        return
    for name, func in getattr(klass, key).items():
        setattr(klass, name, func)
    delattr(klass, key)","not hasattr(klass, key)",18,not key,False,5.804285916064729,N/A
"def __init__(self, addresses=[], authport=1812, acctport=1813, coaport=3799, hosts=None, dict=None, auth_enabled=True, acct_enabled=True, coa_enabled=False):
    """"""Constructor.

        :param     addresses: IP addresses to listen on
        :type      addresses: sequence of strings
        :param      authport: port to listen on for authentication packets
        :type       authport: integer
        :param      acctport: port to listen on for accounting packets
        :type       acctport: integer
        :param       coaport: port to listen on for CoA packets
        :type        coaport: integer
        :param         hosts: hosts who we can talk to
        :type          hosts: dictionary mapping IP to RemoteHost class instances
        :param          dict: RADIUS dictionary to use
        :type           dict: Dictionary class instance
        :param  auth_enabled: enable auth server (default True)
        :type   auth_enabled: bool
        :param  acct_enabled: enable accounting server (default True)
        :type   acct_enabled: bool
        :param   coa_enabled: enable coa server (default False)
        :type    coa_enabled: bool
        """"""
    host.Host.__init__(self, authport, acctport, coaport, dict)
<mask>:
        self.hosts = {}
    else:
        self.hosts = hosts
    self.auth_enabled = auth_enabled
    self.authfds = []
    self.acct_enabled = acct_enabled
    self.acctfds = []
    self.coa_enabled = coa_enabled
    self.coafds = []
    for addr in addresses:
        self.BindToAddress(addr)",hosts is None,155,hosts is None,True,100.00000000000004,N/A
"def BindToAddress(self, addr):
    """"""Add an address to listen on a specific interface.
        String ""0.0.0.0"" indicates you want to listen on all interfaces.

        :param addr: IP address to listen on
        :type  addr: string
        """"""
    addrFamily = self._GetAddrInfo(addr)
    for family, address in addrFamily:
<mask>:
            authfd = socket.socket(family, socket.SOCK_DGRAM)
            authfd.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            authfd.bind((address, self.authport))
            self.authfds.append(authfd)
        if self.acct_enabled:
            acctfd = socket.socket(family, socket.SOCK_DGRAM)
            acctfd.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            acctfd.bind((address, self.acctport))
            self.acctfds.append(acctfd)
        if self.coa_enabled:
            coafd = socket.socket(family, socket.SOCK_DGRAM)
            coafd.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            coafd.bind((address, self.coaport))
            self.coafds.append(coafd)",self.auth_enabled,76,self.auth_enabled,True,100.00000000000004,N/A
"def _AddSecret(self, pkt):
    """"""Add secret to packets received and raise ServerPacketError
        for unknown hosts.

        :param pkt: packet to process
        :type  pkt: Packet class instance
        """"""
<mask>:
        pkt.secret = self.hosts[pkt.source[0]].secret
    elif '0.0.0.0' in self.hosts:
        pkt.secret = self.hosts['0.0.0.0'].secret
    else:
        raise ServerPacketError('Received packet from unknown host')",pkt.source[0] in self.hosts,43,pkt.source[0] in self.hosts,True,100.00000000000004,N/A
"def _HandleAuthPacket(self, pkt):
    """"""Process a packet received on the authentication port.
        If this packet should be dropped instead of processed a
        ServerPacketError exception should be raised. The main loop will
        drop the packet and log the reason.

        :param pkt: packet to process
        :type  pkt: Packet class instance
        """"""
    self._AddSecret(pkt)
<mask>:
        raise ServerPacketError('Received non-authentication packet on authentication port')
    self.HandleAuthPacket(pkt)",pkt.code != packet.AccessRequest,58,not self._CheckAuthPacket(pkt),False,6.567274736060395,N/A
"def _HandleAcctPacket(self, pkt):
    """"""Process a packet received on the accounting port.
        If this packet should be dropped instead of processed a
        ServerPacketError exception should be raised. The main loop will
        drop the packet and log the reason.

        :param pkt: packet to process
        :type  pkt: Packet class instance
        """"""
    self._AddSecret(pkt)
<mask>:
        raise ServerPacketError('Received non-accounting packet on accounting port')
    self.HandleAcctPacket(pkt)","pkt.code not in [packet.AccountingRequest, packet.AccountingResponse]",58,not self._CheckAccountingPort(pkt),False,3.4331054109918173,N/A
"def _HandleProxyPacket(self, pkt):
    """"""Process a packet received on the reply socket.
        If this packet should be dropped instead of processed a
        :obj:`ServerPacketError` exception should be raised. The main loop
        will drop the packet and log the reason.

        :param pkt: packet to process
        :type  pkt: Packet class instance
        """"""
<mask>:
        raise ServerPacketError('Received packet from unknown host')
    pkt.secret = self.hosts[pkt.source[0]].secret
    if pkt.code not in [packet.AccessAccept, packet.AccessReject, packet.AccountingResponse]:
        raise ServerPacketError('Received non-response on proxy socket')",pkt.source[0] not in self.hosts,71,pkt.source[0] not in self.hosts,True,100.00000000000004,N/A
"def _ProcessInput(self, fd):
    """"""Process available data.
        If this packet should be dropped instead of processed a
        `ServerPacketError` exception should be raised. The main loop
        will drop the packet and log the reason.

        This function calls either :obj:`HandleAuthPacket`,
        :obj:`HandleAcctPacket` or :obj:`_HandleProxyPacket` depending on
        which socket is being processed.

        :param  fd: socket to read packet from
        :type   fd: socket class instance
        :param pkt: packet to process
        :type  pkt: Packet class instance
        """"""
<mask>:
        pkt = self._GrabPacket(lambda data, s=self: s.CreatePacket(packet=data), fd)
        self._HandleProxyPacket(pkt)
    else:
        Server._ProcessInput(self, fd)",fd.fileno() == self._proxyfd.fileno(),82,self._ProcessInput(fd),False,9.271103732443692,N/A
"def send_packet(self, packet, future):
<mask>:
        raise Exception('Packet with id %d already present' % packet.id)
    self.pending_requests[packet.id] = {'packet': packet, 'creation_date': datetime.now(), 'retries': 0, 'future': future, 'send_date': datetime.now()}
    self.transport.sendto(packet.RequestPacket())",packet.id in self.pending_requests,27,packet.id in self.pending_requests,True,100.00000000000004,N/A
"def connection_lost(self, exc):
<mask>:
        self.logger.warn('[%s:%d] Connection lost: %s', self.server, self.port, str(exc))
    else:
        self.logger.info('[%s:%d] Transport closed', self.server, self.port)",exc,17,exc,True,100.00000000000004,N/A
"def datagram_received(self, data, addr):
    try:
        reply = Packet(packet=data, dict=self.client.dict)
<mask>:
            req = self.pending_requests[reply.id]
            packet = req['packet']
            reply.dict = packet.dict
            reply.secret = packet.secret
            if packet.VerifyReply(reply, data):
                req['future'].set_result(reply)
                del self.pending_requests[reply.id]
            else:
                self.logger.warn('[%s:%d] Ignore invalid reply for id %d: %s', self.server, self.port, reply.id, data)
        else:
            self.logger.warn('[%s:%d] Ignore invalid reply: %s', self.server, self.port, data)
    except Exception as exc:
        self.logger.error('[%s:%d] Error on decode packet: %s', self.server, self.port, exc)",reply.code and reply.id in self.pending_requests,63,reply.id in self.pending_requests,False,64.11803884299549,N/A
"def __init__(self, server, auth_port=1812, acct_port=1813, coa_port=3799, secret=b'', dict=None, loop=None, retries=3, timeout=30, logger_name='pyrad'):
    """"""Constructor.

        :param    server: hostname or IP address of RADIUS server
        :type     server: string
        :param auth_port: port to use for authentication packets
        :type  auth_port: integer
        :param acct_port: port to use for accounting packets
        :type  acct_port: integer
        :param  coa_port: port to use for CoA packets
        :type   coa_port: integer
        :param    secret: RADIUS secret
        :type     secret: string
        :param      dict: RADIUS dictionary
        :type       dict: pyrad.dictionary.Dictionary
        :param      loop: Python loop handler
        :type       loop:  asyncio event loop
        """"""
<mask>:
        self.loop = asyncio.get_event_loop()
    else:
        self.loop = loop
    self.logger = logging.getLogger(logger_name)
    self.server = server
    self.secret = secret
    self.retries = retries
    self.timeout = timeout
    self.dict = dict
    self.auth_port = auth_port
    self.protocol_auth = None
    self.acct_port = acct_port
    self.protocol_acct = None
    self.protocol_coa = None
    self.coa_port = coa_port",not loop,127,loop is None,False,27.516060407455225,N/A
"def CreateAuthPacket(self, **args):
    """"""Create a new RADIUS packet.
        This utility function creates a new RADIUS packet which can
        be used to communicate with the RADIUS server this client
        talks to. This is initializing the new packet with the
        dictionary and secret used for the client.

        :return: a new empty packet instance
        :rtype:  pyrad.packet.Packet
        """"""
<mask>:
        raise Exception('Transport not initialized')
    return AuthPacket(dict=self.dict, id=self.protocol_auth.create_id(), secret=self.secret, **args)",not self.protocol_auth,64,self.protocol_auth is None,False,61.47881529512643,N/A
"def _SocketOpen(self):
    try:
        family = socket.getaddrinfo(self.server, 80)[0][0]
    except:
        family = socket.AF_INET
<mask>:
        self._socket = socket.socket(family, socket.SOCK_DGRAM)
        self._socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self._poll.register(self._socket, select.POLLIN)",not self._socket,21,family != socket.AF_INET6,False,7.267884212102741,N/A
"def _SendPacket(self, pkt, port):
    """"""Send a packet to a RADIUS server.

        :param pkt:  the packet to send
        :type pkt:   pyrad.packet.Packet
        :param port: UDP port to send packet to
        :type port:  integer
        :return:     the reply packet received
        :rtype:      pyrad.packet.Packet
        :raise Timeout: RADIUS server does not reply
        """"""
    self._SocketOpen()
    for attempt in range(self.retries):
<mask>:
            if 'Acct-Delay-Time' in pkt:
                pkt['Acct-Delay-Time'] = pkt['Acct-Delay-Time'][0] + self.timeout
            else:
                pkt['Acct-Delay-Time'] = self.timeout
        now = time.time()
        waitto = now + self.timeout
        self._socket.sendto(pkt.RequestPacket(), (self.server, port))
        while now < waitto:
            ready = self._poll.poll((waitto - now) * 1000)
            if ready:
                rawreply = self._socket.recv(4096)
            else:
                now = time.time()
                continue
            try:
                reply = pkt.CreateReply(packet=rawreply)
                if pkt.VerifyReply(reply, rawreply):
                    return reply
            except packet.PacketError:
                pass
            now = time.time()
    raise Timeout",attempt and pkt.code == packet.AccountingRequest,114,attempt == 0,False,7.888842466409752,N/A
"def SendPacket(self, pkt):
    """"""Send a packet to a RADIUS server.

        :param pkt: the packet to send
        :type pkt:  pyrad.packet.Packet
        :return:    the reply packet received
        :rtype:     pyrad.packet.Packet
        :raise Timeout: RADIUS server does not reply
        """"""
<mask>:
        if pkt.auth_type == 'eap-md5':
            password = pkt[2][0] if 2 in pkt else pkt[1][0]
            pkt[79] = [struct.pack('!BBHB%ds' % len(password), EAP_CODE_RESPONSE, packet.CurrentID, len(password) + 5, EAP_TYPE_IDENTITY, password)]
        reply = self._SendPacket(pkt, self.authport)
        if reply and reply.code == packet.AccessChallenge and (pkt.auth_type == 'eap-md5'):
            eap_code, eap_id, eap_size, eap_type, eap_md5 = struct.unpack('!BBHB%ds' % (len(reply[79][0]) - 5), reply[79][0])
            client_pw = pkt[2][0] if 2 in pkt else pkt[1][0]
            md5_challenge = hashlib.md5(struct.pack('!B', eap_id) + client_pw + eap_md5[1:]).digest()
            pkt[79] = [struct.pack('!BBHBB', 2, eap_id, len(md5_challenge) + 6, 4, len(md5_challenge)) + md5_challenge]
            pkt[24] = reply[24]
            reply = self._SendPacket(pkt, self.authport)
        return reply
    elif isinstance(pkt, packet.CoAPacket):
        return self._SendPacket(pkt, self.coaport)
    else:
        return self._SendPacket(pkt, self.acctport)","isinstance(pkt, packet.AuthPacket)",134,"isinstance(pkt, packet.Challenge)",False,70.71067811865478,N/A
"def EncodeString(origstr):
<mask>:
        raise ValueError('Can only encode strings of <= 253 characters')
    if isinstance(origstr, str):
        return origstr.encode('utf-8')
    else:
        return origstr",len(origstr) > 253,20,len(origstr) <= 253,False,43.47208719449914,N/A
"def EncodeOctets(octetstring):
<mask>:
        raise ValueError('Can only encode strings of <= 253 characters')
    if isinstance(octetstring, bytes) and octetstring.startswith(b'0x'):
        hexstring = octetstring.split(b'0x')[1]
        encoded_octets = binascii.unhexlify(hexstring)
    elif isinstance(octetstring, str) and octetstring.startswith('0x'):
        hexstring = octetstring.split('0x')[1]
        encoded_octets = binascii.unhexlify(hexstring)
    elif isinstance(octetstring, str) and octetstring.isdecimal():
        encoded_octets = struct.pack('>L', int(octetstring)).lstrip(b'\x00')
    else:
        encoded_octets = octetstring
    if len(encoded_octets) > 253:
        raise ValueError('Can only encode strings of <= 253 characters')
    return encoded_octets",len(octetstring) > 508,62,len(octetstring) > 253,False,75.98356856515926,N/A
"def EncodeAddress(addr):
<mask>:
        raise TypeError('Address has to be a string')
    return IPv4Address(addr).packed","not isinstance(addr, str)",12,"not isinstance(addr, str)",True,100.00000000000004,N/A
"def EncodeIPv6Prefix(addr):
<mask>:
        raise TypeError('IPv6 Prefix has to be a string')
    ip = IPv6Network(addr)
    return struct.pack('2B', *[0, ip.prefixlen]) + ip.ip.packed","not isinstance(addr, str)",20,"not isinstance(addr, string_types)",False,46.713797772819994,N/A
"def EncodeIPv6Address(addr):
<mask>:
        raise TypeError('IPv6 Address has to be a string')
    return IPv6Address(addr).packed","not isinstance(addr, str)",13,"not isinstance(addr, str)",True,100.00000000000004,N/A
"def __init__(self, fd, name, parentdir):
    self.lines = fd.readlines()
    self.length = len(self.lines)
    self.current = 0
    self.name = os.path.basename(name)
    path = os.path.dirname(name)
<mask>:
        self.dir = path
    else:
        self.dir = os.path.join(parentdir, path)",os.path.isabs(path),29,parentdir is None,False,0.0,N/A
"def Next(self):
<mask>:
        return None
    self.current += 1
    return self.lines[self.current - 1]",self.current >= self.length,12,self.current >= len(self.lines),False,40.35278637463991,N/A
"def __ReadNode(self, fil):
    parentdir = self.__CurDir()
<mask>:
        if os.path.isabs(fil):
            fname = fil
        else:
            fname = os.path.join(parentdir, fil)
        fd = open(fname, 'rt')
        node = _Node(fd, fil, parentdir)
        fd.close()
    else:
        node = _Node(fil, '', parentdir)
    self.stack.append(node)","isinstance(fil, str)",34,os.path.isfile(fil),False,12.22307556087252,N/A
"def __GetInclude(self, line):
    line = line.split('#', 1)[0].strip()
    tokens = line.split()
<mask>:
        return ' '.join(tokens[1:])
    else:
        return None",tokens and tokens[0].upper() == '$INCLUDE',17,len(tokens) > 1,False,2.383515454163372,N/A
"def __init__(self, code=0, id=None, secret=b'', authenticator=None, **attributes):
    """"""Constructor

        :param dict:   RADIUS dictionary
        :type dict:    pyrad.dictionary.Dictionary class
        :param secret: secret needed to communicate with a RADIUS server
        :type secret:  string
        :param id:     packet identification number
        :type id:      integer (8 bits)
        :param code:   packet type code
        :type code:    integer (8bits)
        :param packet: raw packet to decode
        :type packet:  string
        """"""
    OrderedDict.__init__(self)
    self.code = code
<mask>:
        self.id = id
    else:
        self.id = CreateID()
    if not isinstance(secret, bytes):
        raise TypeError('secret must be a binary string')
    self.secret = secret
    if authenticator is not None and (not isinstance(authenticator, bytes)):
        raise TypeError('authenticator must be a binary string')
    self.authenticator = authenticator
    self.message_authenticator = None
    self.raw_packet = None
    if 'dict' in attributes:
        self.dict = attributes['dict']
    if 'packet' in attributes:
        self.raw_packet = attributes['packet']
        self.DecodePacket(self.raw_packet)
    if 'message_authenticator' in attributes:
        self.message_authenticator = attributes['message_authenticator']
    for key, value in attributes.items():
        if key in ['dict', 'fd', 'packet', 'message_authenticator']:
            continue
        key = key.replace('_', '-')
        self.AddAttribute(key, value)",id is not None,150,id is not None,True,100.00000000000004,N/A
"def add_message_authenticator(self):
    self.message_authenticator = True
    self['Message-Authenticator'] = 16 * b'\x00'
<mask>:
        self.id = self.CreateID()
    if self.authenticator is None and self.code == AccessRequest:
        self.authenticator = self.CreateAuthenticator()
        self._refresh_message_authenticator()",self.id is None,26,self.id is None,True,100.00000000000004,N/A
"def _refresh_message_authenticator(self):
    hmac_constructor = hmac_new(self.secret)
    self['Message-Authenticator'] = 16 * b'\x00'
    attr = self._PktEncodeAttributes()
    header = struct.pack('!BBH', self.code, self.id, 20 + len(attr))
    hmac_constructor.update(header[0:4])
<mask>:
        hmac_constructor.update(16 * b'\x00')
    else:
        if self.authenticator is None:
            raise Exception('No authenticator found')
        hmac_constructor.update(self.authenticator)
    hmac_constructor.update(attr)
    self['Message-Authenticator'] = hmac_constructor.digest()","self.code in (AccountingRequest, DisconnectRequest, CoARequest, AccountingResponse)",40,self.is_encrypted,False,4.313387938787984,N/A
"def verify_message_authenticator(self, secret=None, original_authenticator=None, original_code=None):
    """"""Verify packet Message-Authenticator.

        :return: False if verification failed else True
        :rtype: boolean
        """"""
<mask>:
        raise Exception('No Message-Authenticator AVP present')
    prev_ma = self['Message-Authenticator']
    if secret is None and self.secret is None:
        raise Exception('Missing secret for HMAC/MD5 verification')
    if secret:
        key = secret
    else:
        key = self.secret
    if self.raw_packet:
        attr = self.raw_packet[20:]
        attr = attr.replace(prev_ma[0], 16 * b'\x00')
    else:
        self['Message-Authenticator'] = 16 * b'\x00'
        attr = self._PktEncodeAttributes()
    header = struct.pack('!BBH', self.code, self.id, 20 + len(attr))
    hmac_constructor = hmac_new(key)
    hmac_constructor.update(header)
    if self.code in (AccountingRequest, DisconnectRequest, CoARequest, AccountingResponse):
        if original_code is None or original_code != StatusServer:
            hmac_constructor.update(16 * b'\x00')
    elif self.code in (AccessAccept, AccessChallenge, AccessReject):
        if original_authenticator is None:
            if self.authenticator:
                original_authenticator = self.authenticator
            else:
                raise Exception('Missing original authenticator')
        hmac_constructor.update(original_authenticator)
    else:
        hmac_constructor.update(self.authenticator)
    hmac_constructor.update(attr)
    self['Message-Authenticator'] = prev_ma[0]
    return prev_ma[0] == hmac_constructor.digest()",self.message_authenticator is None,131,self['Message-Authenticator'] is None,False,15.207218222740094,N/A
"def _DecodeValue(self, attr, value):
<mask>:
        value = self.SaltDecrypt(value)
    if attr.values.HasBackward(value):
        return attr.values.GetBackward(value)
    else:
        return tools.DecodeAttr(attr.type, value)",attr.encrypt == 2,16,attr.type == 'Salt',False,22.957488466614336,N/A
"def connection_lost(self, exc):
<mask>:
        self.logger.warn('[%s:%d] Connection lost: %s', self.ip, self.port, str(exc))
    else:
        self.logger.info('[%s:%d] Transport closed', self.ip, self.port)",exc,17,exc,True,100.00000000000004,N/A
"def datagram_received(self, data, addr):
    self.logger.debug('[%s:%d] Received %d bytes from %s', self.ip, self.port, len(data), addr)
    receive_date = datetime.utcnow()
<mask>:
        remote_host = self.hosts[addr[0]]
    elif '0.0.0.0' in self.hosts:
        remote_host = self.hosts['0.0.0.0']
    else:
        self.logger.warn('[%s:%d] Drop package from unknown source %s', self.ip, self.port, addr)
        return
    try:
        self.logger.debug('[%s:%d] Received from %s packet: %s', self.ip, self.port, addr, data.hex())
        req = Packet(packet=data, dict=self.server.dict)
    except Exception as exc:
        self.logger.error('[%s:%d] Error on decode packet: %s', self.ip, self.port, exc)
        return
    try:
        if req.code in (AccountingResponse, AccessAccept, AccessReject, CoANAK, CoAACK, DisconnectNAK, DisconnectACK):
            raise ServerPacketError('Invalid response packet %d' % req.code)
        elif self.server_type == ServerType.Auth:
            if req.code != AccessRequest:
                raise ServerPacketError('Received non-auth packet on auth port')
            req = AuthPacket(secret=remote_host.secret, dict=self.server.dict, packet=data)
            if self.server.enable_pkt_verify:
                if not req.VerifyAuthRequest():
                    raise PacketError('Packet verification failed')
        elif self.server_type == ServerType.Coa:
            if req.code != DisconnectRequest and req.code != CoARequest:
                raise ServerPacketError('Received non-coa packet on coa port')
            req = CoAPacket(secret=remote_host.secret, dict=self.server.dict, packet=data)
            if self.server.enable_pkt_verify:
                if not req.VerifyCoARequest():
                    raise PacketError('Packet verification failed')
        elif self.server_type == ServerType.Acct:
            if req.code != AccountingRequest:
                raise ServerPacketError('Received non-acct packet on acct port')
            req = AcctPacket(secret=remote_host.secret, dict=self.server.dict, packet=data)
            if self.server.enable_pkt_verify:
                if not req.VerifyAcctRequest():
                    raise PacketError('Packet verification failed')
        self.request_callback(self, req, addr)
    except Exception as exc:
        if self.server.debug:
            self.logger.exception('[%s:%d] Error for packet from %s', self.ip, self.port, addr)
        else:
            self.logger.error('[%s:%d] Error for packet from %s: %s', self.ip, self.port, addr, exc)
    process_date = datetime.utcnow()
    self.logger.debug('[%s:%d] Request from %s processed in %d ms', self.ip, self.port, addr, (process_date - receive_date).microseconds / 1000)",addr[0] in self.hosts,227,addr,False,0.09118819655545167,N/A
"def __init__(self, auth_port=1812, acct_port=1813, coa_port=3799, hosts=None, dictionary=None, loop=None, logger_name='pyrad', enable_pkt_verify=False, debug=False):
<mask>:
        self.loop = asyncio.get_event_loop()
    else:
        self.loop = loop
    self.logger = logging.getLogger(logger_name)
    if hosts is None:
        self.hosts = {}
    else:
        self.hosts = hosts
    self.auth_port = auth_port
    self.auth_protocols = []
    self.acct_port = acct_port
    self.acct_protocols = []
    self.coa_port = coa_port
    self.coa_protocols = []
    self.dict = dictionary
    self.enable_pkt_verify = enable_pkt_verify
    self.debug = debug",not loop,60,loop is None,False,27.516060407455225,N/A
"def __request_handler__(self, protocol, req, addr):
    try:
<mask>:
            self.handle_acct_packet(protocol, req, addr)
        elif protocol.server_type == ServerType.Auth:
            self.handle_auth_packet(protocol, req, addr)
        elif protocol.server_type == ServerType.Coa and req.code == CoARequest:
            self.handle_coa_packet(protocol, req, addr)
        elif protocol.server_type == ServerType.Coa and req.code == DisconnectRequest:
            self.handle_disconnect_packet(protocol, req, addr)
        else:
            self.logger.error('[%s:%s] Unexpected request found', protocol.ip, protocol.port)
    except Exception as exc:
        if self.debug:
            self.logger.exception('[%s:%s] Unexpected error', protocol.ip, protocol.port)
        else:
            self.logger.error('[%s:%s] Unexpected error: %s', protocol.ip, protocol.port, exc)",protocol.server_type == ServerType.Acct,65,protocol.server_type == ServerType.ACCT,False,88.01117367933934,N/A
"def __is_present_proto__(self, ip, port):
<mask>:
        for proto in self.auth_protocols:
            if proto.ip == ip:
                return True
    elif port == self.acct_port:
        for proto in self.acct_protocols:
            if proto.ip == ip:
                return True
    elif port == self.coa_port:
        for proto in self.coa_protocols:
            if proto.ip == ip:
                return True
    return False",port == self.auth_port,45,port == self.auth_port,True,100.00000000000004,N/A
"def __delitem__(self, key):
<mask>:
        del self.backward[self.forward[key]]
        del self.forward[key]
    else:
        del self.forward[self.backward[key]]
        del self.backward[key]",key in self.forward,13,key in self.forward,True,100.00000000000004,N/A
"def __str__(self):
    str = ''
<mask>:
        str += self.file
    if self.line > -1:
        str += '(%d)' % self.line
    if self.file or self.line > -1:
        str += ': '
    str += 'Parse error'
    if self.msg:
        str += ': %s' % self.msg
    return str",self.file,42,self.file,True,100.00000000000004,N/A
"def __init__(self, name, code, datatype, is_sub_attribute=False, vendor='', values=None, encrypt=0, has_tag=False):
<mask>:
        raise ValueError('Invalid data type')
    self.name = name
    self.code = code
    self.type = datatype
    self.vendor = vendor
    self.encrypt = encrypt
    self.has_tag = has_tag
    self.values = bidict.BiDict()
    self.sub_attributes = {}
    self.parent = None
    self.is_sub_attribute = is_sub_attribute
    if values:
        for key, value in values.items():
            self.values.Add(key, value)",datatype not in DATATYPES,54,"datatype not in ['BID', 'VID']",False,20.556680845025987,N/A
"def __init__(self, dict=None, *dicts):
    """"""
        :param dict:  path of dictionary file or file-like object to read
        :type dict:   string or file
        :param dicts: list of dictionaries
        :type dicts:  sequence of strings or files
        """"""
    self.vendors = bidict.BiDict()
    self.vendors.Add('', 0)
    self.attrindex = bidict.BiDict()
    self.attributes = {}
    self.defer_parse = []
<mask>:
        self.ReadDictionary(dict)
    for i in dicts:
        self.ReadDictionary(i)",dict,55,dict,True,100.00000000000004,N/A
"def __ParseAttribute(self, state, tokens):
<mask>:
        raise ParseError('Incorrect number of tokens for attribute definition', name=state['file'], line=state['line'])
    vendor = state['vendor']
    has_tag = False
    encrypt = 0
    if len(tokens) >= 5:

        def keyval(o):
            kv = o.split('=')
            if len(kv) == 2:
                return (kv[0], kv[1])
            else:
                return (kv[0], None)
        options = [keyval(o) for o in tokens[4].split(',')]
        for key, val in options:
            if key == 'has_tag':
                has_tag = True
            elif key == 'encrypt':
                if val not in ['1', '2', '3']:
                    raise ParseError('Illegal attribute encryption: %s' % val, file=state['file'], line=state['line'])
                encrypt = int(val)
        if not has_tag and encrypt == 0:
            vendor = tokens[4]
            if not self.vendors.HasForward(vendor):
                if vendor == 'concat':
                    return None
                else:
                    raise ParseError('Unknown vendor ' + vendor, file=state['file'], line=state['line'])
    attribute, code, datatype = tokens[1:4]
    codes = code.split('.')
    tmp = []
    for c in codes:
        if c.startswith('0x'):
            tmp.append(int(c, 16))
        elif c.startswith('0o'):
            tmp.append(int(c, 8))
        else:
            tmp.append(int(c, 10))
    codes = tmp
    is_sub_attribute = len(codes) > 1
    if len(codes) == 2:
        code = int(codes[1])
        parent_code = int(codes[0])
    elif len(codes) == 1:
        code = int(codes[0])
        parent_code = None
    else:
        raise ParseError('nested tlvs are not supported')
    datatype = datatype.split('[')[0]
    if datatype not in DATATYPES:
        raise ParseError('Illegal type: ' + datatype, file=state['file'], line=state['line'])
    if vendor:
        if is_sub_attribute:
            key = (self.vendors.GetForward(vendor), parent_code, code)
        else:
            key = (self.vendors.GetForward(vendor), code)
    elif is_sub_attribute:
        key = (parent_code, code)
    else:
        key = code
    self.attrindex.Add(attribute, key)
    self.attributes[attribute] = Attribute(attribute, code, datatype, is_sub_attribute, vendor, encrypt=encrypt, has_tag=has_tag)
    if datatype == 'tlv':
        state['tlvs'][code] = self.attributes[attribute]
    if is_sub_attribute:
        state['tlvs'][parent_code].sub_attributes[code] = attribute
        self.attributes[attribute].parent = state['tlvs'][parent_code]","not len(tokens) in [4, 5]",241,len(tokens) < 5,False,23.350308364304226,N/A
"def __ParseValue(self, state, tokens, defer):
<mask>:
        raise ParseError('Incorrect number of tokens for value definition', file=state['file'], line=state['line'])
    attr, key, value = tokens[1:]
    try:
        adef = self.attributes[attr]
    except KeyError:
        if defer:
            self.defer_parse.append((copy(state), copy(tokens)))
            return
        raise ParseError('Value defined for unknown attribute ' + attr, file=state['file'], line=state['line'])
    if adef.type in ['integer', 'signed', 'short', 'byte', 'integer64']:
        value = int(value, 0)
    value = tools.EncodeAttr(adef.type, value)
    self.attributes[attr].values.Add(key, value)",len(tokens) != 4,61,len(tokens) != 2,False,80.91067115702207,N/A
"def datagramReceived(self, datagram, source):
    host, port = source
    try:
        pkt = self.CreatePacket(packet=datagram)
    except packet.PacketError as err:
        log.msg('Dropping invalid packet: ' + str(err))
        return
<mask>:
        log.msg('Dropping packet from unknown host ' + host)
        return
    pkt.source = (host, port)
    try:
        self.processPacket(pkt)
    except PacketError as err:
        log.msg('Dropping packet from %s: %s' % (host, str(err)))",host not in self.hosts,51,not pkt.is_valid,False,9.652434877402245,N/A
"def processPacket(self, pkt):
<mask>:
        raise PacketError('non-AccessRequest packet on authentication socket')",pkt.code != packet.AccessRequest,10,pkt.data != self.access_request_pkt,False,10.127993013562818,N/A
"def processPacket(self, pkt):
<mask>:
        raise PacketError('non-AccountingRequest packet on authentication socket')",pkt.code != packet.AccountingRequest,10,not self.isAccountingRequest(pkt),False,6.770186228657864,N/A
"def generate_list_of_spans(encoding):
    zipkin_attrs = ZipkinAttrs(trace_id=generate_random_64bit_string(), span_id=generate_random_64bit_string(), parent_span_id=generate_random_64bit_string(), is_sampled=True, flags=None)
    inner_span_id = generate_random_64bit_string()
    transport_handler = MockTransportHandler()
    ts = 1538544126.1159
<mask>:
        return (b'\x0c\x00\x00\x00\x02\n\x00\x01\xb5ZX\x14\x81.\x07\xfe\x0b\x00\x03\x00' + b'\x00\x00\ninner_span\n\x00\x04\xc6\xcd\x0c\x1fZ\xb0_\xd1\n\x00\x05BM' + b'\xb5\x02p\xed\x8e\xb1\x0f\x00\x06\x0c\x00\x00\x00\x01\n\x00\x01\x00' + b'\x05wL8\x1b\xc0<\x0b\x00\x02\x00\x00\x00\x02ws\x0c\x00\x03\x08\x00' + b'\x01\n\x00\x00\x00\x06\x00\x02\x1f\x90\x0b\x00\x03\x00\x00\x00\x11' + b'test_service_name\x00\x00\x0f\x00\x08\x0c\x00\x00\x00\x00\x02\x00\t' + b'\x00\n\x00\n\x00\x05wL8\x1b\xc0<\n\x00\x0b\x00\x00\x00\x00\x00LK@\x00' + b'\n\x00\x01\xb5ZX\x14\x81.\x07\xfe\x0b\x00\x03\x00\x00\x00\x0e' + b'test_span_name\n\x00\x04BM\xb5\x02p\xed\x8e\xb1\n\x00\x05\xa1kn\xd3' + b'\x0cA\xba\xbd\x0f\x00\x06\x0c\x00\x00\x00\x02\n\x00\x01\x00\x05wL8' + b'\x1b\xc0<\x0b\x00\x02\x00\x00\x00\x02cs\x0c\x00\x03\x08\x00\x01\n\x00' + b'\x00\x00\x06\x00\x02\x1f\x90\x0b\x00\x03\x00\x00\x00\x11' + b'test_service_name\x00\x00\n\x00\x01\x00\x05wL8\xb4V\xbc\x0b\x00\x02' + b'\x00\x00\x00\x02cr\x0c\x00\x03\x08\x00\x01\n\x00\x00\x00\x06\x00\x02' + b'\x1f\x90\x0b\x00\x03\x00\x00\x00\x11test_service_name\x00\x00\x0f\x00' + b'\x08\x0c\x00\x00\x00\x02\x0b\x00\x01\x00\x00\x00\x08some_key\x0b\x00' + b'\x02\x00\x00\x00\nsome_value\x08\x00\x03\x00\x00\x00\x06\x0c\x00\x04' + b'\x08\x00\x01\n\x00\x00\x00\x06\x00\x02\x1f\x90\x0b\x00\x03\x00\x00' + b'\x00\x11test_service_name\x00\x00\x0b\x00\x01\x00\x00\x00\x02sa\x0b' + b'\x00\x02\x00\x00\x00\x01\x01\x08\x00\x03\x00\x00\x00\x00\x0c\x00\x04' + b'\x08\x00\x01\x00\x00\x00\x00\x06\x00\x02""\xb8\x0b\x00\x03\x00\x00\x00' + b'\nsa_service\x0b\x00\x04\x00\x00\x00\x10 \x01\r\xb8\x85\xa3\x00\x00' + b'\x00\x00\x8a.\x03ps4\x00\x00\x02\x00\t\x00\x00', zipkin_attrs, inner_span_id, ts)
    with mock.patch('time.time', autospec=True) as mock_time:
        mock_time.side_effect = iter([ts, ts, ts + 10, ts + 10, ts + 10])
        with zipkin.zipkin_span(service_name='test_service_name', span_name='test_span_name', transport_handler=transport_handler, binary_annotations={'some_key': 'some_value'}, encoding=encoding, zipkin_attrs=zipkin_attrs, host='10.0.0.0', port=8080, kind=Kind.CLIENT) as span:
            with mock.patch.object(zipkin, 'generate_random_64bit_string', return_value=inner_span_id):
                with zipkin.zipkin_span(service_name='test_service_name', span_name='inner_span', timestamp=ts, duration=5, annotations={'ws': ts}):
                    span.add_sa_binary_annotation(8888, 'sa_service', '2001:0db8:85a3:0000:0000:8a2e:0370:7334')
    return (transport_handler.get_payloads()[0], zipkin_attrs, inner_span_id, ts)",encoding == Encoding.V1_THRIFT,119,encoding == 'gzip',False,21.874242445215206,N/A
"def test_extract_zipkin_attrs_from_headers_invalids():
    span_id = '7e5991634df0c66e'
    parent_span_id = '987d98239475e0a8'
    for invalid in ['', 'a', '-'.join(('a', '')), '-'.join(('', 'b')), '-'.join(('a', '', 'whee')), '-'.join(('a', span_id, 'whee')), '-'.join(('a', span_id, 'whee', parent_span_id)), '-'.join(('a', span_id, 'whee', '')), '-'.join(('a', span_id, '1', 'c', 'd?!'))]:
        print('bad b3 header: %r' % invalid)
        assert None is request_helpers.extract_zipkin_attrs_from_headers({'b3': invalid}, sample_rate=88.2), invalid
        bits = invalid.split('-')
<mask>:
            bad_headers = {'X-B3-TraceId': bits[0]}
        elif len(bits) == 2:
            bad_headers = {'X-B3-TraceId': bits[0], 'X-B3-SpanId': bits[1]}
        elif len(bits) == 3:
            bad_headers = {'X-B3-TraceId': bits[0], 'X-B3-SpanId': bits[1], 'X-B3-Sampled': bits[2]}
        elif len(bits) == 4:
            bad_headers = {'X-B3-TraceId': bits[0], 'X-B3-SpanId': bits[1], 'X-B3-Sampled': bits[2], 'X-B3-ParentSpanId': bits[3]}
        print('bad_headers: %r' % bad_headers)
        assert None is request_helpers.extract_zipkin_attrs_from_headers(bad_headers, sample_rate=88.2)
    assert None is request_helpers.extract_zipkin_attrs_from_headers({'X-B3-TraceId': 'a', 'X-B3-SpanId': span_id, 'X-B3-Sampled': '', 'X-B3-ParentSpanId': parent_span_id}, sample_rate=88.2)
    assert None is request_helpers.extract_zipkin_attrs_from_headers({'X-B3-SpanId': span_id}, sample_rate=88.2)",len(bits) == 1,120,len(bits) == 1,True,100.00000000000004,N/A
"def test_span_inside_trace():
    """"""This tests that nested spans work correctly""""""
    mock_transport_handler, mock_logs = mock_logger()
    with zipkin.zipkin_span(service_name='test_service_name', span_name='test_span_name', transport_handler=mock_transport_handler, sample_rate=100.0, binary_annotations={'some_key': 'some_value'}, encoding=Encoding.V2_JSON):
        with zipkin.zipkin_span(service_name='nested_service', span_name='nested_span', annotations={'nested_annotation': 43.0}, binary_annotations={'nested_key': 'nested_value'}):
            pass
    spans = json.loads(mock_logs[0])
    assert len(spans) == 2
    nested_span = spans[0]
    root_span = spans[1]
    assert nested_span['name'] == 'nested_span'
    assert nested_span['localEndpoint']['serviceName'] == 'nested_service'
    assert nested_span['parentId'] == root_span['id']
    assert nested_span['tags']['nested_key'] == 'nested_value'
    assert nested_span['timestamp'] is not None
    assert nested_span['duration'] is not None
    assert len(nested_span['annotations']) == 1
    assert 'kind' not in nested_span
    assert sorted((ann['value'] for ann in nested_span['annotations'])) == sorted(['nested_annotation'])
    for ann in nested_span['annotations']:
<mask>:
            assert ann['timestamp'] == 43 * USECS",ann['value'] == 'nested_annotation',96,ann['kind'] == 'nested_annotation',False,66.06328636027612,N/A
"def test_annotation_override():
    """"""This is the same as above, but we override an annotation
    in the inner span
    """"""
    mock_transport_handler, mock_logs = mock_logger()
    mock_firehose_handler, mock_firehose_logs = mock_logger()
    with zipkin.zipkin_span(service_name='test_service_name', span_name='test_span_name', transport_handler=mock_transport_handler, sample_rate=100.0, binary_annotations={'some_key': 'some_value'}, firehose_handler=mock_firehose_handler, encoding=Encoding.V1_JSON):
        with zipkin.zipkin_span(service_name='nested_service', span_name='nested_span', kind=Kind.CLIENT, annotations={'nested_annotation': 43, 'cs': 100, 'cr': 300}, binary_annotations={'nested_key': 'nested_value'}):
            pass

    def check_spans(spans):
        nested_span = spans[0]
        root_span = spans[1]
        assert nested_span['name'] == 'nested_span'
        assert nested_span['annotations'][0]['endpoint']['serviceName'] == 'nested_service'
        assert nested_span['parentId'] == root_span['id']
        assert nested_span['binaryAnnotations'][0]['key'] == 'nested_key'
        assert nested_span['binaryAnnotations'][0]['value'] == 'nested_value'
        assert nested_span['timestamp'] is not None
        assert nested_span['duration'] is not None
        assert len(nested_span['annotations']) == 3
        assert sorted((ann['value'] for ann in nested_span['annotations'])) == sorted(['cs', 'cr', 'nested_annotation'])
        for ann in nested_span['annotations']:
<mask>:
                assert ann['timestamp'] == 43 * USECS
            elif ann['value'] == 'cs':
                assert ann['timestamp'] == 100 * USECS
            elif ann['value'] == 'cr':
                assert ann['timestamp'] == 300 * USECS
    check_spans(json.loads(mock_logs[0]))
    check_spans(json.loads(mock_firehose_logs[0]))",ann['value'] == 'nested_annotation',133,ann['value'] == 'nested_annotation',True,100.00000000000004,N/A
"@pytest.mark.parametrize('encoding', [Encoding.V1_JSON, Encoding.V2_JSON])
def test_sr_ss_annotation_override(encoding):
    """"""Here we override ss and sr and expect the output span to contain the
    values we're passing in.
    """"""
    mock_transport_handler, mock_logs = mock_logger()
    with zipkin.zipkin_span(service_name='test_service_name', span_name='test_span_name', transport_handler=mock_transport_handler, sample_rate=100.0, binary_annotations={'some_key': 'some_value'}, encoding=encoding):
        with zipkin.zipkin_span(service_name='nested_service', span_name='nested_span', annotations={'nested_annotation': 43.0, 'sr': 100.0, 'ss': 300.0}, binary_annotations={'nested_key': 'nested_value'}, kind=Kind.SERVER):
            pass
    spans = json.loads(mock_logs[0])
    nested_span = spans[0]
    root_span = spans[1]
    assert nested_span['parentId'] == root_span['id']
    assert nested_span['timestamp'] == 100 * USECS
    assert nested_span['duration'] == 200 * USECS
<mask>:
        assert len(nested_span['annotations']) == 3
        for ann in nested_span['annotations']:
            if ann['value'] == 'nested_annotation':
                assert ann['timestamp'] == 43 * USECS
            elif ann['value'] == 'sr':
                assert ann['timestamp'] == 100 * USECS
            elif ann['value'] == 'ss':
                assert ann['timestamp'] == 300 * USECS",encoding == Encoding.V1_JSON,113,'annotations' in nested_span,False,5.862502026550896,N/A
"@pytest.mark.parametrize('encoding', [Encoding.V1_JSON, Encoding.V2_JSON])
def test_service_span(encoding):
    """"""Tests that zipkin_attrs can be passed in""""""
    mock_transport_handler, mock_logs = mock_logger()
    zipkin_attrs = ZipkinAttrs(trace_id='0', span_id='1', parent_span_id='2', flags='0', is_sampled=True)
    with zipkin.zipkin_span(service_name='test_service_name', span_name='service_span', zipkin_attrs=zipkin_attrs, transport_handler=mock_transport_handler, encoding=encoding):
        pass
    span = json.loads(mock_logs[0])[0]
    assert span['name'] == 'service_span'
    assert span['traceId'] == '0'
    assert span['id'] == '1'
    assert span['parentId'] == '2'
<mask>:
        assert 'timestamp' not in span
        assert 'duration' not in span
    elif encoding == Encoding.V2_JSON:
        assert span['shared'] is True",encoding == Encoding.V1_JSON,68,encoding == Encoding.V1_JSON,True,100.00000000000004,N/A
"@pytest.mark.parametrize('encoding', [Encoding.V1_JSON, Encoding.V2_JSON])
def test_service_span_that_is_independently_sampled(encoding):
    """"""Tests that sample_rate has can turn on sampling for a trace.

    This is the same case as an intermediate service wanting to have an higher
    sampling rate.
    """"""
    mock_transport_handler, mock_logs = mock_logger()
    mock_firehose_handler, mock_firehose_logs = mock_logger()
    zipkin_attrs = ZipkinAttrs(trace_id='0', span_id='1', parent_span_id='2', flags='0', is_sampled=False)
    with zipkin.zipkin_span(service_name='test_service_name', span_name='service_span', zipkin_attrs=zipkin_attrs, transport_handler=mock_transport_handler, port=45, sample_rate=100.0, firehose_handler=mock_firehose_handler, encoding=encoding):
        pass

    def check_span(span):
        assert span['traceId'] == '0'
        assert span['name'] == 'service_span'
        assert 'parentId' not in span
        assert 'timestamp' in span
        assert 'duration' in span
<mask>:
            assert 'shared' not in span
    check_span(json.loads(mock_logs[0])[0])
    check_span(json.loads(mock_firehose_logs[0])[0])",encoding == Encoding.V2_JSON,89,encoding == 'V1_JSON',False,23.263472697663296,N/A
"@pytest.mark.parametrize('encoding', [Encoding.V1_THRIFT, Encoding.V1_JSON, Encoding.V2_JSON, Encoding.V2_PROTO3])
def test_detect_span_version_and_encoding(encoding):
    spans, _, _, _ = generate_list_of_spans(encoding)
    old_type = type(spans)
    assert detect_span_version_and_encoding(spans) == encoding
<mask>:
        assert type(spans) == old_type
        spans = spans.encode()
        assert detect_span_version_and_encoding(spans) == encoding","encoding in [Encoding.V1_JSON, Encoding.V2_JSON]",32,old_type is not None,False,1.8110800993753928,N/A
"def _parse_single_header(b3_header: str) -> B3JSON:
    """"""
    Parse out and return the data necessary for generating ZipkinAttrs.

    Returns a dict with the following keys:
        'trace_id':             str or None
        'span_id':              str or None
        'parent_span_id':       str or None
        'sampled_str':          '0', '1', 'd', or None (defer)
    """"""
    parsed: B3JSON = {'trace_id': None, 'span_id': None, 'parent_span_id': None, 'sampled_str': None}
    bits = b3_header.split('-')
<mask>:
        if bits[0] in ('0', '1', 'd'):
            parsed['sampled_str'] = bits[0]
            return parsed
        raise ValueError('Invalid sample-only value: %r' % bits[0])
    if len(bits) > 4:
        raise ValueError('Too many segments in b3 header: %r' % b3_header)
    parsed['trace_id'] = bits[0]
    if not parsed['trace_id']:
        raise ValueError('Bad or missing TraceId')
    parsed['span_id'] = bits[1]
    if not parsed['span_id']:
        raise ValueError('Bad or missing SpanId')
    if len(bits) > 3:
        parsed['parent_span_id'] = bits[3]
        if not parsed['parent_span_id']:
            raise ValueError('Got empty ParentSpanId')
    if len(bits) > 2:
        if bits[2]:
            parsed['sampled_str'] = bits[2]
            if parsed['sampled_str'] not in ('0', '1', 'd'):
                raise ValueError('Bad SampledState: %r' % parsed['sampled_str'])
    return parsed",len(bits) == 1,150,len(bits) == 1,True,100.00000000000004,N/A
"def _parse_multi_header(headers: Dict[str, str]) -> B3JSON:
    """"""
    Parse out and return the data necessary for generating ZipkinAttrs.

    Returns a dict with the following keys:
        'trace_id':             str or None
        'span_id':              str or None
        'parent_span_id':       str or None
        'sampled_str':          '0', '1', 'd', or None (defer)
    """"""
    parsed: B3JSON = {'trace_id': headers.get('X-B3-TraceId', None), 'span_id': headers.get('X-B3-SpanId', None), 'parent_span_id': headers.get('X-B3-ParentSpanId', None), 'sampled_str': headers.get('X-B3-Sampled', None)}
<mask>:
        parsed['sampled_str'] = 'd'
    if parsed['sampled_str'] == 'true':
        parsed['sampled_str'] = '1'
    elif parsed['sampled_str'] == 'false':
        parsed['sampled_str'] = '0'
    if parsed['sampled_str'] not in (None, '1', '0', 'd'):
        raise ValueError('Got invalid X-B3-Sampled: %s' % parsed['sampled_str'])
    for k in ('trace_id', 'span_id', 'parent_span_id'):
        if parsed[k] == '':
            raise ValueError('Got empty-string %r' % k)
    if parsed['trace_id'] and (not parsed['span_id']):
        raise ValueError('Got X-B3-TraceId but not X-B3-SpanId')
    elif parsed['span_id'] and (not parsed['trace_id']):
        raise ValueError('Got X-B3-SpanId but not X-B3-TraceId')
    if not parsed['trace_id'] and (not parsed['sampled_str']):
        raise ValueError()
    return parsed",headers.get('X-B3-Flags') == '1',140,parsed['sampled_str'] == 'true',False,7.635362674858095,N/A
"def extract_zipkin_attrs_from_headers(headers: Dict[str, str], sample_rate: float=100.0, use_128bit_trace_id: bool=False) -> Optional[ZipkinAttrs]:
    """"""
    Implements extraction of B3 headers per:
        https://github.com/openzipkin/b3-propagation

    The input headers can be any dict-like container that supports ""in""
    membership test and a .get() method that accepts a default value.

    Returns a ZipkinAttrs instance or None
    """"""
    try:
<mask>:
            parsed = _parse_single_header(headers['b3'])
        else:
            parsed = _parse_multi_header(headers)
    except ValueError as e:
        if str(e):
            log.warning(e)
        return None
    if not parsed['trace_id']:
        if parsed['sampled_str'] in ('1', 'd'):
            sample_rate = 100.0
        else:
            sample_rate = 0.0
        attrs = create_attrs_for_span(sample_rate=sample_rate, use_128bit_trace_id=use_128bit_trace_id, flags='1' if parsed['sampled_str'] == 'd' else '0')
        return attrs
    if parsed['sampled_str']:
        if parsed['sampled_str'] in ('1', 'd'):
            is_sampled = True
        else:
            is_sampled = False
    else:
        is_sampled = _should_sample(sample_rate)
    return ZipkinAttrs(parsed['trace_id'], parsed['span_id'], parsed['parent_span_id'], '1' if parsed['sampled_str'] == 'd' else '0', is_sampled)",'b3' in headers,123,'b3' in headers,True,100.00000000000004,N/A
"def create_http_headers(context_stack: Optional[Stack]=None, tracer: Optional[Tracer]=None, new_span_id: bool=False) -> Dict[str, Optional[str]]:
    """"""
    Generate the headers for a new zipkin span.

    .. note::

        If the method is not called from within a zipkin_trace context,
        empty dict will be returned back.

    :returns: dict containing (X-B3-TraceId, X-B3-SpanId, X-B3-ParentSpanId,
                X-B3-Flags and X-B3-Sampled) keys OR an empty dict.
    """"""
<mask>:
        zipkin_attrs = tracer.get_zipkin_attrs()
    elif context_stack:
        zipkin_attrs = context_stack.get()
    else:
        zipkin_attrs = get_default_tracer().get_zipkin_attrs()
    if not zipkin_attrs:
        return {}
    if new_span_id:
        span_id: Optional[str] = generate_random_64bit_string()
        parent_span_id = zipkin_attrs.span_id
    else:
        span_id = zipkin_attrs.span_id
        parent_span_id = zipkin_attrs.parent_span_id
    return {'X-B3-TraceId': zipkin_attrs.trace_id, 'X-B3-SpanId': span_id, 'X-B3-ParentSpanId': parent_span_id, 'X-B3-Flags': '0', 'X-B3-Sampled': '1' if zipkin_attrs.is_sampled else '0'}",tracer,102,tracer,True,100.00000000000004,N/A
"def _get_path_content_type(self, payload: Union[str, bytes]) -> Tuple[str, str]:
    """"""Choose the right api path and content type depending on the encoding.

        This is not something you'd need to do generally when writing your own
        transport since in that case you'd know which encoding you're using.
        Since this is a generic transport, we need to make it compatible with
        any encoding instead.
        """"""
    encoded_payload = payload.encode('utf-8') if isinstance(payload, str) else payload
    encoding = detect_span_version_and_encoding(encoded_payload)
<mask>:
        return ('/api/v1/spans', 'application/json')
    elif encoding == Encoding.V1_THRIFT:
        return ('/api/v1/spans', 'application/x-thrift')
    elif encoding == Encoding.V2_JSON:
        return ('/api/v2/spans', 'application/json')
    elif encoding == Encoding.V2_PROTO3:
        return ('/api/v2/spans', 'application/x-protobuf')
    else:
        raise UnknownEncoding(f'Unknown encoding: {encoding}')",encoding == Encoding.V1_JSON,102,encoding == Encoding.V1_JSON_VERSION,False,75.98356856515926,N/A
"def signed_int_to_unsigned_hex(signed_int: int) -> str:
    """"""Converts a signed int value to a 64-bit hex string.

    Examples:
        1662740067609015813  => '17133d482ba4f605'
        -5270423489115668655 => 'b6dbb1c2b362bf51'

    :param signed_int: an int to convert
    :returns: unsigned hex string
    """"""
    hex_string = hex(struct.unpack('Q', struct.pack('q', signed_int))[0])[2:]
<mask>:
        return hex_string[:-1]
    return hex_string",hex_string.endswith('L'),43,hex_string[-1] == '0',False,17.747405280050266,N/A
"def _should_sample(sample_rate: float) -> bool:
<mask>:
        return False
    elif sample_rate == 100.0:
        return True
    return random.random() * 100 < sample_rate",sample_rate == 0.0,20,sample_rate == 0.0,True,100.00000000000004,N/A
"def create_attrs_for_span(sample_rate: float=100.0, trace_id: Optional[str]=None, span_id: Optional[str]=None, use_128bit_trace_id: bool=False, flags: Optional[str]=None) -> ZipkinAttrs:
    """"""Creates a set of zipkin attributes for a span.

    :param sample_rate: Float between 0.0 and 100.0 to determine sampling rate
    :type sample_rate: float
    :param trace_id: Optional 16-character hex string representing a trace_id.
                    If this is None, a random trace_id will be generated.
    :type trace_id: str
    :param span_id: Optional 16-character hex string representing a span_id.
                    If this is None, a random span_id will be generated.
    :type span_id: str
    :param use_128bit_trace_id: If true, generate 128-bit trace_ids
    :type use_128bit_trace_id: bool
    """"""
<mask>:
        if use_128bit_trace_id:
            trace_id = generate_random_128bit_string()
        else:
            trace_id = generate_random_64bit_string()
    if span_id is None:
        span_id = generate_random_64bit_string()
    is_sampled = _should_sample(sample_rate)
    return ZipkinAttrs(trace_id=trace_id, span_id=span_id, parent_span_id=None, flags=flags or '0', is_sampled=is_sampled)",trace_id is None,119,trace_id is None,True,100.00000000000004,N/A
"def get_tracer(self) -> Tracer:
<mask>:
        return self._tracer
    else:
        return get_default_tracer()",self._tracer is not None,10,self._tracer is not None,True,100.00000000000004,N/A
"def _generate_kind(self, kind: Optional[Kind], include: Optional[str]) -> Kind:
<mask>:
        return kind
    elif include:
        log.warning('The include argument is deprecated. Please use kind.')
        if 'client' in include and 'server' not in include:
            return Kind.CLIENT
        elif 'client' not in include and 'server' in include:
            return Kind.SERVER
        else:
            return Kind.LOCAL
    return Kind.LOCAL",kind,48,kind is not None,False,15.97357760615681,N/A
"def _get_current_context(self) -> Tuple[bool, Optional[ZipkinAttrs]]:
    """"""Returns the current ZipkinAttrs and generates new ones if needed.

        :returns: (report_root_timestamp, zipkin_attrs)
        :rtype: (bool, ZipkinAttrs)
        """"""
<mask>:
        if self.sample_rate is not None:
            if self.zipkin_attrs_override and (not self.zipkin_attrs_override.is_sampled):
                return (True, create_attrs_for_span(sample_rate=self.sample_rate, trace_id=self.zipkin_attrs_override.trace_id))
            elif not self.zipkin_attrs_override:
                return (True, create_attrs_for_span(sample_rate=self.sample_rate, use_128bit_trace_id=self.use_128bit_trace_id))
        if self.firehose_handler and (not self.zipkin_attrs_override):
            return (True, create_attrs_for_span(sample_rate=0.0, use_128bit_trace_id=self.use_128bit_trace_id))
        return (False, self.zipkin_attrs_override)
    else:
        existing_zipkin_attrs = self.get_tracer().get_zipkin_attrs()
        if existing_zipkin_attrs:
            return (False, ZipkinAttrs(trace_id=existing_zipkin_attrs.trace_id, span_id=generate_random_64bit_string(), parent_span_id=existing_zipkin_attrs.span_id, flags=existing_zipkin_attrs.flags, is_sampled=existing_zipkin_attrs.is_sampled))
    return (False, None)",self._is_local_root_span,72,self.is_tracer(),False,12.100518276540289,N/A
"def start(self) -> 'zipkin_span':
    """"""Enter the new span context. All annotations logged inside this
        context will be attributed to this span. All new spans generated
        inside this context will have this span as their parent.

        In the unsampled case, this context still generates new span IDs and
        pushes them onto the threadlocal stack, so downstream services calls
        made will pass the correct headers. However, the logging handler is
        never attached in the unsampled case, so the spans are never logged.
        """"""
    self.do_pop_attrs = False
    report_root_timestamp, self.zipkin_attrs = self._get_current_context()
<mask>:
        return self
    self.get_tracer().push_zipkin_attrs(self.zipkin_attrs)
    self.do_pop_attrs = True
    self.start_timestamp = time.time()
    if self._is_local_root_span:
        if not self.zipkin_attrs.is_sampled and (not self.firehose_handler):
            return self
        if self.get_tracer().is_transport_configured():
            log.info('Transport was already configured, ignoring override from span {}'.format(self.span_name))
            return self
        endpoint = create_endpoint(self.port, self.service_name, self.host)
        self.logging_context = ZipkinLoggingContext(self.zipkin_attrs, endpoint, self.span_name, self.transport_handler, report_root_timestamp or self.report_root_timestamp_override, self.get_tracer, self.service_name, binary_annotations=self.binary_annotations, add_logging_annotation=self.add_logging_annotation, client_context=self.kind == Kind.CLIENT, max_span_batch_size=self.max_span_batch_size, firehose_handler=self.firehose_handler, encoding=self.encoding, annotations=self.annotations)
        self.logging_context.start()
        self.get_tracer().set_transport_configured(configured=True)
    return self",not self.zipkin_attrs,150,self.skip_zipkin_attrs,False,30.739407647563215,N/A
